{"meta":{"title":"PostgreSQL 中文网","subtitle":"Francs's blog","description":null,"author":"francs","url":"https://postgres.fun"},"pages":[{"title":"关于我","date":"2018-06-07T05:08:29.000Z","updated":"2019-01-22T09:04:31.660Z","comments":true,"path":"about/index.html","permalink":"https://postgres.fun/about/index.html","excerpt":"","text":"个人简介谭峰（francs），PostgreSQL DBA，中国开源软件推进联盟PostgreSQL分会特聘专家，《PostgreSQL实战》作者之一，《PostgreSQL 9 Administration Cookbook》译者之一，《PostgreSQL High Performance Cookbook》英文版技术审校者之一，曾在杭州斯凯网络科技有限公司从事PostgreSQL DBA一职六年。热忠于博客分享PostgreSQL经验，分享技术博客500余篇。 提供 PostgreSQL 技术支撑，无偿或有偿，联系邮箱: francs3@163.com 出版的书籍1. 《PostgreSQL实战》语言：中文出版日期：2018年7月作者：谭峰、张文升链接: https://item.jd.com/12405774.html 基于PostgreSQL 10 编写，重点介绍SQL高级特性、并行查询、分区表、物理复制、逻辑复制、备份恢复、高可用、性能优化、PostGIS等，涵盖大量实战用例。 2. 《PostgreSQL 9 Administration Cookbook（第2版）》语言：中文出版日期：2016年5月作者：Simon Riggs、Gianni Ciolli、Hannu Krosing、Gabriele Bartolini译者：黄坚、谭峰链接: https://item.jd.com/11943328.html 一本介绍PostgreSQL数据库运维管理基础书籍，适合新手，全书基于PostgreSQL9.4。 审校的书籍1. 《PostgreSQL 10 High Performance》英文版语言：英语出版日期：2018年4月30日作者：Ibrar Ahmed 、Gregory Smith、Enrico Pirozzi审稿：谭峰（Francs）、Srivathsava Rangarajan链接: 《PostgreSQL 10 High Performance》 一本介绍PostgreSQL性能优化的书籍，难度中等偏上，适合有一定经验的PostgreSQL DBA，全书基于PostgreSQL 10。 2.《PostgreSQL High Performance Cookbook》英文版语言：英语出版日期：2017年3月29日作者：Chitij Chauhan , Dinesh Kumar审稿：Baji Shaik、谭峰（Francs）链接: 《PostgreSQL High Performance Cookbook》英文版 一本介绍PostgreSQL数据库运维管理书籍，难度中等偏下，全书基于PostgreSQL9.6。 关于博客 2010年6月: 开始在网易博客记录学习的点滴，一写就是八年，很吃惊自己能够长期坚持。 2018年6月: 网易博客经常出现日志无法发表，并且20多篇博客被封，决定将博客迁出，几经周折，博客新家终于初步装修好，地址：https://francs.github.io ，网易博客不再更新。 2018年7月: 绑定域名 francs.top。 2018年8月: 更换新域名 postgres.fun。 2018年9月: 博客迁移到腾讯云主机，github仍然放了一份。 2018年10月: 完成域名ICP备案和公安备案。 2019年1月: 给博客添加 Valine 评论模块，包括邮件通知、评论列表头像。 联系方式 Email: francs3@163.com 新博客：https://postgres.fun 老博客：http://francs3.blog.163.com Github：https://github.com/francs Stackoverflow：https://stackoverflow.com/users/527628/francs Weibo: https://weibo.com/u/1992989604 QQ: 151389554"},{"title":"分类","date":"2018-06-07T05:08:04.000Z","updated":"2018-06-07T05:24:24.864Z","comments":true,"path":"categories/index.html","permalink":"https://postgres.fun/categories/index.html","excerpt":"","text":""},{"title":"欢迎大家留言交流","date":"2018-10-14T13:31:32.000Z","updated":"2018-10-14T13:37:42.974Z","comments":true,"path":"guestbook/index.html","permalink":"https://postgres.fun/guestbook/index.html","excerpt":"","text":"关于博客有任何建议欢迎提出。 提供 PostgreSQL 技术支持，欢迎联系。 联系方式 Email: francs3@163.com Weibo: https://weibo.com/u/1992989604"},{"title":"标签","date":"2018-06-07T05:19:23.000Z","updated":"2018-06-07T14:15:07.288Z","comments":true,"path":"tags/index.html","permalink":"https://postgres.fun/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"EnterpriseDB: 使用 edbstart and edbstop 启停数据库","slug":"20190117151500","date":"2019-01-17T07:15:44.000Z","updated":"2019-01-17T07:18:10.003Z","comments":true,"path":"20190117151500.html","link":"","permalink":"https://postgres.fun/20190117151500.html","excerpt":"","text":"EDB 封装了 edbstart 和 edbstop 命令行工具用于启动和关闭 Linux 平台上多个 EDB实例，对于主机上部署了多个EDB实例的场景降低了维护工作量，RPM 安装方式不支持使用这两个脚本工具。 演示这两个工具之前，我们首先创建两个EDB数据库实例。 安装 EDB计划创建两个EDB实例，软件目录为 /opt/edb，端口分别为 5444、5445，数据目录分别为 /edbdata/5444/data, /edbdata/5445/data。 创建系统用户创建操作系统用户，如下: 123# groupadd enterprisedb# useradd -g enterprisedb enterprisedb# passwd enterprisedb 创建数据目录创建两个数据目录，如下: 123[root@pghost6 edb]# mkdir -p /edbdata/5444[root@pghost6 edb]# mkdir -p /edbdata/5445chown -R enterprisedb:enterprisedb /edbdata 定义安装配置文件创建配置文件如下: 1touch /opt/edb/config_edb.conf 给配置文件 /opt/edb/config_edb.conf 加入以下内容: 123456mode=unattendedsuperpassword=xxxxxxprefix=/opt/edbdatadir=/edbdata/5444/dataxlogdir=/edbdata/5444/data/pg_walserverport=5444 安装 EDB执行安装脚本，如下: 1/opt/soft_bak/edb-as10-server-10.5.12-1-linux-x64.run --optionfile /opt/edb/config_edb.conf 设置环境变量安装完成后自动生成环境变量文件 /opt/edb/pgplus_env.sh, 将脚本最后部分的环境变量加到系统用户 enterprisedb 环境变量文件 .bash_profile 中，如下: 1234567export PATH=/opt/edb/bin:$PATHexport EDBHOME=/opt/edbexport PGDATA=/edbdata/5444/dataexport PGDATABASE=edb# export PGUSER=enterprisedbexport PGPORT=5444export PGLOCALEDIR=/opt/edb/share/locale 创建另一个EDB实例使用 initdb 创建另外一个EDB实例，如下: 12345678910111213141516-bash-4.2$ /opt/edb/bin/initdb -D /edbdata/5445/data -E UTF8 --locale=C -U enterprisedb -WThe files belonging to this database system will be owned by user \"enterprisedb\".This user must also own the server process.The database cluster will be initialized with locale \"C\".The default text search configuration will be set to \"english\".Data page checksums are disabled.Enter new superuser password:Enter it again:creating directory /edbdata/5445/data ... okcreating subdirectories ... okselecting default max_connections ... 100...省略 修改此实例 postgresql.conf 的端口号等相关参数，之后启动这个实例，如下: 1-bash-4.2$ pg_ctl start -D /edbdata/5445/data 查看进程，已经有了两个EDB实例，如下: 123456789101112131415161718192021222324-bash-4.2$ ps -ef | grep postroot 1232 1 0 2018 ? 00:00:19 /usr/libexec/postfix/master -wpostfix 1240 1232 0 2018 ? 00:00:04 qmgr -l -t unix -upostfix 29277 1232 0 10:52 ? 00:00:00 pickup -l -t unix -uenterpr+ 29530 1 0 10:55 ? 00:00:00 /opt/edb/bin/edb-postgres -D /edbdata/5444/dataenterpr+ 29531 29530 0 10:55 ? 00:00:00 postgres: logger processenterpr+ 29533 29530 0 10:55 ? 00:00:00 postgres: checkpointer processenterpr+ 29534 29530 0 10:55 ? 00:00:00 postgres: writer processenterpr+ 29535 29530 0 10:55 ? 00:00:00 postgres: wal writer processenterpr+ 29536 29530 0 10:55 ? 00:00:00 postgres: autovacuum launcher processenterpr+ 29537 29530 0 10:55 ? 00:00:00 postgres: stats collector processenterpr+ 29538 29530 0 10:55 ? 00:00:00 postgres: bgworker: dbms_aq launcherenterpr+ 29539 29530 0 10:55 ? 00:00:00 postgres: bgworker: logical replication launcherenterpr+ 32025 1 2 11:34 pts/1 00:00:00 /opt/edb/bin/edb-postgres -D /edbdata/5445/dataenterpr+ 32027 32025 0 11:34 ? 00:00:00 postgres: checkpointer processenterpr+ 32028 32025 0 11:34 ? 00:00:00 postgres: writer processenterpr+ 32029 32025 0 11:34 ? 00:00:00 postgres: wal writer processenterpr+ 32030 32025 0 11:34 ? 00:00:00 postgres: autovacuum launcher processenterpr+ 32031 32025 0 11:34 ? 00:00:00 postgres: stats collector processenterpr+ 32032 32025 0 11:34 ? 00:00:00 postgres: bgworker: dbms_aq launcherenterpr+ 32033 32025 0 11:34 ? 00:00:00 postgres: bgworker: logical replication launcherenterpr+ 32043 32025 0 11:34 ? 00:00:00 postgres: bgworker: dbms_aq worker[postgres] idleenterpr+ 32045 32025 0 11:34 ? 00:00:00 postgres: bgworker: dbms_aq worker[edb] idleenterpr+ 32047 29021 0 11:34 pts/1 00:00:00 grep --color=auto post 关于 edbtab配置文件 /etc/edbtab 用来定义主机上的多个 EDB 实例， edbstart 和 edbstop 脚本会读取 /etc/edbtab 文件内容并进行EDB实例的启停控制。 edbtab 文件位于软件安装目录 /opt/edb/scripts/server/autostart/，将此文件复制到 ‘/etc/‘ 目录，如下: 123456789-bash-4.2$ ll /opt/edb/scripts/server/autostarttotal 16-rwxr-xr-x 1 enterprisedb enterprisedb 1344 Aug 17 14:09 edb_autostartdrwxrwxr-x 3 enterprisedb enterprisedb 18 Jan 17 12:00 edbdata-rwxr-xr-x 1 enterprisedb enterprisedb 1481 Aug 17 14:09 edbstart-rwxr-xr-x 1 enterprisedb enterprisedb 1463 Aug 17 14:09 edbstop-rw-r--r-- 1 enterprisedb enterprisedb 369 Jan 17 11:43 edbtab[root@pghost6 autostart]# cp /opt/edb/scripts/server/autostart/edbtab /etc/ edbtab 文件包含 $EDB_HOME、$PGDATA、&lt;N|Y&gt;三个字段，用冒号分隔，每一行表示一个EDB数据库实例，如下: 12345678910111213141516# This file is used to automatically start EnterpriseDB clusters# at system boot time## Format of entries:## $EDB_HOME:$PGDATA:&lt;N|Y&gt;## Example of usage:## /opt/edb/as10:/opt/edb/as10/data:Y## The 'Y' or 'N' indicate whether the cluster should or should not be# started upon system boot/shutdown#/opt/edb:/edbdata/5444/data:Y/opt/edb:/edbdata/5445/data:Y 这个文件很容易理解，文件的最后两行设置了EDB的两个实例。 关于 edbstart 脚本edbstart 和 edbstop 文件也位于软件安装目录 /opt/edb/scripts/server/autostart/。 edbstart 脚本内容如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#!/bin/sh# EnterpriseDB cluster startup script## This script will start all clusters listed the /etc/edbtab file# that have a 'Y' indicating that the cluster should be started at# system boot time.# This script is meant to be run at boot time# (called by /etc/init.d/edb_autostart) but can be run manually.# If you choose to run this script manually, it will attemp to start all# clusters with a 'Y' unless you provide a data directory as an argument:# i.e ./edbstart /opt/edb/as9.6/data, in which case, only that# cluster will be started.startAll ()&#123; # Retrieve all entries from edbtab for ENTRY in `cat /etc/edbtab | grep -v '^[ \\t]*\\#' | egrep -v ':[nN][ \\t]*'` do EDB_HOME=`echo $ENTRY | awk -F: '&#123;print $1&#125;'` PGDATA=`echo $ENTRY | awk -F: '&#123;print $2&#125;'` EDBLOG=$PGDATA/log/log.txt printf \"Starting $&#123;EDB_HOME&#125; ($&#123;PGDATA&#125;)...\\n\" $EDB_HOME/bin/pg_ctl -D $PGDATA -W start &gt;&gt; $EDBLOG 2&gt;&amp;1 done&#125;startOne ()&#123;PGDATA=$1 # Retrieve entry from edbtab ENTRY=`cat /etc/edbtab | grep \"$&#123;PGDATA&#125;\"` if [ -z $ENTRY ]; then printf \"Entry could not be found!\" exit 1 fi # Extract components EDB_HOME=`echo $ENTRY | awk -F: '&#123;print $1&#125;'` EDBLOG=$PGDATA/log/log.txt printf \"Starting $&#123;EDB_HOME&#125; ($&#123;PGDATA&#125;)...\\n\" $EDB_HOME/bin/pg_ctl -D $PGDATA -w start &gt;&gt; $EDBLOG 2&gt;&amp;1&#125;main ()&#123; if [ -z \"$1\" ]; then startAll else startOne $1 fi&#125;main $* 关于 edbstop 脚本edbstop 脚本内容如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#!/bin/sh# EnterpriseDB cluster shutdown script## This script will stop clusters listed the /etc/edbtab file# that have a 'Y' indicating that the cluster should be stopped at# system shutdown.# This script is meant to be run at system shutdown# (called by /etc/init.d/edb_autostart) but can be run manually.# If you choose to run this script manually, it will attemp to stop all# clusters with a 'Y' unless you provide a data directory as an argument:# i.e ./edbstop /opt/edb/as9.6/data, in which case, only that# cluster will be stopped.stopAll ()&#123; # Retrieve all entries from edbtab for ENTRY in `cat /etc/edbtab | grep -v '^[ \\t]*\\#' | egrep -v ':[nN][ \\t]*'` do EDB_HOME=`echo $ENTRY | awk -F: '&#123;print $1&#125;'` PGDATA=`echo $ENTRY | awk -F: '&#123;print $2&#125;'` printf \"Stopping $&#123;EDB_HOME&#125; ($&#123;PGDATA&#125;)...\\n\" $EDB_HOME/bin/pg_ctl -D $PGDATA -w -m immediate stop done&#125;stopOne ()&#123; PGDATA=$1 # Retrieve entry from edbtab ENTRY=`cat /etc/edbtab | grep \"$&#123;PGDATA&#125;\"` if [ -z $ENTRY ]; then printf \"Entry could not be found!\\n\" exit 1 fi # Extract components EDB_HOME=`echo $ENTRY | awk -F: '&#123;print $1&#125;'` PGDATA=`echo $ENTRY | awk -F: '&#123;print $2&#125;'` printf \"Stopping $&#123;EDB_HOME&#125; ($&#123;PGDATA&#125;)...\\n\" $EDB_HOME/bin/pg_ctl -D $PGDATA -w -m immediate stop&#125;main ()&#123; if [ -z \"$1\" ]; then stopAll else stopOne $1 fi&#125;main $* 验证为了使用方便，将 edbstart 和 edbstop 复制到 $EDBHOME/bin 目录下，如下: 12-bash-4.2$ cp /opt/edb/scripts/server/autostart/edbstart $EDBHOME/bin-bash-4.2$ cp /opt/edb/scripts/server/autostart/edbstop $EDBHOME/bin 启停主机上所有实例两个脚本不接任何参数表示启动或停止主机上的所有实例。 关闭主机上所有EDB实例，如下: 1234567-bash-4.2$ edbstopStopping /opt/edb (/edbdata/5444/data)...waiting for server to shut down.... doneserver stoppedStopping /opt/edb (/edbdata/5445/data)...waiting for server to shut down.... doneserver stopped 启动主机上所有EDB实例,如下: 123-bash-4.2$ edbstartStarting /opt/edb (/edbdata/5444/data)...Starting /opt/edb (/edbdata/5445/data)... 启停主机上指定实例也可以关闭或启动指定实例，指定数据目录即可。 关闭主机上的 /edbdata/5444/data 实例，如下: 1234-bash-4.2$ edbstop /edbdata/5444/dataStopping /opt/edb (/edbdata/5444/data)...waiting for server to shut down.... doneserver stopped 启动主机上的 /edbdata/5444/data 实例，如下: 12-bash-4.2$ edbstart /edbdata/5444/dataStarting /opt/edb (/edbdata/5444/data)... 参考 Using the edbstart and edbstop Utilities EnterpriseDB: 交互式命令行安装 EnterpriseDB: 无人值守安装","categories":[{"name":"EDB","slug":"EDB","permalink":"https://postgres.fun/categories/EDB/"}],"tags":[{"name":"EDB","slug":"EDB","permalink":"https://postgres.fun/tags/EDB/"}]},{"title":"Hexo: 给博客添加近期文章板块","slug":"20190116150800","date":"2019-01-16T07:08:39.000Z","updated":"2019-01-16T07:11:26.218Z","comments":true,"path":"20190116150800.html","link":"","permalink":"https://postgres.fun/20190116150800.html","excerpt":"","text":"博客文章置顶了几篇文章后，总觉得不能直观的找到最近发表的文章，因此想增加近期文章版块，网上查了相关资料，虽然Next模块不直接提供配置项，但很容易实现。 修改sidebar.swig文件参考了文章 Hexo 系列(3) Next 主题配置，具体方法为:将下面代码贴在 next/layout/_macro/sidebar.swig 中的 if theme.links 对应的 endif 后面。 12345678910111213141516&#123;% if theme.recent_posts %&#125; &lt;div class=\"links-of-blogroll motion-element &#123;&#123; \"links-of-blogroll-\" + theme.recent_posts_layout &#125;&#125;\"&gt; &lt;div class=\"links-of-blogroll-title\"&gt; &lt;!-- modify icon to fire by szw --&gt; &lt;i class=\"fa fa-history fa-&#123;&#123; theme.recent_posts_icon | lower &#125;&#125;\" aria-hidden=\"true\"&gt;&lt;/i&gt; &#123;&#123; theme.recent_posts_title &#125;&#125; &lt;/div&gt; &lt;ul class=\"links-of-blogroll-list\"&gt; &#123;% set posts = site.posts.sort('-date') %&#125; &#123;% for post in posts.slice('0', '5') %&#125; &lt;li&gt; &lt;a href=\"&#123;&#123; url_for(post.path) &#125;&#125;\" title=\"&#123;&#123; post.title &#125;&#125;\" target=\"_blank\"&gt;&#123;&#123; post.title &#125;&#125;&lt;/a&gt; &lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &lt;/div&gt; 上面这段代码为博最近文章版块的相关代码。 根据sidebar.swig文件中 if theme.links 这段代码内容，很容易猜到是博客首页右侧友情链接的代码，我想把最近文章的版块放到友情链版块的上面，于是计划将这段代码放到 if theme.links 这段代码的前面，如下: 12345678910111213141516171819202122232425262728293031323334&#123;# Blogroll #&#125;&#123;% if theme.recent_posts %&#125; &lt;div class=\"links-of-blogroll motion-element &#123;&#123; \"links-of-blogroll-\" + theme.recent_posts_layout &#125;&#125;\"&gt; &lt;div class=\"links-of-blogroll-title\"&gt; &lt;!-- modify icon to fire by szw --&gt; &lt;i class=\"fa fa-history fa-&#123;&#123; theme.recent_posts_icon | lower &#125;&#125;\" aria-hidden=\"true\"&gt;&lt;/i&gt; &#123;&#123; theme.recent_posts_title &#125;&#125; &lt;/div&gt; &lt;ul class=\"links-of-blogroll-list\"&gt; &#123;% set posts = site.posts.sort('-date') %&#125; &#123;% for post in posts.slice('0', '5') %&#125; &lt;li&gt; &lt;a href=\"&#123;&#123; url_for(post.path) &#125;&#125;\" title=\"&#123;&#123; post.title &#125;&#125;\" target=\"_blank\"&gt;&#123;&#123; post.title &#125;&#125;&lt;/a&gt; &lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; /div&gt; &#123;% endif %&#125; &#123;% if theme.links %&#125; &lt;div class=\"links-of-blogroll motion-element &#123;&#123; \"links-of-blogroll-\" + theme.links_layout | default('inline') &#125;&#125;\"&gt; &lt;div class=\"links-of-blogroll-title\"&gt; &lt;i class=\"fa fa-fw fa-&#123;&#123; theme.links_icon | default('globe') | lower &#125;&#125;\"&gt;&lt;/i&gt; &#123;&#123; theme.links_title &#125;&#125; &lt;/div&gt; &lt;ul class=\"links-of-blogroll-list\"&gt; &#123;% for name, link in theme.links %&#125; &lt;li class=\"links-of-blogroll-item\"&gt; &lt;a href=\"&#123;&#123; link &#125;&#125;\" title=\"&#123;&#123; name &#125;&#125;\" target=\"_blank\"&gt;&#123;&#123; name &#125;&#125;&lt;/a&gt; &lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt; &lt;/div&gt; &#123;% endif %&#125; 设置主题配置文件在主题配置文件 /d/hexo/themes/next/_config.yml 中添加3个变量，如下： 123recent_posts_title: 近期文章recent_posts_layout: blockrecent_posts: true 验证效果之后重启本地 Hexo 博客，查看效果如下: 已经出来了近期文章版块。 参考 Hexo 系列(3) Next 主题配置","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/tags/Hexo/"}]},{"title":"EnterpriseDB: 无人值守安装","slug":"20190116103400","date":"2019-01-16T02:34:32.000Z","updated":"2019-01-17T00:47:16.275Z","comments":true,"path":"20190116103400.html","link":"","permalink":"https://postgres.fun/20190116103400.html","excerpt":"","text":"上篇博客介绍了EDB 交互式命令行安装方式，EDB同时支持无人值守安装，极大提升了部署效率。 EDB 的无人值守安装是通过预先定义安装配置文件实现的，本文简单演示下。 定义安装配置文件创建配置文件如下 1touch /opt/edb/config_edb.conf 给配置文件 /opt/edb/config_edb.conf加入以下内容: 123456mode=unattendedsuperpassword=xxxxxxprefix=/opt/edbdatadir=/opt/edb/dataxlogdir=/opt/edb/data/pg_walserverport=5444 参数解释如下: mode: 指定 unattended 模式，表示无人值守安装模式。 superpassword: 设置数据库超级用户密码，默认密码为 enterprisedb。 prefix: 设置EDB的软件安装目录。 datadir: 设置EDB的数据文件目录，对应的环境变量为$PGDATA。 xlogdir: 设置wal文件的目录。 serverport: 设置EDB的监听端口号，默认为5444。 其它参数根据需要设置。 无人值守安装 EDB执行安装脚本 edb-as10-server-10.5.12-1-linux-x64.run，并使用 optionfile 指定安装配置文件，如下: 1[root@pghost6 soft_bak]# ./edb-as10-server-10.5.12-1-linux-x64.run --optionfile /opt/edb/config_edb.conf 无人值守安装无输出结果，验证EDB是否已安装成功。 查看EDB安装文件，如下: 123456789101112131415161718192021[root@pghost6 edb]# ll /opt/edbtotal 8200drwxr-xr-x 3 root root 18 Jan 16 10:07 as10drwxr-xr-x 2 root daemon 4096 Jan 16 10:07 bin-rw-r--r-- 1 root root 97 Jan 16 10:06 config_edb.confdrwx------ 20 enterprisedb enterprisedb 4096 Jan 16 10:07 datadrwxr-xr-x 4 root daemon 38 Jan 16 10:06 doc-rw-r--r-- 1 root daemon 72341 Aug 17 14:09 edb-as10-server_commandlinetools_3rd_party_licenses.txt-r--r--r-- 1 root daemon 15216 Aug 17 14:09 edb-as10-server_license.txtdrwxr-xr-x 3 root daemon 23 Jan 16 10:06 etcdrwxr-xr-x 6 root daemon 4096 Jan 16 10:06 include-rw-r--r-- 1 root root 612 Jan 16 10:07 installation_summary.logdrwxr-xr-x 5 root daemon 55 Jan 16 10:06 installerdrwxr-xr-x 4 root daemon 8192 Jan 16 10:07 libdrwxr-xr-x 10 root daemon 164 Jan 16 10:06 pgAdmin4-rwxr-xr-x 1 root daemon 915 Jan 16 10:07 pgplus_env.shdrwxr-xr-x 5 root daemon 73 Jan 16 10:06 scriptsdrwxr-xr-x 10 root daemon 4096 Jan 16 10:06 sharedrwxr-xr-x 7 root daemon 136 Jan 16 10:06 stackbuilderplus-rwx------ 1 root daemon 8118363 Jan 16 10:07 uninstall-edb-as10-server-rw------- 1 root root 141150 Jan 16 10:07 uninstall-edb-as10-server.dat 目录的权限为root，权限需要调整(安装过程中默认创建了 enterprisedb 操作系统用户) 1[root@pghost6 ~]# chown -R enterprisedb:enterprisedb /opt/edb 查看EDB数据库进程，如下: 12345678910111213141516[root@pghost6 soft_bak]# ps -ef | grep postroot 1232 1 0 2018 ? 00:00:18 /usr/libexec/postfix/master -wpostfix 1240 1232 0 2018 ? 00:00:04 qmgr -l -t unix -upostfix 6373 1232 0 08:37 ? 00:00:00 pickup -l -t unix -uenterpr+ 12677 1 0 10:07 ? 00:00:00 /opt/edb/bin/edb-postgres -D /opt/edb/dataenterpr+ 12678 12677 0 10:07 ? 00:00:00 postgres: logger processenterpr+ 12680 12677 0 10:07 ? 00:00:00 postgres: checkpointer processenterpr+ 12681 12677 0 10:07 ? 00:00:00 postgres: writer processenterpr+ 12682 12677 0 10:07 ? 00:00:00 postgres: wal writer processenterpr+ 12683 12677 0 10:07 ? 00:00:00 postgres: autovacuum launcher processenterpr+ 12684 12677 0 10:07 ? 00:00:00 postgres: stats collector processenterpr+ 12685 12677 0 10:07 ? 00:00:00 postgres: bgworker: dbms_aq launcherenterpr+ 12686 12677 0 10:07 ? 00:00:00 postgres: bgworker: logical replication launcherenterpr+ 12698 12677 0 10:07 ? 00:00:00 postgres: bgworker: dbms_aq worker[postgres] idleenterpr+ 12704 12677 0 10:07 ? 00:00:00 postgres: bgworker: dbms_aq worker[edb] idleroot 12713 22990 0 10:07 pts/0 00:00:00 grep --color=auto post 说明 EDB 已安装并且数据库已启动。 设置环境变量安装完成后自动生成环境变量文件 /opt/edb/pgplus_env.sh, 脚本内容如下: 12345678910111213141516171819202122232425262728293031# EnterpriseDB shell environment loader## Instructions:# This file contains additions to the user environment# that make accessing EDB Postgres Advanced Server# executables easier.## To load the environment for a single user:# cp pgplus_env.sh /home/&lt;username&gt;# chown &lt;username&gt; /home/&lt;username&gt;/pgplus_env.sh# vi /home/&lt;username&gt;/.bash_profile# At the bottom, add the line:# . /home/&lt;username&gt;/pgplus_env.sh# ( Note the '.' followed by a space )# To load the environment for all users:# cp pgplus_env.sh /etc# vi /etc/profile# At the bottom, add the line:# . /etc/pgplus_env.sh# ( Note the '.' followed by a space )# Environmentexport PATH=/opt/edb/bin:$PATHexport EDBHOME=/opt/edbexport PGDATA=/opt/edb/dataexport PGDATABASE=edb# export PGUSER=enterprisedbexport PGPORT=5444export PGLOCALEDIR=/opt/edb/share/locale 将整个文件内容或尾部的环境变量加到系统用户 enterprisedb 环境变量文件 .bash_profile 中，如下: 1234[root@pghost6 ~]# su - enterprisedbLast login: Tue Jan 15 11:00:16 CST 2019 on pts/1-bash-4.2$ cp /opt/edb/pgplus_env.sh /opt/edb/.bash_profile 登录EDB验证登录 EDB 数据库，如下: 12345678910111213141516171819202122-bash-4.2$ psql -h 127.0.0.1 -p 5444 edb enterprisedbPassword for user enterprisedb:psql.bin (10.5.12)Type \"help\" for help.edb=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | ICU | Access privileges-----------+--------------+----------+-------------+-------------+-----+------------------------------- edb | enterprisedb | UTF8 | en_US.UTF-8 | en_US.UTF-8 | | postgres | enterprisedb | UTF8 | en_US.UTF-8 | en_US.UTF-8 | | template0 | enterprisedb | UTF8 | en_US.UTF-8 | en_US.UTF-8 | | =c/enterprisedb + | | | | | | enterprisedb=CTc/enterprisedb template1 | enterprisedb | UTF8 | en_US.UTF-8 | en_US.UTF-8 | | =c/enterprisedb + | | | | | | enterprisedb=CTc/enterprisedb(4 rows)edb=# SELECT version(); version-------------------------------------------------------------------------------------------------------------- EnterpriseDB 10.5.12 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-18), 64-bit(1 row) 参考 Performing an Unattended Installation Reference - Command Line Options EnterpriseDB: 交互式命令行安装","categories":[{"name":"EDB","slug":"EDB","permalink":"https://postgres.fun/categories/EDB/"}],"tags":[{"name":"EDB","slug":"EDB","permalink":"https://postgres.fun/tags/EDB/"}]},{"title":"EnterpriseDB: 交互式命令行安装","slug":"20190115143600","date":"2019-01-15T06:36:45.000Z","updated":"2019-01-17T02:57:36.593Z","comments":true,"path":"20190115143600.html","link":"","permalink":"https://postgres.fun/20190115143600.html","excerpt":"","text":"关于 EnterpriseDBEnterpriseDB，简称EDB，是开源数据库PostgreSQL的企业版，在PostgreSQL基础上对企业级特性进行了增强，例如增加了企业级工具、高可用性增强等，同时高度兼容Oracle。 EDB 主要包括以下特点: EDB Advanced Server将PostgreSQL的所有特性与额外的企业级功能结合起来，以增强企业级应用的性能和安全性需求。 EDB高级服务器特性提高了DBA和开发人员的工作效率。 EDB Advanced Server 高度兼容 Oracle语法 ，包括PL/SQL、内置包和许多DBA工具，以加速和简化迁移。 下载EDB安装介质EDB 提供两类数据库可供下载: OPTION A - EDB POSTGRES ADVANCED SERVER OPTION B - POSTGRESQL 我们选择 OPTION A 下载对应 Linux 平台安装介质，目前 Linux 平台交互式安装支持的最新版本为 10.5，RPM 安装支持 11.1 ，我们选择交互式安装方式。 https://www.enterprisedb.com/edb-postgres-advanced-server-10512-linux-x86-64 下载的介质为 edb-as10-server-10.5.12-1-linux-x64.run。 交互式安装EDBEDB封装了安装脚本，安装过程非常简单，交互式安装EDB过程如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187[root@pghost6 soft_bak]# ./edb-as10-server-10.5.12-1-linux-x64.run --mode text----------------------------------------------------------------------------Welcome to the EDB Postgres Advanced Server Setup Wizard.----------------------------------------------------------------------------Please read the following License Agreement. You must accept the terms of this agreement before continuing with the installation.Press [Enter] to continue:Limited Use Software License AgreementVersion 2.9IMPORTANT - READ CAREFULLY...后面省略大量关于许可协议的描述，一路回车Press [Enter] to continue:是否同意许可协议，选择同意。Do you accept this license? [y/n]: y----------------------------------------------------------------------------Please specify the directory where EDB Postgres Advanced Server will be installed.选择安装目录,即 PGHOMEInstallation Directory [/opt/edb/as10]: /opt/edb/----------------------------------------------------------------------------Select the components you want to install; clear the components you do not want to install. Click Next when you are ready to continue.选择需要安装的组件EDB Postgres Advanced Server [Y/n] :ypgAdmin 4 [Y/n] :nStackBuilder Plus [Y/n] :yCommand Line Tools [Y/n] :yIs the selection above correct? [Y/n]: y----------------------------------------------------------------------------选择数据目录和WAL日志目录，即 PGDATAAdditional DirectoriesPlease select a directory under which to store your data.Data Directory [/opt/edb/data]: /opt/edb/dataPlease select a directory under which to store your Write-Ahead Logs.Write-Ahead Log (WAL) Directory [/opt/edb/data/pg_wal]: /opt/edb/data/pg_wal----------------------------------------------------------------------------Advanced Server DialectEDB Postgres Advanced Server can be configured in one of two \"Dialects\" - 1) Compatible with Oracle or 2) Compatible with Postgres.If you select Compatible with Oracle, Advanced Server will be configured with appropriate data type conversions, time and date formats, Oracle-styled operators, dictionary views and more. This makes it easier to migrate or write new applications that are more compatible with the Oracle database.If you select Compatible with Postgres, Advanced Server will be configured with standard PostgeSQL data types, time/date formats and operators.Advanced Server Dialect选择兼容Oracle还是兼容PostgreSQL，我们选择兼容Oracle，关于Oracle兼容性本文末尾有详细说明。[1] Compatible with Oracle[2] Compatible with PostgresPlease choose an option [1] : 1----------------------------------------------------------------------------Please provide a password for the database superuser (enterprisedb). A locked Unix user account (enterprisedb) will be created if not present.安装过程中会创建系统用户 enterprisedb，设置密码Password :Retype Password :---------------------------------------------------------------------------- Additional ConfigurationPlease select the port number the server should listen on.设置EDB端口Port [5444]: 5444Select the locale to be used by the new database cluster.Locale...省略[232] en_US[233] en_US.iso88591[234] en_US.iso885915[235] en_US.utf8[236] en_ZA[237] en_ZA.iso88591...省略设置字符集Please choose an option [1] : 235Would you like to install sample tables and procedures?Install sample tables and procedures. [Y/n]: y----------------------------------------------------------------------------Dynatune Dynamic Tuning:Server UtilizationPlease select the type of server to determine the amount of system resources that may be utilized:设置数据库环境类型[1] Development (e.g. a developers laptop)[2] General Purpose (e.g. a web or application server)[3] Dedicated (a server running only Advanced Server)Please choose an option [2] : 2----------------------------------------------------------------------------Dynatune Dynamic Tuning:Workload ProfilePlease select the type of workload this server will be used for:设置OLTP或OLAP[1] Transaction Processing (OLTP systems)[2] General Purpose (OLTP and reporting workloads)[3] Reporting (Complex queries or OLAP workloads)Please choose an option [1] : 1----------------------------------------------------------------------------Update Notification ServiceInstall Update Notification Service [Y/n]: nThe Update Notification Service informs, downloads and installs whenever security patches and other updates are available for your EDB Postgres Advanced Server installation.EDB安装选项汇总----------------------------------------------------------------------------Pre Installation SummaryThe following settings will be used for the installation::Installation Directory: /opt/edbServer Installation Directory: /opt/edbData Directory: /opt/edb/dataWAL Directory: /opt/edb/data/pg_walDatabase Port: 5444Database Superuser: enterprisedbOperating System Account: enterprisedbDatabase Service: edb-as-10Command Line Tools Installation Directory: /opt/edbStackBuilderPlus Installation Directory: /opt/edb/stackbuilderplusPress [Enter] to continue:----------------------------------------------------------------------------Setup is now ready to begin installing EDB Postgres Advanced Server on your computer.Do you want to continue? [Y/n]: y开始安装----------------------------------------------------------------------------Please wait while Setup installs EDB Postgres Advanced Server on your computer. Installing EDB Postgres Advanced Server 0% ______________ 50% ______________ 100% #########################################----------------------------------------------------------------------------Setup has finished installing EDB Postgres Advanced Server on your computer. 安装完成，安装文件如下 123456789101112131415161718[root@pghost6 ~]# ll /opt/edbtotal 8124drwxr-xr-x 2 root daemon 4096 Jan 15 10:34 bindrwx------ 21 enterprisedb enterprisedb 4096 Jan 15 10:35 datadrwxr-xr-x 4 root daemon 38 Jan 15 10:34 doc-rw-r--r-- 1 root daemon 72341 Aug 17 14:09 edb-as10-server_commandlinetools_3rd_party_licenses.txt-r--r--r-- 1 root daemon 15216 Aug 17 14:09 edb-as10-server_license.txtdrwxr-xr-x 3 root daemon 23 Jan 15 10:34 etcdrwxr-xr-x 6 root daemon 4096 Jan 15 10:34 include-rw-r--r-- 1 root root 556 Jan 15 10:35 installation_summary.logdrwxr-xr-x 5 root daemon 55 Jan 15 10:34 installerdrwxr-xr-x 4 root daemon 8192 Jan 15 10:34 lib-rwxr-xr-x 1 root daemon 915 Jan 15 10:35 pgplus_env.shdrwxr-xr-x 5 root daemon 73 Jan 15 10:34 scriptsdrwxr-xr-x 10 root daemon 4096 Jan 15 10:34 sharedrwxr-xr-x 7 root daemon 136 Jan 15 10:34 stackbuilderplus-rwx------ 1 root daemon 8118363 Jan 15 10:35 uninstall-edb-as10-server-rw------- 1 root root 68383 Jan 15 10:35 uninstall-edb-as10-server.dat 目录的权限为root，权限需要调整(安装过程中默认创建了 enterprisedb 操作系统用户) 1[root@pghost6 ~]# chown -R enterprisedb:enterprisedb /opt/edb 设置环境变量安装完成后自动生成环境变量文件 /opt/edb/pgplus_env.sh, 脚本内容如下: 12345678910111213141516171819202122232425262728293031# EnterpriseDB shell environment loader## Instructions:# This file contains additions to the user environment# that make accessing EDB Postgres Advanced Server# executables easier.## To load the environment for a single user:# cp pgplus_env.sh /home/&lt;username&gt;# chown &lt;username&gt; /home/&lt;username&gt;/pgplus_env.sh# vi /home/&lt;username&gt;/.bash_profile# At the bottom, add the line:# . /home/&lt;username&gt;/pgplus_env.sh# ( Note the '.' followed by a space )# To load the environment for all users:# cp pgplus_env.sh /etc# vi /etc/profile# At the bottom, add the line:# . /etc/pgplus_env.sh# ( Note the '.' followed by a space )# Environmentexport PATH=/opt/edb/bin:$PATHexport EDBHOME=/opt/edbexport PGDATA=/opt/edb/dataexport PGDATABASE=edb# export PGUSER=enterprisedbexport PGPORT=5444export PGLOCALEDIR=/opt/edb/share/locale 将整个文件内容或尾部的环境变量加到系统用户 enterprisedb 环境变量文件 .bash_profile中，如下: 1234[root@pghost6 ~]# su - enterprisedbLast login: Tue Jan 15 11:00:16 CST 2019 on pts/1-bash-4.2$ cp /opt/edb/pgplus_env.sh /opt/edb/.bash_profile 登录EDB验证登录 EDB 数据库，如下: 12345678910111213141516171819202122-bash-4.2$ psql -h 127.0.0.1 -p 5444Password: psql.bin (10.5.12)Type \"help\" for help.edb=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | ICU | Access privileges -----------+--------------+----------+------------+------------+-----+------------------------------- edb | enterprisedb | UTF8 | en_US.utf8 | en_US.utf8 | | postgres | enterprisedb | UTF8 | en_US.utf8 | en_US.utf8 | | template0 | enterprisedb | UTF8 | en_US.utf8 | en_US.utf8 | | =c/enterprisedb + | | | | | | enterprisedb=CTc/enterprisedb template1 | enterprisedb | UTF8 | en_US.utf8 | en_US.utf8 | | =c/enterprisedb + | | | | | | enterprisedb=CTc/enterprisedb(4 rows)edb=# SELECT version(); version -------------------------------------------------------------------------------------------------------------- EnterpriseDB 10.5.12 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-18), 64-bit(1 row) EDB的启动与关闭EDB 的启停有多种方式，这里主要介绍 pg_ctl 和 systemctl 方式(RHEL7或CentOS7以上系统)。 pg_ctl 启停pg_ctl 的启停兼容 PostgreSQL 的语法，不需要ROOT用户执行，关闭数据库如下: 123-bash-4.2$ pg_ctl stop -m fast waiting for server to shut down.... doneserver stopped 启动数据库，如下:12345678-bash-4.2$ pg_ctl startwaiting for server to start....2019-01-15 11:05:59 CST LOG: listening on IPv4 address \"0.0.0.0\", port 54442019-01-15 11:05:59 CST LOG: listening on IPv6 address \"::\", port 54442019-01-15 11:05:59 CST LOG: listening on Unix socket \"/tmp/.s.PGSQL.5444\"2019-01-15 11:05:59 CST LOG: redirecting log output to logging collector process2019-01-15 11:05:59 CST HINT: Future log output will appear in directory \"log\". doneserver started systemctl 启停如果是 CentOS7 或 RHEL7 以上操作系统，通过 systemctl启动或关闭EDB，如下: 12[root@pghost6 ~]# systemctl stop edb-as-10.service [root@pghost6 ~]# systemctl start edb-as-10.service EDB与Oracle的兼容性以下是EDB官网对Oracle兼容性的描述，支持大部分Oracle语法，如下: Installing Advanced Server in Compatible with Oracle mode provides the following functionality: Data dictionary views and data type conversions compatible with Oracle databases. Date values displayed in a format compatible with Oracle syntax. Oracle-styled concatenation rules (if you concatenate a string value with a NULL value, the returned value is the value of the string). Schemas (dbo and sys) compatible with Oracle databases added to the SEARCH_PATH. Support for the following Oracle built-in packages: Package Functionality Compatible with Oracle Databases dbms_alert Provides the ability to register for, send and receive alerts. dbms_aq Provides queueing functionality for Advanced Server. dbms_aqadm Provides supporting functionality for dbms_aq. dbms_crypto Provides a way to encrypt or decrypt RAW, BLOB or CLOB data. dbms_job Implements job-scheduling functionality. dbms_lob Provides the ability to manage large objects. dbms_lock Provides support for the DBMS_LOCK.SLEEP procedure. dbms_mview Provides a way to manage and refresh materialized views. dbms_output Provides the ability to display a message on the client. dbms_pipe Provides the ability to send a message from one session and read it in another session. dbms_profiler Collects and stores performance data about PL/pgSQL and SPL statements. dbms_random Provides a way to generate random numbers. dbms_rls Implements row level security. dbms_scheduler Provides a way to create and manage Oracle-style jobs. dbms_session A partial implementation that provides support for DBMS_SESSION.SET_ROLE. dbms_sql Implements use of Dynamic SQL dbms_utility Provides a collection of misc functions and procedures. utl_encode Provides a way to encode or decode data. utl_file Provides a way for a function, procedure or anonymous block to interact with files stored in the server’s file system. utl_http Provides a way to use HTTP or HTTPS to retrieve information found at a URL. utl_mail Provides a simplified interface for sending email and attachments. utl_raw Provides a way to manipulate or retrieve the length of raw data types. utl_smtp Implements smtp email functions. utl_url Provides a way to escape illegal and reserved characters in a URL. 总结以上是EDB交互式安装整体过程，EDB也支持无人值守安装，详见 EnterpriseDB: 无人值守安装 参考 企业级PG：EnterpriseDB的安装使用 Performing a Text Mode Installation EnterpriseDB: 无人值守安装","categories":[{"name":"EDB","slug":"EDB","permalink":"https://postgres.fun/categories/EDB/"}],"tags":[{"name":"EDB","slug":"EDB","permalink":"https://postgres.fun/tags/EDB/"}]},{"title":"Hexo: 添加Valine评论(邮件通知、评论列表头像)","slug":"20190107095300","date":"2019-01-07T01:53:22.000Z","updated":"2019-01-08T01:12:04.091Z","comments":true,"path":"20190107095300.html","link":"","permalink":"https://postgres.fun/20190107095300.html","excerpt":"","text":"Hexo 博客和 Valine 模块部署有好几个月了，Valine 模块部署本身并不复杂，但部署后发现头像无法显示设置的图像，这个问题困扰了至少1个月，今天请教高手终于解决了这个问题，是时候好好总结下 Valine 模块的部署历程了。 之所以选择 Valine 模块，一方面因为 Valine 评论模块延续了Hexo模板简洁的风格，两者匹配得可谓天衣无缝，另一方面国内可选的评论模块有些已经不再支持，有些模块的UI各人不太喜欢。 关于 Valine 模块Valine 诞生于2017年8月7日，是一款基于 Leancloud 的快速、简洁且高效的无后端评论系统，支持但不限于静态博客。 具有以下特性： 快速 安全 Emoji 无后端实现 MarkDown 全语法支持 轻量易用(~15kb gzipped) 文章阅读量统计 v1.2.0+ Valine 部署Valine 是基于 leancloud的评论模块，评论数据都存储在 Leancloud 平台，因此需要先在 leancloud 申请帐号。 申请 Leancloud 帐号申请 Leancloud 步骤比较简单，首先进行 Leancloud 控制台创建应用(个人使用开发版)，之后获取应用的 App ID 和 App Key，后面配置博客的主题配置文件会用到。 配置安全域名登录leancloud控制台，选择 设置 -&gt; 安全中心 -&gt; Web 安全域名，这里设置成博客的域名地址和本地地址即可，如图: 配置主题配置文件修改博客主题配置文件 /d/hexo/themes/next/_config.yml，配置 Valine 的 enable、appid、appkey 参数，如下: 12345678910111213# Valine.# You can get your appid and appkey from https://leancloud.cn# more info please open https://valine.js.orgvaline: enable: true appid: your leancloud app id appkey: your leancloud app key notify: true # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 留言 # comment box placeholder avatar: # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size 以上的 appid和appkey为本文开始在Leancloud创建应用的 App ID 和 App Key。 评论模块验证之后访问博客，可以看到如下评论模块： 并且可以留言评论了，以上就是 Valine 模块的基本部署过程。 开启 Valine 评论邮件通知以上使用 Valine 模块实现了最基本的评论功能，但功能还需要完善，例如评论邮件通知、评论管理等功能，正好 Valine Admin 可以满足需求，Valine Admin 是Valine评论模块的扩展和增强。 这块内容主要参考 valine-admin-document。 云引擎”一键”部署在Leancloud云引擎设置界面，填写代码库并保存：https://github.com/DesertsP/Valine-Admin.git，如图: 设置 Web 二级域名Valine Admin 同时提供评论WEB管理后台，能方便的管理评论，例如标记为垃圾评论或删除评论。 在云引擎设置页面，设置Web二级域名，如下 二级域名用于评论后台管理，这里设置为 https://postgres.leanapp.cn 。 之后访问出现登录界面，如图: 用户名为leancloud的登录邮箱, 密码设置如下: 点请示重置后Leancloud会给你发一封密码重置的邮件，这里设置的密码为评论管理WEB后台的密码。之前不知道密码存储在云引擎的user对象里，多亏了博採眾長的指导。 设置环境变量设置环境变量，如下图: 以上参数需正确配置，我的参数配置如下: 配置完参数后，之后切换到云引擎页面点部署。 验证评论邮件通知功能假设我为博主且游客评论时填写了邮箱，以下场景能收到博客评论通知邮件: 1、用博主的帐号评论时，博主能收到通知邮件。2、游客评论后，博主能收到通知邮件。3、博主回复游客，游客能收到通知邮件。 Valine 头像设置Valine 部署完成后使用了默认图像，查阅 Valine官网后，了解到 Valine 使用的是 Gravatar作为评论列表头像，评论时提供 Gravatar 注册的邮箱即可显示设置的头像。之后我在 Gravatar 修改头像一直没生效(gravatar.cat.net 有七天的缓存)，评论列表头像没生效问题折腾了一个月左右，早已超出了7天的缓存期，后来咨询了一位高手终于解决，这里记录下。 评论头像没有刷新的原因为：Valine 默认用的是第三方CDN，第一次请求的时候缓存源站的头像，当再请求的时候就不需要返回源站请求，可能这个第三方CDN缓存刷新时有问题导致有些用户刷新成功，有些用户没有刷新，可以通过更换其它CDN解决，这里设置成 Gravatar 的CDN。 avatar 参数Valine 的 avatar 参数用来设置评论头像，avatar 参数可选值如下: 我这里将 avatar 设置成空，表示使用的是默认的评论图像。 Gravatar 设置登录 gravatar，注册账号并设置头像，我的设置如下： Next 主题设置主题配置文件增加 avatar_cdn 参数，如下: 1234567891011121314# Valine.# You can get your appid and appkey from https://leancloud.cn# more info please open https://valine.js.orgvaline: enable: true appid: UK8DCa4kSCj0Th48U4nGPxrO-gzGzoHsz appkey: gxPalmYtqejoIqRUifA1Lqg6 notify: true # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 留言 # comment box placeholder avatar: # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size avatar_cdn: https://www.gravatar.com/avatar/ 目前Valine的版本不支持 avatar_cdn 参数，这里手工加一个并设置成Gravatar的CDN。 之后修改Valine模板文件 /d/hexo/themes/next/layout/_third-party/comments/valine.swig，增加 avatar_cnd这行代码，如下: 123456789101112131415161718192021222324&#123;% if theme.valine.enable and theme.valine.appid and theme.valine.appkey %&#125; &lt;script src=\"//cdn1.lncld.net/static/js/3.0.4/av-min.js\"&gt;&lt;/script&gt; &lt;script src=\"//unpkg.com/valine/dist/Valine.min.js\"&gt;&lt;/script&gt; &lt;script type=\"text/javascript\"&gt; var GUEST = ['nick','mail','link']; var guest = '&#123;&#123; theme.valine.guest_info &#125;&#125;'; guest = guest.split(',').filter(item=&gt;&#123; return GUEST.indexOf(item)&gt;-1; &#125;); new Valine(&#123; el: '#comments' , verify: &#123;&#123; theme.valine.verify &#125;&#125;, notify: &#123;&#123; theme.valine.notify &#125;&#125;, appId: '&#123;&#123; theme.valine.appid &#125;&#125;', appKey: '&#123;&#123; theme.valine.appkey &#125;&#125;', placeholder: '&#123;&#123; theme.valine.placeholder &#125;&#125;', avatar:'&#123;&#123; theme.valine.avatar &#125;&#125;', guest_info:guest, pageSize:'&#123;&#123; theme.valine.pageSize &#125;&#125;' || 10, avatar_cdn:'&#123;&#123; theme.valine.avatar_cdn &#125;&#125;',&#125;); &lt;/script&gt;&#123;% endif %&#125; 评论头像终于刷新来了，如下图: 参考 valine Valine Admin 配置手册","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/tags/Hexo/"}]},{"title":"PostgreSQL 2018 中国技术大会圆满举行","slug":"20181219135100","date":"2018-12-19T05:51:47.000Z","updated":"2018-12-20T07:56:04.545Z","comments":true,"path":"20181219135100.html","link":"","permalink":"https://postgres.fun/20181219135100.html","excerpt":"","text":"“Postgres中国技术大会2018（大象会，第8届）”已于2018年12月15、16日在浙江吉利控股集团有限公司总部(杭州市滨江区江陵路1760号)举行。 大会由中国Postgres用户会（China Postgres User Group，简称：CPUG）主办，是一场汇聚各界PostgreSQL大拿、交流最新业界技术动态和真实应用案例的盛宴。 本次大会参会者、嘉宾共400余人，议题丰富。 本届大会的议题如下: 主会场 – 12/15日 赖宝华_开源分布式NewSQL数据库CockroachDB架构及最佳实践 李海龙_Qunar的PostgreSQL运维实践 周正中_如何构建PostgreSQL大生态之我见 潘娟ApacheShardingSphere云架构演化 邵宗文_图数据库及应用场景 汪洋_PG之高可用特性、工具及架构设计 王昊Greenplum5智能运维管理实例及展望 主会场 – 12/16日 陈华军_citus在苏宁的大规模应用 梅白帆_PostGIS在传统行业中的应用简介 石勇虎_庖丁解牛之平安vacuum优化之路 唐建法MongoDB4.0_开创NoSQL＿+＿ACID新纪元 王帅_从Oracle到PostgreSQL的数据迁移 张启程_为什么我们抛弃MongoDB和MySQL，选择PgSQL 赵振平_PostgreSQL和Greenplum数据库故障排查 周飞_PG在Hellobike的应用 分会场1 – 12/15日 陈河堆_PostgreSQL基于PaaS平台的高可用集群方案V3 董红禹SQLServer迁移PG经验分享 桑栎_PipelineDB体系结构和使用场景 杨杰PostgreSQL-FlashbackQuery实现与介绍 分会场1 – 12/16日 曾文旌阿里云RDSfor_PostgreSQL在PostgreSQL功能和性能改进 樊文凯_ORACLE数据库和应用异构迁移最佳实践 冯慧媛_GIS地理信息-postgres在时空大数据方面的应用 黄晓涛_通过FDW对大容量非结构化文件的管理和访问 赖思超PostgreSQL10hash索引的WAL日志修改版final 刘成伟_oracle到Postgres数据库迁移工具 陶征霖_新一代数据仓库OushuDB架构剖析 分会场2 – 12/15日 党宏博_亚信AntDB在业务驱动下的创新-v0.2 马鹏玮_以标准推动数据库新技术的快速落地 权宗亮_基于odyssey连接池实现企业级PostgreSQL数据分布中间件 肖斐_PostgreSQL数据库时空引擎Ganos 分会场2 – 12/16日 胡森_TBase分布式架构以及OLAP性能优化 刘东明_PostgreSQL并行查询 余鹏_gogudb—基于FDW实现的PG分库分表插件 资料下载本次大会的资料可在PosgreSQL中文社区官网下载 http://postgres.cn/v2/news/viewone/1/377","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL何以支持丰富的NoSQL特性?","slug":"20181218101700","date":"2018-12-18T02:17:26.000Z","updated":"2018-12-20T00:26:56.730Z","comments":true,"path":"20181218101700.html","link":"","permalink":"https://postgres.fun/20181218101700.html","excerpt":"","text":"一、引言上篇文章 介绍了PostgreSQL的典型高级SQL特性，PostgreSQL不仅是关系型数据库，同时支持丰富的NoSQL特性，本文将从 《PostgreSQL实战》 一书的“ 第9章 PostgreSQL的NoSQL特性”中摘选部分内容介绍。 本文主要包含以下三部分内容: PostgreSQL的 JSON和JSONB数据类型简介 JSON与JSONB读写性能测试 PostgreSQL全文检索支持JSON和JSONB（PosgreSQL 10 新特性） 二、PostgreSQL的JSON和JSONB数据类型PostgreSQL支持非关系数据类型json (JavaScript Object Notation)，本节介绍json类型、json与jsonb差异、json与jsonb操作符和函数，以及jsonb键值的追加、删除、更新。 JSON类型简介PotgreSQL早在9.2版本已经提供了json类型，并且随着大版本的演进，PostgreSQL对json的支持趋于完善，例如提供更多的json函数和操作符方便应用开发，一个简单的json类型例子如下：1234mydb=&gt; SELECT '&#123;\"a\":1,\"b\":2&#125;'::json; json--------------- &#123;\"a\":1,\"b\":2&#125; 为了更好演示json类型，接下来创建一张表，如下所示：12mydb=&gt; CREATE TABLE test_json1 (id serial primary key,name json);CREATE TABLE 以上示例定义字段name为json类型，插入表数据，如下所示：1234567mydb=&gt; INSERT INTO test_json1 (name)VALUES ('&#123;\"col1\":1,\"col2\":\"francs\",\"col3\":\"male\"&#125;');INSERT 0 1mydb=&gt; INSERT INTO test_json1 (name)VALUES ('&#123;\"col1\":2,\"col2\":\"fp\",\"col3\":\"female\"&#125;');INSERT 0 1 查询表test_json1数据：12345mydb=&gt; SELECT * FROM test_json1; id | name----+------------------------------------------ 1 | &#123;\"col1\":1,\"col2\":\"francs\",\"col3\":\"male\"&#125; 2 | &#123;\"col1\":2,\"col2\":\"fp\",\"col3\":\"female\"&#125; 查询JSON数据通过-&gt;操作符可以查询json数据的键值，如下所示：12345mydb=&gt; SELECT name -&gt; 'col2' FROM test_json1 WHERE id=1; ?column?---------- \"francs\"(1 row) 如果想以文本格式返回json字段键值可以使用-&gt;&gt;符，如下所示：12345mydb=&gt; SELECT name -&gt;&gt; 'col2' FROM test_json1 WHERE id=1; ?column?---------- francs(1 row) JSONB与JSON差异PostgreSQL支持两种JSON数据类型：json和jsonb，两种类型在使用上几乎完全相同，两者主要区别为以下：json存储格式为文本而jsonb存储格式为二进制 ，由于存储格式的不同使得两种json数据类型的处理效率不一样，json类型以文本存储并且存储的内容和输入数据一样，当检索json数据时必须重新解析，而jsonb以二进制形式存储已解析好的数据，当检索jsonb数据时不需要重新解析，因此json写入比jsonb快，但检索比jsonb慢，后面会通过测试验证两者读写性能差异。除了上述介绍的区别之外，json与jsonb在使用过程中还存在差异，例如jsonb输出的键的顺序和输入不一样，如下所示：12345mydb=&gt; SELECT '&#123;\"bar\": \"baz\", \"balance\": 7.77, \"active\":false&#125;'::jsonb; jsonb-------------------------------------------------- &#123;\"bar\": \"baz\", \"active\": false, \"balance\": 7.77&#125;(1 row) 而json的输出键的顺序和输入完全一样，如下所示：12345mydb=&gt; SELECT '&#123;\"bar\": \"baz\", \"balance\": 7.77, \"active\":false&#125;'::json; json------------------------------------------------- &#123;\"bar\": \"baz\", \"balance\": 7.77, \"active\":false&#125;(1 row) 另外，jsonb类型会去掉输入数据中键值的空格，如下所示：12345mydb=&gt; SELECT ' &#123;\"id\":1, \"name\":\"francs\"&#125;'::jsonb; jsonb----------------------------- &#123;\"id\": 1, \"name\": \"francs\"&#125;(1 row) 上例中id键与name键输入时是有空格的，输出显示空格键被删除，而json的输出和输入一样，不会删掉空格键：12345mydb=&gt; SELECT ' &#123;\"id\":1, \"name\":\"francs\"&#125;'::json; json------------------------------- &#123;\"id\":1, \"name\":\"francs\"&#125;(1 row) 另外，jsonb会删除重复的键，仅保留最后一个，如下所示：123456789mydb=&gt; SELECT ' &#123;\"id\":1,\"name\":\"francs\",\"remark\":\"a good guy!\",\"name\":\"test\"&#125;'::jsonb; jsonb---------------------------------------------------- &#123;\"id\": 1, \"name\": \"test\", \"remark\": \"a good guy!\"&#125;(1 row) 上面name键重复，仅保留最后一个name键的值，而json数据类型会保留重复的键值。相比json大多数应用场景建议使用jsonb，除非有特殊的需求，比如对json的键顺序有特殊的要求。 JSONB与JSON操作符PostgreSQL支持丰富的JSONB和JSON的操作符，举例如下：以文本格式返回json类型的字段键值可以使用-&gt;&gt;符，如下所示：12345mydb=&gt; SELECT name -&gt;&gt; 'col2' FROM test_json1 WHERE id=1; ?column?---------- francs(1 row) 字符串是否作为顶层键值，如下所示：12345mydb=&gt; SELECT '&#123;\"a\":1, \"b\":2&#125;'::jsonb ? 'a'; ?column?---------- t(1 row) 删除json数据的键/值，如下所示：12345mydb=&gt; SELECT '&#123;\"a\":1, \"b\":2&#125;'::jsonb - 'a'; ?column?---------- &#123;\"b\": 2&#125;(1 row) JSONB与JSON函数json与jsonb相关的函数非常丰富，举例如下：扩展最外层的json对象成为一组键/值结果集，如下所示：123456mydb=&gt; SELECT * FROM json_each('&#123;\"a\":\"foo\", \"b\":\"bar\"&#125;'); key | value-----+------- a | \"foo\" b | \"bar\"(2 rows) 以文本形式返回结果，如下所示：123456mydb=&gt; SELECT * FROM json_each_text('&#123;\"a\":\"foo\", \"b\":\"bar\"&#125;'); key | value-----+------- a | foo b | bar(2 rows) 一个非常重要的函数为row_to_json()函数，能够将行作为json对象返回，此函数常用来生成json测试数据，比如将一个普通表转换成json类型表：1234567891011mydb=&gt; SELECT * FROM test_copy WHERE id=1; id | name----+------ 1 | a(1 row)mydb=&gt; SELECT row_to_json(test_copy) FROM test_copy WHERE id=1; row_to_json--------------------- &#123;\"id\":1,\"name\":\"a\"&#125;(1 row) 返回最外层的json对像中的键的集合，如下所示：123456mydb=&gt; SELECT * FROM json_object_keys('&#123;\"a\":\"foo\", \"b\":\"bar\"&#125;'); json_object_keys------------------ a b(2 rows) jsonb键/值的追加、删除、更新jsonb键/值追加可通过||操作符，如下增加sex键/值：123456mydb=&gt; SELECT '&#123;\"name\":\"francs\",\"age\":\"31\"&#125;'::jsonb ||'&#123;\"sex\":\"male\"&#125;'::jsonb; ?column?------------------------------------------------ &#123;\"age\": \"31\", \"sex\": \"male\", \"name\": \"francs\"&#125;(1 row) jsonb键/值的删除有两种方法，一种是通过操作符号-删除，另一种通过操作符#-删除指定键/值。通过操作符号-删除键/值如下：1234567891011mydb=&gt; SELECT '&#123;\"name\": \"James\", \"email\": \"james@localhost\"&#125;'::jsonb - 'email'; ?column?------------------- &#123;\"name\": \"James\"&#125;(1 row)mydb=&gt; SELECT '[\"red\",\"green\",\"blue\"]'::jsonb - 0; ?column?------------------- [\"green\", \"blue\"] 第二种方法是通过操作符#-删除指定键/值，通常用于有嵌套json数据删除的场景，如下删除嵌套contact中的fax键/值：12345mydb=&gt; SELECT '&#123;\"name\": \"James\", \"contact\": &#123;\"phone\": \"01234 567890\", \"fax\": \"01987 543210\"&#125;&#125;'::jsonb #- '&#123;contact,fax&#125;'::text[]; ?column?--------------------------------------------------------- &#123;\"name\": \"James\", \"contact\": &#123;\"phone\": \"01234 567890\"&#125;&#125;(1 row) 删除嵌套aliases中的位置为1的键/值，如下所示：12345mydb=&gt; SELECT '&#123;\"name\": \"James\", \"aliases\": [\"Jamie\",\"The Jamester\",\"J Man\"]&#125;'::jsonb #- '&#123;aliases,1&#125;'::text[]; ?column?-------------------------------------------------- &#123;\"name\": \"James\", \"aliases\": [\"Jamie\", \"J Man\"]&#125;(1 row) 键/值的更新也有两种方式，第一种方式为||操作符，||操作符可以连接json键，也可覆盖重复的键值，如下修改age键的值：123456mydb=&gt; SELECT '&#123;\"name\":\"francs\",\"age\":\"31\"&#125;'::jsonb ||'&#123;\"age\":\"32\"&#125;'::jsonb; ?column?--------------------------------- &#123;\"age\": \"32\", \"name\": \"francs\"&#125;(1 row) 第二种方式是通过jsonb_set函数，语法如下：1jsonb_set(target jsonb, path text[], new_value jsonb[, create_missing boolean]) target指源jsonb数据，path指路径，new_value指更新后的键值，create_missing 值为 true表示如果键不存在则添加，create_missing 值为 false表示如果键不存在则不添加，示例如下：1234567891011mydb=&gt; SELECT jsonb_set('&#123;\"name\":\"francs\",\"age\":\"31\"&#125;'::jsonb,'&#123;age&#125;','\"32\"'::jsonb,false); jsonb_set--------------------------------- &#123;\"age\": \"32\", \"name\": \"francs\"&#125;(1 row)mydb=&gt; SELECT jsonb_set('&#123;\"name\":\"francs\",\"age\":\"31\"&#125;'::jsonb,'&#123;sex&#125;','\"male\"'::jsonb,true); jsonb_set------------------------------------------------ &#123;\"age\": \"31\", \"sex\": \"male\", \"name\": \"francs\"&#125;(1 row) 给JSONB类型创建索引这一小节介绍给jsonb数据类型创建索引，jsonb数据类型支持GIN索引，为了便于说明，假如一个json字段内容如下，并且以jsonb格式存储。123456&#123; \"id\": 1, \"user_id\": 1440933, \"user_name\": \"1_francs\", \"create_time\": \"2017-08-03 16:22:05.528432+08\"&#125; 假如存储以上jsonb数据的字段名为user_info，表名为tbl_user_jsonb，在user_info字段上创建GIN索引语法如下：1CREATE INDEX idx_gin ON tbl_user_jsonb USING gin(user_info); jsonb上的GIN索引支持@&gt;、?、 ?&amp;、?|操作符，例如以下查询将会使用索引。1SELECT * FROM tbl_user_jsonb WHERE user_info @&gt; '&#123;\"user_name\": \"1_frans\"&#125;' 但是以下基于jsonb键值的查询不会走索引idx_gin，如下所示：1SELECT * FROM tbl_user_jsonb WHERE user_info-&gt;&gt;'user_name'= '1_francs'; 如果要想提升基于jsonb类型的键值检索效率，可以在jsonb数据类型对应的键值上创建索引，如下所示：12CREATE INDEX idx_gin_user_infob_user_name ON tbl_user_jsonb USING btree((user_info -&gt;&gt; 'user_name')); 创建以上索引后，上述根据user_info-&gt;&gt;’user_name’键值查询的SQL将会走索引。 三、JSON与JSONB读写性能测试上一小节介绍了jsonb数据类型索引创建相关内容，本小节将对json、jsonb读写性能进行简单对比，在第3章数据类型章节中介绍json、jsonb数据类型时提到了两者读写性能的差异，主要表现为json写入时比jsonb快，但检索时比jsonb慢，主要原因为：json存储格式为文本而jsonb存储格式为二进制，存储格式的不同使得两种json数据类型的处理效率不一样，json类型存储的内容和输入数据一样，当检索json数据时必须重新解析，而jsonb以二进制形式存储已解析好的数据，当检索jsonb数据时不需要重新解析。 构建JSON、JSONB测试表下面通过一个简单的例子测试下json、jsonb的读写性能差异，计划创建以下三张表： user_ini：基础数据表，并插入200万测试数据； tbl_user_json:： json 数据类型表，200万数据； tbl_user_jsonb： jsonb 数据类型表，200万数据； 首先创建user_ini表并插入200万测试数据，如下：123456789mydb=&gt; CREATE TABLE user_ini(id int4 ,user_id int8, user_name charactervarying(64),create_time timestamp(6) with time zone defaultclock_timestamp());CREATE TABLEmydb=&gt; INSERT INTO user_ini(id,user_id,user_name)SELECT r,round(random()*2000000), r || '_francs'FROM generate_series(1,2000000) as r;INSERT 0 2000000 计划使用user_ini表数据生成json、jsonb数据，创建user_ini_json、user_ini_jsonb表，如下所示：1234mydb=&gt; CREATE TABLE tbl_user_json(id serial, user_info json);CREATE TABLEmydb=&gt; CREATE TABLE tbl_user_jsonb(id serial, user_info jsonb);CREATE TABLE JSON与JSONB表写性能测试根据user_ini数据通过row_to_json函数向表user_ini_json插入200万json数据，如下：1234567mydb=&gt; \\timingTiming is on.mydb=&gt; INSERT INTO tbl_user_json(user_info) SELECT row_to_json(user_ini)FROM user_ini;INSERT 0 2000000Time: 13825.974 ms (00:13.826) 从以上结果看出tbl_user_json插入200万数据花了13秒左右；接着根据user_ini表数据生成200万jsonb数据并插入表tbl_user_jsonb，如下：1234mydb=&gt; INSERT INTO tbl_user_jsonb(user_info) SELECT row_to_json(user_ini)::jsonb FROM user_ini;INSERT 0 2000000Time: 20756.993 ms (00:20.757) 从以上看出tbl_user_jsonb表插入200万jsonb数据花了20秒左右，正好验证了json数据写入比jsonb快，比较两表占用空间大小，如下所示：12345678910111213mydb=&gt; \\dt+ tbl_user_json List of relations Schema | Name | Type | Owner | Size | Description--------+---------------+-------+--------+--------+------------- pguser | tbl_user_json | table | pguser | 281 MB |(1 row)mydb=&gt; \\dt+ tbl_user_jsonb List of relations Schema | Name | Type | Owner | Size | Description--------+----------------+-------+--------+--------+------------- pguser | tbl_user_jsonb | table | pguser | 333 MB |(1 row) 从占用空间来看，同样的数据量jsonb数据类型占用空间比json稍大。 查询tbl_user_json表的一条测试数据，如下：：12345mydb=&gt; SELECT * FROM tbl_user_json LIMIT 1; id | user_info---------+------------------------------------------------------------------------------------ 2000001 | &#123;\"id\":1,\"user_id\":1182883,\"user_name\":\"1_francs\",\"create_time\":\"2017-08-03T20:59:27.42741+08:00\"&#125;(1 row) JSON与JSONB表读性能测试对于json、jsonb读性能测试我们选择基于json、jsonb键值查询的场景，例如，根据user_info字段的user_name键的值查询，如下所示：123456789mydb=&gt; EXPLAIN ANALYZE SELECT * FROM tbl_user_jsonb WHERE user_info-&gt;&gt;'user_name'='1_francs'; QUERY PLAN------------------------------------------------------------------------------------- Seq Scan on tbl_user_jsonb (cost=0.00..72859.90 rows=10042 width=143) (actual time=0.023..524.843 rows=1 loops=1) Filter: ((user_info -&gt;&gt; 'user_name'::text) = '1_francs'::text) Rows Removed by Filter: 1999999 Planning time: 0.091 ms Execution time: 524.876 ms(5 rows) 上述SQL执行时间为524毫秒左右，基于user_info字段的user_name键值创建btree索引如下：12mydb=&gt; CREATE INDEX idx_jsonb ON tbl_user_jsonb USING btree((user_info-&gt;&gt;'user_name')); 再次执行上述查询，如下所示：1234567891011mydb=&gt; EXPLAIN ANALYZE SELECT * FROM tbl_user_jsonb WHERE user_info-&gt;&gt;'user_name'='1_francs'; QUERY PLAN------------------------------------------------------------------------------------- Bitmap Heap Scan on tbl_user_jsonb (cost=155.93..14113.93 rows=10000 width=143) (actual time=0.027..0.027 rows=1 loops=1) Recheck Cond: ((user_info -&gt;&gt; 'user_name'::text) = '1_francs'::text) Heap Blocks: exact=1 -&gt; Bitmap Index Scan on idx_jsonb (cost=0.00..153.43 rows=10000 width=0) (actual time=0.021..0.021 rows=1 loops=1) Index Cond: ((user_info -&gt;&gt; 'user_name'::text) = '1_francs'::text) Planning time: 0.091 ms Execution time: 0.060 ms(7 rows) 根据上述执行计划看出走了索引，并且SQL时间下降到0.060ms。为更好的对比tbl_user_json、tbl_user_jsonb表基于键值查询的效率，计划根据user_info字段id键进行范围扫描对比性能，创建索引如下：1234567mydb=&gt; CREATE INDEX idx_gin_user_info_id ON tbl_user_json USING btree(((user_info -&gt;&gt; 'id')::integer));CREATE INDEXmydb=&gt; CREATE INDEX idx_gin_user_infob_id ON tbl_user_jsonb USING btree(((user_info -&gt;&gt; 'id')::integer));CREATE INDEX 索引创建后，查询tbl_user_json表如下：1234567891011121314mydb=&gt; EXPLAIN ANALYZE SELECT id,user_info-&gt;'id',user_info-&gt;'user_name' FROM tbl_user_jsonWHERE (user_info-&gt;&gt;'id')::int4&gt;1 AND (user_info-&gt;&gt;'id')::int4&lt;10000; QUERY PLAN------------------------------------------------------------------------------------- Bitmap Heap Scan on tbl_user_json (cost=166.30..14178.17 rows=10329 width=68) (actual time=1.167..26.534 rows=9998 loops=1) Recheck Cond: ((((user_info -&gt;&gt; 'id'::text))::integer &gt; 1) AND (((user_info -&gt;&gt; 'id'::text))::integer &lt; 10000)) Heap Blocks: exact=338 -&gt; Bitmap Index Scan on idx_gin_user_info_id (cost=0.00..163.72 rows=10329 width=0) (actual time=1.110..1.110 rows=19996 loops=1) Index Cond: ((((user_info -&gt;&gt; 'id'::text))::integer &gt; 1) AND (((user_info -&gt;&gt; 'id'::text))::integer &lt; 10000)) Planning time: 0.094 ms Execution time: 27.092 ms(7 rows) 根据以上看出，查询表tbl_user_json的user_info字段id键值在1到10000范围内的记录走了索引，并且执行时间为27.092毫秒，接着测试tbl_user_jsonb表同样SQL的检索性能，如下所示：1234567891011121314mydb=&gt; EXPLAIN ANALYZE SELECT id,user_info-&gt;'id',user_info-&gt;'user_name' FROM tbl_user_jsonbWHERE (user_info-&gt;&gt;'id')::int4&gt;1 AND (user_info-&gt;&gt;'id')::int4&lt;10000; QUERY PLAN------------------------------------------------------------------------------------- Bitmap Heap Scan on tbl_user_jsonb (cost=158.93..14316.93 rows=10000 width=68) (actual time=1.140..8.116 rows=9998 loops=1) Recheck Cond: ((((user_info -&gt;&gt; 'id'::text))::integer &gt; 1) AND (((user_info -&gt;&gt; 'id'::text))::integer &lt; 10000)) Heap Blocks: exact=393 -&gt; Bitmap Index Scan on idx_gin_user_infob_id (cost=0.00..156.43 rows=10000 width=0) (actual time=1.058..1.058 rows=18992 loops=1) Index Cond: ((((user_info -&gt;&gt; 'id'::text))::integer &gt; 1) AND (((user_info -&gt;&gt; 'id'::text))::integer &lt; 10000)) Planning time: 0.104 ms Execution time: 8.656 ms(7 rows) 根据以上看出，查询表tbl_user_jsonb的user_info字段id键值在1到10000范围内的记录走了索引并且执行时间为8.656毫秒，从这个测试看出jsonb检索比json效率高。从以上两个测试看出，正好验证了“json写入比jsonb快，但检索时比jsonb慢”的观点，值得一提的是如果需要通过key/value进行检索，例如以下。1SELECT * FROM tbl_user_jsonb WHERE user_info @&gt; '&#123;\"user_name\": \"2_francs\"&#125;'; 这时执行计划为全表扫描，如下所示：123456789mydb=&gt; EXPLAIN ANALYZE SELECT * FROM tbl_user_jsonb WHERE user_info @&gt; '&#123;\"user_name\": \"2_francs\"&#125;'; QUERY PLAN------------------------------------------------------------------------------------ Seq Scan on tbl_user_jsonb (cost=0.00..67733.00 rows=2000 width=143) (actual time=0.018..582.207 rows=1 loops=1) Filter: (user_info @&gt; '&#123;\"user_name\": \"2_francs\"&#125;'::jsonb) Rows Removed by Filter: 1999999 Planning time: 0.065 ms Execution time: 582.232 ms(5 rows) 从以上看出执行时间为582毫秒左右，在tbl_user_jsonb字段user_info上创建gin索引，如下所示：123mydb=&gt; CREATE INDEX idx_tbl_user_jsonb_user_Info ON tbl_user_jsonb USING gin (user_Info);CREATE INDEX 索引创建后，再次执行以下，如下所示：123456789101112mydb=&gt; EXPLAIN ANALYZE SELECT * FROM tbl_user_jsonb WHERE user_info @&gt; '&#123;\"user_name\": \"2_francs\"&#125;'; QUERY PLAN------------------------------------------------------------------------------------- Bitmap Heap Scan on tbl_user_jsonb (cost=37.50..3554.34 rows=2000 width=143) (actual time=0.079..0.080 rows=1 loops=1) Recheck Cond: (user_info @&gt; '&#123;\"user_name\": \"2_francs\"&#125;'::jsonb) Heap Blocks: exact=1 -&gt; Bitmap Index Scan on idx_tbl_user_jsonb_user_info (cost=0.00..37.00 rows=2000 width=0) (actual time=0.069..0.069 rows=1 loops=1) Index Cond: (user_info @&gt; '&#123;\"user_name\": \"2_francs\"&#125;'::jsonb) Planning time: 0.094 ms Execution time: 0.114 ms(7 rows) 从以上看出走了索引，并且执行时间下降到了0.114毫秒。这一小节测试了json、jsonb数据类型读写性能差异，验证了json写入时比jsonb快，但检索时比jsonb慢的观点。 四、PostgreSQ全文检索支持JSON和JSONB这一小节将介绍PostgreSQL10的一个新特性：全文检索支持json、jsonb数据类型，本小节分两部分，第一部分简单介绍PostgreSQL全文检索，第二部分演示全文检索对json、jsonb数据类型的支持。 PostgreSQL全文检索简介对于大多数应用全文检索很少放到数据库中实现，一般使用单独的全文检索引擎，例如基于SQL全文检索引擎Sphinx。PostgreSQL支持全文检索，对于规模不大的应用如果不想搭建专门的搜索引擎，PostgreSQL的全文检索也可以满足需求。如果没有使用专门的搜索引擎，大部检索需要通过数据库like操作匹配，这种检索方式主要缺点在于： 不能很好的支持索引，通常需全表扫描检索数据，数据量大时检索性能很低。 不提供检索结果排序，当输出结果数据量非常大时表现更加明显。 PostgreSQL全文检索能有效地解决这个问题，PostgreSQL全文检索通过以下两种数据类型来实现。 tsvectortsvector全文检索数据类型代表一个被优化的可以基于搜索的文档，将一串字符串转换成tsvector全文检索数据类型，如下：12345mydb=&gt; SELECT 'Hello,cat,how are u? cat is smiling! '::tsvector; tsvector-------------------------------------------------- 'Hello,cat,how' 'are' 'cat' 'is' 'smiling!' 'u?'(1 row) 可以看到，字符串的内容被分隔成好几段，但通过::tsvector只是做类型转换，没有进行数据标准化处理，对于英文全文检索可通过函数to_tsvector进行数据标准化，如下所示：12345mydb=&gt; SELECT to_tsvector('english','Hello cat,'); to_tsvector------------------- 'cat':2 'hello':1(1 row) tsquerytsquery表示一个文本查询，存储用于搜索的词，并且支持布尔操作&amp;、|、!，将字符串转换成tsquery，如下所示：12345mydb=&gt; SELECT 'hello&amp;cat'::tsquery; tsquery----------------- 'hello' &amp; 'cat'(1 row) 上述只是转换成tsquery类型，而并没有做标准化，使用to_tsquery函数可以执行标准化，如下所示：12345mydb=&gt; SELECT to_tsquery( 'hello&amp;cat' ); to_tsquery----------------- 'hello' &amp; 'cat'(1 row) 一个全文检索示例如下，检索字符串是否包括hello和cat字符，本例中返回真。123456mydb=&gt; SELECT to_tsvector('english','Hello cat,how are u') @@to_tsquery( 'hello&amp;cat' ); ?column?---------- t(1 row) 检索字符串是否包含字符hello和dog，本例中返回假。123456mydb=&gt; SELECT to_tsvector('english','Hello cat,how are u') @@ to_tsquery( 'hello&amp;dog' ); ?column?---------- f(1 row) 有兴趣的读者可以测试tsquery的其他操作符，例如|、!等。 注意：这里使用了带双参数的to_tsvector函数，函数to_tsvector双参数的格式如下：1to_tsvector([ config regconfig , ] document text)，本节to_tsvector函数指定了config参数为english，如果不指定config参数，则默认使用default_text_search_config参数的配置。 英文全文检索例子下面演示一个英文全文检索示例，创建一张测试表并插入200万测试数据，如下所示：123456mydb=&gt; CREATE TABLE test_search(id int4,name text);CREATE TABLEmydb=&gt; INSERT INTO test_search(id,name) SELECT n, n||'_francs'FROM generate_series(1,2000000) n;INSERT 0 2000000 执行以下SQL，查询test_search表name字段包含字符1_francs的记录。12345mydb=&gt; SELECT * FROM test_search WHERE name LIKE '1_francs'; id | name----+---------- 1 | 1_francs(1 row) 执行计划如下：12345678mydb=&gt; EXPLAIN ANALYZE SELECT * FROM test_search WHERE name LIKE '1_francs'; QUERY PLAN-------------------------------------------------------------------------------------Seq Scan on test_search (cost=0.00..38465.04 rows=204 width=18) (actual time=0.022..261.766 rows=1 loops=1) Filter: (name ~~ '1_francs'::text) Rows Removed by Filter: 1999999 Planning time: 0.101 ms Execution time: 261.796 ms(5 rows) 以上执行计划走了全表扫描，执行时间为261毫秒左右，性能很低，接着创建索引，如下所示：123mydb=&gt; CREATE INDEX idx_gin_search ON test_search USING gin(to_tsvector('english',name));CREATE INDEX 执行以下SQL，查询test_search表name字段包含字符1_francs的记录。123456mydb=&gt; SELECT * FROM test_search WHERE to_tsvector('english',name) @@to_tsquery('english','1_francs'); id | name----+---------- 1 | 1_francs(1 row) 再次查看执行计划和执行时间，如下所示：123456789101112mydb=&gt; EXPLAIN ANALYZE SELECT * FROM test_search WHERE to_tsvector('english',name) @@to_tsquery('english','1_francs'); QUERY PLAN------------------------------------------------------------------------------------- Bitmap Heap Scan on test_search (cost=18.39..128.38 rows=50 width=36) (actual time=0.071..0.071 rows=1 loops=1) Recheck Cond: (to_tsvector('english'::regconfig, name) @@ '''1'' &amp; ''franc'''::tsquery) Heap Blocks: exact=1 -&gt; Bitmap Index Scan on idx_gin_search (cost=0.00..18.38 rows=50 width=0) (actual time=0.064..0.064 rows=1 loops=1) Index Cond: (to_tsvector('english'::regconfig, name) @@ '''1'' &amp; ''franc'''::tsquery) Planning time: 0.122 ms Execution time: 0.104 ms(7 rows) 创建索引后，以上查询走了索引并且执行时间下降到0.104毫秒，性能提升了3个数量级，值得一提的是如果SQL改成以下，则不走索引，如下所示：12345678910mydb=&gt; EXPLAIN ANALYZE SELECT * FROM test_search WHERE to_tsvector(name) @@ to_tsquery('1_francs'); QUERY PLAN------------------------------------------------------------------------------------- Seq Scan on test_search (cost=0.00..1037730.00 rows=50 width=18) (actual time=0.036..10297.764 rows=1 loops=1) Filter: (to_tsvector(name) @@ to_tsquery('1_francs'::text)) Rows Removed by Filter: 1999999 Planning time: 0.098 ms Execution time: 10297.787 ms(5 rows) 由于创建索引时使用的是to_tsvector(‘english’,name)函数索引，带了两个参数，因此where条件中的to_tsvector函数带两个参数才能走索引，而to_tsvector(name)不走索引。 JSON、JSONB全文检索实践在PostgreSQL10版本之前全文检索不支持json和jsonb数据类型，10版本的一个重要特性是全文检索支持json和jsonb数据类型，这一小节演示下10版本的这个新特性。 PostgreSQL 10版本与9.6版本to_tsvector函数的差异先来看下9.6版本to_tsvector函数，如下：123456789101112[postgres@pghost1 ~]$ psql francs francspsql (9.6.3)Type \"help\" for help.mydb=&gt; \\df *to_tsvector* List of functions Schema | Name | Result data type | Argument data types | Type------------+-------------------+------------------+---------------------+-------- pg_catalog | array_to_tsvector | tsvector | text[] | normal pg_catalog | to_tsvector | tsvector | regconfig, text | normal pg_catalog | to_tsvector | tsvector | text | normal(3 rows) 从以上看出9.6版本to_tsvector函数的输入参数仅支持text、text[]数据类型，接着看下10版本的to_tsvector函数，如下所示： 123456789101112131415[postgres@pghost1 ~]$ psql mydb pguserpsql (10.0)Type \"help\" for help.mydb=&gt; \\df *to_tsvector* List of functions Schema | Name | Result data type | Argument data types | Type------------+-------------------+------------------+---------------------+-------- pg_catalog | array_to_tsvector | tsvector | text[] | normal pg_catalog | to_tsvector | tsvector | json | normal pg_catalog | to_tsvector | tsvector | jsonb | normal pg_catalog | to_tsvector | tsvector | regconfig, json | normal pg_catalog | to_tsvector | tsvector | regconfig, jsonb | normal pg_catalog | to_tsvector | tsvector | regconfig, text | normal pg_catalog | to_tsvector | tsvector | text | normal(7 rows) 从以上看出，10版本的to_tsvector函数支持的数据类型增加了json和jsonb。 创建数据生成函数为了便于生成测试数据，创建以下两个函数用来随机生成指定长度的字符串，创建random_range(int4, int4)函数如下：123456CREATE OR REPLACE FUNCTION random_range(int4, int4)RETURNS int4LANGUAGE SQLAS $$ SELECT ($1 + FLOOR(($2 - $1 + 1) * random() ))::int4;$$; 接着创建random_text_simple(length int4)函数，此函数会调用random_range(int4, int4)函数。12345678910111213141516171819CREATE OR REPLACE FUNCTION random_text_simple(length int4)RETURNS textLANGUAGE PLPGSQLAS $$DECLARE possible_chars text := '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'; output text := ''; i int4; pos int4;BEGIN FOR i IN 1..length LOOP pos := random_range(1, length(possible_chars)); output := output || substr(possible_chars, pos, 1); END LOOP; RETURN output;END;$$; random_text_simple(length int4)函数可以随机生成指定长度字符串，如下随机生成含三位字符的字符串。12345mydb=&gt; SELECT random_text_simple(3); random_text_simple-------------------- LL9(1 row) 随机生成含六位字符的字符串，如下所示：12345mydb=&gt; SELECT random_text_simple(6); random_text_simple-------------------- B81BPW(1 row) 后面会用到这个函数生成测试数据。 创建JSON测试表创建user_ini测试表，并通过random_text_simple(length int4)函数插入100万随机生成六位字符的字符串测试数据，如下所示：123456789mydb=&gt; CREATE TABLE user_ini(id int4 ,user_id int8,user_name character varying(64),create_time timestamp(6) with time zone default clock_timestamp());CREATE TABLEmydb=&gt; INSERT INTO user_ini(id,user_id,user_name)SELECT r,round(random()*1000000), random_text_simple(6)FROM generate_series(1,1000000) as r;INSERT 0 1000000 创建tbl_user_search_json表，并通过row_to_json函数将表user_ini行数据转换成json数据，如下所示：123456mydb=&gt; CREATE TABLE tbl_user_search_json(id serial, user_info json);CREATE TABLEmydb=&gt; INSERT INTO tbl_user_search_json(user_info) SELECT row_to_json(user_ini) FROM user_ini;INSERT 0 1000000 生成的数据如下：12345mydb=&gt; SELECT * FROM tbl_user_search_json LIMIT 1; id | user_info----+----------------------------------------------------------------------------------------------- 1 | &#123;\"id\":1,\"user_id\":186536,\"user_name\":\"KTU89H\",\"create_time\":\"2017-08-05T15:59:25.359148+08:00\"&#125;(1 row) JSON数据全文检索测试使用全文检索查询表tbl_user_search_json的user_info字段中包含KTU89H字符的记录，如下所示：123456mydb=&gt; SELECT * FROM tbl_user_search_jsonWHERE to_tsvector('english',user_info) @@ to_tsquery('ENGLISH','KTU89H'); id | user_info----+---------------------------------------------------------------------------------------- 1 | &#123;\"id\":1,\"user_id\":186536,\"user_name\":\"KTU89H\",\"create_time\":\"2017-08-05T15:59:25.359148+08:00\"&#125;(1 row) 以上SQL能正常执行说明全文检索支持json数据类型，只是上述SQL走了全表扫描性能低，执行时间为8061毫秒，如下所示：12345678910mydb=&gt; EXPLAIN ANALYZE SELECT * FROM tbl_user_search_json WHERE to_tsvector('english',user_info) @@ to_tsquery('ENGLISH','KTU89H'); QUERY PLAN----------------------------------------------------------------------------------- Seq Scan on tbl_user_search_json (cost=0.00..279513.00 rows=5000 width=104) (actual time=0.046..8061.858 rows=1 loops=1) Filter: (to_tsvector('english'::regconfig, user_info) @@ '''ktu89h'''::tsquery) Rows Removed by Filter: 999999 Planning time: 0.091 ms Execution time: 8061.880 ms(5 rows) 创建如下索引：123mydb=&gt; CREATE INDEX idx_gin_search_json ON tbl_user_search_json USINGgin(to_tsvector('english',user_info)); CREATE INDEX 索引创建后，再次执行以下SQL，如下所示：1234567891011mydb=&gt; EXPLAIN ANALYZE SELECT * FROM tbl_user_search_json WHERE to_tsvector('english',user_info) @@ to_tsquery('ENGLISH','KTU89H'); QUERY PLAN------------------------------------------------------------------------------------- Bitmap Heap Scan on tbl_user_search_json (cost=50.75..7876.06 rows=5000 width=104) (actual time=0.024..0.024 rows=1 loops=1) Recheck Cond: (to_tsvector('english'::regconfig, user_info) @@ '''ktu89h'''::tsquery) Heap Blocks: exact=1 -&gt; Bitmap Index Scan on idx_gin_search_json (cost=0.00..49.50 rows=5000 width=0) (actual time=0.018..0.018 rows=1 loops=1) Index Cond: (to_tsvector('english'::regconfig, user_info) @@ '''ktu89h'''::tsquery) Planning time: 0.113 ms Execution time: 0.057 ms(7 rows) 从上述执行计划看出走了索引，并且执行时间降为0.057毫秒，性能非常不错。这一小节前一部分对PostgreSQL全文检索的实现做了简单介绍，并且给出了一个英文检索的例子，后一部分通过示例介绍了PostgreSQL10的一个新特性，即全文检索支持json、jsonb类型。 五、总结本文介绍了PostgreSQL的NoSQL特性，首先介绍了json和jsonb数据类型，之后通过示例对比json、jsonb数据类型读写性能差异，最后介绍了PostgreSQL全文检索对json、jsonb类型的支持（PostgreSQL10新特性）；值得一提的是，PostgreSQL对中文全文检索也是支持的，有兴趣的读者可自行测试。 六、参考 PostgreSQL用户应掌握的高级SQL特性","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL用户应掌握的高级SQL特性","slug":"20181214145600","date":"2018-12-14T06:56:20.000Z","updated":"2018-12-20T00:27:43.574Z","comments":true,"path":"20181214145600.html","link":"","permalink":"https://postgres.fun/20181214145600.html","excerpt":"","text":"引言PostgreSQL数据库在SQL和NoSQL方面具有很多丰富的特性，本文将从 《PostgreSQL实战》 一书的“ 第4章 SQL高级特性”中摘选部分内容介绍。 这一部分主要介绍PostgreSQL在SQL方面的高级特性，例如WITH查询、批量插入、RETURNING返回修改的数据、UPSERT、数据抽样、聚合函数、窗口函数。 WITH查询WITH查询是PostgreSQL支持的高级SQL特性之一，这一特性常称为CTE(Common Table Expressions)，WITH查询在复杂查询中定义一个辅助语句（可理解成在一个查询中定义的临时表），这一特性常用于复杂查询或递归查询应用场景 先通过一个简单的CTE示例了解WITH查询，如下所示：1234WITH t as ( SELECT generate_series(1,3))SELECT * FROM t; 执行结果如下：123456generate_series ----------------- 1 2 3(3 rows) 这个简单的CTE示例中，一开始定义了一条辅助语句t取数，之后在主查询语句中查询t，定义的辅助语句就像是定义了一张临时表，对于复杂查询如果不使用CTE，可以通过创建视图方式简化SQL。 WITH查询的一个重要属性是RECURSIVE，使用RECURSIVE属性可以引用自己的输出，从而实现递归，一般用于层次结构或树状结构的应用场景。 例如，存在一张包含如下数据的表。123456789id name fatherid1 中国 02 辽宁 13 山东 14 沈阳 25 大连 26 济南 37 和平区 48 沈河区 4 使用PostgreSQL的WITH查询检索ID为7以及以上的所有父节点，如下：123456WITH RECURSIVE r AS ( SELECT * FROM test_area WHERE id = 7 UNION ALL SELECT test_area.* FROM test_area, r WHERE test_area.id = r.fatherid ) SELECT * FROM r ORDER BY id; 查询结果如下：1234567id | name | fatherid ----+--------+---------- 1 | 中国 | 0 2 | 辽宁 | 1 4 | 沈阳 | 2 7 | 和平区 | 4(4 rows) 批量插入批量插入是指一次性插入多条数据，主要用于提升数据插入效率，PostgreSQL有多种方法实现批量插入。 方式一：INSERT INTO..SELECT.过表数据或函数批量插入，这种方式大部分关系数据库都支持，语法如下：1INSERT INTO table_name SELECT …FROM source_table 方式二：INSERT INTO VALUES (),(),…()这种批量插入方式为一条INSERT语句中通过VALUES关键字插入多条记录，通过一个例子就很容易理解，如下所示：12345mydb=&gt; CREATE TABLE tbl_batch3(id int4,info text);CREATE TABLEmydb=&gt; INSERT INTO tbl_batch3(id,info) VALUES (1,'a'),(2,'b'),(3,'c');INSERT 0 3 数据如下：1234567mydb=&gt; SELECT * FROM tbl_batch3; id | info ----+------ 1 | a 2 | b 3 | c(3 rows) 这种批量插入方式非常独特，一条SQL插入多行数据，相比一条SQL插入一条数据的方式能减少和数据库的交互，减少数据库WAL（Write-Ahead Logging）日志的生成，提升插入效率，通常很少有开发人员了解PostgreSQL的这种批量插入方式。 方式三：COPY或\\COPY元命令COPY或\\COPY元命令能够将一定格式的文件数据导入到数据库中，相比INSERT命令插入效率更高，通常大数据量的文件导入一般在数据库服务端主机通过PostgreSQL超级用户使用COPY命令导入。 将文件tbl_batch4.txt的一千万数据导入到表中，如下所示：12345mydb=# TRUNCATE TABLE pguser.tbl_batch4;TRUNCATE TABLEmydb=# COPY pguser.tbl_batch4 FROM '/home/pg10/tbl_batch4.txt';COPY 10000000 RETURNING返回修改的数据PostgreSQL的RETURNING特性可以返回DML修改的数据，具体为以下三个场景 ，INSERT语句后接RETURNING属性返回插入的数据，UPDATE语句后接RETURNING属性返回更新后的新值，DELETE语句后接RETURNING属性返回删除的数据，这个特性的优点在于不需要额外的SQL获取这些值，能够方便应用开发，接着通过示例演示。 RETURNING返回插入的数据INSERT语句后接RETURNING属性返回插入的值，以下创建测试表，并返回已插入的整行数据。123456789mydb=&gt; CREATE TABLE test_r1(id serial,flag char(1));CREATE TABLEmydb=&gt; INSERT INTO test_r1(flag) VALUES ('a') RETURNING *; id | flag ----+------ 1 | a(1 row)INSERT 0 1 RETURNING返回更新后数据UPDATE后接RETURNING属性返回UPDATE语句更新后的值，如下所示：123456789101112mydb=&gt; SELECT * FROM test_r1 WHERE id=1; id | flag ----+------ 1 | a(1 row)mydb=&gt; UPDATE test_r1 SET flag='p' WHERE id=1 RETURNING *; id | flag ----+------ 1 | p(1 row)UPDATE 1 RETURNING返回删除的数据DELETE后接RETURNING属性返回删除的数据，如下所示：123456mydb=&gt; DELETE FROM test_r1 WHERE id=2 RETURNING *; id | flag ----+------ 2 | b(1 row)DELETE 1 UPSERTPostgreSQL的UPSERT特性是指 INSERT ... ON CONFLICT UPDATE，用来解决在数据插入过程中数据冲突的情况，比如违反用户自定义约束，日志数据应用场景通常在事务中批量插入日志数据，如果其中有一条数据违反表上的约束，则整个插入事务将会回滚，PostgreSQL的UPSERT特性可解决这一问题。 接下来通过例子来理解UPSERT的功能，定义一张用户登录日志表并插入一条数据，如下， 1234567mydb=&gt; CREATE TABLE user_logins(user_name text primary key,login_cnt int4,last_login_time timestamp(0) without time zone);CREATE TABLEmydb=&gt; INSERT INTO user_logins(user_name,login_cnt) VALUES ('francs',1);INSERT 0 1 在user_logins表user_name字段上定义主键，批量插入数据中如有重复会报错，如下所示：1234mydb=&gt; INSERT INTO user_logins(user_name,login_cnt) VALUES ('matiler',1),('francs',1);ERROR: duplicate key value violates unique constraint \"user_logins_pkey\"DETAIL: Key (user_name)=(francs) already exists. 上述SQL试图插入两条数据，其中matiler这条数据不违反主键冲突，而francs这条数据违反主键冲突，结果两条数据都不能插入。PostgreSQL的UPSERT可以处理冲突的数据，比如当插入的数据冲突时不报错，同时更新冲突的数据，如下所示：123456mydb=&gt; INSERT INTO user_logins(user_name,login_cnt) VALUES ('matiler',1),('francs',1)ON CONFLICT(user_name) DO UPDATE SET login_cnt=user_logins.login_cnt+EXCLUDED.login_cnt,last_login_time=now();INSERT 0 2 上述INSERT语句插入两条数据，并设置规则：当数据冲突时更新登录次数字段login_cnt值加1，同时更新最近登录时间last_login_time，ON CONFLICT(user_name)定义冲突类型为user_name字段，DO UPDATE SET是指冲突动作，后面定义了一个UPDATE语句，注意上述SET命令中引用了user_loins表和内置表EXCLUDED，引用原表user_loins访问表中已存在的冲突记录，内置表EXCLUDED引用试图插入的值，再次查询表user_login，如下所示：123456mydb=&gt; SELECT * FROM user_logins ; user_name | login_cnt | last_login_time -----------+-----------+--------------------- matiler | 1 | francs | 2 | 2017-08-08 15:23:13(2 rows) 一方面冲突的francs这条数据被更新了login_cnt和last_login_time字段，另一方面新的数据matiler记录已正常插入。 数据抽样数据抽样（TABLESAMPLE）在数据处理方面经常用到，特别是当表数据量比较大时，随机查询表一定数量记录很常见，PostgreSQL早在9.5版时就已经提供了TABLESAMPLE数据抽样功能，9.5版前通常通过ORDER BY random()方式实现数据抽样，这种方式虽然在功能上满足随机返回指定行数据，但性能很低，如下：1234567891011mydb=&gt; EXPLAIN ANALYZE SELECT * FROM user_ini ORDER BY random() LIMIT 1; QUERY PLAN ---------------------------------------------------------------------------------- Limit (cost=25599.98..25599.98 rows=1 width=35) (actual time=367.867..367.868 rows=1 loops=1) -&gt; Sort (cost=25599.98..28175.12 rows=1030056 width=35) (actual time=367.866..367.866 rows=1 loops=1) Sort Key: (random()) Sort Method: top-N heapsort Memory: 25kB -&gt; Seq Scan on user_ini (cost=0.00..20449.70 rows=1030056 width=35) (actual time=0.012..159.569 rows=1000000 loops=1) Planning time: 0.083 ms Execution time: 367.909 ms(7 rows) 表user_ini数据量为100万，从100万随机取一条上述SQL执行时间为367ms，这种方法走了全表扫描和排序，效率非常低，当表数据量大时，性能几乎无法接受。 9.5版本以后PostgreSQL支持TABLESAMPLE数据抽样，语法为以下：123SELECT …FROM table_nameTABLESAMPLE sampling_method ( argument [, ...] ) [ REPEATABLE ( seed ) ] sampling_method指抽样方法，主要有两种：SYSTEM和BERNOULLI，接下来详细介绍这两种抽样方式，argument指抽样百分比。 SYSTEM抽样方式SYSTEM抽样方式为随机抽取表上数据块上的数据，理论上被抽样表的每个数据块被检索的概率是一样的，SYSTEM抽样方式基于数据块级别，后接抽样参数，被选中的块上的所有数据将被检索。 创建test_sample测试表，并插入150万数据，抽样因子设置成0.01，意味着返回1500000*0.01%=150条记录，执行如下SQL。 12345678mydb=&gt; EXPLAIN ANALYZE SELECT * FROM test_sample TABLESAMPLE SYSTEM(0.01); QUERY PLAN ---------------------------------------------------------------------------------- Sample Scan on test_sample (cost=0.00..3.50 rows=150 width=45) (actual time=0.099..0.146 rows=107 loops=1) Sampling: system ('0.01'::real) Planning time: 0.053 ms Execution time: 0.166 ms(4 rows) 以上执行计划主要有两点，一方面走了Sample Scan扫描(抽样方式为SYSTEM)，执行时间为0.166毫秒，性能较好，另一方面优化器预计访问150条记录，实际返回107条。 BERNOULLI抽样方式BERNOULLI抽样方式随机抽取表的数据行，并返回指定百分比数据，BERNOULLI抽样方式基于数据行级别，理论上被抽样表的每行记录被检索的概率是一样的，因此BERNOULLI抽样方式抽取的数据相比SYSTEM抽样方式具有更好的随机性，但性能上相比SYSTEM抽样方式低很多，下面演示下BERNOULLI抽样方式，同样基于test_sample测试表。 设置抽样方式为BERNOULLI，抽样因子为0.01，如下所示。12345678mydb=&gt; EXPLAIN ANALYZE SELECT * FROM test_sample TABLESAMPLE BERNOULLI (0.01); QUERY PLAN ---------------------------------------------------------------------------------- Sample Scan on test_sample (cost=0.00..14020.50 rows=150 width=45) (actual time=0.025..22.541 rows=152 loops=1) Sampling: bernoulli ('0.01'::real) Planning time: 0.063 ms Execution time: 22.569 ms(4 rows) 从以上执行计划看出走了Sample Scan扫描（抽样方式为BERNOULLI），执行计划预计返回150条记录，实际返回152条，从返回的记录数来看，非常接近150条（1000000*0.01%），但执行时间却要22.569毫秒，性能相比SYSTEM抽样方式0.166毫秒差了136倍。 多次执行以下查询，查看返回记录数的变化，如下所示：1234567891011mydb=&gt; SELECT count(*) FROM test_sample TABLESAMPLE BERNOULLI(0.01); count ------- 151(1 row)mydb=&gt; SELECT count(*) FROM test_sample TABLESAMPLE BERNOULLI(0.01); count ------- 147(1 row) 从以上看出，BERNOULLI抽样方式返回的数据量非常接近抽样数据的百分比，而SYSTEM抽样方式数据返回以数据块为单位，被抽样的块上的所有数据都被返回，因此SYSTEM抽样方式的数据量返回的偏差较大。 这里演示了SYSTEM和BERNOULLI抽样方式，SYSTEM抽样方式基于数据块级别，随机抽取表数据块上的记录，因此这种方式抽取的记录的随机性不是很好，但返回的数据以数据块为单位，抽样性能很高，适用于抽样效率优先的场景，例如抽样大小为GB的日志表；而BERNOULLI抽样方式基于数据行，相比SYSTEM抽样方式所抽样的数据随机性更好，但性能相比SYSTEM差很多，适用于抽样随机性优先的场景，读者可根据实际应用场景选择抽样方式。 聚合函数聚合函数可以对结果集进行计算，常用的聚合函数有avg()、sum()、min()、max()、count()等，本节将介绍PostgreSQL两个特殊功能的聚合函数并给出测试示例。 在介绍两个聚合函数之前，先来看一个应用场景，假如一张表有以下数据，如下：12345678country | city ---------+------ 中国 | 台北 中国 | 香港 中国 | 上海 日本 | 东京 日本 | 大阪(5 rows) 要求得到如下结果集：12中国 台北，香港，上海 日本 东京，大阪 这个SQL读者想想如何写？ string_agg函数首先介绍string_agg函数，此函数语法如下：1string_agg(expression, delimiter) 简单的说string_agg函数能将结果集某个字段的所有行连接成字符串，并用指定delimiter分隔符分隔，expression表示要处理的字符类型数据；参数的类型为(text, text) 或 (bytea, bytea)，函数返回的类型同输入参数类型一致，bytea属于二进制类型，使用情况不多，我们主要介绍text类型输入参数，本节开头的场景正好可以用string_agg函数处理。 将city字段连接成字符串如下：12345mydb=&gt; SELECT string_agg(city,',') FROM city; string_agg -------------------------- 台北,香港,上海,东京,大阪(1 row) 可见string_agg函数将输出的结果集连接成了字符串，并用指定的逗号分隔符分隔，回到本文开头的问题，通过以下SQL实现，如下所示：12345mydb=&gt; SELECT country,string_agg(city,',') FROM city GROUP BY country; country | string_agg ---------+---------------- 日本 | 东京,大阪 中国 | 台北,香港,上海 array_agg函数array_agg函数和string_agg函数类似，最主要的区别为返回的类型为数组，数组数据类型同输入参数数据类型一致，array_agg函数支持两种语法，第一种如下：array_agg(expression) –输入参数为任何非数组类型 输入参数可以是任何非数组类型，返回的结果是一维数组，array_agg函数将结果集某个字段的所有行连接成数组，执行以下查询。12345mydb=&gt; SELECT country,array_agg(city) FROM city GROUP BY country; country | array_agg ---------+------------------ 日本 | &#123;东京,大阪&#125; 中国 | &#123;台北,香港,上海&#125; array_agg函数输出的结果为字符类型数组，其他无明显区别，使用array_agg函数主要优点在于可以使用数组相关函数和操作符。 窗口函数PostgreSQL提供内置的窗口函数，例如row_num()、rank()、lag()等，除了内置的窗口函数外，聚合函数、自定义函数后接OVER属性也可作为窗口函数。 窗口函数的调用语法稍复杂，如下所示：1function_name ([expression [, expression ... ]]) [ FILTER ( WHERE filter_clause ) ] OVER ( window_definition ) 其中window_definition语法如下：1234[ existing_window_name ][ PARTITION BY expression [, ...] ][ ORDER BY expression [ ASC | DESC | USING operator ] [ NULLS &#123; FIRST | LAST &#125; ] [, ...] ][ frame_clause ] OVER表示窗口函数的关键字。 PARTITON BY 属性对查询返回的结果集进行分组，之后窗口函数处理分组的数据。 ORDER BY 属性设定结果集的分组数据的排序。 row_number() 窗口函数创建一张成绩表并插入测试数据，如下所示：1234567891011121314CREATE TABLE score ( id serial primary key, subject character varying(32), stu_name character varying(32), score numeric(3,0) );INSERT INTO score ( subject,stu_name,score ) VALUES ('Chinese','francs',70);INSERT INTO score ( subject,stu_name,score ) VALUES ('Chinese','matiler',70);INSERT INTO score ( subject,stu_name,score) VALUES ('Chinese','tutu',80);INSERT INTO score ( subject,stu_name,score ) VALUES ('English','matiler',75);INSERT INTO score ( subject,stu_name,score ) VALUES ('English','francs',90);INSERT INTO score ( subject,stu_name,score ) VALUES ('English','tutu',60);INSERT INTO score ( subject,stu_name,score ) VALUES ('Math','francs',80);INSERT INTO score ( subject,stu_name,score ) VALUES ('Math','matiler',99);INSERT INTO score ( subject,stu_name,score ) VALUES ('Math','tutu',65); row_number()窗口函数对结果集分组后的数据标注行号，从1开始，如下。12345678910111213mydb=&gt; SELECT row_number() OVER (partition by subject ORDER BY score desc),* FROM score; row_number | id | subject | stu_name | score------------+----+---------+----------+------- 1 | 3 | Chinese | tutu | 80 2 | 1 | Chinese | francs | 70 3 | 2 | Chinese | matiler | 70 1 | 5 | English | francs | 90 2 | 4 | English | matiler | 75 3 | 6 | English | tutu | 60 1 | 8 | Math | matiler | 99 2 | 7 | Math | francs | 80 3 | 9 | Math | tutu | 65(9 rows) 以上row_number()窗口函数显示的是分组后记录的行号，如果不指定partition属性，row_number()窗口函数显示表所有记录的行号，类似oracle里的ROWNUM，如下。 12345678910111213mydb=&gt; SELECT row_number() OVER (ORDER BY id) AS rownum ,* FROM score; rownum | id | subject | stu_name | score--------+----+---------+----------+------- 1 | 1 | Chinese | francs | 70 2 | 2 | Chinese | matiler | 70 3 | 3 | Chinese | tutu | 80 4 | 4 | English | matiler | 75 5 | 5 | English | francs | 90 6 | 6 | English | tutu | 60 7 | 7 | Math | francs | 80 8 | 8 | Math | matiler | 99 9 | 9 | Math | tutu | 65(9 rows) avg() OVER()窗口函数聚合函数后接OVER属性的窗口函数表示在一个查询结果集上应用聚合函数，本小节将演示avg()聚合函数后接OVER属性的窗口函数，此窗口函数用来计算分组后数据的平均值。 查询每名学生学习成绩并且显示课程的平均分，通常是先计算出课程的平均分，之后score表再与平均分表关联查询，如下所示：12345678910111213141516mydb=&gt; SELECT s.subject, s.stu_name,s.score, tmp.avgscore FROM score s LEFT JOIN (SELECT subject, avg(score) avgscore FROM score GROUP BY subject) tmp ON s.subject = tmp.subject; subject | stu_name | score | avgscore ---------+----------+-------+--------------------- Chinese | francs | 70 | 73.3333333333333333 Chinese | matiler | 70 | 73.3333333333333333 Chinese | tutu | 80 | 73.3333333333333333 English | matiler | 75 | 75.0000000000000000 English | francs | 90 | 75.0000000000000000 English | tutu | 60 | 75.0000000000000000 Math | francs | 80 | 81.3333333333333333 Math | matiler | 99 | 81.3333333333333333 Math | tutu | 65 | 81.3333333333333333(9 rows) 使用窗口函数很容易实现以上需求，如下所示：12345678910111213mydb=&gt; SELECT subject,stu_name, score, avg(score) OVER(PARTITION BY subject) FROM score; subject | stu_name | score | avg ---------+----------+-------+--------------------- Chinese | francs | 70 | 73.3333333333333333 Chinese | matiler | 70 | 73.3333333333333333 Chinese | tutu | 80 | 73.3333333333333333 English | matiler | 75 | 75.0000000000000000 English | francs | 90 | 75.0000000000000000 English | tutu | 60 | 75.0000000000000000 Math | francs | 80 | 81.3333333333333333 Math | matiler | 99 | 81.3333333333333333 Math | tutu | 65 | 81.3333333333333333(9 rows) 以上查询前三列来源于表score，第四列表示取课程的平均分，PARTITION BY subject表示根据字段subject进行分组。 rank()窗口函数rank()窗口函数和row_number()窗口函数相似，主要区别为当组内某行字段值相同时，行号重复并且行号产生间隙（手册上解释为gaps），如下：12345678910111213mydb=&gt; SELECT rank() OVER(PARTITION BY subject ORDER BY score),* FROM score; rank | id | subject | stu_name | score ------+----+---------+----------+------- 1 | 2 | Chinese | matiler | 70 1 | 1 | Chinese | francs | 70 3 | 3 | Chinese | tutu | 80 1 | 6 | English | tutu | 60 2 | 4 | English | matiler | 75 3 | 5 | English | francs | 90 1 | 9 | Math | tutu | 65 2 | 7 | Math | francs | 80 3 | 8 | Math | matiler | 99(9 rows) 以上示例中，Chinese课程前两条记录的score字段值都为70，因此前两行的rank字段值1，而第三行的rank字段值为3，产生了间隙。 dense_rank ()窗口函数dense_rank ()窗口函数和rank ()窗口函数相似，主要区别为当组内某行字段值相同时，虽然行号重复，但行号不产生间隙（手册上解释为gaps），如下：12345678910111213mydb=&gt; SELECT dense_rank() OVER(PARTITION BY subject ORDER BY score),* FROM score; dense_rank | id | subject | stu_name | score ------------+----+---------+----------+------- 1 | 2 | Chinese | matiler | 70 1 | 1 | Chinese | francs | 70 2 | 3 | Chinese | tutu | 80 1 | 6 | English | tutu | 60 2 | 4 | English | matiler | 75 3 | 5 | English | francs | 90 1 | 9 | Math | tutu | 65 2 | 7 | Math | francs | 80 3 | 8 | Math | matiler | 99(9 rows) 以上示例中，Chinese课程前两行的rank字段值1，而第三行的rank字段值为2，没有产生间隙。 PostgreSQL还支持很多其它内置窗口函数，例如、lag()、first_values()、last_values()等，篇幅关系不再介绍。 总结本篇文章主要介绍了PostgreSQL支持的一些高级SQL特性，例如WITH查询、批量插入、RETURNING返回DML修改的数据、UPSERT、数据抽样、聚合函数、窗口函数，了解这些功能能够简化SQL代码，提升开发效率，并且实现普通查询不容易实现的功能，希望通过阅读本章读者能够在实际工作中应用SQL高级特性，同时挖掘PostgreSQL的其他高级SQL特性。 PosgreSQL不仅是关系型数据库，同时支持NoSQL特性，关于PostgreSQL的NoSQL特性将在下篇文章中介绍。 参考 PostgreSQL何以支持丰富的NoSQL特性?","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Hexo: 推荐一个网站访问速度测试工具","slug":"20181108144300","date":"2018-11-08T06:34:25.000Z","updated":"2018-11-08T06:41:31.398Z","comments":true,"path":"20181108144300.html","link":"","permalink":"https://postgres.fun/20181108144300.html","excerpt":"","text":"经同事推荐一个不错的网站速度测试工具，地址: http://tool.chinaz.com/speedtest 输入网站链接，即可返回不同地域访问网站的速度情况。 国内测速输入博客地址，返回下图: 从上图看出一片绿色，大部分地区访问博客时间在1秒以下，访问速度还不错！","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/tags/Hexo/"}]},{"title":"Hexo: 给每篇文章添加宣传栏","slug":"20181107111000","date":"2018-11-07T03:10:53.000Z","updated":"2018-11-07T03:22:26.178Z","comments":true,"path":"20181107111000.html","link":"","permalink":"https://postgres.fun/20181107111000.html","excerpt":"","text":"最近琢磨如何在Hexo的每一篇博客底部添加宣传栏，用于推广，一时半会没找到，咨询了博採眾長(Hexo高手)，可以通过修改打赏文件添加宣传信息，也就是说在打赏模块嵌入宣传栏。 Hexo部署博客最好要会一点前端，但这方面几乎不会，组内有个妹子会前端开发，在她的帮助下，终于实现了这个功能，对二位的帮助再次表示感谢！ 修改 reward.swig打赏模块对应的文件为 /d/hexo/themes/next/layout/_macro/reward.swig ，修改此文件添加5行代码，用于添加宣传栏功能，修改前记得做好文件备份，如下: 1234567891011121314151617181920212223242526272829303132333435363738&lt;div style=\"padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;\"&gt;# 以下5行新增&lt;div&gt;&lt;p style=\"text-align:left\"&gt;最后推荐和张文升共同编写的《PostgreSQL实战》，本书基于PostgreSQL 10 编写，共18章，重点介绍SQL高级特性、并行查询、分区表、物理复制、逻辑复制、备份恢复、高可用、性能优化、PostGIS等，涵盖大量实战用例！&lt;/p&gt;&lt;p style=\"text-align:left\"&gt;购买链接：&lt;a href=\"https://item.jd.com/12405774.html\"&gt;https://item.jd.com/12405774.html&lt;/a&gt;&lt;/p&gt;&lt;img src=\"/images/PostgreSQL实战_small.png\" alt=\"PostgreSQL实战\" style=\"margin: 0 auto;\" /&gt;&lt;/div&gt;&lt;div&gt;&#123;&#123; theme.reward_comment &#125;&#125;&lt;/div&gt; &lt;button id=\"rewardButton\" disable=\"enable\" onclick=\"var qr = document.getElementById('QR'); if (qr.style.display === 'none') &#123;qr.style.display='block';&#125; else &#123;qr.style.display='none'&#125;\"&gt; &lt;span&gt;&#123;&#123; __('reward.donate') &#125;&#125;&lt;/span&gt; &lt;/button&gt; &lt;div id=\"QR\" style=\"display: none;\"&gt; &#123;% if theme.wechatpay %&#125; &lt;div id=\"wechat\" style=\"display: inline-block\"&gt; &lt;img id=\"wechat_qr\" src=\"&#123;&#123; theme.wechatpay &#125;&#125;\" alt=\"&#123;&#123; theme.author &#125;&#125; &#123;&#123; __('reward.wechatpay') &#125;&#125;\"/&gt; &lt;p&gt;&#123;&#123; __('reward.wechatpay') &#125;&#125;&lt;/p&gt; &lt;/div&gt; &#123;% endif %&#125; &#123;% if theme.alipay %&#125; &lt;div id=\"alipay\" style=\"display: inline-block\"&gt; &lt;img id=\"alipay_qr\" src=\"&#123;&#123; theme.alipay &#125;&#125;\" alt=\"&#123;&#123; theme.author &#125;&#125; &#123;&#123; __('reward.alipay') &#125;&#125;\"/&gt; &lt;p&gt;&#123;&#123; __('reward.alipay') &#125;&#125;&lt;/p&gt; &lt;/div&gt; &#123;% endif %&#125; &#123;% if theme.bitcoin %&#125; &lt;div id=\"bitcoin\" style=\"display: inline-block\"&gt; &lt;img id=\"bitcoin_qr\" src=\"&#123;&#123; theme.bitcoin &#125;&#125;\" alt=\"&#123;&#123; theme.author &#125;&#125; &#123;&#123; __('reward.bitcoin') &#125;&#125;\"/&gt; &lt;p&gt;&#123;&#123; __('reward.bitcoin') &#125;&#125;&lt;/p&gt; &lt;/div&gt; &#123;% endif %&#125; &lt;/div&gt;&lt;/div&gt; 修改完后需重启博客生效。 效果演示查看本文底部的书籍宣传信息查看效果，达到了预期效果! 设置指定文章不打赏如何设置指定文章不打赏呢？Hexo 官方不提供是否打赏参数，可以通过修改 post.swig 源码实现此功能，详见 如何设置指定文章不打赏？ 参考 如何设置指定文章不打赏？","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/tags/Hexo/"}]},{"title":"PostgreSQL 11 有哪些引人瞩目的新特性？","slug":"20181102084300","date":"2018-11-02T00:43:12.000Z","updated":"2018-11-05T14:05:40.918Z","comments":true,"path":"20181102084300.html","link":"","permalink":"https://postgres.fun/20181102084300.html","excerpt":"","text":"2018-10-18 PostgreSQL官网 宣布 PostgreSQL 11 正式版发行，PostgreSQL 11 重点对性能进行了提升和功能完善，特别是对大数据库和高计算负载的情况下进行了增强，主要包括以下: 对分区表进行了大幅的改进和增强。 增加了对存储过程的支持，存储过程支持事务。 增强了并行查询能力和并行数据定义能力。 增加了对 just-in-time (JIT) 编译的支持，加速SQL中的表达式执行效率。 最近对PostgreSQL以上新特性和其它功能完善做了演示，希望对PostgreSQL从业者有帮助，详见以下。 分区表的改进PostgreSQL 11 对分区表进行了重大的改进，例如增加了哈希分区、支持创建主键、外键、索引、支持UPDATE分区键以及增加了默认分区，这些功能的完善极大的增强了分区表的可用性，详见以下: PostgreSQL11: 分区表增加哈希分区 PostgreSQL11：分区表支持创建主键、外键、索引 PostgreSQL11: 分区表支持UPDATE分区键 PostgreSQL11: 分区表增加 Default Partition 支持存储过程PostgreSQL 11 版本一个重量级新特性是对存储过程的支持，同时支持存储过程嵌入事务，存储过程是很多 PostgreSQL 从业者期待已久的特性，尤其是很多从Oracle转到PostgreSQL朋友。 尽管PostgreSQL提供函数可以实现大多数存储过程的功能，但函数不支持部分提交，换句话说，函数中的SQL要么都执行成功，要不全部返回失败，详见以下: PostgreSQL11: 支持存储过程(SQL Stored Procedures) 并行能力的增强PostgreSQL 11 版本在并行方面得到较大增强，例如支持并行创建索引、并行Hash Join、并行 CREATE TABLE .. AS等，详见以下: PostgreSQL11：支持并行创建索引(Parallel Index Builds) PostgreSQL11：支持并行哈希连接(Parallel Hash Joins) 增加对Just-in-Time (JIT)编译的支持PostgreSQL 11 版本的一个重量级新特性是引入了 JIT (Just-in-Time) 编译来加速SQL中的表达式计算效率。 JIT 表达式的编译使用LLVM项目编译器来提升在WHERE条件、指定列表、聚合以及一些内部操作表达式的编译执行，详见以下: PostgreSQL11: 增加对JIT(just-in-time)编译的支持提升分析型SQL执行效率 其它功能完善此外， PostgreSQL 11 增强了其它新特性以增加用户体验，以下列举了主要的几点，详见以下: PostgreSQL11: 新增非空默认值字段不需要重写表 PostgreSQL11: Indexs With Include Columns PostgreSQL11: 新增三个默认角色 PostgreSQL11: 可通过GRNAT权限下放的四个系统函数 PostgreSQL11: Initdb/Pg_resetwal支持修改WAL文件大小 PostgreSQL11: psql 新增 \\gdesc 显示查询结果的列名和类型 PostgreSQL11: psql 新增变量记录SQL语句的执行情况和错误 关于PostgreSQLPostgreSQL 号称世界上最先进的开源关系型数据库，PostgreSQL 全球社区是一个由数千名用户、开发人员、公司或其他组织组成。 PostgreSQL 起源于加利福利亚的伯克利大学，有30年以上历史，经历了无数次开发升级。 PostgreSQL 的出众之处在于不仅具有商业数据库的功能特性，同时在扩展性、安全性、稳定性等高级数据库特性方面超越了它们。 若想获取到更多关于PostgreSQL的信息或者加入PostgreSQL社区，请浏览官网 PostgreSQL.org 。 参考 PostgreSQL 11 Released! PostgreSQL 11.0 正式版更新版本发布说明","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL11: 增加对JIT(just-in-time)编译的支持提升分析型SQL执行效率","slug":"20181031161700","date":"2018-10-31T08:17:15.000Z","updated":"2018-11-16T00:20:02.813Z","comments":true,"path":"20181031161700.html","link":"","permalink":"https://postgres.fun/20181031161700.html","excerpt":"","text":"关于 JITPostgreSQL 11 版本的一个重量级新特性是引入了 JIT (Just-in-Time) 编译来加速SQL中的表达式计算效率。 JIT 表达式的编译使用LLVM项目编译器的架构来提升在WHERE条件、指定列表、聚合以及一些内部操作表达式的编译执行。 使用 JIT 必须在首先编译安装 LLVM ，之后编译安装 PostgreSQL 时设置 --with-llvm 选项，本文主要包括两部分，如下: CentOS7 编译安装 LLVM。 CentOS7 编译安装PostgreSQL 11，启用并演示 JIT。 JIT 使用场景JIT 常用于CPU密集型SQL(分析统计SQL)，执行很快的SQL使用JIT由于产生一定开销，反而可能引起性能下降。 手册 Release说明 Add Just-in-Time (JIT) compilation of some parts of query plans to improve execution speed (Andres Freund)This feature requires LLVM to be available. It is not currently enabled by default, even in builds that support it. 安装环境操作系统: CentOS Linux release 7.4.1708 (Core)硬件环境: 8核4G/80G 的云主机 LLVM安装前提条件LLVM 安装依赖较多，如下： 1234The minimum required version of LLVM is currently 3.9 --本实验使用 LLVM 5.0.2CMake. Version 3.4.3 is the minimum required. --本实验使用 Cmake 3.12.3Python 2.7 or newer is required --本实验使用 Python 2.7.9GCC version must be at least 4.8! --本实验使用 gcc 4.8.5 安装 Cmake 3.12.3下载并编译安装 cmake 3.12.3，如下:123456# wget -c https://cmake.org/files/v3.12/cmake-3.12.3.tar.gz# tar xvf cmake-3.12.3.tar.gz# cd cmake-3.12.3# ./bootstrap# make -j 4# make install 安装Python 2.7.9下载并编译安装 python 2.7.9，如下: 123456# wget -c https://www.python.org/downloads/release/python-279/# tar jxvf Python-2.7.9.tgz# cd Python-2.7.9# ./configure# make# make install 安装 LLVM 5.0.2LLVM 的安装步骤较繁琐，并且编译安装过程时间较长，性能好的机器能减少编译时间，注意操作系统需启用 swap，否则编译过程中会报错,本人开始编译安装时没有启用 swap，折腾了很久。 下载LLVM安装介质在 LLVM官网下载安装介质，如下: 1234567llvm-5.0.2.src.tar.xzcfe-5.0.2.src.tar.xzclang-tools-extra-5.0.2.src.tar.xzcompiler-rt-5.0.2.src.tar.xzlibcxx-5.0.2.src.tar.xzlibcxxabi-5.0.2.src.tar.xzlibunwind-5.0.2.src.tar.xz 编译安装LLVM解压 llvm-5.0.2.src.tar.xz123# cd /opt/soft_bak/# tar xvf llvm-5.0.2.src.tar.xz # mv llvm-5.0.2.src llvm 解压安装包并重命名，目录结构对应如下，如下: 安装包 安装目录 llvm-5.0.2.src.tar.xz /opt/soft_bak/llvm cfe-5.0.2.src.tar.xz /opt/soft_bak/tools/clang clang-tools-extra-5.0.2.src.tar.xz /opt/soft_bak/tools/clang/tools/extra compiler-rt-5.0.2.src.tar.xz /opt/soft_bak/projects/compiler-rt libcxx-5.0.2.src.tar.xz /opt/soft_bak/projects/libcxx libcxxabi-5.0.2.src.tar.xz /opt/soft_bak/projects/libcxxabi libunwind-5.0.2.src.tar.xz /opt/soft_bak/projects/libunwind LLVM 官网的其它安装包非必须，可根据情况选择。 编译安装 LLVM，如下:12345# mkdir -p /opt/soft_bak/llvm_build/# cd /opt/soft_bak/llvm_build/# cmake -G \"Unix Makefiles\" -DCMAKE_INSTALL_PREFIX=/usr/local/llvm -DCLANG_DEFAULT_CXX_STDLIB=libc++ -DCMAKE_BUILD_TYPE=\"Release\" /opt/soft_bak/llvm# make -j 4# make install 设置环境变量，如下:12export PATH=$PATH:/usr/local/llvm/binexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/llvm/lib 查看版本123456789101112[root@pghost7 ~]# llvm-cat --versionLLVM (http://llvm.org/): LLVM version 5.0.2 Optimized build. Default target: x86_64-unknown-linux-gnu Host CPU: broadwell[root@pghost7 ~]# clang --versionclang version 5.0.2 (tags/RELEASE_502/final)Target: x86_64-unknown-linux-gnuThread model: posixInstalledDir: /usr/local/llvm/bin 至此 LLVM 已安装成功。 PostgreSQL 11 安装安装相关包，如下:1# yum -y install gcc readline readline-devel zlib zlib-devel python-devel 下载PostgreSQL 11 并编译安装，编译时指定 --with-llvm 选项， 如下: 12345# wget -c https://ftp.postgresql.org/pub/source/v11.0/postgresql-11.0.tar.bz2# tar xvf postgresql-11.0.tar.bz2#./configure --prefix=/opt/pgsql_11.0 --with-wal-blocksize=16 -with-pgport=1930 --with-llvm LLVM_CONFIG='/usr/local/llvm/bin/llvm-config'# make wolrd -j 4# make install-world 设置 .bash_profile ，如下:123456789101112export PGPORT=1930export PGUSER=postgresexport PGDATA=/database/pg11/pg_rootexport LANG=en_US.utf8export PGHOME=/opt/pgsql_11.0export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/libexport DATE=`date +\"%Y%m%d%H%M\"`export PATH=$PGHOME/bin:$PATH:.export MANPATH=$PGHOME/share/man:$MANPATHalias rm='rm -i'alias ll='ls -lh' 使用 initdb 初始化数据库，如下:1[pg11@pghost7 pg_root]$ initdb -D /database/pg11/pg_root -E=UTF8 --locale=C -U postgres -W postgresql.conf 设置以下 JIT 配置参数，其它参数按需配置，这里不贴出，如下: 12345# - Other Defaults -#dynamic_library_path = '$libdir'jit = on # allow JIT compilationjit_provider = 'llvmjit' # JIT implementation to use 设置 pg_hba.conf，如下:1host all all 0.0.0.0/0 md5 之后启动数据库，如下:12345678[pg11@pghost7 pg_root]$ pg_ctl startwaiting for server to start....2018-10-31 11:13:26.154 CST [19742] LOG: listening on IPv4 address \"0.0.0.0\", port 19302018-10-31 11:13:26.154 CST [19742] LOG: listening on IPv6 address \"::\", port 19302018-10-31 11:13:26.159 CST [19742] LOG: listening on Unix socket \"/tmp/.s.PGSQL.1930\"2018-10-31 11:13:26.185 CST [19742] LOG: redirecting log output to logging collector process2018-10-31 11:13:26.185 CST [19742] HINT: Future log output will appear in directory \"log\". doneserver started JIT 测试以下大致演示 JIT，测试样例很简单，不做充分的性能测试，有兴趣的朋友可以做 TPC-H 性能测试。 测试数据准备创建一张5千万的数据表，如下: 12CREATE TABLE t_llvm1(a int4, b int4, info text, ctime timestamp(6) without time zone);INSERT INTO t_llvm1 (a,b,info,ctime) SELECT n,n*2,n||'_llvm1',clock_timestamp() FROM generate_series(1,50000000) n; 查看 JIT 相关参数1234567891011121314postgres=# SELECT name,setting FROM pg_settings WHERE name LIKE 'jit%'; name | setting-------------------------+--------- jit | on jit_above_cost | 100000 jit_debugging_support | off jit_dump_bitcode | off jit_expressions | on jit_inline_above_cost | 500000 jit_optimize_above_cost | 500000 jit_profiling_support | off jit_provider | llvmjit jit_tuple_deforming | on(10 rows) 开启 JIT开启 JIT，执行计划如下: 12345678910111213141516171819202122postgres=# SET JIT = on;SETpostgres=# EXPLAIN ANALYZE SELECT count(*),sum(a) FROM t_llvm1 WHERE (a+b) &gt; 10; QUERY PLAN-------------------------------------------------------------------------------------------------------------------------------------------------- Finalize Aggregate (cost=576982.30..576982.31 rows=1 width=16) (actual time=2148.607..2148.608 rows=1 loops=1) -&gt; Gather (cost=576981.86..576982.28 rows=4 width=16) (actual time=2148.457..2153.185 rows=5 loops=1) Workers Planned: 4 Workers Launched: 4 -&gt; Partial Aggregate (cost=575981.86..575981.88 rows=1 width=16) (actual time=2134.919..2134.919 rows=1 loops=5) -&gt; Parallel Seq Scan on t_llvm1 (cost=0.00..555148.48 rows=4166677 width=4) (actual time=105.597..1516.253 rows=9999999 loops=5) Filter: ((a + b) &gt; 10) Rows Removed by Filter: 1 Planning Time: 0.078 ms JIT: Functions: 28 Options: Inlining true, Optimization true, Expressions true, Deforming true Timing: Generation 5.842 ms, Inlining 226.589 ms, Optimization 191.071 ms, Emission 107.027 ms, Total 530.529 ms Execution Time: 2154.870 ms(14 rows) 从以上看出执行计划中包含 JIT 编译信息，执行时间为 2154 ms 左右。 关闭 JIT关闭 JIT,查看执行计划和扫行时间，如下:123456789101112131415161718postgres=# SET JIT = off;SETpostgres=# EXPLAIN ANALYZE SELECT count(*),sum(a) FROM t_llvm1 WHERE (a+b) &gt; 10; QUERY PLAN------------------------------------------------------------------------------------------------------------------------------------------------ Finalize Aggregate (cost=576982.30..576982.31 rows=1 width=16) (actual time=2382.035..2382.035 rows=1 loops=1) -&gt; Gather (cost=576981.86..576982.28 rows=4 width=16) (actual time=2381.939..2385.143 rows=5 loops=1) Workers Planned: 4 Workers Launched: 4 -&gt; Partial Aggregate (cost=575981.86..575981.88 rows=1 width=16) (actual time=2371.143..2371.143 rows=1 loops=5) -&gt; Parallel Seq Scan on t_llvm1 (cost=0.00..555148.48 rows=4166677 width=4) (actual time=0.560..1600.125 rows=9999999 loops=5) Filter: ((a + b) &gt; 10) Rows Removed by Filter: 1 Planning Time: 0.083 ms Execution Time: 2385.209 ms(10 rows) 从以上看出执行计划中没有包含 JIT 信息，执行时间为 2385 ms 左右，开启JIT性能提升了9.7% 左右。 参考 Chapter 32. Just-in-Time Compilation (JIT) The LLVM Compiler Infrastructure PostgreSQL 已包含对 LLVM JIT 支持的提交性能将飙升 How to compile PostgreSQL 11 with support for JIT compilation on RHEL/CentOS 7 PostgreSQL 11 and Just In Time Compilation of Queries Speeding up query execution in PostgreSQL using LLVM JIT compiler PostgreSQL 11.0 正式版更新版本发布说明","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Hexo: 声明博客的许可协议","slug":"20181028211900","date":"2018-10-28T13:19:09.000Z","updated":"2018-10-28T13:27:17.605Z","comments":true,"path":"20181028211900.html","link":"","permalink":"https://postgres.fun/20181028211900.html","excerpt":"","text":"Hexo 博客可以声明许可协议，在 creativecommons 网站上找适合自己的许可。 常用的许可协议如下: CC BY(署名) CC BY-SA(署名-相同方式共享) CC BY-ND(署名-禁止演绎) CC BY-NC(署名-非商业性使用) CC BY-NC-SA(署名-非商业性使用-相同方式共享 ) CC BY-NC-ND(署名-非商业性使用-禁止演绎) 关于以上协议的解释详见 https://creativecommons.org/licenses/ 。 本博客计划使用 CC BY-SA 协议。 关于CC BY-SA协议 You are free to: Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.ShareAlike — If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. Under the following terms: Attribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.ShareAlike — If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. 简单的说， CC BY-SA 协议允许其它人下载、发行、修改，即使是出于商业目的，但必须给出原创出处并指明原创身份，他人发行此作品采取的许可必须和原创作者采取的许可相同。 设置网站的许可协议修改主题配置文件 /d/hexo/themes/next/_config.yml 如下参数: 12345# Declare license on postspost_copyright: enable: true license: CC BY-SA 4.0 license_url: https://creativecommons.org/licenses/by-sa/4.0/ 之后刷新博客，如下。 12hexo ghexo d 验证之后博客的每篇文章的底部显示许可信息，如下: 参考 creativecommons","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/tags/Hexo/"}]},{"title":"Hexo: 给博客添加百度统计","slug":"20181027203300","date":"2018-10-27T12:33:52.000Z","updated":"2018-10-28T12:21:56.387Z","comments":true,"path":"20181027203300.html","link":"","permalink":"https://postgres.fun/20181027203300.html","excerpt":"","text":"当Hexo博客被百度、必应、谷歌搜索引擎收录以后，有件重要的工作是统计博客的访问情况，比如博客的历史访问量、搜索关键字、访问来源、访问地域等统计数据。 百度统计 能方便的完成网站访问量分析统计，本文简单演示下Hexo+Next博客配置百度统计功能。 开通百度统计帐号在 百度统计 注册帐号。 帐号注册成功后，在网站列表中添加目标网站。 获取跟踪代码网站添加之后在代码管理模块选择代码获取，可以看到如下代码: 123456789&lt;script&gt;var _hmt = _hmt || [];(function() &#123; var hm = document.createElement(\"script\"); hm.src = \"https://hm.baidu.com/hm.js?____________________\"; var s = document.getElementsByTagName(\"script\")[0]; s.parentNode.insertBefore(hm, s);&#125;)();&lt;/script&gt; 这段代码需要用户添加到网站全部页面的 &lt;/head&gt; 标签前，Next主题已对百度统计进行配置优化，只需要配置主题配置文件即可，下面会详细介绍。 其中 hm.js? 后面的字符串为用户的 key 值，将 key 值记录下来，后面会用到。 配置主题配置文件配置主题配置文件 /d/hexo/themes/next/_config.yml ，配置 baidu_analytics 参数，如下: 12# Baidu Analytics IDbaidu_analytics: 上面步骤中记录的百度统计里用户的key值。 修改完参数后执行 hexo g 和 hexo d 命令部署博客。 验证百度统计之后仍然在代码管理模块的代码获取页面进行验证，如下图: 上图表示验证通过。 一般过20分钟左右就可以看到网站分析数据，过了几小时后，已经看到博客的访问统计分析数据，如下图: 参考 Hexo添加百度统计 Hexo博客添加SEO-评论系统-阅读统计-站长统计","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/tags/Hexo/"}]},{"title":"经历了20多天的闭站备案，终于完成了网站备案。","slug":"20181026113200","date":"2018-10-26T03:32:22.000Z","updated":"2018-10-26T06:15:52.237Z","comments":true,"path":"20181026113200.html","link":"","permalink":"https://postgres.fun/20181026113200.html","excerpt":"","text":"经历了20多天的闭站备案，终于完成了网站备案，博客总算有了个合法的身份，这里简单分离下备案经历。 备案分两部分: 域名备案 公安备案 域名备案原则上部署到国内服务器的网站都需要进行域名备案（部署到外海服务器不需要进行域名备案），到空间提供商备案即可，例如，我的博客部署在腾讯云主机，到腾讯云提交备案申请即可。 备案主要分为以下四个步骤： 填写备案类型 填写备案信息 办理幕布拍照 提交管局审核 其中前三个步骤较快，每一步仅需1-2工作日；第四步为提交管局审核，通常为20个工作日以内，我这次从提交管局审核到审批通过仅需要9个工作日，比预期快，整个域名备案大概花了15个工作日。 关于备案的详细信息， 参考 域名备案参考 。 域名备案审核通过之后可以到 域名信息备案系统 进行验证，如下: 公安备案完成域名备案后，会提示请于备案完成后的30日内登陆全国公安机关互联网站安全管理服务平台办理公安备案，虽然是个人博客，建议做下公安备案 ，公安备案在 互联网安全管理服务平台 进行。 公安备案手册，详见 公安备案教程1 和 公安备案教程2 公安备案审核通过之后可以到 互联网安全管理服务平台 进行查询，如下: 公安备案比较快，上午提交，下午就收到审核通过消息了，各地审批时间会有差异。 添加备案信息到博客备案完成后建议将网站备案号放到博客底部，具体操作如下: 修改 /d/hexo/themes/next/layout/_third-party/analytics/busuanzi-counter.swig 文件，底部添加如下一段代码: 123&lt;div &gt;&lt;a target=\"_blank\" href=\"http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33010402003707\" style=\"display:inline-block;text-decoration:none;height:20px;line-height:20px;\"&gt;&lt;img src=\"/images/gongan.png\" style=\"float:left;\"/&gt;浙公网安备 33010402003707号&lt;/p&gt;&lt;/a&gt;&lt;span class=\"post-meta-divider\"&gt;|&lt;/span&gt;&lt;span&gt;&lt;a href=\"http://www.miitbeian.gov.cn\"&gt;浙ICP备18045927号&lt;/a&gt;&lt;/span&gt;&lt;/div&gt; 效果如下: 参考 https://cloud.tencent.com/product/ba http://bbs.qcloud.com/thread-28158-1-1.html https://boke112.com/3338.html","categories":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/categories/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"https://postgres.fun/tags/Hexo/"}]},{"title":"PostgreSQL11: psql 新增变量记录SQL语句的执行情况和错误","slug":"20181018084300","date":"2018-10-18T00:43:41.000Z","updated":"2018-10-18T00:44:57.641Z","comments":true,"path":"20181018084300.html","link":"","permalink":"https://postgres.fun/20181018084300.html","excerpt":"","text":"PostgreSQL 11 版本新增加 ERROR、SQLSTATE、ROW_COUNT、LAST_ERROR_MESSAGE、LAST_ERROR_SQLSTATE 五个变量用来记录SQL语句的执行结果状态和错误信息。 这些变量的值随着SQL执行后进行刷新，主要用来编写脚本时捕获SQL的执行结果。 Release 说明 Add psql variables to report query activity and errors (Fabien Coelho)Specifically, the new variables are ERROR, SQLSTATE, ROW_COUNT, LAST_ERROR_MESSAGE, and LAST_ERROR_SQLSTATE. 变量解释关于这几个变量的说明如下: 123456789101112**ERROR**true if the last SQL query failed, false if it succeeded. See also SQLSTATE.**SQLSTATE**The error code (see Appendix A) associated with the last SQL query is failure, or 00000 if it succeeded.**ROW_COUNT**The number of rows returned or affected by the last SQL query, or 0 if the query failed or did not report a row count.**LAST_ERROR_MESSAGE****LAST_ERROR_SQLSTATE**The primary error message and associated SQLSTATE code for the most recent failed query in the current psql session, or an empty string and 00000 if no error has occurred in the current session. 这几个变更的解释很容易理解，下面演示下。 演示：SQL执行成功正常执行一条SQL,执行后查看 ERROR、SQLSTATE、ROW_COUNT 变量，如下: 1234567891011121314151617181920[pg11@pghost2 ~]$ psql francs francspsql (11beta3)Type \"help\" for help.francs=&gt; SELECT * FROM generate_series(1,3); generate_series----------------- 1 2 3(3 rows)francs=&gt; \\echo :ERRORfalsefrancs=&gt; \\echo :SQLSTATE00000francs=&gt; \\echo :ROW_COUNT3 演示：SQL执行失败SQL执行失败,执行后查看 ERROR、SQLSTATE、ROW_COUNT 变更，如下: 123456789101112131415161718francs=&gt; SELECT *,afcd FROM generate_series(1,3);ERROR: column \"afcd\" does not existLINE 1: SELECT *,afcd FROM generate_series(1,3); ^francs=&gt; \\echo :ERRORtruefrancs=&gt; \\echo :SQLSTATE42703francs=&gt; \\echo :ROW_COUNT0francs=&gt; \\echo :LAST_ERROR_MESSAGEcolumn \"afcd\" does not existfrancs=&gt; \\echo :LAST_ERROR_SQLSTATE42703 SQLSTATE 变量返回SQL报错代码，SQL报错代码可参考手册 PostgreSQL Error Codes 。 参考 psql PostgreSQL Error Codes Waiting for PostgreSQL 11 – Add psql variables to track success/failure of SQL queries.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"psql","slug":"psql","permalink":"https://postgres.fun/tags/psql/"}]},{"title":"PostgreSQL11: psql 新增 \\gdesc 显示查询结果的列名和类型","slug":"20181017123400","date":"2018-10-17T04:34:42.000Z","updated":"2018-10-17T13:10:53.928Z","comments":true,"path":"20181017123400.html","link":"","permalink":"https://postgres.fun/20181017123400.html","excerpt":"","text":"PostgreSQL 11 的 psql 新增 \\gdesc 选项，此选项可以返回查询结果的列名和类型，而不实际执行SQL。 Release 说明 psqlAdd psql command \\gdesc to display the column names and types of the query output (Pavel Stehule) \\gdesc 选项说明 \\gdescShows the description (that is, the column names and data types) of the result of the current query buffer. The query is not actually executed; however, if it contains some type of syntax error, that error will be reported in the normal way.If the current query buffer is empty, the most recently sent query is described instea \\gdesc 只是显示查询结果的列名和类型，并不实际执行SQL，下面演示下。 \\gdesc 选项演示数据库中存在一张大表big，结构如下: 1234567891011121314[pg11@pghost2 ~]$ psql francs francspsql (11beta3)Type \"help\" for help.francs=&gt; \\d big Table \"francs.big\" Column | Type | Collation | Nullable | Default-----------+--------------------------------+-----------+----------+------------------- user_id | integer | | | user_name | text | | | ctime | timestamp(6) without time zone | | | clock_timestamp()Indexes: \"idx_big_ctime\" btree (ctime) \"idx_big_username\" btree (user_name) 执行以下查询，如下:12345678910francs=&gt; \\timingTiming is on.francs=&gt; SELECT count(*),sum(hashtext(user_name)) FROM big; count | sum----------+---------------- 30000000 | 11924569894736(1 row)Time: 1347.527 ms (00:01.348) 执行时间为 1347 ms 左右。 使用 \\gdesc 选项查询，如下:12345678francs=&gt; SELECT count(*),sum(hashtext(user_name)) FROM big \\gdesc Column | Type--------+-------- count | bigint sum | bigint(2 rows)Time: 0.634 ms 以上返回了查询结果的列和数据类型，执行很快，只需要 0.634 ms，可见没有实际执行SQL。 另一个示例，查询 pg_class 系统表，如下:12345678910111213141516171819202122232425262728293031323334353637francs=&gt; SELECT * FROM pg_class \\gdesc Column | Type---------------------+-------------- relname | name relnamespace | oid reltype | oid reloftype | oid relowner | oid relam | oid relfilenode | oid reltablespace | oid relpages | integer reltuples | real relallvisible | integer reltoastrelid | oid relhasindex | boolean relisshared | boolean relpersistence | \"char\" relkind | \"char\" relnatts | smallint relchecks | smallint relhasoids | boolean relhasrules | boolean relhastriggers | boolean relhassubclass | boolean relrowsecurity | boolean relforcerowsecurity | boolean relispopulated | boolean relreplident | \"char\" relispartition | boolean relrewrite | oid relfrozenxid | xid relminmxid | xid relacl | aclitem[] reloptions | text[] relpartbound | pg_node_tree(33 rows) 这个特性不需要实际执行SQL就能返回查询结果的列和数据类型，在某些特定场景比较有用。 参考 Waiting for PostgreSQL 11 – Add \\gdesc psql command psql","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"psql","slug":"psql","permalink":"https://postgres.fun/tags/psql/"}]},{"title":"PostgreSQL11: Initdb/Pg_resetwal支持修改WAL文件大小","slug":"20181016214500","date":"2018-10-16T13:45:25.000Z","updated":"2018-10-16T13:50:07.882Z","comments":true,"path":"20181016214500.html","link":"","permalink":"https://postgres.fun/20181016214500.html","excerpt":"","text":"PostgreSQL 11 版本的一个重要调整是支持 initdb 和 pg_resetwal 修改 WAL 文件大小，而 11 版本之前只能在编译安装 PostgreSQL 时设置 WAL 文件大小。这一特性能够方便 WAL 文件的管理。 Release 的说明 Allow the WAL file size to be set via initdb (Beena Emerson)Previously the 16MB default could only be changed at compile time. 下面分别演示通过 initdb 和 pg_resetwal 修改 WAL 文件大小。 使用 initdb 调整WAL文件大小initdb 命令关于修改 WAL 文件大小选项，如下: –wal-segsize=sizeSet the WAL segment size, in megabytes. This is the size of each individual file in the WAL log. The default size is 16 megabytes. The value must be a power of 2 between 1 and 1024 (megabytes). This option can only be set during initialization, and cannot be changed later.It may be useful to adjust this size to control the granularity of WAL log shipping or archiving. Also, in databases with a high volume of WAL, the sheer number of WAL files per directory can become a performance and management problem. Increasing the WAL file size will reduce the number of WAL files. WAL 日志文件大小默认为16MB，该值必须是1到1024之间的2的次方，增大WAL文件大小能够减少WAL日志文件的产生。 初始化一个新的 PostgreSQL 数据库实例，指定WAL文件大小64MB，如下: 1234567891011121314151617181920212223242526272829[pg11@pghost2 ~]$ initdb -E UTF8 --locale=C --wal-segsize=64 -D /home/pg11/data01 -U postgres -WThe files belonging to this database system will be owned by user \"pg11\".This user must also own the server process.The database cluster will be initialized with locale \"C\".The default text search configuration will be set to \"english\".Data page checksums are disabled.Enter new superuser password: Enter it again: creating directory /home/pg11/data01 ... okcreating subdirectories ... okselecting default max_connections ... 100selecting default shared_buffers ... 128MBselecting dynamic shared memory implementation ... posixcreating configuration files ... okrunning bootstrap script ... okperforming post-bootstrap initialization ... oksyncing data to disk ... okWARNING: enabling \"trust\" authentication for local connectionsYou can change this by editing pg_hba.conf or using the option -A, or--auth-local and --auth-host, the next time you run initdb.Success. You can now start the database server using: pg_ctl -D /home/pg11/data01 -l logfile start 修改 postgresql.conf 相关配置，之后启动数据库。 12345678[pg11@pghost2 data01]$ pg_ctl start -D /home/pg11/data01waiting for server to start....2018-10-16 15:58:16.714 CST [10583] LOG: listening on IPv6 address \"::1\", port 19502018-10-16 15:58:16.714 CST [10583] LOG: listening on IPv4 address \"127.0.0.1\", port 19502018-10-16 15:58:16.731 CST [10583] LOG: listening on Unix socket \"/tmp/.s.PGSQL.1950\"2018-10-16 15:58:16.762 CST [10584] LOG: database system was shut down at 2018-10-16 15:56:46 CST2018-10-16 15:58:16.782 CST [10583] LOG: database system is ready to accept connections doneserver started 验证WAL文件大小，如下: 1234[pg11@pghost2 ~]$ ll /home/pg11/data01/pg_waltotal 65M-rw------- 1 pg11 pg11 64M Oct 16 16:03 000000010000000000000001drwx------ 2 pg11 pg11 4.0K Oct 16 15:56 archive_status 可见WAL文件大小为64MB。 使用 pg_resetwal 调整WAL文件大小pg_resetwal 用来重置WAL日志和一些控制信息，常用于数据库恢复场景，不到万不得已不轻易使用，以下演示使用pg_resetwal命令调整WAL日志文件大小，仅供测试参考，生产环境慎用。 pg_resetwal 命令关于调整WAL文件大小的选项，如下: –wal-segsize=wal_segment_sizeSet the new WAL segment size, in megabytes. The value must be set to a power of 2 between 1 and 1024 (megabytes). See the same option of initdb for more information. 以下演示在已有PostgreSQL实例基础上调整WAL日志文件大小。 查看当前数据库的 pg_wal 目录，如下: 1234567891011121314[pg11@pghost2 pg_wal]$ ll /database/pg11/pg_root/pg_wal/total 2.3G-rw------- 1 pg11 pg11 16M Sep 30 14:45 000000010000001700000013-rw------- 1 pg11 pg11 16M Sep 30 14:45 000000010000001700000014-rw------- 1 pg11 pg11 16M Sep 30 14:45 000000010000001700000015-rw------- 1 pg11 pg11 16M Sep 30 14:45 000000010000001700000016-rw------- 1 pg11 pg11 16M Sep 30 14:45 000000010000001700000017-rw------- 1 pg11 pg11 16M Sep 30 14:45 000000010000001700000018-rw------- 1 pg11 pg11 16M Sep 30 14:45 000000010000001700000019-rw------- 1 pg11 pg11 16M Sep 30 14:45 00000001000000170000001A-rw------- 1 pg11 pg11 16M Sep 30 14:45 00000001000000170000001B...省略drwx------ 2 pg11 pg11 16K Oct 16 08:38 archive_status pg_wal 目录中已有大量WAL日志文件，WAL文件大小为16MB，计划将WAL日志文件调整成64MB。 pg_resetwal 操作时需要关闭数据库，如下。 123[pg11@pghost2 ~]$ pg_ctl stop -m fastwaiting for server to shut down.... doneserver stopped pg_resetwal 命令调整WAL日志文件大小为 64MB:12[pg11@pghost2 ~]$ pg_resetwal --wal-segsize=64 -D /database/pg11/pg_rootWrite-ahead log reset 验证WAL文件大小,如下: 1234[pg11@pghost2 ~]$ ll /database/pg11/pg_root/pg_wal/total 65M-rw------- 1 pg11 pg11 64M Oct 16 08:55 000000010000001700000029drwx------ 2 pg11 pg11 16K Oct 16 08:55 archive_status 发现 pg_wal 目录中原有的WAL日志被清理，同时生成了大小为64MB新的WAL文件。 启动数据库提示 min_wal_size 参数至少需设置成 wal_segment_size 大小为 2 倍。123456[pg11@pghost2 ~]$ pg_ctl startwaiting for server to start....2018-10-16 09:01:26.096 CST [24318] FATAL: \"min_wal_size\" must be at least twice \"wal_segment_size\".2018-10-16 09:01:26.096 CST [24318] LOG: database system is shut down stopped waitingpg_ctl: could not start serverExamine the log output. 根据提示调整 postgresql.conf，设置如下：1min_wal_size = 128MB 启动数据库正常，如下:123456[pg11@pghost2 ~]$ pg_ctl startwaiting for server to start....2018-10-16 09:02:45.680 CST [24614] LOG: listening on IPv4 address \"0.0.0.0\", port 19302018-10-16 09:02:45.680 CST [24614] LOG: listening on IPv6 address \"::\", port 19302018-10-16 09:02:45.687 CST [24614] LOG: listening on Unix socket \"/tmp/.s.PGSQL.1930\"2018-10-16 09:02:45.715 CST [24614] LOG: redirecting log output to logging collector process2018-10-16 09:02:45.715 CST [24614] HINT: Future log output will appear in directory \"log\". 总结 以上演示了 11 版本通过 initdb 和 pg_resetwal 调整WAL文件大小。 pg_resetwal 会清除pg_wal目录的WAL文件，本博客的测试样例仅供参考，生产环境使用需慎重。 参考 initdb pg_resetwal","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL11: 可通过GRNAT权限下放的四个系统函数","slug":"20181015113300","date":"2018-10-15T03:33:49.000Z","updated":"2018-10-15T03:40:53.253Z","comments":true,"path":"20181015113300.html","link":"","permalink":"https://postgres.fun/20181015113300.html","excerpt":"","text":"涉及到数据库服务端文件读取的系统函数通常需要管理员权限，例如 pg_ls_dir()等系统函数，PostgreSQL 11 版本支持少量文件读取的系统函数权限下放，可通过 GRANT/REVOKE 将权限赋给普通用户，目前以下四个文件读取系统函数支持权限下放： pg_ls_dir(): List the contents of a directory. Restricted to superusers by default, but other users can be granted EXECUTE to run the function. pg_read_file(): Return the contents of a text file. Restricted to superusers by default, but other users can be granted EXECUTE to run the function. pg_read_binary_file(): Return the contents of a file. Restricted to superusers by default, but other users can be granted EXECUTE to run the functio pg_stat_file(): Return information about a file. Restricted to superusers by default, but other users can be granted EXECUTE to run the function. 这四个函数在11版本之前只有超级用户才有权限使用。 一、Release说明 Allow access to file system functions to be controlled by GRANT/REVOKE permissions, rather than superuser checks (Stephen Frost)Specifically, these functions were modified: pg_ls_dir(), pg_read_file(), pg_read_binary_file(), pg_stat_file(). 以上四个函数使用上差异不大，本文仅演示其中两个函数。 二、pg_ls_dir()pg_ls_dir()函数可以列出数据库服务端数据目录的文件，11版本前只有超级用户才有权限调用。 PostgreSQL 10 测试10 版本测试如下:123456[postgres@pghost1 ~]$ psql mydb pguserpsql (10.0)Type \"help\" for help.mydb=&gt; SELECT pg_ls_dir('pg_wal');ERROR: must be superuser to get directory listings 以上显示只有超级用户才有权限。 尝试将函数 pg_ls_dir()的可执行权限赋给普通用户 pguser。 1234567891011mydb=&gt; \\c mydb postgresYou are now connected to database \"mydb\" as user \"postgres\".mydb=# GRANT EXECUTE ON FUNCTION pg_ls_dir(text) TO pguser;GRANTmydb=# \\c mydb pguserYou are now connected to database \"mydb\" as user \"pguser\".mydb=&gt; select pg_ls_dir('pg_wal');ERROR: must be superuser to get directory listings 以上看出，将函数pg_ls_dir()的执行权限赋给普通用户后，普通用户依然没有权限执行。 PostgreSQL 11 测试将函数 pg_ls_dir()的可执行权限赋给普通用户 role11 。 123456[pg11@pghost2 ~]$ psql francs postgrespsql (11beta3)Type \"help\" for help.francs=# GRANT EXECUTE ON FUNCTION pg_ls_dir(text) TO role11;GRANT 以 role11 用户登录 francs 库测试:123456789101112[pg11@pghost2 ~]$ psql francs role11psql (11beta3)Type \"help\" for help.francs=&gt; SELECT pg_ls_dir('pg_wal'); pg_ls_dir-------------------------- 00000001000000170000002B 000000010000001700000025 000000010000001700000034 000000010000001700000073 ...省略 普通用户执行 pg_ls_dir(‘pg_wal’) 函数成功，已查看到数据库服务端的 pg_wal 目录文件。 三、pg_read_file()pg_read_file()函数可以显示数据库服务端文本文件的内容，11版本前只有超级用户才有权限调用。 PostgreSQL 10 测试10 版本测试，如下:123456[postgres@pghost1 ~]$ psql mydb pguserpsql (10.0)Type \"help\" for help.mydb=&gt; SELECT pg_read_file('/home/postgres/t_copy2.txt');ERROR: must be superuser to read files 显示只有超级用户才有权限执行。 PostgreSQL 11 测试将函数 pg_read_file()的可执行权限赋给普通用户 role11 。 123456[pg11@pghost2 ~]$ psql francs postgrespsql (11beta3)Type \"help\" for help.francs=# GRANT EXECUTE ON FUNCTION pg_read_file(text) TO role11;GRANT 以role11用户登录francs库测试，如下:1234567891011[pg11@pghost2 ~]$ psql francs role11psql (11beta3)Type \"help\" for help.francs=&gt; select pg_read_file ('/home/pg11/t_copy2.txt'); pg_read_file-------------- 1 a + 2 b +(1 row) 赋权后，普通用户role11有权限执行 pg_read_file() 函数查看数据库服务端文件内容。 四、参考 Generic File Access Functions PostgreSQL11: 新增三个默认角色","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL11: 新增三个默认角色","slug":"20181013215100","date":"2018-10-13T13:51:35.000Z","updated":"2018-10-13T13:55:36.147Z","comments":true,"path":"20181013215100.html","link":"","permalink":"https://postgres.fun/20181013215100.html","excerpt":"","text":"PostgreSQL 11 新增三个默认系统角色，如下： pg_read_server_files pg_write_server_files pg_execute_server_program 这三个角色主要涉及数据库服务端文件的读写权限，例如使用copy命令或file_fdw模块读写数据库端文件的权限。 这些权限之前版本只有超级用户才具备，这三个默认角色的出现，使得可以将数据库服务端的文件的访问权限（目前仅包含copy命令或file_fdw模块）下放给普通用户。 Release说明 Add default roles which control file system access (Stephen Frost)Specifically, the new roles are: pg_read_server_files, pg_write_server_files, pg_execute_server_program. These roles now also control who can use COPY and extension file_fdw. Previously only superusers could use these functions, and that is still the default behavior. 手册中说明很清楚，下面演示这三种角色的权限。 pg_read_server_filespg_read_server_files 角色具有数据库服务端文件的读权限，例如使用copy命令或file_fdw模块读数据库端文件的权限。 在数据库主机 pghost2 家目录创建 t_copy.txt 文件并写入两行数据，如下：121 a2 b 以 francs 用户登录数据库 francs 创建测试表 t_copy 如下:123456[pg11@pghost2 ~]$ psql francs francspsql (11beta3)Type \"help\" for help.francs=&gt; CREATE TABLE t_copy(id int4, name text);CREATE TABLE 创建 role11 用户，如下12postgres=# CREATE ROLE role11 NOSUPERUSER PASSWORD 'role11' LOGIN;CREATE ROLE 以 role11 用户登录到 francs 数据库，执行 copy 命令，尝试将数据库服务端文件 t_copy.txt 文件的数据加载到表 t_copy 中，如下:1234567[pg11@pghost2 ~]$ psql francs role11psql (11beta3)Type \"help\" for help.francs=&gt; COPY t_copy FROM '/home/pg11/t_copy.txt';ERROR: must be superuser or a member of the pg_read_server_files role to COPY from a fileHINT: Anyone can COPY to stdout or from stdin. psql s \\copy command also works for anyone. 以上报错，提示需要超级用户或具有pg_read_server_files权限才能使用 COPY 命令读取数据库服务端文件。 给 role11 用户赋 pg_read_server_files 角色权限，如下:1234567891011francs=&gt; \\c francs postgresYou are now connected to database \"francs\" as user \"postgres\".francs=# GRANT pg_read_server_files TO role11;GRANT ROLEfrancs=# GRANT USAGE ON SCHEMA francs TO role11;GRANTfrancs=# GRANT INSERT ON francs.t_copy TO role11;GRANT francs库中创建了模式 francs ，因此也需要将模式的使用权限赋给 role11，否则访问表时会报没有使用模式权限的错误；之后再赋予表的写权限。 再次测试成功，如下。123456[pg11@pghost2 ~]$ psql francs role11psql (11beta3)Type \"help\" for help.francs=&gt; COPY francs.t_copy FROM '/home/pg11/t_copy.txt';COPY 2 pg_write_server_filespg_write_server_files 角色具有数据库服务端文件的写权限，例如使用copy命令或file_fdw模块写数据库端文件的权限，接着演示。 以 role11 用户登录数据库 francs ，尝试导出表数据到数据库服务端。1234567[pg11@pghost2 ~]$ psql francs role11;psql (11beta3)Type \"help\" for help.francs=&gt; COPY francs.t_copy TO '/home/pg11/t_copy2.txt';ERROR: must be superuser or a member of the pg_write_server_files role to COPY to a fileHINT: Anyone can COPY to stdout or from stdin. psql s \\copy command also works for anyone. 赋权如下：123456789[pg11@pghost2 ~]$ psql francs postgrespsql (11beta3)Type \"help\" for help.francs=# GRANT pg_write_server_files TO role11;GRANT ROLEfrancs=# GRANT SELECT ON francs.t_copy TO role11;GRANT 再次测试成功，如下12345678910[pg11@pghost2 ~]$ psql francs role11;psql (11beta3)Type \"help\" for help.francs=&gt; COPY francs.t_copy TO '/home/pg11/t_copy2.txt';COPY 2francs=&gt; \\! cat '/home/pg11/t_copy2.txt'1 a2 b 可见，已将数据导出到数据库服务端上的文件。 pg_execute_server_programpg_execute_server_program 角色具有执行数据库服务端的程序权限，以file_fdw外部表举例如下。 首先准备数据文件，将 t_copy 文件进行压缩，如下:12345[pg11@pghost2 ~]$ cat t_copy.txt 1 a2 b[pg11@pghost2 ~]$ gzip t_copy.txt 创建 file_fdw 外部扩展和外部表，以超级用户postgres登录francs库，如下:123456789101112[pg11@pghost2 ~]$ psql francs postgrespsql (11beta3)Type \"help\" for help.francs=# CREATE EXTENSION file_fdw;CREATE EXTENSIONfrancs=# CREATE SERVER srv_file FOREIGN DATA WRAPPER file_fdw ;CREATE SERVERfrancs=# GRANT USAGE ON FOREIGN SERVER srv_file TO role11;GRANT 以普通用户role11登录francs库，创建带OPTIONS(program)选项的外部表，如下:123456[pg11@pghost2 ~]$ psql francs role11psql (11beta3)Type \"help\" for help.francs=&gt; CREATE FOREIGN TABLE ft_t_copy(id int4,name text) SERVER srv_file OPTIONS(program 'gunzip &lt; /home/pg11/t_copy.txt.gz');ERROR: only superuser or a member of the pg_execute_server_program role may specify the program option of a file_fdw foreign table 以上报错，提示需要 superuser 或 pg_execute_server_program 权限才有权限指定 file_fdw 外部表的 program 选项。 将 pg_execute_server_program 角色赋予 role11用户，注意以下以postgres超级用户执行。12francs=# GRANT pg_execute_server_program TO role11;GRANT ROLE 再次以role11用户登录francs库测试，如下:123456789francs=&gt; CREATE FOREIGN TABLE ft_t_copy(id int4,name text) SERVER srv_file OPTIONS(program 'gunzip &lt; /home/pg11/t_copy.txt.gz');CREATE FOREIGN TABLEfrancs=&gt; SELECT * FROM ft_t_copy ; id | name ----+------ 1 | a 2 | b(2 rows) 创建带带OPTIONS(program)选项的外部表成功。 总结 pg_read_server_files、pg_write_server_files、pg_execute_server_program 角色涉及到读写数据库服务端文件，权限较大，分配此角色权限给数据库用户时需谨慎考虑。 参考 Default Roles Postgres 11 highlight - New System Roles PostgreSQL9.1新特性之三：基于文件访问的 SQL/MED","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL11: Indexs With Include Columns ","slug":"20180930094300","date":"2018-09-30T01:43:45.000Z","updated":"2018-09-30T02:29:43.004Z","comments":true,"path":"20180930094300.html","link":"","permalink":"https://postgres.fun/20180930094300.html","excerpt":"","text":"PostgreSQL 11 版本索引方面一个显著的新特性是创建索引时支持 INCLUDE COLUMNS ，语法如下1CREATE INDEX idx_name ON table_name USING BTREE (column_a) INCLUDE (column_b); 一、Release 中的说明 Allow indexes to INCLUDE columns that are not part of the unique constraint but are available for index-only scans (Anastasia Lubennikova, Alexander Korotkov, Teodor Sigaev) This is also useful for including columns that dont have btree support 此特性主要用途和使用场景： 如果字段不支持btree索引，可以使用INCLUDE方式索引。 使表上的更多SQL能走 Index-Only Scans。 以上描述颇为费力，以下通过实例演示。 二、验证: 不支持Btree索引的字段，使用Include方式索引首先验证第一点：不支持Btree索引的字段支持使用INCLUDE方式索引，创建测试表。12francs=&gt; CREATE TABLE t_json1(a serial, user_info json);CREATE TABLE 在(a,user_info) 字段上创建 btree 索引，如下:123francs=&gt; CREATE INDEX t_json1_idx1 ON t_json1 USING BTREE(a,user_info);ERROR: data type json has no default operator class for access method \"btree\"HINT: You must specify an operator class for the index or define a default operator class for the data type. 以上创建索引报错，是由于 json 不支持 btree 索引。 使用 INCLUDE 方式创建索引成功，如下:12francs=&gt; CREATE INDEX t_json1_idx_include ON t_json1 USING btree(a) INCLUDE(user_info);CREATE INDEX 三、验证: Include Columns 支持 Index-Only Scans创建测试表并插入300万数据，如下：12345francs=&gt; CREATE TABLE t_include(a int4, name text);CREATE TABLEfrancs=&gt; INSERT INTO t_include(a,name) SELECT n,n || '_INCLUDE TEST' FROM generate_series(1,3000000) n;INSERT 0 3000000 在字段a上创建索引，如下：12francs=&gt; CREATE INDEX idx_t_include_a ON t_include USING BTREE (a);CREATE INDEX where条件中只包含a，查询a字段，以下SQL走了 Index Only Scan。123456789francs=&gt; EXPLAIN ANALYZE SELECT a FROM t_include WHERE a&lt;5; QUERY PLAN-------------------------------------------------------------------------------------------------------------------------------- Index Only Scan using idx_t_include_a on t_include (cost=0.43..8.50 rows=4 width=4) (actual time=0.017..0.018 rows=4 loops=1) Index Cond: (a &lt; 5) Heap Fetches: 4 Planning Time: 0.272 ms Execution Time: 0.038 ms(5 rows) 加入 name 字段后，不走 Index Only Scan，如下。12345678francs=&gt; EXPLAIN ANALYZE SELECT a,name FROM t_include WHERE a&lt;5; QUERY PLAN---------------------------------------------------------------------------------------------------------------------------- Index Scan using idx_t_include_a on t_include (cost=0.43..8.50 rows=4 width=24) (actual time=0.005..0.007 rows=4 loops=1) Index Cond: (a &lt; 5) Planning Time: 0.125 ms Execution Time: 0.025 ms(4 rows) 加入 name 后走了 Index Scan using ，根据索引回表查询name字段。 创建索引时使用 INCLUDE(name)，如下12francs=&gt; CREATE INDEX idx_t_include ON t_include USING BTREE (a) INCLUDE (name);CREATE INDEX 再次执行查询，走了 Index Only Scan 123456789101112francs=&gt; VACUUM ANALYZE t_include;VACUUMfrancs=&gt; EXPLAIN ANALYZE SELECT a,name FROM t_include WHERE a&lt;5; QUERY PLAN------------------------------------------------------------------------------------------------------------------------------- Index Only Scan using idx_t_include on t_include (cost=0.43..4.50 rows=4 width=24) (actual time=0.017..0.018 rows=4 loops=1) Index Cond: (a &lt; 5) Heap Fetches: 0 Planning Time: 0.175 ms Execution Time: 0.038 ms(5 rows) 只查询name字段，也走了 Index Only Scan，如下123456789francs=&gt; EXPLAIN ANALYZE SELECT name FROM t_include WHERE a&lt;5; QUERY PLAN------------------------------------------------------------------------------------------------------------------------------- Index Only Scan using idx_t_include on t_include (cost=0.43..4.50 rows=4 width=20) (actual time=0.012..0.014 rows=4 loops=1) Index Cond: (a &lt; 5) Heap Fetches: 0 Planning Time: 0.163 ms Execution Time: 0.038 ms(5 rows) 四、Include 索引的限制 目前只有 Btree 索引支持 INCLUDE COLUMNS(INCLUDE中的字段物理上位于btree索引叶子节点)。 INCLUDE COLUMNS 中的字段不支持函数索引。 INCLUDE COLUMNS 中的字段数据类型可以不支持 btree 索引，例如 JSON 等数据类型。 五、两种索引方式差异或许有朋友问以下两种索引方式有啥区别？ 这个问题欢迎大家留言讨论。12CREATE INDEX idx_t_not_include ON t_include USING BTREE (a,name);CREATE INDEX idx_t_include ON t_include USING BTREE (a) INCLUDE (name); 六、参考 CREATE INDEX Postgres 11 highlight - Covering Indexes WAITING FOR POSTGRESQL 11 – INDEXES WITH INCLUDE COLUMNS AND THEIR SUPPORT IN B-TREE","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL11: 新增非空默认值字段不需要重写表","slug":"20180929100400","date":"2018-09-29T02:04:49.000Z","updated":"2018-09-30T07:17:57.405Z","comments":true,"path":"20180929100400.html","link":"","permalink":"https://postgres.fun/20180929100400.html","excerpt":"","text":"PostgreSQL 10 版本前表新增不带默认值的DDL不需要重写表，只需要更新数据字典，因此DDL能瞬间执行，如下: 1ALTER TABLE table_name ADD COLUMN flag text; 如果新增的字段带默认值，则需要重写表，表越大，执行时间越长，如下。 1ALTER TABLE table_name ADD COLUMN flag text DEFAULT 'default values'; 生产环境下给大表添加带 Default 值的字段将非常吃力，通常分两步进行： 第一步: 先添加不带 Default值的字段。 第二步: 写函数批量刷新新增字段的默认值。 上述第二步比较麻烦，也可以在业务低谷或申请停服窗口一次性完成带DEFAUL值字段的新增。 PostgreSQL 11 版本这方面进一步增强，表新增带非空默认值的字段不再需要重写表，Release 中的说明如下： Release中的说明 Allow ALTER TABLE to add a column with a non-null default without a table rewrite 本文分别在 10 版本和 11 版本进行测试。 PostgreSQL 10 版本创建测试表并插入1000万数据，如下。123456789101112[pg10@pghost1 ~]$ psql mydb pguserpsql (10.0)Type \"help\" for help.mydb=&gt; CREATE TABLE t1(id int4, name text);CREATE TABLEmydb=&gt; INSERT INTO t1 (id,name ) SELECT n, n || '_ALTER TABLE TEST ' FROM generate_series (1,10000000) n;INSERT 0 10000000mydb=&gt; ANALYZE t1;ANALYZE 查看表的 relfilenode 和 relpages 信息，relfilenode 表示表的物理文件号。12345mydb=&gt; SELECT relname,relfilenode, relpages FROM pg_class WHERE relname='t1'; relname | relfilenode | relpages---------+-------------+---------- t1 | 25672 | 73530(1 row) 新增带默认值的非空字段，如下。123456mydb=&gt; \\timingTiming is on.mydb=&gt; ALTER TABLE t1 ADD COLUMN flag text DEFAULT 'abcdefg';ALTER TABLETime: 15540.002 ms (00:15.540) 执行时间较长，需要15秒左右。 表分析后再次查看表的 relfilenode 和 relpages信息12345678mydb=&gt; ANALYZE t1;ANALYZEmydb=&gt; SELECT relname,relfilenode, relpages FROM pg_class WHERE relname='t1'; relname | relfilenode | relpages---------+-------------+---------- t1 | 25679 | 83334(1 row) 发现 relfilenode 有变化 ，之前的 relfilenode 值为 25672 ，说明表被重写。另一方面 relpages 变大了。 PostgreSQL 11 版本创建测试表并插入1000万数据，如下。123456789101112[pg11@pghost2 ~]$ psql francs francspsql (11beta3)Type \"help\" for help.francs=&gt; CREATE TABLE t1(id int4, name text);CREATE TABLEfrancs=&gt; INSERT INTO t1 (id,name ) SELECT n, n || '_ALTER TABLE TEST ' FROM generate_series (1,10000000) n;INSERT 0 10000000francs=&gt; ANALYZE t1;ANALYZE 查看表的 relfilenode 和 relpages信息，如下:1234francs=&gt; SELECT relname,relfilenode, relpages FROM pg_class WHERE relname='t1'; relname | relfilenode | relpages---------+-------------+---------- t1 | 16802 | 73530 新增带默认值的非空字段，如下。123456francs=&gt; \\timingTiming is on.francs=&gt; ALTER TABLE t1 ADD COLUMN flag text DEFAULT 'abcdefg';ALTER TABLETime: 40.743 ms 执行时间只需要 40 ms，瞬间完成。 表分析后再次查看表的 relfilenode 和 relpages信息12345678francs=&gt; ANALYZE t1;ANALYZEfrancs=&gt; SELECT relname,relfilenode, relpages FROM pg_class WHERE relname='t1'; relname | relfilenode | relpages---------+-------------+---------- t1 | 16802 | 73530(1 row) 发现 relfilenode 没有变化，依然是 16802，同时 relpages 也没有变化。 增加1000字段PostgreSQL 11 版本给表 t1 增加了一个带默认值的字段后表占用空间没有变化，是不是增加的字段数不够多？接着往下测试，增加1000个带默认值的字段，看看情况如何？ 创建测试表并插入1000万测试数据，如下:1234567891011francs=&gt; DROP TABLE t1;DROP TABLEfrancs=&gt; CREATE TABLE t1(id int4, name text);CREATE TABLEfrancs=&gt; INSERT INTO t1 (id,name ) SELECT n, n || '_ALTER TABLE TEST ' FROM generate_series (1,10000000) n;INSERT 0 10000000francs=&gt; ANALYZE t1;ANALYZE 查看表的 relfilenode 和 relpages信息，如下:12345francs=&gt; SELECT relname,relfilenode, relpages FROM pg_class WHERE relname='t1'; relname | relfilenode | relpages---------+-------------+---------- t1 | 34187 | 73530(1 row) 查看表大小，如下：12345francs=&gt; SELECT pg_size_pretty(pg_relation_size('t1')); pg_size_pretty---------------- 574 MB(1 row) 创建函数，此函数用来给表 t1 添加 1000 个带默认值的字段，如下:123456789101112131415CREATE OR REPLACE FUNCTION add_column() RETURNS INTEGER AS $BODY$DECLARE column_name text; default_value text;BEGIN default_value:= repeat(md5('1'),10); FOR i in 1..1000 LOOP column_name:= 'flag' || i; EXECUTE $$ ALTER TABLE t1 ADD COLUMN $$ || column_name || $$ text default' $$ || default_value || $$'$$ ; END LOOP; RETURN 1;END$BODY$ LANGUAGE 'plpgsql'; 执行函数，如下：12345francs=&gt; SELECT add_column(); add_column------------ 1(1 row) 这时表t1已增加了1000个字段，如下：12345678910111213francs=&gt; SELECT * FROM t1 LIMIT 1;-----------------------------------------[ RECORD 1 ]-----------------------------------------------------------------------------------------------------------------------id | 1name | 1_ALTER TABLE TESTflag1 | c4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bflag2 | c4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bflag3 | c4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bflag4 | c4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bflag5 | c4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bflag6 | c4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bflag7 | c4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849bc4ca4238a0b923820dcc509a6f75849b...省略 查看表 relfilenode 和 relpages，没有变化。12345francs=&gt; SELECT relname,relfilenode, relpages FROM pg_class WHERE relname='t1'; relname | relfilenode | relpages---------+-------------+---------- t1 | 34187 | 73530(1 row) 再次确认表大小，依然还是 574MB。12345francs=&gt; SELECT pg_size_pretty(pg_relation_size('t1')); pg_size_pretty---------------- 574 MB(1 row) 从以上看出给表t1增加了1000个带默认值的字段后，t1表大小依然没有变化。 参考 WAITING FOR POSTGRESQL 11 – FAST ALTER TABLE ADD COLUMN WITH A NON-NULL DEFAULT PostgreSQL 11 preview - 添加非空默认值不需要 rewrite table - online add column with default value","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL11: 支持存储过程(SQL Stored Procedures)","slug":"20180925162500","date":"2018-09-25T08:25:48.000Z","updated":"2018-09-25T09:00:25.654Z","comments":true,"path":"20180925162500.html","link":"","permalink":"https://postgres.fun/20180925162500.html","excerpt":"","text":"PostgreSQL 11 版本一个重量级新特性是对存储过程的支持，同时支持存储过程嵌入事务，存储过程是很多 PostgreSQL 从业者期待已久的特性，尤其是很多从Oracle转到PostgreSQL朋友，尽管PostgreSQL提供函数可以实现大多数存储过程的功能，但在函数中无法执行事务实现部分提交，换句话说，函数中的SQL要么都执行成功，要不全部返回失败。 PostgreSQL 11 版本对存储过程的支持，从兼容层面考虑和Oracle的兼容性进一步增强，本文演示下PostgreSQL存储过程的创建和调用，并且演示存储过程支持嵌入事务样例。 发行说明 SQL stored procedures, with support for embedded transactions 存储过程创建语法123456789CREATE [ OR REPLACE ] PROCEDURE name ( [ [ argmode ] [ argname ] argtype [ &#123; DEFAULT | = &#125; default_expr ] [, ...] ] ) &#123; LANGUAGE lang_name | TRANSFORM &#123; FOR TYPE type_name &#125; [, ... ] | [ EXTERNAL ] SECURITY INVOKER | [ EXTERNAL ] SECURITY DEFINER | SET configuration_parameter &#123; TO value | = value | FROM CURRENT &#125; | AS 'definition' | AS 'obj_file', 'link_symbol' &#125; ... 存储过程调用语法1CALL name ( [ argument ] [, ...] ) 存储过程调用比较简单，使用 CALL 命令即可，而函数的调用是使用 SELECT 命令。 存储过程嵌入事务测试创建一个简单的存储过程演示下。 创建测试表12CREATE TABLE t1 (id int4);CREATE TABLE t1_log (ctime timestamp(6) without time zone , operation text,ins_values int4); 创建存储过程 ins_t1123456CREATE OR REPLACE PROCEDURE ins_t1(a integer) AS $$BEGIN INSERT INTO t1(id) VALUES(a); INSERT INTO t1_log(ctime,operation,ins_values) VALUES (clock_timestamp(),'INSERT',a);END$$ LANGUAGE 'plpgsql'; 调用存储过程 ins_t1()，如下12francs=&gt; CALL ins_t1(1);CALL 查看表 t1 和 t1_log 记录。1234567891011francs=&gt; SELECT * FROM t1; id---- 1(1 row)francs=&gt; SELECT * FROM t1_log; ctime | operation | ins_values----------------------------+-----------+------------ 2018-09-25 15:08:16.026122 | INSERT | 1(1 row) 以上看出两条SQL都已提交。 创建存储过程 ins_t1_part123456789CREATE OR REPLACE PROCEDURE ins_t1_part(a integer) AS $$BEGIN INSERT INTO t1(id) VALUES(a); COMMIT; INSERT INTO t1_log(ctime,operation,ins_values) VALUES (clock_timestamp(),'INSERT',a); ROLLBACK;END$$ LANGUAGE 'plpgsql'; 调用存储过程 ins_t1_part()，如下12francs=&gt; CALL ins_t1_part(2);CALL 查看表 t1 和 t1_log 记录。123456789101112francs=&gt; SELECT * FROM t1; id---- 1 2(2 rows)francs=&gt; SELECT * FROM t1_log; ctime | operation | ins_values----------------------------+-----------+------------ 2018-09-25 15:08:16.026122 | INSERT | 1(1 row) 发现 t1 表的数据已提交，而 t1_log 表的数据没有提交，验证了存储过程支持嵌入式事务。 参考 CREATE PROCEDURE TECH PREVIEW: POSTGRESQL 11 – CREATE PROCEDURE","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL11：支持并行哈希连接(Parallel Hash Joins)","slug":"20180924134200","date":"2018-09-24T05:42:27.000Z","updated":"2018-09-24T05:52:42.159Z","comments":true,"path":"20180924134200.html","link":"","permalink":"https://postgres.fun/20180924134200.html","excerpt":"","text":"PostgreSQL 11 版本在并行方面得到增强，例如支持并行创建索引(Parallel Index Build)、并行哈希连接(Parallel Hash Join)、并行 CREATE TABLE .. AS等，上篇博客介绍了并行创建索引，本文介绍并行 Hash Join。 测试环境准备创建大表t_big并插入5000万条数据。1234567CREATE TABLE t_big(id int4,name text,create_time timestamp without time zone );INSERT INTO t_big(id,name,create_time)SELECT n, n|| '_test',clock_timestamp() FROM generate_series(1,50000000) n ; 创建小表t_small并插入800万条数据1234CREATE TABLE t_small(id int4, name text);INSERT INTO t_small(id,name)SELECT n, n|| '_small' FROM generate_series(1,8000000) n ; 验证并行哈希连接PostgreSQL 10 版本查看以下SQL执行计划，如下： 1234567891011121314des=&gt; EXPLAIN SELECT t_small.name FROM t_big JOIN t_small ON (t_big.id = t_small.id) AND t_small.id &lt; 100; QUERY PLAN-------------------------------------------------------------------------------------- Gather (cost=151870.58..661385.28 rows=4143 width=13) Workers Planned: 4 -&gt; Hash Join (cost=150870.58..659970.98 rows=1036 width=13) Hash Cond: (t_big.id = t_small.id) -&gt; Parallel Seq Scan on t_big (cost=0.00..470246.58 rows=10358258 width=4) -&gt; Hash (cost=150860.58..150860.58 rows=800 width=17) -&gt; Seq Scan on t_small (cost=0.00..150860.58 rows=800 width=17) Filter: (id &lt; 100)(8 rows) PostgreSQL 11 版本查看以下SQL执行计划，如下：1234567891011121314francs=&gt; EXPLAIN SELECT t_small.name FROM t_big JOIN t_small ON (t_big.id = t_small.id) AND t_small.id &lt; 100; QUERY PLAN----------------------------------------------------------------------------------------- Gather (cost=76862.42..615477.60 rows=800 width=13) Workers Planned: 4 -&gt; Parallel Hash Join (cost=75862.42..614397.60 rows=200 width=13) Hash Cond: (t_big.id = t_small.id) -&gt; Parallel Seq Scan on t_big (cost=0.00..491660.86 rows=12499686 width=4) -&gt; Parallel Hash (cost=75859.92..75859.92 rows=200 width=17) -&gt; Parallel Seq Scan on t_small (cost=0.00..75859.92 rows=200 width=17) Filter: (id &lt; 100)(8 rows) 对比10版本的执行计划，不同之处为11版本走了 Parallel Hash Join，而 10 版本走的 Hash Join，Parallel Hash Join 为 11 版本的新特性。 并行哈希连接性能测试开启并行哈希连接相比不开启性能上有何变化？接着测试。 开启并行哈希连接PostgreSQL 11 版本执行以下SQL，如下：1234567891011121314151617181920francs=&gt; EXPLAIN ANALYZE SELECT t_small.name FROM t_big JOIN t_small ON (t_big.id = t_small.id) AND t_small.id &lt; 100; QUERY PLAN------------------------------------------------------------------------------------------------------------------------------------------ Gather (cost=76862.42..615477.60 rows=800 width=13) (actual time=197.399..2738.010 rows=99 loops=1) Workers Planned: 4 Workers Launched: 4 -&gt; Parallel Hash Join (cost=75862.42..614397.60 rows=200 width=13) (actual time=2222.347..2729.943 rows=20 loops=5) Hash Cond: (t_big.id = t_small.id) -&gt; Parallel Seq Scan on t_big (cost=0.00..491660.86 rows=12499686 width=4) (actual time=0.038..1330.836 rows=10000000 loops=5) -&gt; Parallel Hash (cost=75859.92..75859.92 rows=200 width=17) (actual time=191.484..191.484 rows=20 loops=5) Buckets: 1024 Batches: 1 Memory Usage: 40kB -&gt; Parallel Seq Scan on t_small (cost=0.00..75859.92 rows=200 width=17) (actual time=152.436..191.385 rows=20 loops=5) Filter: (id &lt; 100) Rows Removed by Filter: 1599980 Planning Time: 0.183 ms Execution Time: 2738.068 ms(13 rows) 以上SQL执行多次，取最快时间，执行时间为 2738.068 ms。 关闭并行哈希连接会话级设置enable_parallel_hash参数为off表示关闭并行哈希连接，测试性能有何变化，如下。 1234567891011121314151617181920212223francs=&gt; set enable_parallel_hash = off;SETfrancs=&gt; EXPLAIN ANALYZE SELECT t_small.name FROM t_big JOIN t_small ON (t_big.id = t_small.id) AND t_small.id &lt; 100; QUERY PLAN------------------------------------------------------------------------------------------------------------------------------------------ Gather (cost=151869.66..690486.34 rows=800 width=13) (actual time=996.137..3496.940 rows=99 loops=1) Workers Planned: 4 Workers Launched: 4 -&gt; Hash Join (cost=150869.66..689406.34 rows=200 width=13) (actual time=2990.847..3490.557 rows=20 loops=5) Hash Cond: (t_big.id = t_small.id) -&gt; Parallel Seq Scan on t_big (cost=0.00..491660.86 rows=12499686 width=4) (actual time=0.240..1392.062 rows=10000000 loops=5) -&gt; Hash (cost=150859.66..150859.66 rows=800 width=17) (actual time=890.943..890.943 rows=99 loops=5) Buckets: 1024 Batches: 1 Memory Usage: 13kB -&gt; Seq Scan on t_small (cost=0.00..150859.66 rows=800 width=17) (actual time=884.288..890.906 rows=99 loops=5) Filter: (id &lt; 100) Rows Removed by Filter: 7999901 Planning Time: 0.154 ms Execution Time: 3496.982 ms(13 rows) 以上SQL执行多次，取最快时间，从以上看出，关闭并行哈希连接时SQL的执行时间为 3496.982 ms ，相比开启并行哈希连接执行时间长了 27%。 可见开启并行哈希连接后，性能有较大幅度提升。 参考 PostgreSQL11：支持并行创建索引(Parallel Index Builds) PostgreSQL10：Parallel Queries 增强","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Parallel Query","slug":"Parallel-Query","permalink":"https://postgres.fun/tags/Parallel-Query/"}]},{"title":"PostgreSQL11：支持并行创建索引(Parallel Index Builds)","slug":"20180922222100","date":"2018-09-22T14:21:38.000Z","updated":"2018-09-24T05:46:21.808Z","comments":true,"path":"20180922222100.html","link":"","permalink":"https://postgres.fun/20180922222100.html","excerpt":"","text":"PostgreSQL 11 版本在并行方面得到增强，例如支持并行创建索引、并行Hash Join、并行 CREATE TABLE .. AS等，本文先介绍并行创建索引。 PostgreSQL 11 版本并行创建索引仅支持 B-tree 索引，其它类型索引现阶段不支持并行创建。 并行进程相关参数介绍并行创建索引之前先来看看并行进程的相关 postgresql.conf 参数。 max_parallel_maintenance_workersmax_parallel_maintenance_workers 参数设置维护命令(例如 CREATE INDEX) 命令允许的最大并行进程数，默认值为2。 max_parallel_workers设置系统支持的最大并行进程数，默认值为8。 max_worker_processes设置数据库的最大后台进程数，默认值为8。 max_parallel_workers_per_gather设置单个Gather或Gather Merge节点能够启用的最大并行进程数，默认值为2，并行查询(Parallel Query)的并行度受此参数的影响，CREATE INDEX 命令的并行度不受此参数影响。 以上参数可能并不容易理解，进一步解释如下： max_worker_processes 参数设置的是数据库允许的最大后台进程数，并行进程属于后台进程的一种； max_parallel_workers 参数设置数据库允许的最大并行进程数，这个值小于或等于 max_worker_processes。 并行进程数设置分为两类，第一类是并行查询，并行查询的并行度由 max_parallel_workers_per_gather 参数控制，第二类是维护命令(例如 CREATE INDEX)，维护命令的并行度由 max_parallel_maintenance_workers 参数控制。 max_parallel_workers_per_gather+max_parallel_maintenance_workers值应小于或等于 max_parallel_workers。 postgresql.conf 设置以下并行度参数如下：1234max_worker_processes = 16 # (change requires restart)max_parallel_maintenance_workers = 4 # taken from max_parallel_workersmax_parallel_workers_per_gather = 4 # taken from max_parallel_workersmax_parallel_workers = 8 测试环境准备创建测试表big并插入3000万条数据，如下：12CREATE TABLE big(user_id int4,user_name text,ctime timestamp(6) without time zone default clock_timestamp() );INSERT INTO big(user_id,user_name) SELECT n ,n || '_data' FROM generate_series(1,30000000) n;; 并行创建索引在会话级设置max_parallel_maintenance_workers值为4。12francs=&gt; set max_parallel_maintenance_workers =4;SET 创建索引，如下12francs=&gt; CREATE INDEX idx_big_ctime ON big USING BTREE(ctime);CREATE INDEX 在主机上通过 top 命令可以看到 CREATE INDEX 命令的进程号为 21164，并且开启了4个并发子进程。 创建索引并行度测试设置 max_parallel_maintenance_workers 值不同并行度，测试并行索引创建的时间。 本测试环境为一台4核8GB内存的虚机，测试结果如下： max_parallel_maintenance_workers 索引创建时间(毫秒) 0 14938.738 2 10469.283 4 10439.237 6 11577.147 8 17020.216 从以上看出，当 max_parallel_maintenance_workers 值为4时索引创建时间出现拐点。 关闭指定表并行创建索引通过前面介绍大家知道可通过设置max_parallel_maintenance_workers参数为0关闭所有表的并行创建索引，如何关闭指定表的并行索引创建呢？ 可通过 ALTER TABLE 方式禁止表上的并行创建索引，如下禁止表big上的所有并行创建索引。12francs=&gt; ALTER TABLE big SET (parallel_workers=0);ALTER TABLE 如果想恢复指定表上的parallel_workers参数设置，使用 RESET 选项即可，如下：12francs=&gt; ALTER TABLE big RESET(parallel_workers);ALTER TABLE 参考 CREATE TABLE WAITING FOR POSTGRESQL 11 – SUPPORT PARALLEL BTREE INDEX BUILDS. PostgreSQL10：Parallel Queries 增强","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Parallel Query","slug":"Parallel-Query","permalink":"https://postgres.fun/tags/Parallel-Query/"}]},{"title":"MySQL数据库迁移PostgreSQL实践","slug":"20180921202900","date":"2018-09-21T12:29:25.000Z","updated":"2018-09-26T13:03:43.754Z","comments":true,"path":"20180921202900.html","link":"","permalink":"https://postgres.fun/20180921202900.html","excerpt":"","text":"前几年杭州PostgreSQL交流会上分享了一个MySQL数据库迁移到PostgreSQL的案例，虽然时间有些久了，仍有参考意义，方便有需要的朋友。 这个PPT在我的百度网盘上可下载，如下： Converting MySQL to PostgreSQL","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"mysql_fdw","slug":"mysql-fdw","permalink":"https://postgres.fun/tags/mysql-fdw/"}]},{"title":"PostgreSQL11: 分区表增加 Default Partition","slug":"20180921101300","date":"2018-09-21T02:13:17.000Z","updated":"2018-11-13T00:18:02.333Z","comments":true,"path":"20180921101300.html","link":"","permalink":"https://postgres.fun/20180921101300.html","excerpt":"","text":"PostgreSQL 10 版本当往分区表写入的数据不在分区定义范围内时会报错，无法插入数据，PostgreSQL 11 版本分区表增加了 Default 分区用来存储不满足分区定义范围的数据。 本节以创建范围分区表为例进行测试。 PostgreSQL 10 版本在PostgreSQL 10 版本进行测试，详见以下。 创建父表12345create table tbl_log ( id serial, create_time timestamp(0) without time zone, remark char(1)) PARTITION BY RANGE(create_time); 创建子表12CREATE TABLE tbl_log_p201801 PARTITION OF tbl_log FOR VALUES FROM ('2018-01-01') TO ('2018-02-01');CREATE TABLE tbl_log_p201802 PARTITION OF tbl_log FOR VALUES FROM ('2018-02-01') TO ('2018-03-01'); 插入测试PostgreSQL 10 版本插入一条不在分区定义范围的记录，如下123mydb=&gt; INSERT INTO tbl_log(id,create_time,remark) VALUES (3,'2018-03-01','a');ERROR: no partition of relation \"tbl_log\" found for rowDETAIL: Partition key of the failing row contains (create_time) = (2018-03-01 00:00:00). 插入报错，提示找不到相应分区无法插入数据。 PostgreSQL 11 版本PostgreSQL 11 版本支持创建 Default 分区，用来存储不满足分区定义的数据，测试如下。 创建父表12345create table tbl_log ( id serial, create_time timestamp(0) without time zone, remark char(1)) PARTITION BY RANGE(create_time); 创建常规分区12CREATE TABLE tbl_log_p201801 PARTITION OF tbl_log FOR VALUES FROM ('2018-01-01') TO ('2018-02-01');CREATE TABLE tbl_log_p201802 PARTITION OF tbl_log FOR VALUES FROM ('2018-02-01') TO ('2018-03-01'); 创建默认分区1CREATE TABLE tbl_log_default PARTITION OF tbl_log DEFAULT; 查看表tbl_log定义123456789101112francs=&gt; \\d+ tbl_log Table \"francs.tbl_log\" Column | Type | Collation | Nullable | Default | Storage | Stats target | Description -------------+--------------------------------+-----------+----------+-------------------------------------+----------+--------------+------------- id | integer | | not null | nextval('tbl_log_id_seq'::regclass) | plain | | create_time | timestamp(0) without time zone | | | | plain | | remark | character(1) | | | | extended | | Partition key: RANGE (create_time)Partitions: tbl_log_p201801 FOR VALUES FROM ('2018-01-01 00:00:00') TO ('2018-02-01 00:00:00'), tbl_log_p201802 FOR VALUES FROM ('2018-02-01 00:00:00') TO ('2018-03-01 00:00:00'), tbl_log_default DEFAULT 以上看出 tbl_log 表包含三个分区，其中一个是 DEFAULT 分区。 插入数据12francs=&gt; INSERT INTO tbl_log(id,create_time,remark) VALUES (3,'2018-03-01','a');INSERT 0 1 验证数据可以直接查询子表数据验证，如下：12345francs=&gt; SELECT * FROM tbl_log_default ; id | create_time | remark ----+---------------------+-------- 3 | 2018-03-01 00:00:00 | a(1 row) 也可以统计各子表记录总数统计，如下：123456789francs=&gt; SELECT table_name,count(*) FROM tbl_log, LATERAL (SELECT relname FROM pg_class WHERE pg_class.oid=tbl_log.tableoid) AS table_nameGROUP BY table_name ORDER BY table_name; table_name | count -------------------+------- (tbl_log_default) | 1(1 row) 从以上测试看出，不满足分区定义的数据存储到了DEFAULT分区，那么问题来了，定义了DEFAULT分区的分区表如何添加分区？ 定义了Default分区的分区表如何添加分区？出于管理或业务需求，需要给分区表 tbl_log 新增 tbl_log_p201803 分区用来存储 2018年3月份的日志数据，如下。12francs=&gt; CREATE TABLE tbl_log_p201803 PARTITION OF tbl_log FOR VALUES FROM ('2018-03-01') TO ('2018-04-01');ERROR: updated partition constraint for default partition \"tbl_log_default\" would be violated by some row 以上添加分区报错，需要解绑default分区，之后再添加，如下： 解绑Default分区12francs=&gt; ALTER TABLE tbl_log DETACH PARTITION tbl_log_default;ALTER TABLE 之后再创建新分区 tbl_log_p201803。 创建新分区12francs=&gt; CREATE TABLE tbl_log_p201803 PARTITION OF tbl_log FOR VALUES FROM ('2018-03-01') TO ('2018-04-01');CREATE TABLE 分区创建成功，分区创建之后需把DEFAULT分区连接。 连接Default分区连接DEFAULT分区报错，如下:12francs=&gt; ALTER TABLE tbl_log ATTACH PARTITION tbl_log_default DEFAULT;ERROR: partition constraint is violated by some row 以上是由于 2018-03-01 的记录存储在了DEFAULT分区上，根据分区规则这条记录应该存储到 tbl_log_p201803 分区。 需将 tbl_log_default 数据转移到 tbl_log_p201803 分区，如下：12345francs=&gt; INSERT INTO tbl_log_p201803 SELECT * FROM tbl_log_default ;INSERT 0 1francs=&gt; DELETE FROM tbl_log_default ;DELETE 1 数据量大的话建议使用 TRUNCATE 清除数据，不产生垃圾数据。 再次连接DEFAULT分区成功，如下。12francs=&gt; ALTER TABLE tbl_log ATTACH PARTITION tbl_log_default DEFAULT;ALTER TABLE 查看表定义再次查看分区表定义，1234567891011121314francs=&gt; \\d+ tbl_log Table \"francs.tbl_log\" Column | Type | Collation | Nullable | Default | Storage | Stats target | Description -------------+--------------------------------+-----------+----------+-------------------------------------+----------+------------- id | integer | | not null | nextval('tbl_log_id_seq'::regclass) | plain | | create_time | timestamp(0) without time zone | | | | plain | | remark | character(1) | | | | extended | | Partition key: RANGE (create_time)Partitions: tbl_log_p201801 FOR VALUES FROM ('2018-01-01 00:00:00') TO ('2018-02-01 00:00:00'), tbl_log_p201802 FOR VALUES FROM ('2018-02-01 00:00:00') TO ('2018-03-01 00:00:00'), tbl_log_p201803 FOR VALUES FROM ('2018-03-01 00:00:00') TO ('2018-04-01 00:00:00'), tbl_log_default DEFAULT tbl_log_p201803 分区添加成功。 总结 PostgreSQL 11 版本分区表增加了DEFAULT分区，支持将不满足分区定义的数据存储到默认分区。 对于添加了DEFAULT分区的分区表如果想增加分区，需参照“解绑DEFAULT分区，创建新分区，转移分区数据，连接DEFAULT分区”的步骤进行。 参考 PostgreSQL11: 分区表增加哈希分区 PostgreSQL11: 分区表支持创建主键、外键、索引 PostgreSQL11: 分区表支持UPDATE分区键 PostgreSQL11: 分区表增加 Default Partition PostgreSQL11: Partitioning Improvements","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"PostgreSQL11: 分区表支持UPDATE分区键","slug":"20180920214600","date":"2018-09-20T13:46:22.000Z","updated":"2018-09-23T14:13:41.526Z","comments":true,"path":"20180920214600.html","link":"","permalink":"https://postgres.fun/20180920214600.html","excerpt":"","text":"PostgreSQL 10 版本已支持分区表，但不支持分区表根据分区键UPDATE记录，PostgreSQL 11 版本这方面得到增加，当分区表的分区键字段被UPDATE后，会自动将该记录转移至新的分区中。 官网Release说明如下： UPDATE statements that change a partition key now move affected rows to the appropriate partitions 本文以UPDATE列表分区表分区键为例进行演示。 测试环境准备创建列表分区表并插入测试数据，为演示做准备。 创建父表12345CREATE TABLE cities ( city_id bigserial not null, name text not null, population bigint) PARTITION BY LIST (left(lower(name), 1)); 创建子表123CREATE TABLE cities_a PARTITION OF cities ( CONSTRAINT city_id_nonzero CHECK (city_id != 0)) FOR VALUES IN ('a');CREATE TABLE cities_b PARTITION OF cities ( CONSTRAINT city_id_nonzero CHECK (city_id != 0)) FOR VALUES IN ('b');CREATE TABLE cities_c PARTITION OF cities ( CONSTRAINT city_id_nonzero CHECK (city_id != 0)) FOR VALUES IN ('c'); 插入测试数据123INSERT INTO cities(city_id,name,population) VALUES (1,'a_city','100000');INSERT INTO cities(city_id,name,population) VALUES (2,'b_city','200000');INSERT INTO cities(city_id,name,population) VALUES (3,'c_city','300000'); PostgreSQL 10 测试PostgreSQL 10 版本UPDATE分区键报错，如下：12345678910mydb=&gt; SELECT version(); version--------------------------------------------------------------------------------------------------------- PostgreSQL 10.0 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-11), 64-bit(1 row)mydb=&gt; UPDATE cities SET name='ca_city' WHERE city_id=1;ERROR: new row for relation \"cities_a\" violates partition constraintDETAIL: Failing row contains (1, ca_city, 100000). PostgreSQL 11 测试PostgreSQL 11 版本支持更新分区键，如下:12345678910111213141516francs=&gt; SELECT version(); version------------------------------------------------------------------------------------------------------------ PostgreSQL 11beta3 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-11), 64-bit(1 row)francs=&gt; UPDATE cities SET name='ca_city' WHERE city_id=1;UPDATE 1francs=&gt; SELECT * FROM cities; city_id | name | population---------+---------+------------ 2 | b_city | 200000 3 | c_city | 300000 1 | ca_city | 100000(3 rows) 查看cities_c分区，发现city_id为1的记录已转移到此分区，如下：123456francs=&gt; SELECT * from cities_c; city_id | name | population---------+---------+------------ 3 | c_city | 300000 1 | ca_city | 100000(2 rows) 参考 PostgreSQL10: 重量级新特性-支持分区表 PostgreSQL11: 分区表增加哈希分区 PostgreSQL11: 分区表支持创建主键、外键、索引 PostgreSQL11: 分区表支持UPDATE分区键 PostgreSQL11: 分区表增加 Default Partition PostgreSQL11: Partitioning Improvements","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"PostgreSQL11：分区表支持创建主键、外键、索引","slug":"20180920155600","date":"2018-09-20T07:56:18.000Z","updated":"2018-09-23T14:14:04.403Z","comments":true,"path":"20180920155600.html","link":"","permalink":"https://postgres.fun/20180920155600.html","excerpt":"","text":"PostgreSQL 10 版本虽然支持创建范围分区表和列表分区表，但创建过程依然比较繁琐，需要手工定义子表索引、主键，详见 PostgreSQL10：重量级新特性-支持分区表，PostgreSQL 11 版本得到增强，在父表上创建索引、主键、外键后，子表上将自动创建，本文演示这三种场景。 值得一提的是，11 版本之前 PostgreSQL 的分区表不支持全局主键，虽然可以在父表和子表上分别定义主键，但不支持全局主键，也就是说，父表和子表、子表和子表的主键数据可以重复。 手册上的 Release 说明 Support for PRIMARY KEY, FOREIGN KEY, indexes, and triggers on partitioned tables 本文以创建哈希分区表为例进行测试。 测试环境准备创建分区表并插入测试数据，为后续测试做准备。 创建父表12345CREATE TABLE userinfo ( userid int4, username character varying(64), ctime timestamp(6) without time zone) PARTITION BY HASH(userid); 创建子表1234CREATE TABLE userinfo_p0 PARTITION OF userinfo FOR VALUES WITH(MODULUS 4, REMAINDER 0);CREATE TABLE userinfo_p1 PARTITION OF userinfo FOR VALUES WITH(MODULUS 4, REMAINDER 1);CREATE TABLE userinfo_p2 PARTITION OF userinfo FOR VALUES WITH(MODULUS 4, REMAINDER 2);CREATE TABLE userinfo_p3 PARTITION OF userinfo FOR VALUES WITH(MODULUS 4, REMAINDER 3); 插入数据给分区表插入100万数据，如下:1INSERT INTO userinfo(userid,username,ctime) SELECT n, n || '_username',clock_timestamp() FROM generate_series(1,1000000) n; 测试一: 创建主键在父表上创建主键，如下。1ALTER TABLE userinfo ADD PRIMARY KEY (userid); 在父表上查看，如下。1234567891011francs=&gt; \\d userinfo Table \"francs.userinfo\" Column | Type | Collation | Nullable | Default----------+--------------------------------+-----------+----------+--------- userid | integer | | not null | username | character varying(64) | | | ctime | timestamp(6) without time zone | | |Partition key: HASH (userid)Indexes: \"userinfo_pkey\" PRIMARY KEY, btree (userid)Number of partitions: 4 (Use \\d+ to list them.) 查看子表，发现子表上也有了主键。12345678910francs=&gt; \\d userinfo_p0 Table \"francs.userinfo_p0\" Column | Type | Collation | Nullable | Default----------+--------------------------------+-----------+----------+--------- userid | integer | | not null | username | character varying(64) | | | ctime | timestamp(6) without time zone | | |Partition of: userinfo FOR VALUES WITH (modulus 4, remainder 0)Indexes: \"userinfo_p0_pkey\" PRIMARY KEY, btree (userid) 此主键为全局主键，子表间的主键之间不能有重复数据。 测试二: 创建索引在父表上创建索引，如下12francs=&gt; CREATE INDEX idx_userinfo_username ON userinfo USING BTREE(username);CREATE INDEX 发现父表和子表上都创建了索引，如下。123456789101112131415161718192021222324francs=&gt; \\d userinfo Table \"francs.userinfo\" Column | Type | Collation | Nullable | Default----------+--------------------------------+-----------+----------+--------- userid | integer | | not null | username | character varying(64) | | | ctime | timestamp(6) without time zone | | |Partition key: HASH (userid)Indexes: \"userinfo_pkey\" PRIMARY KEY, btree (userid) \"idx_userinfo_username\" btree (username)Number of partitions: 4 (Use \\d+ to list them.)francs=&gt; \\d userinfo_p1 Table \"francs.userinfo_p1\" Column | Type | Collation | Nullable | Default----------+--------------------------------+-----------+----------+--------- userid | integer | | not null | username | character varying(64) | | | ctime | timestamp(6) without time zone | | |Partition of: userinfo FOR VALUES WITH (modulus 4, remainder 1)Indexes: \"userinfo_p1_pkey\" PRIMARY KEY, btree (userid) \"userinfo_p1_username_idx\" btree (username) 测试三: 创建外键例如两张表，supplier_groups 和 supplier 分别用来存储供应商分组和供应商信, 如下。 12345678910CREATE TABLE supplier_groups( group_id int4 PRIMARY KEY, group_name text);CREATE TABLE suppliers ( supplier_id int4 PRIMARY KEY, supplier_name text, group_id int4 REFERENCES supplier_groups(group_id)) PARTITION BY HASH (supplier_id); 创建子表1234CREATE TABLE suppliers_p0 PARTITION OF suppliers FOR VALUES WITH(MODULUS 4, REMAINDER 0);CREATE TABLE suppliers_p1 PARTITION OF suppliers FOR VALUES WITH(MODULUS 4, REMAINDER 1);CREATE TABLE suppliers_p2 PARTITION OF suppliers FOR VALUES WITH(MODULUS 4, REMAINDER 2);CREATE TABLE suppliers_p3 PARTITION OF suppliers FOR VALUES WITH(MODULUS 4, REMAINDER 3); 查看子表查看子表，发现子表上也自动创建了外键。123456789101112francs=&gt; \\d suppliers_p0 Table \"francs.suppliers_p0\" Column | Type | Collation | Nullable | Default---------------+---------+-----------+----------+--------- supplier_id | integer | | not null | supplier_name | text | | | group_id | integer | | |Partition of: suppliers FOR VALUES WITH (modulus 4, remainder 0)Indexes: \"suppliers_p0_pkey\" PRIMARY KEY, btree (supplier_id)Foreign-key constraints: \"suppliers_group_id_fkey\" FOREIGN KEY (group_id) REFERENCES supplier_groups(group_id) 总结以上演示了 PostgreSQL 11 分区表在父表上创建索引、主键、外键后，子表会自动创建相应索引、主键、外键，相比10版本极大减少了分区表维护工作量。 参考 PostgreSQL10: 重量级新特性-支持分区表 PostgreSQL11: 分区表增加哈希分区 PostgreSQL11: 分区表支持创建主键、外键、索引 PostgreSQL11: 分区表支持UPDATE分区键 PostgreSQL11: 分区表增加 Default Partition PostgreSQL11: Partitioning Improvements","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"PostgreSQL11: 分区表增加哈希分区","slug":"20180920082700","date":"2018-09-20T00:27:30.000Z","updated":"2018-09-21T12:50:13.506Z","comments":true,"path":"20180920082700.html","link":"","permalink":"https://postgres.fun/20180920082700.html","excerpt":"","text":"PostgreSQL 11 Beta3 版本已经发布，最近忙于工作和家里事情没有研究，后面抽时间补上。 PostgreSQL 11 的一个重量级新特性为分区表得到较大增强，例如支持哈希分区(HASH)表，因此 PostgreSQL 支持范围分区(RANGE)、列表分区(LIST)、&gt;哈希分区(HASH)三种分区方式，本文简单演示下哈希分区表。 Hash Partitioning The table is partitioned by specifying a modulus and a remainder for each partition. Each partition will hold the rows for which the hash value of the partition key divided by the specified modulus will produce the specified remainder. Hash分区表的分区定义包含两个属性，如下： modulus: 指Hash分区个数。 remainder: 指Hash分区键取模余。 创建分区表语法12345CREATE TABLE table_name ( ... )[ PARTITION BY &#123; RANGE | LIST | HASH &#125; ( &#123; column_name | ( expression ) &#125; CREATE TABLE table_namePARTITION OF parent_table [ () ] FOR VALUES partition_bound_spec 创建数据生成函数为了便于生成测试数据，创建以下两个函数用来随机生成指定长度的字符串，创建 random_range(int4, int4) 函数如下：123456CREATE OR REPLACE FUNCTION random_range(int4, int4)RETURNS int4LANGUAGE SQLAS $$ SELECT ($1 + FLOOR(($2 - $1 + 1) * random() ))::int4;$$; 接着创建random_text_simple(length int4)函数，此函数会调用random_range(int4, int4)函数。12345678910111213141516171819CREATE OR REPLACE FUNCTION random_text_simple(length int4)RETURNS textLANGUAGE PLPGSQLAS $$DECLARE possible_chars text := '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'; output text := ''; i int4; pos int4;BEGIN FOR i IN 1..length LOOP pos := random_range(1, length(possible_chars)); output := output || substr(possible_chars, pos, 1); END LOOP; RETURN output;END;$$; random_text_simple(length int4)函数可以随机生成指定长度字符串，如下随机生成含三位字符的字符串。12345mydb=&gt; SELECT random_text_simple(3); random_text_simple -------------------- LL9(1 row) 随机生成含六位字符的字符串，如下所示：12345mydb=&gt; SELECT random_text_simple(6); random_text_simple -------------------- B81BPW(1 row) 后面会用到这个函数生成测试数据。 创建哈希分区父表1234CREATE TABLE student ( stuname text , ctime timestamp(6) without time zone) PARTITION BY HASH(stuname); 创建索引1CREATE INDEX idx_stuendt_stuname on student using btree(stuname); 创建子表1234CREATE TABLE student_p0 PARTITION OF student FOR VALUES WITH(MODULUS 4, REMAINDER 0);CREATE TABLE student_p1 PARTITION OF student FOR VALUES WITH(MODULUS 4, REMAINDER 1);CREATE TABLE student_p2 PARTITION OF student FOR VALUES WITH(MODULUS 4, REMAINDER 2);CREATE TABLE student_p3 PARTITION OF student FOR VALUES WITH(MODULUS 4, REMAINDER 3); 查看分区表定义12345678910111213francs=&gt; \\d+ student Table \"francs.student\" Column | Type | Collation | Nullable | Default | Storage | Stats target | Description ---------+--------------------------------+-----------+----------+---------+----------+--------------+------------- stuname | text | | | | extended | | ctime | timestamp(6) without time zone | | | | plain | | Partition key: HASH (stuname)Indexes: \"idx_stuendt_stuname\" btree (stuname)Partitions: student_p0 FOR VALUES WITH (modulus 4, remainder 0), student_p1 FOR VALUES WITH (modulus 4, remainder 1), student_p2 FOR VALUES WITH (modulus 4, remainder 2), student_p3 FOR VALUES WITH (modulus 4, remainder 3) 从以上看出表 student 和它的四个分区。 插入测试数据使用之前创建的函数 random_text_simple() 生成100万测试数据，如下。1INSERT INTO student(stuname,ctime) SELECT random_text_simple(6),clock_timestamp() FROM generate_series(1,1000000); 查看分区表数据表数据如下1234567francs=&gt; SELECT * FROM student LIMIT 3; stuname | ctime ---------+--------------------- 4JJOPN | 2018-09-20 10:45:06 NHQONC | 2018-09-20 10:45:06 8V5BGH | 2018-09-20 10:45:06(3 rows) 统计分区数据量12345678francs=&gt; SELECT tableoid::regclass,count(*) from student group by 1 order by 1; tableoid | count ------------+-------- student_p0 | 250510 student_p1 | 249448 student_p2 | 249620 student_p3 | 250422(4 rows) 可见数据均匀分布到了四个分区。 根据分区键查询1234567891011francs=&gt; EXPLAIN ANALYZE SELECT * FROM student WHERE stuname='3LXBEV'; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ Append (cost=0.42..8.44 rows=1 width=15) (actual time=0.017..0.018 rows=1 loops=1) -&gt; Index Scan using student_p3_stuname_idx on student_p3 (cost=0.42..8.44 rows=1 width=15) (actual time=0.017..0.017 rows=1 loops=1) Index Cond: (stuname = '3LXBEV'::text) Planning Time: 0.198 ms Execution Time: 0.042 ms(5 rows) 根据分区键stuname查询仅扫描分区 student_p3，并走了索引。 根据非分区键查询12345678910111213141516171819202122francs=&gt; EXPLAIN ANALYZE SELECT * FROM student WHERE ctime='2018-09-20 10:53:55.48392'; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------- Gather (cost=1000.00..13761.36 rows=4 width=15) (actual time=37.891..39.183 rows=1 loops=1) Workers Planned: 2 Workers Launched: 2 -&gt; Parallel Append (cost=0.00..12760.96 rows=4 width=15) (actual time=23.753..35.006 rows=0 loops=3) -&gt; Parallel Seq Scan on student_p0 (cost=0.00..3196.99 rows=1 width=15) (actual time=0.014..28.550 rows=1 loops=1) Filter: (ctime = '2018-09-20 10:53:55.48392'::timestamp without time zone) Rows Removed by Filter: 250509 -&gt; Parallel Seq Scan on student_p3 (cost=0.00..3195.34 rows=1 width=15) (actual time=29.543..29.543 rows=0 loops=1) Filter: (ctime = '2018-09-20 10:53:55.48392'::timestamp without time zone) Rows Removed by Filter: 250422 -&gt; Parallel Seq Scan on student_p2 (cost=0.00..3185.44 rows=1 width=15) (actual time=8.260..8.260 rows=0 loops=3) Filter: (ctime = '2018-09-20 10:53:55.48392'::timestamp without time zone) Rows Removed by Filter: 83207 -&gt; Parallel Seq Scan on student_p1 (cost=0.00..3183.18 rows=1 width=15) (actual time=22.135..22.135 rows=0 loops=1) Filter: (ctime = '2018-09-20 10:53:55.48392'::timestamp without time zone) Rows Removed by Filter: 249448 Planning Time: 0.183 ms Execution Time: 39.219 ms(18 rows) 根据非分区键ctime查询扫描了分区表所有分区。 总结本文演示了 PostgreSQL 哈希分区表的创建、测试数据的生成导入和查询计划，后面博客演示分区表增强的其它方面。 参考 CREATE TABLE Table Partitioning WAITING FOR POSTGRESQL 11 – ADD HASH PARTITIONING PostgreSQL11: 分区表增加哈希分区 PostgreSQL11: 分区表支持创建主键、外键、索引 PostgreSQL11: 分区表支持UPDATE分区键 PostgreSQL11: 分区表增加 Default Partition PostgreSQL11: Partitioning Improvements","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"新书推荐 |《PostgreSQL实战》出版","slug":"20180727084300","date":"2018-07-27T00:44:03.000Z","updated":"2018-11-06T06:22:48.085Z","comments":true,"path":"20180727084300.html","link":"","permalink":"https://postgres.fun/20180727084300.html","excerpt":"","text":"很高兴《PostgreSQL实战》一书终于出版，本书大体上系统总结了笔者 PostgreSQL DBA 职业生涯的经验总结，本书的另一位作者张文升拥有丰富的PostgreSQL运维经验，目前就职于探探科技任首席PostgreSQL DBA，他的加入极大地丰富了此书的内容。 本书基于 PostgreSQL 10 编写，以实践为主，希望对 PGer 带来帮助。 作者: 谭峰、张文升出版日期: 2018年7月页数: 415页定价: 89元 本书特色中国开源软件推进联盟PostgreSQL分会特聘专家撰写，国内多位开源数据库专家鼎力推荐。 基于PostgreSQL 10 编写，重点介绍SQL高级特性、并行查询、分区表、物理复制、逻辑复制、备份恢复、高可用、性能优化、PostGIS等，涵盖大量实战用 内容简介本书由资深数据库专业开发人员撰写，系统介绍PostgreSQL 10的丰富特性，及其在生产实践运维中的技巧，全书分为基础篇、核心篇、进阶篇，共18章。基础篇包括第1~4章，主要介绍PostgreSQL基础知识，例如安装与配置、客户端工具、数据类型、SQL高级特性等，为读者阅读核心篇和进阶篇做好准备；核心篇包括第5~9章，主要介绍PostgreSQL核心内容，例如体系结构、并行查询、事务与并发控制、分区表等；进阶篇包括第10~18章，主要介绍PostgreSQL进阶内容，例如性能优化、物理复制、逻辑复制、备份与恢复、高可用、版本升级、扩展模块、Oracle数据库迁移PostgreSQL实战、PostGIS等。 作者简介谭峰网名francs，中国开源软件推进联盟PostgreSQL分会特聘专家，《PostgreSQL 9 Administration Cookbook》译者之一，《PostgreSQL High Performance Cookbook》英文版技术审校者之一，曾在杭州斯凯网络科技有限公司从事PostgreSQL DBA一职六年。热忠于博客分享PostgreSQL经验，分享技术博客500余篇。现就职于浙江移动负责应用上云架构管控、资源分配以及私有云建设工作。 张文升中国开源软件推进联盟PostgreSQL分会核心成员之一。常年活跃于PostgreSQL、MySQL、Redis等开源技术社区，坚持推动PostgreSQL在中国地区的发展，多次参与组织PostgreSQL全国用户大会。近年来致力于推动PostgreSQL在互联网企业的应用以及企业PostgreSQL培训与技术支持。 购买链接京东: https://item.jd.com/12405774.html当当网: http://product.dangdang.com/25310839.html 样章试读https://pan.baidu.com/s/1QeKlAkU8Prpo9lW3JFGSoA 本书目录篇幅有限，完整目录详见购买链接。 第1章 安装与配置基础1.1 初识PostgreSQL21.2 安装PostgreSQL31.3 客户端程序和服务器程序81.4 创建数据库实例111.5 启动和停止数据库服务器141.6 数据库配置基础171.7 本章小结22 第2章 客户端工具2.1 pgAdmin 4简介232.2 psql功能及应用262.3 本章小结43 第3章 数据类型3.1 数字类型443.2 字符类型473.3 时间/日期类型493.4 布尔类型523.5 网络地址类型533.6 数组类型563.7 范围类型603.8 json/jsonb类型633.9 数据类型转换683.10 本章小结70 第4章 SQL高级特性4.1 WITH查询714.2 批量插入744.3 RETURNING返回修改的数据764.4 UPSERT784.5 数据抽样804.6 聚合函数844.7 窗口函数864.8 本章小结93 第5章 体系结构5.1 逻辑和物理存储结构965.2 进程结构1055.3 内存结构1065.4 本章小结107 第6章 并行查询6.1 并行查询相关配置参数1096.2 并行扫描1116.3 并行聚合1176.4 多表关联1196.5 本章小结124 第7章 事务与并发控制7.1 事务和并发控制的概念1257.2 PostgreSQL的事务隔离级别1307.3 PostgreSQL的并发控制1337.4 本章小结141 第8章 分区表8.1 分区表的意义1428.2 传统分区表1438.3 内置分区表1558.4 本章小结163 第9章 PostgreSQL的NoSQL特性9.1 为jsonb类型创建索引1649.2 json、jsonb读写性能测试1659.3 全文检索对json和jsonb数据类型的支持1699.4 本章小结176 第10章 性能优化10.1 服务器硬件18010.2 操作系统优化18110.3 数据库调优19310.4 本章小结203 第11章 基准测试与pgbench11.1 关于基准测试20411.2 使用pgbench进行测试20611.3 本章小结214 第12章 物理复制和逻辑复制12.1 异步流复制21612.2 同步流复制22412.3 单实例、异步流复制、同步流复制性能测试22712.4 流复制监控23112.5 流复制主备切换23612.6 延迟备库24412.7 同步复制优选提交24712.8 级联复制25112.9 流复制维护生产案例25512.10 逻辑复制26512.11 本章小结280 第13章 备份与恢复13.1 备份与恢复概述28113.2 增量备份28313.3 指定时间和还原点的恢复28813.4 SQL转储和文件系统级别的备份29813.5 本章小结301 第14章 高可用14.1 Pgpool-II+异步流复制实现高可用30314.2 基于Keepalived+异步流复制实现高可用32114.3 本章小结333 第15章 版本升级15.1 版本介绍33415.2 小版本升级33515.3 大版本升级33615.4 本章小结350 第16章 扩展模块16.1 CREATE EXTENSION35116.2 pg_stat_statements35316.3 auto_explain35616.4 pg_prewarm35716.5 file_fdw35916.6 postgres_fdw36416.7 Citus36916.8 本章小结377 第17章 Oracle数据库迁移PostgreSQL实践17.1 项目准备37817.2 数据库对象迁移37917.3 应用代码改造38017.4 数据迁移测试38417.5 功能测试和性能测试38817.6 生产割接38917.7 oracle_fdw部署过程中的常见错误38917.8 本章小结391 第18章 PostGIS18.1 安装与配置39218.2 创建GIS数据库39318.3 几何对象39318.4 应用场景：圈人与地理围栏39718.5 本章小结399","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Book","slug":"PostgreSQL-Book","permalink":"https://postgres.fun/tags/PostgreSQL-Book/"}]},{"title":"《PostgreSQL 10 High Performance》英文版技术审校","slug":"20180515204700","date":"2018-05-15T12:47:00.000Z","updated":"2018-12-14T07:42:51.209Z","comments":true,"path":"20180515204700.html","link":"","permalink":"https://postgres.fun/20180515204700.html","excerpt":"","text":"今天收到了 Packt Publishing 出版社寄出的 《PostgreSQL 10 High Performance》 一书的纸质版，这已经是第二次进行英文原版书籍的技术审校了，之前完成了 《PostgreSQL High Performance Cookbook》 英文书籍的审校并收到这本书的纸质版，至今记得当时是多么地兴奋，啥也不说，直接上图。 语言：英语出版日期：2018年4月30日作者：Ibrar Ahmed 、Gregory Smith、Enrico Pirozzi审稿：谭峰（Francs）、Srivathsava Rangarajan 主要内容本书基于PostgreSQL10 编写，难度中上，适合中级PostgreSQL DBA，着重从硬件、操作系统、文件系统、基准测试、缓存、参数配置、索引、查询计划、监控、连接池、数据分区方面介绍数据库性能优化。本书侧重介绍PostgreSQL性能优化，关于PostgreSQL的重要特性、复制技术方面没有做太多的介绍，在性能优化方面，本书值得推荐！ 章节目录这本书共十六章，如下： 1: POSTGRESQL VERSIONS 2: DATABASE HARDWARE 3: DATABASE HARDWARE BENCHMARKING 4: DISK SETUP 5: MEMORY FOR DATABASE CACHING 6: SERVER CONFIGURATION TUNING 7: ROUTINE MAINTENANCE 8: DATABASE BENCHMARKING 9: DATABASE INDEXING 10: QUERY OPTIMIZATION 11: DATABASE ACTIVITY AND STATISTICS 12: MONITORING AND TRENDING 13: POOLING AND CACHING 14: SCALING WITH REPLICATION 15: PARTITIONING DATA 16: AVOIDING COMMON PROBLEMS 关于技术审校Packt Publishing 的技术书籍出版前会联系行业相关人士做技术审校，技术审校者主要从技术方面审校全书的内容，审校者对全书的内容反馈建议。书籍出版后，作为对技术审校者工作的回报，Packt Publishing 出版社一方面将技术审校者的姓名公布在书籍上，另一方面会寄一本纸质书籍给技术审校者，以下是关于我的个人介绍。 希望国内有更多的同行加入到外版书籍技术审校工作中。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Book","slug":"PostgreSQL-Book","permalink":"https://postgres.fun/tags/PostgreSQL-Book/"}]},{"title":"PostgreSQL 10 新特性汇总 ","slug":"20170710155008","date":"2017-07-10T07:50:08.000Z","updated":"2018-11-02T00:29:59.272Z","comments":true,"path":"20170710155008.html","link":"","permalink":"https://postgres.fun/20170710155008.html","excerpt":"","text":"PostgreSQL10Beta1 版本于 2017年5月18日发行，PostgreSQL 10 新增了大量新特性，其中特重量级新特性如下： 内置分区表（ Native Table Partitioning） 逻辑复制（Logical Replication） 并行功能增强（Enhancement of Parallel Query） Quorum Commit for Synchronous Replication 全文检索支持JSON和JSONB数据类型 其它新特性详见 PostgreSQL10 Release ，这里不详细列出，由于时间和精力的关系，目前仅对部分新特性进行演示，详见以下博客： PostgreSQL10：重量级新特性-支持分区表 PostgreSQL10：Parallel Queries 增强 PostgreSQL10：Additional FDW Push-Down PostgreSQL10：逻辑复制（Logical Replication）之一 PostgreSQL10：逻辑复制（Logical Replication）之二 PostgreSQL10：Quorum Commit for Synchronous Replication PostgreSQL10：Multi-column Correlation Statistics PostgreSQL10：新增 pg_hba_file_rules 视图 PostgreSQL10：全文检索支持 JSON 和 JSONB PostgreSQL10：Identity Columns 特性介绍 PostgreSQL10：Incompatible Changes PostgreSQL10：新增 pg_sequence 系统表","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL:10：新增 pg_sequence 系统表","slug":"20170701151506","date":"2017-07-01T07:15:06.000Z","updated":"2018-09-04T01:34:21.816Z","comments":true,"path":"20170701151506.html","link":"","permalink":"https://postgres.fun/20170701151506.html","excerpt":"","text":"Pg_sequence patch 说明 Add pg_sequence system catalogMove sequence metadata (start, increment, etc.) into a proper systemcatalog instead of storing it in the sequence heap object. Thisseparates the metadata from the sequence data. Sequence metadata is nowoperated on transactionally by DDL commands, whereas previouslyrollbacks of sequence-related DDL commands would be ignored.Reviewed-by: Andreas Karlsson Release Note 中的说明 Create a pg_sequence system catalog to store sequence metadata (Peter Eisentraut) Sequence metadata includes start, increment, etc, which is now transactional. Sequence counters are still stored in separate heap relations. Also add pg_sequences view to show all sequences. PostgreSQL10 增加 pg_sequence 系统表存储序列的元数据，元数据是指序列的 seqstart、seqincremnet、seqmax、seqmin、seqcache 等值，但序列的 last value 依然存储在序列本身中；10版本之前序列所有以上信息都存储在序列中。 PostgreSQL9.6 序列创建测试表, serial 类型默认创建序列123456789francs=&gt; create table test_seq(id serial,name text);CREATE TABLEfrancs=&gt; \\d test_seqTable \"francs.test_seq\"Column | Type | Modifiers --------+---------+-------------------------------------------------------id | integer | not null default nextval('test_seq_id_seq'::regclass)name | text| 查看序列, 注意 last_value 值为 1123456789101112francs=&gt; select * from test_seq_id_seq ;-[ RECORD 1 ]-+--------------------sequence_name | test_seq_id_seqlast_value | 1start_value | 1increment_by | 1max_value | 9223372036854775807min_value | 1cache_value | 1log_cnt | 0is_cycled | fis_called | f 插入三条数据12francs=&gt; insert into test_seq(name) values('a'),('b'),('c');INSERT 0 3 再次查看序列, 注意 last_value 值为 3 123456789101112francs=&gt; select * from test_seq_id_seq ;-[ RECORD 1 ]-+--------------------sequence_name | test_seq_id_seqlast_value | 3start_value | 1increment_by | 1max_value | 9223372036854775807min_value | 1cache_value | 1log_cnt | 30is_cycled | fis_called | t 备注：PostgreSQL9.6 序列的元数据、last_values 值都存在序列中。 PostgreSQL10 序列创建测试表并查看序列1234567891011121314francs=&gt; create table test_seq(id serial,name text);CREATE TABLE francs=&gt; \\d test_seq Table \"francs.test_seq\"Column | Type | Collation | Nullable | Default --------+---------+-----------+----------+--------------------------------------id | integer | | not null | nextval('test_seq_id_seq'::regclass)name | text| | | francs=&gt; select * from test_seq_id_seq ;last_value | log_cnt | is_called------------+---------+----------- 1 | 0 | f 备注：序列仅存储 last_value、log_cnt、is_called 值，序列的其它值存储在哪了呢？ 查看 pg_sequence1234567891011121314151617181920212223242526francs=&gt; \\d pg_sequenceTable \"pg_catalog.pg_sequence\" Column | Type | Collation | Nullable | Default--------------+---------+-----------+----------+---------seqrelid | oid | | not null |seqtypid | oid | | not null |seqstart | bigint| | not null |seqincrement | bigint| | not null |seqmax | bigint| | not null |seqmin | bigint| | not null |seqcache | bigint| | not null |seqcycle | boolean | | not null |Indexes: \"pg_sequence_seqrelid_index\" UNIQUE, btree (seqrelid)francs=&gt; select seqrelid::regclass,* from pg_sequence where seqrelid='test_seq_id_seq'::regclass;-[ RECORD 1 ]+----------------seqrelid | test_seq_id_seqseqrelid | 24996seqtypid | 23seqstart | 1seqincrement | 1seqmax | 2147483647seqmin | 1seqcache | 1seqcycle | f 备注：序列的 seqstart、seqincrement、seqmax、seqmin、seqcache、seqcycle 元数据存储在了新增加 pg_sequence 系统表中， 10版本的这个特性变化值得关注，可通过 pg_sequence 系统表查看一个数据库中所有序列属性；而序列的last_value、log_cnt、is_called 值依然存储在序列中。 参考 WAITING FOR POSTGRESQL 10 – ADD PG_SEQUENCE SYSTEM CATALOG","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL10 ：Incompatible Changes","slug":"20170625210855","date":"2017-06-25T13:08:55.000Z","updated":"2018-09-04T01:34:21.754Z","comments":true,"path":"20170625210855.html","link":"","permalink":"https://postgres.fun/20170625210855.html","excerpt":"","text":"PostgreSQL10 的 Incompatible changes 文档中有详细描述，以下列举主要 Incompatible changes，这些 Incompatible change 会影响到日常的数据库维护工作，需要注意： 1、Change the default log directory from pg_log to log (Andreas Karlsson)log_directory (string) 参数默认值由 pg_log 调整成 log。 2、Rename pg_xlog to pg_wal (Michael Paquier)This prevents the write-ahead log directory from being confused as containing server activity logs, and erroneously truncated.$PGDATA 目录下的 pg_xlog 目录调整成 pg_wal， pg_wal 目录用来存放 WAL 日志。 3、Rename SQL functions, tools, and options that reference “xlog” to “wal” (Robert Haas)For example, pg_switch_xlog() becomes pg_switch_wal(), pg_receivexlog becomes pg_receivewal,and pg_basebackup –xlog-method becomes –wal-methodxlog 相关函数调整成 WAL，上面举了几个例子，例如 pg_switch_xlog() 调整成 pg_switch_wal()， pg_switch_xlog() 调整成 pg_switch_wal(), pg_basebackup 的–xlog-method 选项调整成 –wal-method。 4、Rename WAL-related functions and views to use lsn instead of locationWAL 相关的函数或视图的 location 调整成了 LSN， 例如pg_current_xlog_location 调整成 pg_current_wal_lsn， pg_current_xlog_insert_location 调整成 pg_current_wal_insert_lsn，pg_xlog_location_diff 调整成了 pg_wal_lsn_diff。 5、Rename transaction status directory pg_clog directory to pg_xact (Michael Paquier)transaction 状态目录 pg_clog 调整成 pg_xact。 6、Add GUCs min_parallel_table_scan_size and min_parallel_index_scan_size to control parallel operation (Amit Kapila, Robert Haas)This replaces min_parallel_relation_size, which was too generic.min_parallel_relation_size 参数被 min_parallel_table_scan_size 和 min_parallel_index_scan_size 替换。 7、Have pg_basebackup stream the WAL needed to restore the backup by default (Magnus Hagander)This changes the pg_basebackup -X/–xlog-method default to stream. An option value none has been added to recreate the old behavior. The pg_basebackup option -x has been removed (use -X fetch).a)pg_basebackup -X 选项默认值调整成 stream；b) -X 选项增加 none 参数；c) 不再支持 -x 参数。 8、Make all pg_ctl actions wait by default for completion (Peter Eisentraut)Previously some pg_ctl actions didn not wait for completion, and required the use of -w to do so.pg_ctl 所有操作默认为 wait，pg_ctl 会等待命令执行完成后再退出。 9、Remove the ability to store unencrypted passwords on the server (Heikki Linnakangas)The server-side variable password_encryption no longer supports off or plain. The UNENCRYPTED option is no longer supported for CREATE/ALTER USER … PASSSWORD. Similarly, the –unencrypted has been removed from createuser. The default for password_encryption is still md5, and users migrating passwords from older systems will have them stored encrypted by default in this release.CREATE/ALTER USER/createuser 不再支持非加密的密码。 10、pg_upgrade-ed hash indexes from previous major Postgres versions must be rebuilt.Major hash storage improvements necessitated this requirement.使用 pg_upgrade 升级 PostgreSQL 大版本后 Hash Index 需重建。 11、Remove pg_dump/pg_dumpall support for dumping from pre-8.0 servers (Tom Lane)Users needing dump support for pre-8.0 servers need to use dump binaries from Postgres 9.6.不支持 pg_dump/pg_dumpall 程序导出 8.0 版本之前的 PostgreSQL 库。 12、Remove createlang and droplang command-line applications (Peter Eisentraut)不再支持 createlang、droplang 操作系统命令。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL10：Identity Columns 特性介绍","slug":"20170615083732","date":"2017-06-15T00:37:32.000Z","updated":"2018-09-04T01:34:21.691Z","comments":true,"path":"20170615083732.html","link":"","permalink":"https://postgres.fun/20170615083732.html","excerpt":"","text":"Identity columns This is the SQL standard-conforming variant of PostgreSQL is serialcolumns. It fixes a few usability issues that serial columns have: CREATE TABLE / LIKE copies default but refers to same sequence cannot add/drop serialness with ALTER TABLE dropping default does not drop sequence need to grant separate privileges to sequence other slight weirdnesses because serial is some kind of special macro identity columns 和 serial 功能很像，是 PostgreSQL 对 SQL 兼容性的提升，并且修复了 serial 类型存在的以下问题： CREATE TABLE / LIKE 命令复制表时指定相同的序列 不能使用 ALTER TABLE 增加或删除 serialness 表删除 default 属性时不会删除序列 需要对序列进行额外赋权 接下来根据这四种情况进行验证，先来创建两张表，一张基于 serial 表，一张基于 identity columns表。 测试准备创建两张测试表，创建 serial表1234567891011francs=&gt; create table t_serial(id serial primary key, name text);CREATE TABLE francs=&gt; \\d t_serial Table \"francs.t_serial\"Column | Type | Collation | Nullable | Default --------+---------+-----------+----------+--------------------------------------id | integer | | not null | nextval('t_serial_id_seq'::regclass)name | text| | |Indexes: \"t_serial_pkey\" PRIMARY KEY, btree (id) 创建 identity 表123456789101112131415161718francs=&gt; create table t_identity(id int8 generated by default AS IDENTITY primary key, name text);CREATE TABLE francs=&gt; \\d t_identityTable \"francs.t_identity\"Column | Type | Collation | Nullable | Default --------+--------+-----------+----------+----------------------------------id | bigint | | not null | generated by default as identityname | text | | |Indexes: \"t_identity_pkey\" PRIMARY KEY, btree (id) francs=&gt; \\dsList of relationsSchema | Name | Type | Owner --------+----------------------+----------+--------francs | t_identity_id_seq| sequence | francsfrancs | t_serial_id_seq| sequence | francs 插入数据测试123456789101112131415161718192021francs=&gt; insert into t_serial (name) values ('a'),('b'),('c');INSERT 0 3 francs=&gt; select * from t_serial;id | name----+------ 1 | a 2 | b 3 | c(3 rows) francs=&gt; insert into t_identity (name) values('a'),('b'),('c');INSERT 0 3 francs=&gt; select * from t_identity;id | name----+------ 1 | a 2 | b 3 | c(3 rows) 备注：两张表同时插入数据，没啥不同。 都使用序列123456francs=&gt; \\dsList of relationsSchema | Name | Type | Owner --------+----------------------+----------+--------francs | t_identity_id_seq| sequence | francsfrancs | t_serial_id_seq| sequence | francs 区别一: Serial 表通过CREATE TABLE / LIKE 命令复制表时指定相同的序列通过 CREATE TABLE LIKE 命令复制一张 serial 表1234567891011francs=&gt; create table t_serial2 (like t_serial including all);CREATE TABLE francs=&gt; \\d t_serial2 Table \"francs.t_serial2\"Column | Type | Collation | Nullable | Default --------+---------+-----------+----------+--------------------------------------id | integer | | not null | nextval('t_serial_id_seq'::regclass)name | text| | |Indexes: \"t_serial2_pkey\" PRIMARY KEY, btree (id) 备注：可见 t_serial2 表也是用的 t_serial 的序列。 复制一张 identity 类型表1234567891011121314151617181920francs=&gt; create table t_identity2 ( like t_identity including all);CREATE TABLE francs=&gt; \\d t_identity2 Table \"francs.t_identity2\"Column | Type | Collation | Nullable | Default --------+--------+-----------+----------+----------------------------------id | bigint | | not null | generated by default as identityname | text | | |Indexes: \"t_identity2_pkey\" PRIMARY KEY, btree (id) francs=&gt; \\d+ t_identity2_id_seq Sequence \"francs.t_identity2_id_seq\"Column | Type | Value | Storage------------+---------+-------+---------last_value | bigint| 1 | plainlog_cnt | bigint| 0 | plainis_called | boolean | f | plainSequence for identity column: francs.t_identity2.id 备注：新复制的表 t_identity2 使用新建的序列 t_identity2_id_seq，而serial 表复制表后仍然使用老序列，到这里就很清楚了。 区别二: Serial 表不能使用 ALTER TABLE 增加或删除 serialnessserial 表需要分两步删除 serial 属性1234567891011121314francs=&gt; ALTER TABLE t_serial ALTER COLUMN id drop default;ALTER TABLE francs=&gt; drop sequence t_serial_id_seq;DROP SEQUENCE francs=&gt; \\d t_serial Table \"francs.t_serial\"Column | Type | Collation | Nullable | Default--------+---------+-----------+----------+---------id | integer | | not null |name | text| | |Indexes: \"t_serial_pkey\" PRIMARY KEY, btree (id) identity 表可通过 ALTER TABLE 一条命令删除字段 IDENTITY 属性1234567891011121314151617francs=&gt; ALTER TABLE t_identity ALTER COLUMN id DROP IDENTITY;ALTER TABLE francs=&gt; \\d t_identity Table \"francs.t_identity\"Column | Type | Collation | Nullable | Default--------+--------+-----------+----------+---------id | bigint | | not null |name | text | | |Indexes: \"t_identity_pkey\" PRIMARY KEY, btree (id) francs=&gt; insert into t_identity(name) values('4');ERROR: null value in column \"id\" violate francs=&gt; \\d t_identity_id_seqDid not find any relation named \"t_identity_id_seq\". 备注：删除 t_identity 表 id 字段 IDENTITY 属性的同时，序列 t_identity_id_seq 也被删除了。 区别三: Serial 表表删除 default 属性时不会删除序列这部分上面已经演示过了。 1234567891011francs=&gt; ALTER TABLE t_serial ALTER COLUMN id drop default;ALTER TABLE francs=&gt; \\d t_serial_id_seqSequence \"francs.t_serial_id_seq\"Column | Type | Value------------+---------+-------last_value | bigint| 1log_cnt | bigint| 0is_called | boolean | fOwned by: francs.t_serial.id 区别四: Serial 表需要对序列进行额外赋权创建 readonly 用户并赋权1234567891011121314postgres=# create role readonly login encrypted password 'readonly';CREATE ROLE postgres=# \\c francs francsYou are now connected to database \"francs\" as user \"francs\". francs=&gt; grant connect on database francs to readonly;GRANT francs=&gt; grant usage on schema francs to readonly;GRANT francs=&gt; grant select,insert on t_serial,t_identity to readonly;GRANT t_serial 表12345678910111213141516171819202122232425francs=&gt; \\c francs readonlyYou are now connected to database \"francs\" as user \"readonly\".francs=&gt; select * from francs.t_serial;id | name----+------ 1 | a 2 | b 3 | c(3 rows) francs=&gt; insert into francs.t_serial (name) values('4');ERROR: permission denied for sequence t_serial_id_seq提示没有权限，序列赋权后执行正常。 francs=&gt; \\c francs francsYou are now connected to database \"francs\" as user \"francs\". francs=&gt; grant usage on SEQUENCE t_serial_id_seq to readonly;GRANT francs=&gt; \\c francs readonlyYou are now connected to database \"francs\" as user \"readonly\". francs=&gt; insert into francs.t_serial (name) values('4');INSERT 0 1 t_identity 表1234567891011francs=&gt; \\c francs readonlyfrancs=&gt; select * from francs.t_identity;id | name----+------ 1 | a 2 | b 3 | c(3 rows) francs=&gt; insert into francs.t_identity (name) values('4');INSERT 0 1 备注：identity 表不需要额外给序列赋权，减少了维护成本。 参考 WAITING FOR POSTGRESQL 10 – IDENTITY COLUMNS PostgreSQL 10 identity columns explained","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL10：全文检索支持 JSON 和 JSONB","slug":"20170611204225","date":"2017-06-11T12:42:25.000Z","updated":"2018-09-04T01:34:21.629Z","comments":true,"path":"20170611204225.html","link":"","permalink":"https://postgres.fun/20170611204225.html","excerpt":"","text":"Add full text search support for JSON and JSONB (Dmitry Dolgov)This is accessed via ts_headline() and to_tsvector. PostgreSQ10 全文检索开始支持 JSON 和 JSONB 数据类型，to_tsvector 函数中的输入参数可以看到支持 JSON 和 JSONB。 一、to_tsvector 函数1234567891011121314151617181920212223PostgreSQL 10beta1francs=&gt; \\df *to_tsvector* List of functionsSchema | Name | Result data type | Argument data types | Type ------------+-------------------+------------------+---------------------+--------pg_catalog | array_to_tsvector | tsvector | text[] | normalpg_catalog | to_tsvector | tsvector | json| normalpg_catalog | to_tsvector | tsvector | jsonb | normalpg_catalog | to_tsvector | tsvector | regconfig, json | normalpg_catalog | to_tsvector | tsvector | regconfig, jsonb| normalpg_catalog | to_tsvector | tsvector | regconfig, text | normalpg_catalog | to_tsvector | tsvector | text| normal(7 rows) PostgreSQL 9.6.3des=&gt; \\df *to_tsvector* List of functionsSchema | Name | Result data type | Argument data types | Type ------------+-------------------+------------------+---------------------+--------pg_catalog | array_to_tsvector | tsvector | text[] | normalpg_catalog | to_tsvector | tsvector | regconfig, text | normalpg_catalog | to_tsvector | tsvector | text| normal(3 rows) 备注：接下来在10版本做个测试，验证JSON是否支持全文检索。 二、Json 全文检索测试创建 random_range （）函数123456CREATE OR REPLACE FUNCTION random_range(INTEGER, INTEGER) RETURNS INTEGER LANGUAGE SQL AS $$SELECT ($1 + FLOOR(($2 - $1 + 1) * random() ))::INTEGER; $$; 创建 random_text_simple（）函数12345678910111213141516171819 CREATE OR REPLACE FUNCTION random_text_simple(length INTEGER) RETURNS TEXT LANGUAGE PLPGSQL AS $$ DECLAREpossible_chars TEXT := '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ';output TEXT := ''; i INT4; pos INT4; BEGIN FOR i IN 1..length LOOPpos := random_range(1, length(possible_chars));output := output || substr(possible_chars, pos, 1); END LOOP; RETURN output; END; $$; 备注：random_text_simple(integer) 函数随机返回指定长度字符串，用来生产测试数据，示例如下：创建测试函数12345francs=&gt; select random_text_simple(6);random_text_simple--------------------T87GL1(1 row) 创建测试表并生成测试数据12345678910111213create table user_ini(id int4 ,user_id int8, user_name character varying(64),create_time timestamp(6) with time zone default clock_timestamp());insert into user_ini(id,user_id,user_name) select r,round(random()*1000000), random_text_simple(6) from generate_series(1,1000000) as r; create table tbl_user_json(id serial, user_info json);insert into tbl_user_json(user_info) select row_to_json(user_ini) from user_ini; francs=&gt; select * from tbl_user_json limit 3; id | user_info----------------------------------------------------------------------------------------6000001 | &#123;\"id\":1,\"user_id\":999960,\"user_name\":\"GO9H59\",\"create_time\":\"2017-06-11T17:48:57.178684+08:00\"&#125;6000002 | &#123;\"id\":2,\"user_id\":915581,\"user_name\":\"7HZEMH\",\"create_time\":\"2017-06-11T17:48:57.178881+08:00\"&#125;6000003 | &#123;\"id\":3,\"user_id\":68310,\"user_name\":\"L1P1OU\",\"create_time\":\"2017-06-11T17:48:57.178897+08:00\"&#125;(3 rows) JSON 数据全文检索测试12345678910111213141516171819francs=&gt; select * from tbl_user_json where to_tsvector('english',user_info) @@ to_tsquery('english','7HZEMH'); id | user_info---------+-------------------------------------------------------------------------------------------------6000002 | &#123;\"id\":2,\"user_id\":915581,\"user_name\":\"7HZEMH\",\"create_time\":\"2017-06-11T17:48:57.178881+08:00\"&#125;(1 row) francs=&gt; explain analyze select * from tbl_user_json where to_tsvector('english',user_info) @@ to_tsquery('english','7HZEMH'); QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------Gather (cost=1000.00..94627.42 rows=5000 width=104) (actual time=0.863..2964.579 rows=1 loops=1)Workers Planned: 3Workers Launched: 2-&gt; Parallel Seq Scan on tbl_user_json(cost=0.00..93127.42 rows=1613 width=104) (actual time=1972.931..2960.835 rows=0 loops=3)Filter: (to_tsvector('english'::regconfig, user_info) @@ '''7hzemh'''::tsquery)Rows Removed by Filter: 333333Planning time: 0.117 msExecution time: 2966.479 ms(8 rows) 备注：从上面示例看出， JSON 数据支持全文检索操作，只是速度慢，花了近3秒，接下来创建索引。 创建 gin 索引1234567891011121314francs=&gt; create index idx_gin_tbl_user_json_user_info on tbl_user_json using gin(to_tsvector('english',user_info));CREATE INDEX francs=&gt; explain analyze select * from tbl_user_json where to_tsvector('english',user_info) @@ to_tsquery('english','7HZEMH'); QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------------------Bitmap Heap Scan on tbl_user_json(cost=48.75..6471.39 rows=5000 width=104) (actual time=0.027..0.027 rows=1 loops=1)Recheck Cond: (to_tsvector('english'::regconfig, user_info) @@ '''7hzemh'''::tsquery)Heap Blocks: exact=1-&gt; Bitmap Index Scan on idx_gin_tbl_user_json_user_info(cost=0.00..47.50 rows=5000 width=0) (actual time=0.019..0.019 rows=1 loops=1)Index Cond: (to_tsvector('english'::regconfig, user_info) @@ '''7hzemh'''::tsquery)Planning time: 0.127 msExecution time: 0.058 ms 备注：创建 gin 索引后，走了 Bitmap Index Scan，执行时间下降到 0.058 ms。 三、总结今天只测试了全文检索对 JSON 数据类型的支持，JSONB 数据类型还没有测试，有兴趣的朋友可测试下，根据 to_tsvector 的定义，可以看到对 JSONB 是支持的；从10版本开始，PostgreSQL 对 JSON 、JSONB 的支持得到了增强。 四、参考 PostgreSQL 中文全文检索(之一)：Zhparser 安装及使用 PostgreSQL9.4: Jsonb 性能测试 PostgreSQL 9.4: 新增 Jsonb 数据类型","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"JSON/JSONB","slug":"JSON-JSONB","permalink":"https://postgres.fun/tags/JSON-JSONB/"}]},{"title":"中国开源软件推进联盟PostgreSQL分会成立发布会","slug":"20170611105747","date":"2017-06-11T02:57:47.000Z","updated":"2018-09-04T01:34:21.566Z","comments":true,"path":"20170611105747.html","link":"","permalink":"https://postgres.fun/20170611105747.html","excerpt":"","text":"2017年4月，由工信部批准，中国开源软件推进联盟PostgreSQL分会宣告成立，发布会将于6月21日在北京丽亭华苑酒店举行，届时将举行2017年PostgreSQL技术论坛，本次论坛将是一场汇聚各界PostgreSQL大拿、交流最新业界技术动态和真实应用案例的盛宴，欢迎各位PGer莅临交流！ 详见 http://postgresql621-tcwechatshare.eventdove.com/","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL10：新增 pg_hba_file_rules 视图","slug":"20170607205537","date":"2017-06-07T12:55:37.000Z","updated":"2018-09-04T01:34:21.503Z","comments":true,"path":"20170607205537.html","link":"","permalink":"https://postgres.fun/20170607205537.html","excerpt":"","text":"PostgreSQL10 新增pg_hba_file_rules视图，根据视图名字大概能猜到这个视图的内容，的确，此视图显示 $PGDATA/pg_hba.conf 文件中的内容，pg_hba.conf 中每一条正常或异常的策略在此视图中都会保存一行记录。 pg_hba_file_rules 视图一个重要的意义是能够快速诊断 pg_hba.conf 文件中的错误，以下会做测试。 pg_hba.conf 文件内容1234567891011121314151617# TYPE DATABASE USER ADDRESS METHOD # \"local\" is for Unix domain socket connections only#local francs francs md5#host francs francs 127.0.0.1/32 md5 local all all trust# IPv4 local connections:host all all 127.0.0.1/32 trust# IPv6 local connections:# Allow replication connections from localhost, by a user with the# replication privilege.host replication repuser 20.26.28.74/32 md5host replication repuser 20.26.28.75/32 md5host replication repuser 20.26.28.76/32 md5host all repuser 0.0.0.0/0 md5host all all 0.0.0.0/0 md5 备注： pg_hba.conf 中有7条策略。 查看 pg_hba_file_rules1234567891011postgres=# select line_number,type,database,user_name,address,netmask,auth_method,error from pg_hba_file_rules ;line_number | type | database | user_name | address | netmask | auth_method | error-------------+-------+---------------+-----------+-------------+-----------------+-------------+------- 87 | local | &#123;all&#125; | &#123;all&#125; | | | trust | 89 | host | &#123;all&#125; | &#123;all&#125; | 127.0.0.1 | 255.255.255.255 | trust | 93 | host | &#123;replication&#125; | &#123;repuser&#125; | 20.26.28.74 | 255.255.255.255 | md5 | 94 | host | &#123;replication&#125; | &#123;repuser&#125; | 20.26.28.75 | 255.255.255.255 | md5 | 95 | host | &#123;replication&#125; | &#123;repuser&#125; | 20.26.28.76 | 255.255.255.255 | md5 | 96 | host | &#123;all&#125; | &#123;repuser&#125; | 0.0.0.0 | 0.0.0.0 | md5 | 97 | host | &#123;all&#125; | &#123;all&#125; | 0.0.0.0 | 0.0.0.0 | md5 |(7 rows) 备注：刚好 pg_hba_file_rules 也有7条策略，内容和 pg_hba.conf 一致。接下来计划在 pg_hba.conf 文件中新增一条错误的策略。 pg_hba.conf 测试pg_hba.conf 文件末尾增加以下内容1host all all 0.0.0.0 md5 备注：IP字段故意没写掩码。 重新载入 pg_hba.conf12[pg10@csv-tfcs01 ~]$ pg_ctl reloadserver signaled 备注：pg_ctl reload 操作并没有返回异常信息，个人认为这里不太友好，需要到日志里查看 pg_hba.conf 中的策略是否填写正确。 查看 pg_log1232017-06-07 20:42:13.459 CST,,,5862,,592fceaf.16e6,7,,2017-06-01 16:22:07 CST,,0,LOG,00000,\"received SIGHUP, reloading configuration files\",,,,,,,,,\"\"2017-06-07 20:42:13.460 CST,,,5862,,592fceaf.16e6,8,,2017-06-01 16:22:07 CST,,0,LOG,F0000,\"invalid IP mask \"\"md5\"\": Name or service not known\",,,,,\"line 98 of configuration file \"\"/database/pg10/pg_root/pg_hba.conf\"\"\",,,,\"\"2017-06-07 20:42:13.460 CST,,,5862,,592fceaf.16e6,9,,2017-06-01 16:22:07 CST,,0,LOG,00000,\"pg_hba.conf was not reloaded\",,,,,,,,,\"\" 备注： pg_hba.conf 文件第98 行有报错信息。 查看 pg_hba_file_rules1234567891011121314postgres=# select line_number,type,database,user_name,address,netmask,auth_method,error from pg_hba_file_rules ;line_number | type | database | user_name | address | netmask | auth_method | error -------------+-------+---------------+-----------+-------------+-----------------+-------------+-------------------------------------------------- 87 | local | &#123;all&#125; | &#123;all&#125; | | | trust | 89 | host | &#123;all&#125; | &#123;all&#125; | 127.0.0.1 | 255.255.255.255 | trust | 93 | host | &#123;replication&#125; | &#123;repuser&#125; | 20.26.28.74 | 255.255.255.255 | md5 | 94 | host | &#123;replication&#125; | &#123;repuser&#125; | 20.26.28.75 | 255.255.255.255 | md5 | 95 | host | &#123;replication&#125; | &#123;repuser&#125; | 20.26.28.76 | 255.255.255.255 | md5 | 96 | host | &#123;all&#125; | &#123;repuser&#125; | 0.0.0.0 | 0.0.0.0 | md5 | 97 | host | &#123;all&#125; | &#123;all&#125; | 0.0.0.0 | 0.0.0.0 | md5 | 98 | | | | | | | invalid IP mask \"md5\": Name or service not known(8 rows) 备注：pg_hba_file_rules 增加了一条line_number=98记录， line_number 字段显示错误策略所在 pg_hba.conf 文件中的行号，error 字段显示具体错误信息。 注意 pg_hba_file_rules 只有超级用户才有读的权限； pg_hba.conf 异常的策略在 pg_hba_file_rules 视图中只有 line_number，error 两个字段有信息。 参考 pg_hba_file_rules","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL10：Multi-column Correlation Statistics","slug":"20170604203859","date":"2017-06-04T12:38:59.000Z","updated":"2018-09-04T01:34:21.457Z","comments":true,"path":"20170604203859.html","link":"","permalink":"https://postgres.fun/20170604203859.html","excerpt":"","text":"Multi-column Correlation Statistics123Add multi-column optimizer statistics to compute the correlation ratio and number of distinct values (Tomas Vondra, David Rowley, álvaro Herrera) Real-world data frequently contains correlated data in table columns, which can easily fool the query planner into thinking WHERE clauses are more selective than they really are, which can cause some queries to become very slow. Multivariate statistics objects can be used to let the planner learn about this, which proofs it against making such mistakes. This manual section explains the feature in more detail, and this section shows some examples. This feature in PostgreSQL represents an advance in the state of the art for all SQL databases. PostgreSQL10 增加多字段相关统计信息，听起来比较费解，通过以下例子演示下： 多字段相关性测试创建测试表12345678910111213francs=&gt; create table test_t( a int4, b int4);CREATE TABLE francs=&gt; insert into test_t(a,b) select n%100,n%100 from generate_series(1,10000) n;INSERT 0 10000 francs=&gt; select * from test_t limit 3;a | b---+---1 | 12 | 23 | 3(3 rows) 根据字段a查询执行计划如下:123456789francs=&gt; explain analyze select * from test_t where a=1; QUERY PLAN ------------------------------------------------------------------------------------------------------Seq Scan on test_t (cost=0.00..148.00 rows=100 width=8) (actual time=0.017..0.743 rows=100 loops=1)Filter: (a = 1)Rows Removed by Filter: 9900Planning time: 0.137 msExecution time: 0.765 ms(5 rows) 备注： Seq Scan on test_t (cost=0.00..148.00 rows=100 width=8) 表示执行计划分析的信息，这时SQL还没执行，(actual time=0.017..0.743 rows=100 loops=1) 表示SQL实际执行时返回的信息， 可以看到SQL实际扫描的行数和执行计划一致，都为 100 rows。 根据字段b查询执行计划如下:123456789francs=&gt; explain analyze select * from test_t where b=1; QUERY PLAN ------------------------------------------------------------------------------------------------------Seq Scan on test_t (cost=0.00..148.00 rows=100 width=8) (actual time=0.018..0.837 rows=100 loops=1)Filter: (b = 1)Rows Removed by Filter: 9900Planning time: 0.087 msExecution time: 0.861 ms(5 rows) 备注：可以看到SQL实际扫描的行数和执行计划一致，都为 100 rows。 根据 a=1 and b=1 查询执行计划如下：123456789francs=&gt; explain analyze select * from test_t where a=1 and b=1; QUERY PLAN ----------------------------------------------------------------------------------------------------Seq Scan on test_t (cost=0.00..173.00 rows=1 width=8) (actual time=0.029..0.765 rows=100 loops=1)Filter: ((a = 1) AND (b = 1))Rows Removed by Filter: 9900Planning time: 0.078 msExecution time: 0.790 ms(5 rows) 备注：这里执行计划的 rows=1，而实际SQL返回 rows=100，执行计划认为 a 字段和 b 字段为互不相关独立的两个字段，因此将两个字段的选择性相乘，0.01*0.01=0.0001，所以执行计划中的 rows=1；在生产案例中这种场景会得到错误的执行计划，影响SQL性能， PG 10 可以应对这个问题，支持多字段的相关性统计信息，如下： 创建统计信息CREATE STATISTICS12345678910111213141516francs=&gt; CREATE STATISTICS stts_test_t ON a, b FROM test_t;WARNING: unrecognized node type: 333CREATE STATISTICS francs=&gt; analyze test_t;ANALYZE francs=&gt; explain analyze select * from test_t where a=1 and b=1; QUERY PLAN ------------------------------------------------------------------------------------------------------Seq Scan on test_t (cost=0.00..173.00 rows=100 width=8) (actual time=0.018..0.741 rows=100 loops=1)Filter: ((a = 1) AND (b = 1))Rows Removed by Filter: 9900Planning time: 0.130 msExecution time: 0.765 ms(5 rows) 备注：创建 a,b 字段的相关性后，执行计划中的 rows 值变成 100 了。 查询 STATISTICS12345francs=&gt; select * from pg_statistic_ext where stxname='stts_test_t';stxrelid | stxname | stxnamespace | stxowner | stxkeys | stxkind | stxndistinct| stxdependencies----------+-------------+--------------+----------+---------+---------+---------------+------------------------------------------ 24875 | stts_test_t | 16387 | 16384 | 1 2 | &#123;d,f&#125; | &#123;\"1, 2\": 100&#125; | &#123;\"1 =&gt; 2\": 1.000000, \"2 =&gt; 1\": 1.000000&#125;(1 row) stxname 指 statistics 名称； stxkeys 指 statistics 对应的字段名称； stxkind 指 statistic 的类型，d=n-distinct statistics ，f=functional dependency statistics； stxndistinct 组合字段的 distinct 值。 注意事项 当SQL 语句where 中出现多个字段等于号后接常量的场景时(where a=? and b=? …)，可以使用此特性； 手册上指出 Functional Dependencies 目前仅支持等于号后接常量的场景 ，其它场景暂不支持，比如等于号后接表达式、&gt;=、&lt;=、LIKE 等操作 参考 implement-multivariate-n-distinct-coefficients CREATE STATISTICS Multivariate Statistics Examples","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL10：Quorum Commit for Synchronous Replication","slug":"20170601130120","date":"2017-06-01T05:01:20.000Z","updated":"2018-09-04T01:34:21.378Z","comments":true,"path":"20170601130120.html","link":"","permalink":"https://postgres.fun/20170601130120.html","excerpt":"","text":"10 版本同步复制支持 Quorum Commit，如下： 关于 Quorum Commit 说明12345678910111213141516171819202122232425262728293031323334353637commit: 3901fd70cc7ccacef1b0549a6835bb7d8dcaae43author: Fujii Masao &lt;fujii@postgresql.org&gt;date: Mon, 19 Dec 2016 21:15:30 +0900Support quorum-based synchronous replication. This feature is also known as \"quorum commit\" especially in discussionon pgsql-hackers. This commit adds the following new syntaxes into synchronous_standby_namesGUC. By using FIRST and ANY keywords, users can specify the method tochoose synchronous standbys from the listed servers. FIRST num_sync (standby_name [, ...])ANY num_sync (standby_name [, ...]) The keyword FIRST specifies a priority-based synchronous replicationwhich was available also in 9.6 or before. This method makes transactioncommits wait until their WAL records are replicated to num_syncsynchronous standbys chosen based on their priorities. The keyword ANY specifies a quorum-based synchronous replicationand makes transaction commits wait until their WAL records arereplicated to *at least* num_sync listed standbys. In this method,the values of sync_state.pg_stat_replication for the listed standbysare reported as \"quorum\". The priority is still assigned to each standby,but not used in this method. The existing syntaxes having neither FIRST nor ANY keyword are stillsupported. They are the same as new syntax with FIRST keyword, i.e.,a priority-based synchronous replication. Author: Masahiko SawadaReviewed-By: Michael Paquier, Amit Kapila and meDiscussion: &lt;CAD21AoAACi9NeC_ecm+Vahm+MMA6nYh=Kqs3KB3np+MBOS_gZg@mail.gmail.com&gt; Many thanks to the various individuals who were involved indiscussing and developing this feature. 备注：具体是说 synchronous_standby_names 参数支持 FIRST 和 ANY 两种模式指定同步复制备节点，语法如下：12FIRST num_sync (standby_name [, ...])ANY num_sync (standby_name [, ...]) num_sync 是指需要同步复制的备节点个数； standby_name 是指同步复制备节点的名称，这个名称在备节点 recovery.conf 中的 primary_conninfo 参数 application_name 选项指定； FIRST 表示列表中的同步节点优先按前后顺序排序，列表中越往前的节点优先级越高，同步节点为num_sync个；FIRST 1(node2,node3)，表示 node2 为同步备节点，并且同步节点数为1个； ANY 表示 quorum-based 同步复制，同步备节点为任意 num_sync 个。 搭建一主两从流复制具体搭建步骤略，这里列出部分配置项环境信息123xx.xx.xx.74 node1PRIMARYxx.xx.xx.75 node2STADNBYxx.xx.xx.76 node3 STANDBY 三节点 postgresql.conf 主要参数123wal_level = logicalsynchronous_commit = onsynchronous_standby_names = 按需配置，详见文章后面的的演示 三节点 pg_hba.conf 添加 以下123host replication repuser xx.xx.xx.74/32 md5host replication repuser xx.xx.xx.75/32 md5host replication repuser xx.xx.xx.76/32 md5 三节点 .pgpass 添加 以下1234$ cat .pgpassxx.xx.xx.74:1921:replication:repuser:repuser [pg93@redhat6 ~]$ chmod 0600 .pgpass pg_basebackup 命令参考123pg_start_backup('bak1');pg_basebackup -D /database/pg10/pg_root -Fp -Xs -v -P -h xx.xx.xx.74 -p 1921 -U repuserpg_stop_backup(); node2 节点 recovery.conf 配置123recovery_target_timeline = 'latest'standby_mode = onprimary_conninfo = 'host=xx.xx.xx.74 port=1921 user=repuser application_name=node2' node3 节点 recovery.conf 配置123recovery_target_timeline = 'latest'standby_mode = onprimary_conninfo = 'host=xx.xx.xx.74 port=1921 user=repuser application_name=node3' FIRST 同步复制策略node1设置 synchronous_standby_names 参数12345678postgres=# ALTER SYSTEM SET synchronous_standby_names = 'first 1(node2,node3)';ALTER SYSTEM postgres=# select pg_reload_conf();pg_reload_conf----------------t(1 row) node1上查看123456postgres=# select pid,usename,application_name,client_addr,state,sync_priority,sync_state from pg_stat_replication where application_name in ('node2','node3') order by application_name;pid | usename | application_name | client_addr | state | sync_priority | sync_state------+---------+------------------+-------------+-----------+---------------+------------3313 | repuser | node2 | xx.xx.xx.75 | streaming | 1 | sync3322 | repuser | node3 | xx.xx.xx.76 | streaming | 2 | potential(2 rows) 备注： 注意sync_state值 ，sync 表示 同步备节点；potential 表示目前是异步备节点，当同步备节点宕机时potential节点有可能升级为同步备节点。 关闭 node2，再次在node1节点上查看123456789[pg10@db-tfcs02 ~]$ pg_ctl stop -m fastwaiting for server to shut down.... doneserver stopped postgres=# select pid,usename,application_name,client_addr,state,sync_priority,sync_state from pg_stat_replication where application_name in ('node2','node3') order by application_name;pid | usename | application_name | client_addr | state | sync_priority | sync_state------+---------+------------------+-------------+-----------+---------------+------------3322 | repuser | node3 | xx.xx.xx.76 | streaming | 2 | sync(1 row) 备注：node2关闭后， node3升级为同步备节点。 node1执行12postgres=# insert into test_sr(id) values(10);INSERT 0 1 备注：node1 节点上的操作不受影响。 关闭 node3，再次在node1节点上查看123[pg10@db-tfcs03 ~]$ pg_ctl stop -m fastwaiting for server to shut down.... doneserver stopped node1执行12postgres=# insert into test_sr(id) values(12);等待状态，INSERT 命令下不去，主库操作将处于等待状态 备注：synchronous_standby_names = ‘first 1(node2,node3)’时，当一个同步节点宕机时主库操作不受影响，当两个同步节点宕机时主库操作将处于等待状态。 ANY 同步复制策略设备列表中任意两个节点为同步节点，按照预想，列表中如果宕掉一个同步备节点，主库上的操作将被等待，测试下node1 设置 synchronous_standby_names12345678postgres=# ALTER SYSTEM SET synchronous_standby_names = 'ANY 2(node2,node3)';ALTER SYSTEM postgres=# SELECT pg_reload_conf();pg_reload_conf----------------t(1 row) node1 查看12345postgres=# select pid,usename,application_name,client_addr,state,sync_priority,sync_state from pg_stat_replication where application_name in ('node2','node3') order by application_name;pid | usename | application_name | client_addr | state | sync_priority | sync_state------+---------+------------------+-------------+-----------+---------------+------------3525 | repuser | node2 | xx.xx.xx.75 | streaming | 1 | quorum3322 | repuser | node3 | xx.xx.xx.76 | streaming | 1 | quorum 关闭nod2123[pg10@db-tfcs02 ~]$ pg_ctl stop -m fastwaiting for server to shut down.... doneserver stopped node1 上操作1234567postgres=# select pid,usename,application_name,client_addr,state,sync_priority,sync_state from pg_stat_replication where application_name in ('node2','node3') order by application_name;pid | usename | application_name | client_addr | state | sync_priority | sync_state------+---------+------------------+-------------+-----------+---------------+------------3322 | repuser | node3 | xx.xx.xx.76 | streaming | 1 | quorum(1 row) postgres=# insert into test_sr(id) values(8); 等待状态，INSERT 命令下不去，因为有一个同步节点宕掉了，验证了实验前的预想。 附 pg_stat_replication.sync_state字段详细解释如下：12345678Synchronous state of this standby server. Possible values are:async: This standby server is asynchronous. potential: This standby server is now asynchronous, but can potentially become synchronous if one of current synchronous ones fails. sync: This standby server is synchronous. quorum: This standby server is considered as a candidate for quorum standbys. 参考 PostgreSQL：使用 pg_basebackup 搭建流复制环境 PostgreSQL9.1新特性之五：同步复制 ( Synchronous Replication ) Postgres 10 highlight - Quorum Set of Synchronous Standbys pg_stat_replication.sync_state Major Features: Postgres 10","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"PostgreSQL10：逻辑复制（Logical Replication）之二","slug":"20170530165846","date":"2017-05-30T08:58:46.000Z","updated":"2018-09-04T01:34:21.316Z","comments":true,"path":"20170530165846.html","link":"","permalink":"https://postgres.fun/20170530165846.html","excerpt":"","text":"上篇博客介绍了 PostgreSQL10重量级新特性逻辑复制并进行了演示，详见PostgreSQL10：逻辑复制（Logical Replication）之一， 逻辑复制的延迟如何呢？以下做个压测。 INSERT 压测测试准备发布节点创建测试表并赋权12345678francs=&gt; create table test_per2(id int4,name text, create_time timestamp(0) without time zone DEFAULT '2000-01-01 00:00:00'::timestamp without time zone);CREATE TABLE francs=# grant select on francs.test_per2 to repuser ;GRANT francs=# ALTER PUBLICATION pub1 ADD TABLE francs.test_per2;ALTER PUBLICATION 订阅节点创建表结构123456des=# create table francs.test_per2(id int4,name text, create_time timestamp(0) without time zone DEFAULT '2000-01-01 00:00:00'::timestamp without time zone);CREATE TABLE des=# ALTER SUBSCRIPTION sub1 REFRESH PUBLICATION ;NOTICE: added subscription for table francs.test_per2ALTER SUBSCRIPTION Insert 压力测试insert 脚本123[pg10@csv-tfcs01 loadtest]$ cat insert_1.sqlset v_id random(1,1000000)insert into francs.test_per2(id,name) values (:vid,:vid||'_a'); pgbench12[pg10@csv-tfcs01 loadtest]$ nohup pgbench -c 5 -T 120 -d francs -U francs -n N -M prepared -f insert_1.sql &gt; insert_1.out &amp;[2] 11911 发布节点查看延时123456789101112131415161718192021francs=# select * from pg_stat_replication ;-[ RECORD 1 ]----+------------------------------pid | 14790usesysid | 24809usename | repuserapplication_name | sub1client_addr | 127.0.0.1client_hostname |client_port | 56547backend_start | 2017-05-28 13:24:58.254526+08backend_xmin |state | streamingsent_lsn | 3/1A26FFE0write_lsn | 3/1A26FFE0flush_lsn | 3/1A23AE70replay_lsn | 3/1A26FFE0write_lag | 00:00:00.069027flush_lag | 00:00:00.069027replay_lag | 00:00:00.069027sync_priority | 0sync_state | async pgbench 过程中持续执行以下命令，取最大值12345francs=# select pg_wal_lsn_diff(sent_lsn,replay_lsn) as \"Delay bytes\" from pg_stat_replication;Delay bytes------------- 433296(1 row) 备注：通过 pg_wal_lsn_diff 函数查看发布节点最大延时值为 433KB 订阅节点查看延时12345678910111213des=# \\d pg_stat_subscription View \"pg_catalog.pg_stat_subscription\" Column | Type | Collation | Nullable | Default-----------------------+--------------------------+-----------+----------+---------subid | oid | | |subname | name | | |pid | integer | | |relid | oid | | |received_lsn | pg_lsn | | |last_msg_send_time | timestamp with time zone | | |last_msg_receipt_time | timestamp with time zone | | |latest_end_lsn | pg_lsn | | |latest_end_time | timestamp with time zone | | | pgbench 过程中持续执行以下命令，取最大值12345des=# select pg_wal_lsn_diff(received_lsn,latest_end_lsn) from pg_stat_subscription ;pg_wal_lsn_diff----------------- 532264(1 row) 备注： 订阅节点最大延时为 532 KB。 查看 INSERT 压测TPS1234567891011[pg10@csv-tfcs01 loadtest]$ tail -f insert_1.outtransaction type: insert_1.sqlscaling factor: 1query mode: preparednumber of clients: 5number of threads: 1duration: 120 snumber of transactions actually processed: 1621615latency average = 0.370 mstps = 13512.982187 (including connections establishing)tps = 13513.233304 (excluding connections establishing) 备注：以上逻辑复制延时 WAL 字节数在几百KB范围内，INSERT 场景下逻辑复制延时可控，不算大。 UPDATE 压测测试准备测试表，一千万数据 12345678910111213141516francs=# \\d francs.test_per1 Table \"francs.test_per1\" Column | Type | Collation | Nullable | Default ------------+--------------------------------+-----------+----------+----------------------------------------------------id | bigint | | not null |name | text | | |creat_time | timestamp(0) without time zone | | | '2000-01-01 00:00:00'::timestamp without time zoneIndexes: \"test_per1_pkey\" PRIMARY KEY, btree (id)Publications: \"pub1\"francs=# select count(*) from francs.test_per1; count ----------10000000(1 row) Update 压力测试update 脚本1234[pg10@csv-tfcs01 loadtest]$ cat update_1.sqlset v_id random(1,1000000) update test_per1 set creat_time=clock_timestamp() where id=:v_id; pgbench12[pg10@csv-tfcs01 loadtest]$ nohup pgbench -c 5 -T 120 -d francs -U francs -n N -M prepared -f update_1.sql &gt; update_1.out &amp;[2] 12536 发布结点查看最大延时12345francs=# select pg_wal_lsn_diff(sent_lsn,replay_lsn) as \"Delay bytes\" from pg_stat_replication;Delay bytes------------- 15152304(1 row) 订阅节点查看最大延时12345des=# select pg_wal_lsn_diff(received_lsn,latest_end_lsn) from pg_stat_subscription ;pg_wal_lsn_diff----------------- 18757656(1 row) 查看 UPDATE 压测TPS1234567891011[pg10@csv-tfcs01 loadtest]$ tail -f update_1.outtransaction type: update_1.sqlscaling factor: 1query mode: preparednumber of clients: 5number of threads: 1duration: 120 snumber of transactions actually processed: 1302798latency average = 0.461 mstps = 10856.066911 (including connections establishing)tps = 10856.228038 (excluding connections establishing) 备注：发布结点最大延时达到 15MB， 订阅节点最大延时达到 18MB，可见 UPDATE 场景下延时较大。 参考 PostgreSQL10：逻辑复制（Logical Replication）之一 pg_wal_lsn_diff Table 28.7. pg_stat_subscription view","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"PostgreSQL10：逻辑复制（Logical Replication）之一","slug":"20170528142004","date":"2017-05-28T06:20:04.000Z","updated":"2018-09-04T01:34:21.253Z","comments":true,"path":"20170528142004.html","link":"","permalink":"https://postgres.fun/20170528142004.html","excerpt":"","text":"PostgreSQL10另一重量级新特性是支持内置的逻辑复制（ Logical Replication），这一新特性主要提交者为来自 2ndquadrant 的开发者，感谢他们的付出！ 关于 Logical Replication介绍Logical Replication之前，先介绍下Streaming Replication，中文常称之为流复制，流复制最早在 9.0 版本出现 ，生产环境使用非常普遍，常用在高可用、读写分离场景，流复制是基于 WAL 日志的物理复制，适用于实例级别的复制，而Logical Replication 属于逻辑复制，可基于表级别复制，是一种粒度可细的复制，主要用在以下场景： 满足业务上需求，实现某些指定表数据同步； 报表系统，采集报表数据； PostgreSQL 跨版本数据同步 PostgreSQL 大版本升级 在10版本之前，虽然没有内置的逻辑复制，也可以通过其它方式实现，例如触发器、自定义脚本实现表级别同步，另外也可以通过外部工具 Londiste3 实现，详见 Londiste3：搭建简单的基于表的复制 接下来做个演示，本机装了两个 pg10 实例，端口分别为 1921，1923，计划将端口1921上的francs 库作为源库，端口1923上的des库作为目标库。 演示 Logical Replication两节点上设置postgresql.conf1wal_level = logical # minimal, replica, or logical create user repuser(1921 实例上操作)12345CREATE USER repuser REPLICATION LOGINCONNECTION LIMIT 10ENCRYPTED PASSWORD 'repuser'; 备注：用于逻辑复制的用户必须是 replication 角色 superuser 角色 赋权需要给 repuser 用户对源库、源表、源schmea 赋权，便于演示，这里暂不赋权限。 1921实例francs库操作1234567891011121314151617181920创建测试表francs=&gt; create table test_lr1(id int4 primary key ,name text);CREATE TABLE francs=&gt; insert into test_lr1 values (1,'a');INSERT 0 1创建 PUBLICATION pub1francs=&gt; create PUBLICATION pub1 FOR TABLE test_lr1;CREATE PUBLICATION查看 PUBLICATIONfrancs=# select * from pg_publication;-[ RECORD 1 ]+------pubname | pub1pubowner | 16384puballtables | fpubinsert | tpubupdate | tpubdelete | t 1923实例des库操作12345678910111213141516des=&gt; create schema francs;CREATE SCHEMA des=&gt; create table francs.test_lr1(id int4 ,name text);CREATE TABLE des=&gt; select * from francs.test_lr1 ;id | name----+------(0 rows) 创建subscriptiondes=# create subscription sub1 connection 'host=127.0.0.1 port=1921 dbname=francs user=repuser' publication pub1;NOTICE: synchronized table statesNOTICE: created replication slot \"sub1\" on publisherCREATE SUBSCRIPTION 备注：必须是具有superuser权限的用户才能创建subscription。 查看subscription12345678910des=# select * from pg_subscription;[ RECORD 1 ]---+----------------------------------------------------subdbid | 16387subname | sub1subowner | 10subenabled | tsubconninfo | host=127.0.0.1 port=1921 dbname=francs user=repusersubslotname | sub1subsynccommit | offsubpublications | &#123;pub1&#125; 数据验证1234des=# select * from francs.test_lr1 ;id | name----+------(0 rows) 备注：这时看数据还没有同步过来。 查看 francs 库日志122017-05-28 11:11:57.609 CST,\"repuser\",\"francs\",13332,\"127.0.0.1:56500\",592a3ffd.3414,2,\"COPY\",2017-05-28 11:11:57 CST,5/1338,999,ERROR,42501,\"permission denied for schema francs\",,,,,,\"COPY francs.test_lr1 TO STDOUT\",,,\"sub1_16416_sync_16409\"备注：原来没有给 repuser 相应权限，赋权即可。 1921实例francs库操作1234567给 repuser 赋权francs=&gt; grant connect on database francs to repuser;GRANTfrancs=&gt; grant usage on schema francs to repuser;GRANTfrancs=&gt; grant select on test_lr1 to repuser;GRANT 1923实例des库操作12345des=# select * from francs.test_lr1 ;id | name----+------ 1 | a(1 row) 备注：数据有了。 1921实例francs库再插入一条数据12francs=&gt; insert into test_lr1 values (2,'b');INSERT 0 1 1923实例des库操作123456des=&gt; select * from francs.test_lr1 ;id | name----+------ 1 | a 2 | b(2 rows) 备注：新插入的数据被同步过来了，这时再演示下删除操作 1921实例francs库删除一条数据12francs=&gt; delete from test_lr1 where id=1;DELETE 1 1923实例des库操作12345des=# select * from francs.test_lr1 ;id | name----+------ 2 | b(1 rows) 备注：id=1 的数据在备库也被删掉了，这个时候如果要加入一个逻辑复制表如何操作？ 1921实例francs库新建一张大表，插入1000万数据1234567891011francs=&gt; create table test_big2(id int4 primary key, create_time timestamp without time zone default clock_timestamp(), name character varying(32));CREATE TABLE francs=&gt; insert into test_big2(id,name) select n,n*random()*10000 from generate_series(1,10000000) n ;INSERT 0 10000000 francs=&gt; grant select on test_big2 to repuser;GRANT francs=# ALTER PUBLICATION pub1 add TABLE francs.test_big2;ALTER PUBLICATION 1923实例des库操作123456789101112des=&gt; create table francs.test_big2(id int4 primary key, create_time timestamp without time zone default clock_timestamp(), name character varying(32));CREATE TABLE des=# ALTER SUBSCRIPTION sub1 REFRESH PUBLICATION ;NOTICE: added subscription for table francs.test_big2ALTER SUBSCRIPTIO des=# select count(*) from francs.test_big2; count ----------10000000(1 row) 备注：PUBLICATION 节点增加表后，SUBSCRIPTION 节点需要执行 ALTER SUBSCRIPTION sub1 REFRESH PUBLICATION 命令进行刷新；1000万数据同步大概用了不到30秒，速度很快，在同步过程中看到了 COPY 进程，推测在数据初始化时用的是 copy SUBSCRIPTION 进程可以 DISABLE 或 ENABLE12345des=# ALTER SUBSCRIPTION sub1 DISABLE ;ALTER SUBSCRIPTION des=# ALTER SUBSCRIPTION sub1 ENABLE ;ALTER SUBSCRIPTION Logical Replication 原理Logical Replication 原理实际上用到了 Replication slots 的概念，关于这个可参考之前写的博客： PostgreSQL9.4: 初识逻辑解析 ( logical decoding ) PostgreSQL 9.4: Replication Slots 总结publication - 发布者 逻辑复制的前提是将数据库 wal_level 参数设置成 logical； 源库上逻辑复制的用户必须具有 replicatoin 或 superuser 角色； 逻辑复制目前仅支持数据库表逻辑复制，其它对象例如函数、视图不支持； 逻辑复制支持DML(UPDATE、INSERT、DELETE)操作，TRUNCATE 和 DDL 操作不支持； 需要发布逻辑复制的表，须配置表的 REPLICA IDENTITY 特性； 一个数据库中可以有多个publication，通过 pg_publication 查看； 允许一次发布所有表，语法： CREATE PUBLICATION alltables FOR ALL TABLES; subscription - 订阅者 订阅节点需要指定发布者的连接信息； 一个数据库中可以有多个订阅者； 可以使用enable/disable启用/暂停该订阅； 发布节点和订阅节点表的模式名、表名必须一致，订阅节点允许表有额外字段； 发布节点增加表名，订阅节点需要执行： ALTER SUBSCRIPTION sub1 REFRESH PUBLICATION 参考 Chapter 31. Logical Replication Logical Replication in PostgreSQL 10 PostgreSQL9.4: 初识逻辑解析 ( logical decoding ) PostgreSQL 9.4: Replication Slots WAITING FOR POSTGRESQL 10 – LOGICAL REPLICATION PostgreSQL 10.0 逻辑复制原理与最佳实践","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"PostgreSQL10: Additional FDW Push-Down","slug":"20170525231345","date":"2017-05-25T15:13:45.000Z","updated":"2018-09-04T01:34:21.191Z","comments":true,"path":"20170525231345.html","link":"","permalink":"https://postgres.fun/20170525231345.html","excerpt":"","text":"在外部表上做 aggregates 操作时，10版本之前的处理过程是先将远程库的数据全部取到本地库，之后在本地库上aggregates操作，10版本后部分 aggregates 操作可下推到远程库，这样将大幅减少远程库传到本地库的数据量，性能会有较大提升，关于这个特性，文档中有以下描述 Committed Patch 的内容 postgres_fdw: Push down aggregates to remote servers.Now that the upper planner uses paths, and now that we have proper hooks to inject paths into the upper planning process, its possible for foreign data wrappers to arrange to push aggregates to the remote side instead of fetching all of the rows and aggregating them locally. This figures to be a massive win for performance, so teach postgres_fdw to do it.Jeevan Chalke and Ashutosh Bapat. Reviewed by Ashutosh Bapat withadditional testing by Prabhat Sahu. Various mostly cosmetic changesby me. PostgreSQL10 Release Note 中的描述 Push aggregates to foreign data wrapper servers, where possible (Jeevan Chalke, Ashutosh Bapat)This reduces the amount of data that must be passed from the foreign data wrapper server, and offloads aggregate computation from the requesting server. The postgres_fdw is able to perform this optimization. There are also improvements in pushing down joins involving extensions. 接下来分别在 9.6 版本和 10 版本测试。 PostgreSQL9.6 版本测试创建 postgres_fdw123456[pg96@db1 pg_root]$ psqlpsql (9.6beta1)Type \"help\" for help. francs=# create extension postgres_fdw;CREATE EXTENSION 创建远程PG库SERVER和MAPPING USER1234567francs=# grant usage on foreign data wrapper postgres_fdw to francs;GRANT francs=&gt; CREATE SERVER pgsql_srv FOREIGN DATA WRAPPER postgres_fdw OPTIONS (host '127.0.0.1', port '1921', dbname 'francs');CREATE SERVERfrancs=&gt; CREATE USER MAPPING FOR public SERVER pgsql_srv OPTIONS (user 'francs', password 'francs');CREATE USER MAPPING 创建测试表并插入数据123456create table test_fdw3 ( id int4, flag int4); insert into test_fdw3(id,flag) select n, mod(n,3) from generate_series(1,100000) n; 创建外部表1234CREATE FOREIGN TABLE ft_test_fdw3 ( id int4 , flag int4) SERVER pgsql_srv OPTIONS (schema_name 'francs', table_name 'test_fdw3'); 查看执行计划123456789101112francs=&gt; explain (analyze on,verbose on) select flag,count(*) from ft_test_fdw3 group by flag order by flag; QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------------GroupAggregate (cost=100.00..222.22 rows=200 width=12) (actual time=202.422..440.217 rows=3 loops=1)Output: flag, count(*)Group Key: ft_test_fdw3.flag-&gt; Foreign Scan on francs.ft_test_fdw3 (cost=100.00..205.60 rows=2925 width=4) (actual time=59.773..395.126 rows=100000 loops=1)Output: id, flagRemote SQL: SELECT flag FROM francs.test_fdw3 ORDER BY flag ASC NULLS LASTPlanning time: 0.185 msExecution time: 441.758 ms(8 rows) PostgreSQL10 版本测试重复创建外部表、测试表等以上操作，最后查看以下执行计划123456789101112131415161718[pg10@csv-tfcs01 ~]$ psql francs francs Password for user francs: psql (10beta1) Type \"help\" for help. francs=&gt; explain (analyze on,verbose on) select flag,count(*) from ft_test_fdw3 group by flag order by flag; QUERY PLAN ---------------------------------------------------------------------------------------------------------Sort (cost=211.41..211.91 rows=200 width=12) (actual time=19.662..19.662 rows=3 loops=1)Output: flag, (count(*))Sort Key: ft_test_fdw3.flagSort Method: quicksortMemory: 25kB-&gt; Foreign Scan (cost=129.25..203.76 rows=200 width=12) (actual time=19.648..19.649 rows=3 loops=1)Output: flag, (count(*))Relations: Aggregate on (francs.ft_test_fdw3)Remote SQL: SELECT flag, count(*) FROM francs.test_fdw3 GROUP BY flagPlanning time: 0.212 msExecution time: 19.928 备注：仔细观察这份执行计划，发现变化有三 多了Relations: Aggregate on 步骤，Aggregate 操作被下推到远程库上执行。 Foreign Scan 的 actual rows=3，而9.6版本这步操作的 Foreign Scan 的 actual rows=100000 执行时间快很多，10 版本这个查询仅需 19.9ms，9.6版本这个查询需要 441 ms，这里说明下9.6、10所在的的硬件环境不对等，尽管如此，可以推测即使是在同样的硬件环境下，由于 Aggregate 操作的下推，10 版本这类 SQL 的性能比 9.6 版本要好很多。 说明今天仅做了一个Aggregate操作下推到远程库的测试案例，是否所有Aggregate操作都能下推，值得思考与测试，今天先测试到这里，有兴趣的朋友可测试其它Aggregate案例。 参考 PostgreSQL10 Beta Release Notes WAITING FOR POSTGRESQL 10 – POSTGRES_FDW: PUSH DOWN AGGREGATES TO REMOTE SERVERS.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL10：Parallel Queries 增强","slug":"20170521162007","date":"2017-05-21T08:20:07.000Z","updated":"2018-09-04T01:34:21.128Z","comments":true,"path":"20170521162007.html","link":"","permalink":"https://postgres.fun/20170521162007.html","excerpt":"","text":"PostgreSQL9.6 版本时并行查询仅支持并行 sequential scans、并行 aggregates 操作，PostgreSQL10版本对 Parallel Queries 功能有了较大增强，如下： 1、Support parallel btree index scans (Rahila Syed, Amit Kapila, Robert Haas, Rafia Sabih)Allows btree index pages to be checked by separate parallel workers. 2、Support parallel bitmap heap scans (Dilip Kumar)This allows a single index scan to dispatch parallel workers to process different areas of the heap. 3、Allow merge joins to be performed in parallel (Dilip Kumar) 4、Allow non-correlated subqueries to be run in parallel (Amit Kapila) 5、Improve ability of parallel workers to return pre-sorted data (Rushabh Lathia) 6、Increase parallel query usage in procedural language functions (Robert Haas, Rafia Sabih) 备注：今天主要演示下前两种情况，后面几种情况有兴趣的朋友可自行构建测试模型测试。 环境准备创建测试表并插入1000万数据12345678910create table test_big1(id int4 primary key, create_time timestamp without time zone default clock_timestamp(), name character varying(32));insert into test_big1(id,name) select n,n*random()*10000 from generate_series(1,10000000) n ; francs=&gt; select * from test_big1 limit 3;id | create_time | name----+----------------------------+------------------ 1 | 2017-05-21 16:02:24.921751 | 2298.13809040934 2 | 2017-05-21 16:02:24.922051 | 7580.18649183214 3 | 2017-05-21 16:02:24.922064 | 24218.4893181548(3 rows) Parallel Index Only Scan1234567891011121314151617181920francs=&gt; show max_parallel_workers;max_parallel_workers----------------------4(1 row) francs=&gt; explain analyze select count(*) from test_big1 where id &lt;1000000 ; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------Finalize Aggregate (cost=18576.59..18576.60 rows=1 width=8) (actual time=73.362..73.362 rows=1 loops=1)-&gt; Gather (cost=18576.17..18576.58 rows=4 width=8) (actual time=73.200..73.355 rows=5 loops=1)Workers Planned: 4Workers Launched: 4-&gt; Partial Aggregate (cost=17576.17..17576.18 rows=1 width=8) (actual time=68.992..68.992 rows=1 loops=5)-&gt; Parallel Index Only Scan using test_big1_pkey on test_big1(cost=0.43..16947.37 rows=251523 width=0) (actual time=0.053..54.343 rows=200000 loops=5)Index Cond: (id &lt; 1000000)Heap Fetches: 174195Planning time: 0.105 msExecution time: 74.572 ms(10 rows) 备注：从执行计划中看到了 “Parallel Index Only Scan”。 关闭并行1234567891011121314151617francs=&gt; set max_parallel_workers=0;SET francs=&gt; explain analyze select count(*) from test_big1 where id &lt;1000000 ; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------Finalize Aggregate (cost=18576.59..18576.60 rows=1 width=8) (actual time=257.585..257.585 rows=1 loops=1)-&gt; Gather (cost=18576.17..18576.58 rows=4 width=8) (actual time=257.579..257.579 rows=1 loops=1)Workers Planned: 4Workers Launched: 0-&gt; Partial Aggregate (cost=17576.17..17576.18 rows=1 width=8) (actual time=257.251..257.251 rows=1 loops=1)-&gt; Parallel Index Only Scan using test_big1_pkey on test_big1(cost=0.43..16947.37 rows=251523 width=0) (actual time=0.042..183.384 rows=999999 loops=1)Index Cond: (id &lt; 1000000)Heap Fetches: 999999Planning time: 0.102 msExecution time: 257.717 ms(10 rows) 备注：开启并行查询时，此SQL执行时间为74ms，关闭并行查询时执行时间257ms，慢了3倍多。 Parallel Index Scan12345678910111213francs=&gt; explain analyze select count(name) from test_big1 where id &lt;1000000 ; QUERY PLAN -----------------------------------------------------------------------------------------------------------------------------------------------------------------------Finalize Aggregate (cost=18580.76..18580.77 rows=1 width=8) (actual time=76.601..76.601 rows=1 loops=1)-&gt; Gather (cost=18580.34..18580.75 rows=4 width=8) (actual time=75.541..76.595 rows=5 loops=1)Workers Planned: 4Workers Launched: 4-&gt; Partial Aggregate (cost=17580.34..17580.35 rows=1 width=8) (actual time=71.752..71.752 rows=1 loops=5)-&gt; Parallel Index Scan using idx_test_big1_id on test_big1(cost=0.43..16951.53 rows=251527 width=16) (actual time=0.036..54.157 rows=200000 loops=5)Index Cond: (id &lt; 1000000)Planning time: 0.126 msExecution time: 76.919 ms(9 rows) 备注：从执行计划中，走了“Parallel Index Scan”。 Parallel Bitmap Heap Scan12345678910111213141516171819francs=&gt; explain analyze select count(*) from test_big1 where id &lt;1000000 or id &gt; 9000000; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------------------------------------Finalize Aggregate (cost=72069.09..72069.10 rows=1 width=8) (actual time=194.016..194.017 rows=1 loops=1)-&gt; Gather (cost=72068.67..72069.08 rows=4 width=8) (actual time=193.676..194.011 rows=5 loops=1)Workers Planned: 4Workers Launched: 4-&gt; Partial Aggregate (cost=71068.67..71068.68 rows=1 width=8) (actual time=189.722..189.722 rows=1 loops=5)-&gt; Parallel Bitmap Heap Scan on test_big1(cost=21277.11..69887.94 rows=472292 width=0) (actual time=84.034..155.426 rows=400000 loops=5)Recheck Cond: ((id &lt; 1000000) OR (id &gt; 9000000))Heap Blocks: exact=1291-&gt; BitmapOr (cost=21277.11..21277.11 rows=1987954 width=0) (actual time=85.468..85.468 rows=0 loops=1)-&gt; Bitmap Index Scan on test_big1_pkey(cost=0.00..10290.13 rows=1006093 width=0) (actual time=42.868..42.868 rows=999999 loops=1)Index Cond: (id &lt; 1000000)-&gt; Bitmap Index Scan on test_big1_pkey(cost=0.00..10042.39 rows=981861 width=0) (actual time=42.597..42.597 rows=1000000 loops=1)Index Cond: (id &gt; 9000000)Planning time: 0.120 msExecution time: 195.811 ms(15 rows) 备注：从执行计划中看到了“Parallel Bitmap Heap Scan on test_big1”。 参考 PostgreSQL9.6：Parallel Aggregates 初体验 PostgreSQL9.6：Parallel Sequential Scans 初体验 PostgreSQL10 Beta Release Notes PostgreSQL 9.6 并行计算优化器算法浅析","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Parallel Query","slug":"Parallel-Query","permalink":"https://postgres.fun/tags/Parallel-Query/"}]},{"title":"PostgreSQL10：重量级新特性-支持分区表","slug":"20170521123452","date":"2017-05-21T04:34:52.000Z","updated":"2018-09-04T01:34:21.066Z","comments":true,"path":"20170521123452.html","link":"","permalink":"https://postgres.fun/20170521123452.html","excerpt":"","text":"PostgreSQL10 一个重量级新特性是支持分区表，在这之前，PostgreSQL不支持内置分区表，若要实现功能，需通过继承的方式实现，详见PostgreSQL: 分区表应用二(取模分区)。 PostgreSQL 内置分区表目前仅支持以下两种形式分区 范围分区（Range Partitioning） The table is partitioned into “ranges” defined by a key column or set of columns, with no overlap between the ranges of values assigned to different partitions. For example, one might partition by date ranges, or by ranges of identifiers for particular business objects. 列表分区（List Partitioning） The table is partitioned by explicitly listing which key values appear in each partition. 分区表语法12345CREATE TABLE table_name ( ... )[ PARTITION BY &#123; RANGE | LIST &#125; ( &#123; column_name | ( expression ) &#125; CREATE TABLE table_namePARTITION OF parent_table [ () ] FOR VALUES partition_bound_spec 创建范围分区表下面通过一个例子演示下：创建一个范围分区的分区表创建父表123456789101112131415161718192021create table tmp_log ( id serial, create_time timestamp(0) without time zone,remark char(1)) partition by RANGE(create_time); CREATE TABLE tmp_log_p2016_befor PARTITION OF tmp_log FOR VALUES FROM (UNBOUNDED) TO ('2016-01-01');CREATE TABLE tmp_log_p201601 PARTITION OF tmp_log FOR VALUES FROM ('2016-01-01') TO ('2016-02-01');CREATE TABLE tmp_log_p201602 PARTITION OF tmp_log FOR VALUES FROM ('2016-02-01') TO ('2016-03-01');CREATE TABLE tmp_log_p201603 PARTITION OF tmp_log FOR VALUES FROM ('2016-03-01') TO ('2016-04-01');CREATE TABLE tmp_log_p201604 PARTITION OF tmp_log FOR VALUES FROM ('2016-04-01') TO ('2016-05-01');CREATE TABLE tmp_log_p201605 PARTITION OF tmp_log FOR VALUES FROM ('2016-05-01') TO ('2016-06-01');CREATE TABLE tmp_log_p201606 PARTITION OF tmp_log FOR VALUES FROM ('2016-06-01') TO ('2016-07-01');CREATE TABLE tmp_log_p201607 PARTITION OF tmp_log FOR VALUES FROM ('2016-07-01') TO ('2016-08-01');CREATE TABLE tmp_log_p201608 PARTITION OF tmp_log FOR VALUES FROM ('2016-08-01') TO ('2016-09-01');CREATE TABLE tmp_log_p201609 PARTITION OF tmp_log FOR VALUES FROM ('2016-09-01') TO ('2016-10-01');CREATE TABLE tmp_log_p201610 PARTITION OF tmp_log FOR VALUES FROM ('2016-10-01') TO ('2016-11-01');CREATE TABLE tmp_log_p201611 PARTITION OF tmp_log FOR VALUES FROM ('2016-11-01') TO ('2016-12-01');CREATE TABLE tmp_log_p201612 PARTITION OF tmp_log FOR VALUES FROM ('2016-12-01') TO ('2017-01-01');CREATE TABLE tmp_log_p201701 PARTITION OF tmp_log FOR VALUES FROM ('2017-01-01') TO ('2017-02-01');CREATE TABLE tmp_log_p201702 PARTITION OF tmp_log FOR VALUES FROM ('2017-02-01') TO ('2017-03-01'); 创建索引123456789101112131415create index idx_tmp_log_p2016_befor_ctime on tmp_log_p2016_befor using btree (create_time);create index idx_tmp_log_p201601_ctime on tmp_log_p201601 using btree (create_time);create index idx_tmp_log_p201602_ctime on tmp_log_p201602 using btree (create_time);create index idx_tmp_log_p201603_ctime on tmp_log_p201603 using btree (create_time);create index idx_tmp_log_p201604_ctime on tmp_log_p201604 using btree (create_time);create index idx_tmp_log_p201605_ctime on tmp_log_p201605 using btree (create_time);create index idx_tmp_log_p201606_ctime on tmp_log_p201606 using btree (create_time);create index idx_tmp_log_p201607_ctime on tmp_log_p201607 using btree (create_time);create index idx_tmp_log_p201608_ctime on tmp_log_p201608 using btree (create_time);create index idx_tmp_log_p201609_ctime on tmp_log_p201609 using btree (create_time);create index idx_tmp_log_p201610_ctime on tmp_log_p201610 using btree (create_time);create index idx_tmp_log_p201611_ctime on tmp_log_p201611 using btree (create_time);create index idx_tmp_log_p201612_ctime on tmp_log_p201612 using btree (create_time);create index idx_tmp_log_p201701_ctime on tmp_log_p201701 using btree (create_time);create index idx_tmp_log_p201702_ctime on tmp_log_p201702 using btree (create_time); 备注：主要通过以上三步完成分区表创建，注意 constraint_exclusion 设备成 partition ；目前分区上的索引、约束、主键需要使用单独的命令创建。 插入测试数据123francs=&gt; insert into tmp_log (create_time,remark)select generate_series('2015-11-01'::date, '2017-02-28'::date, '1 hour'),'1';INSERT 0 11641 备注：利用generate_series 函数生成时间戳测试数据，一天插入24条数据，测试样例数据如下。 数据插入溢出1234francs=&gt; insert into tmp_log (create_time,remark)francs-&gt; select generate_series('2015-11-01'::date, '2018-01-01'::date, '1 hour'),'1';ERROR: no partition of relation \"tmp_log\" found for rowDETAIL: Partition key of the failing row contains (create_time) = (2017-03-01 00:00:00). 备注：如果插入的数据没有对应的分区，报如上错误。 查看数据量12345678910111213141516171819202122232425262728293031francs=&gt; select * from tmp_log limit 3;id | create_time | remark-------+---------------------+--------24795 | 2016-01-01 00:00:00 | 124796 | 2016-01-01 01:00:00 | 124797 | 2016-01-01 02:00:00 | 1(3 rows) francs=&gt; select count(*) from tmp_log;count-------11641(1 row) francs=&gt; select count(*) from tmp_log_p201601;count-------744(1 row) francs=&gt; select count(*) from tmp_log_p201602;count-------696(1 row) francs=&gt; select count(*) from tmp_log_p201609;count-------720(1 row) 查看执行计划123456789francs=&gt; explain analyze select * from tmp_log where create_time &gt; '2016-01-01' and create_time &lt; '2016-01-02'; QUERY PLAN -----------------------------------------------------------------------------------------------------------------Append (cost=0.15..4.63 rows=24 width=14) (actual time=0.009..0.015 rows=23 loops=1)-&gt; Index Scan using idx_tmp_log_p201601_ctime on tmp_log_p201601(cost=0.15..4.63 rows=24 width=14) (actual time=0.008..0.012 rows=23 loops=1)Index Cond: ((create_time &gt; '2016-01-01 00:00:00'::timestamp without time zone) AND (create_time &lt; '2016-01-02 00:00:00'::timestamp without time zone))Planning time: 0.511 msExecution time: 0.037 ms(5 rows) 备注：从PLAN看出，索引扫描指定分区表 tmp_log_p201601。 分区表管理以下介绍常用的分区表管理操作：断开分区、连接分区、删除分区。 断开分区12francs=&gt; alter table tmp_log DETACH PARTITION tmp_log_p201702;ALTER TABLE 备注：DETACH 操作是指将分区从分区表断开，类似从一列火车中断开一节车厢类似，这个表将转变成普通表，仍然可读写。 连接分区12francs=&gt; alter table tmp_log ATTACH PARTITION tmp_log_p201702 FOR VALUES FROM ('2017-02-01') TO ('2017-03-01');ALTER TABLE 备注：ATTACH 操作是指将普通表连接到指定分区表，有一点要注意，ATTACH 和 DETACH 操作过程中，会在父表、此张分区表上加上 AccessExclusiveLock 排它锁，因此分区表的这两个操作应该在业务低谷时进行，避免影响业务。 删除分区12francs=&gt; drop table tmp_log_p201702;DROP TABLE 备注：删除对就分区表即可。 参考 Table Partitioning PostgreSQL: 分区表应用二(取模分区) CREATE TABLE","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"《PostgreSQL High Performance Cookbook》英文版技术审校","slug":"20170413090417","date":"2017-04-13T01:04:00.000Z","updated":"2018-12-14T07:41:38.372Z","comments":true,"path":"20170413090417.html","link":"","permalink":"https://postgres.fun/20170413090417.html","excerpt":"","text":"前几天收到 Packt Publishing 出版社编辑的邮件，她很兴奋地告诉我 《PostgreSQL High Performance Cookbook》 一书已出版， 并感谢我对此书的技术校对工作，同时给我寄了一本样书，我很高兴，毕竟利用业余时间完成了本书的校对工作，从 Packt Publishing 编辑联系我到此书出版大概有两年时间。 语言：英语出版日期：2017年3月29日作者：Chitij Chauhan , Dinesh Kumar审稿：Baji Shaik、谭峰（Francs） 主要内容此书介绍了PostgreSQL管理、压力测试、监控、高可用、复制、性能调优的内容，是一本难得的PostgreSQL管理类书籍，第一次做外版技术书籍的校对工作，很新鲜也很期望，我总共对此书的八个章节进行技术校对， 在校对过程中尽可能地对书中技术细节进行校对，遇到不熟悉的内容查阅了相关资料， 校对过程中学到了不少新东西。 章节目录 1: DATABASE BENCHMARKING 2: SERVER CONFIGURATION AND CONTROL 3: DEVICE OPTIMIZATION 4: MONITORING SERVER PERFORMANCE 5: CONNECTION POOLING AND DATABASE PARTITIONING 6: HIGH AVAILABILITY AND REPLICATION 7: WORKING WITH THIRD-PARTY REPLICATION MANAGEMENT UTILITIES 8: DATABASE MONITORING AND PERFORMANCE 9: VACUUM INTERNALS 10: DATA MIGRATION FROM OTHER DATABASES TO POSTGRESQL AND UPGRADING THE POSTGRESQL CLUSTER 11: QUERY OPTIMIZATION 12: DATABASE INDEXING 关于技术审校下面是本书关于Reviewer 的介绍，我用了一段比较别扭的英文介绍自己","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Book","slug":"PostgreSQL-Book","permalink":"https://postgres.fun/tags/PostgreSQL-Book/"}]},{"title":"FIO之一： 硬盘性能测试工具FIO初步使用","slug":"20170307085812","date":"2017-03-07T00:58:12.000Z","updated":"2018-09-04T01:34:20.941Z","comments":true,"path":"20170307085812.html","link":"","permalink":"https://postgres.fun/20170307085812.html","excerpt":"","text":"FIO（flexible I/O tester）是一个测试硬盘性能非常好的工具，这里测试下。 安装 FIO下载http://freecode.com/projects/fio 安装相关包1yum install libaio libaio-devel 解压安装1234[root@csv-tfcs01 fio-2.1.10]# cd /opt/soft_bak/fio-2.1.10[root@csv-tfcs01 fio-2.1.10]# ./configure[root@csv-tfcs01 fio-2.1.10]# make[root@csv-tfcs01 fio-2.1.10]# make install 备注：FIO 的安装非常简单，接下对计划在物理机上做个 FIO 测试，物理机上用了两块900G SAS 盘做了系统盘，SSD 盘做了数据盘，计划对系统盘和数据盘做性能测试， 主要测试指标：测试8K随机写、8K随机读、8K混合读写、1MB顺序写、1MB顺序读、1MB顺序读写；测试命令模板如下 FIO 测试命令1234567891011121314151617--8k 随机写fio -name=8krandw -runtime=120 -filename=/data/rand.txt -ioengine=libaio -direct=1 -bs=8K -size=10g -iodepth=128 -numjobs=1 -rw=randwrite -group_reporting -time_based --8K 随机读fio -name=8krandr -runtime=120 -filename=/data/rand.txt -ioengine=libaio -direct=1 -bs=8K -size=10g -iodepth=128 -numjobs=1 -rw=randread -group_reporting -time_based --8k 混合读写fio -name=8krandrw -runtime=120 -filename=/data/rand.txt -ioengine=libaio -direct=1 -bs=8k -size=10g -iodepth=128 -numjobs=1 -rw=randrw -rwmixwrite=30 -group_reporting -time_based --1Mb 顺序写fio -name=1mseqw -runtime=120 -filename=/data/seq.txt -ioengine=libaio -direct=1 -bs=1024k -size=20g -iodepth=128 -numjobs=1 -rw=write -group_reporting -time_based --1Mb 顺序读fio -name=1mseqr -runtime=120 -filename=/data/seq.txt -ioengine=libaio -direct=1 -bs=1024k -size=20g -iodepth=128 -numjobs=1 -rw=read -group_reporting -time_based --1Mb 顺序读写fio -name=1mseqrw -runtime=120 -filename=/data/seq.txt -ioengine=libaio -direct=1 -bs=1024k -size=20g -iodepth=128 -numjobs=1 -rw=rw -rwmixwrite=30 -group_reporting -time_based 8K随机写命令输出 测试结果设备型号：RH5885 V3； 硬盘：900G/SAS 10K/2.5寸硬盘/RAID11234568k 随机写:iops=7988k 随机读:iops=111908k 混合读写(读写7:3):读iops=1114 写iops=4791m 顺序写:iops=1701m 顺序读:iops=2461m 顺序读写(读写7:3):读iops=134 写iops=56 型号：RH5885 V3； 硬盘：480G/Intel SATA接口SSD/2.5寸硬盘/RAID51234568k 随机写:iops=128738k 随机读:iops=686368k 混合读写(读写7:3):读iops=24846 写iops=106431m 顺序写:iops=16871m 顺序读:iops=23471m 顺序读写(读写7:3):读iops=1019 写iops=431 虚拟机 4c/8G 接软件定义存储1234568k 随机写:iops=20234 8k 随机读:iops=20192 8k 混合读写:读iops=14510 写iops=6215 1m 顺序写:iops=923 1m 顺序读:iops=699 1m 顺序读写:读iops=571 写iops=241 备注：从测试结果来看， SATA 接口的 SSD 盘 8K随机读写、1M顺序读写能力远远超过 SAS 盘 附：FIO 相关参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849readwrite=str, rw=strType of I/O pattern. Accepted values are: read Sequential reads. write Sequential writes. trim Sequential trim (Linux block devices only). randreadRandom reads. randwriteRandom writes. randtrimRandom trim (Linux block devices only). rw, readwrite Mixed sequential reads and writes. randrw Mixed random reads and writes. direct=bool If true, use non-buffered I/O (usually O_DIRECT). Default: false.iodepth=int Number of I/O units to keep in flight against the file. Note that increasing iodepth beyond 1 willnotaffect synchronous ioengines (except for small degress when verify_async is in use). Even async enginesmay impose OS restrictions causing the desired depth not to be achieved. This may happen on Linux when using libaio and not setting direct=1, since buffered IO is not async on that OS. Keep an eye on the IOdepth distribution in the fio output to verify that the achieved depth is as expected. Default: 1. ioengine=str Defines how the job issues I/O. The following types are defined:sync Basic read(2) or write(2) I/O. fseek(2) is used to position the I/O location. psync Basic pread(2) or pwrite(2) I/O. vsync Basic readv(2) or writev(2) I/O. Will emulate queuing by coalescing adjacent IOs into a single submission.pvsync Basic preadv(2) or pwritev(2) I/O.libaio Linux native asynchronous I/O. This ioengine defines engine specific options.blocksize=int[,int], bs=int[,int] Block size for I/O units. Default: 4k. Values for reads, writes, and trims can be specified separately in the format read,write,trim either of which may be empty to leave that value at its default. If a trailing comma isn’t given, the remainder will inherit the last value set.size=int Total size of I/O for this job. fio will run until this many bytes have been transferred, unless lim- ited by other options(runtime, for instance). Unless nrfiles and filesize options are given, thisamount will be divided between the available files for the job. If not set, fio will use the full size of the given files or devices. If the files do not exist, size must be given. It is also possible togive size as a percentage between 1 and 100. If size=20% is given, fio will use 20% of the full size ofthe given files or devices.numjobs=int Number of clones (processes/threads performing the same workload) of this job. Default: 1. 参考 https://segmentfault.com/a/1190000003880571 https://linux.die.net/man/1/fio http://www.liusuping.com/storage/iops-ceshi-shuju.html","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"Postgres中国用户大会 2016（PG大象会）","slug":"20161018092210","date":"2016-10-18T01:22:10.000Z","updated":"2018-09-04T01:34:20.878Z","comments":true,"path":"20161018092210.html","link":"","permalink":"https://postgres.fun/20161018092210.html","excerpt":"","text":"自2011年以来，PG大象会（Postgres Conference China）已经举办至第6届。2016年大会由平安科技和平安壹钱包作为总冠名赞助商，中国Postgres用户会（China Postgres User Group，简称：CPUG）主办，是一场汇聚各界PostgreSQL大拿、交流最新业界技术动态和真实应用案例的盛宴。PG大象会的成功举办推动了PostgreSQL数据库在中国的发展，也起到了联系PG中国社区和国外社区的作用。时间：2016年10月27日至10月28日地点：上海浦东假日酒店组委会：胡怡文、萧少聪、赵振平、侯松、周正中、杨楂文媒体合作：赵振平（18911070380）联系人： 活动报名：余星 演讲主题：谈伟 赞助事事宜：朱贤文 媒体宣传：周到京、谭峰、媛媛 摄影: 蒋齐飞报名链接：http://www.huodongxing.com/event/8352217821400 大会主题自1996年PostgreSQL正式发布支持SQL引擎的关系型数据库产品，自今已经20年，2016年是PostgreSQL的20周年华诞，同时2016年也是可以说是PostgreSQL中国发展的元年。今年大会将设立5大主题分会场，分享并探讨PostgreSQL在新互联网时代的技术架构。 PostgreSQL PostGIS+JSON加速移动互联网及物联网发展 用户经验分享，PostgreSQL迁移的最佳实践 各社区互动：No Only PostgreSQL (PG + MySQL/Redis/MongoDB/Hadoop/ES/云计算等) PostgreSQL的分布式架构设计(PG-XC/XL/X2/XZ、Greenplum、CitusDB等) PostgreSQL黑科技的实际使用（流式计算、GPU、LLVM等） 大会简介会议报名规模预计将超过500人，嘉宾阵容强大，特邀欧洲，俄罗斯，日本数据库方面专家助阵，国内顶级PostgreSQL数据库专家也悉数到场：Postgres-XL的项目发起人：Mason SharpEnterpriseDB亚太区资深数据库专家：Katsuji Takatsuru平安科技数据库技术部总监：汪洋阿里云数据库专家：周正中(德哥)，PostgreSQL中国用户会创始人之一……嘉宾们所属的单位也都在业内享负盛名，包括：国外企业：Pivotal（美国EMC旗下）、EnterpriseDB（美国）、NTT（日本）等国内企业：平安科技、平安壹钱包、阿里云、腾讯云、山东瀚高、去哪儿网、探探科技、云徙科技、成都文武信息、飞象数据等知名学府：武汉大学、上海师范大学、上海交通大学 大会日程 27日上午：主会场时间 主题 嘉宾 公司09:30 - 09:50 开幕致辞之社区发言 萧少聪 PG中国社区09:50 - 10:10 开幕致辞之企业发言 陈立明 平安科技10:10 - 10:55 PostgreSQL 数据库前世今生 周正中 PG中国社区10:55 - 11:40 PG+金融的架构演进之路 汪洋 平安科技11:40 - 12:00 集 体 照 27日下午：分会场1（PostgreSQL+应用实践、运维技巧）时间 主题 嘉宾 公司14:00 - 14:40 PostgreSQL应用开发最佳实践 周正中 阿里云14:40 - 15:20 PostgreSQL和探探见证四亿次心动 张文升 探探科技15:20 - 15:40 中场休息15:40 - 16:20 PostgreSQL预写式日志解析于应用实践 王硕 山东瀚高16:20 - 17:00 实现 PostgreSQL逻辑复制实战 王青松 飞象数据 27日下午：分会场2（GIS+JSON加速移动互联网及物联网发展）时间 主题 嘉宾 公司14:00 - 14:40 工业大数据初探 赵振平 北京太阳塔信息科技14:40 - 15:20 The power of PostgreSQL exposed with automatically generated API endpoints. Sylvain Verly Coderbunker15:20 - 15:40 中场休息15:40 - 16:20 智慧物流与PostgreSQL 何祖文 贵州云飞科技16:20 - 17:00 数据库设计中对JSON的使用 孙鹏 北京英资教育科技 27日下午：分会场3（PostgreSQL在金融行业的应用）时间 主题 嘉宾 公司14:00 - 14:40 金融级PostgreSQL数据库监控与优化 梁海安 平安科技14:40 - 15:20 从金融架构师的视角看数据库 张晓通 平安壹钱包15:20 - 15:40 中场休息15:40 - 16:20 浅谈PostgreSQL 高可用架构 余星 平安壹钱包16:20 - 17:00 PostgreSQL“反向代理”Redis 高金芳 平安科技 28日上午：分会场1（PostgreSQL迁移的最佳实践）时间 主题 嘉宾 公司09:00 - 09:40 从ACID的D看三种主流关系型数据库 王鹏冲 平安科技09:40 - 10:20 从Sybase迁移到PostgreSQL的那些事儿 赖伟 飞象数据10:20 - 10:40 中场休息10:40 - 11:20 Oracle到PostgreSQL/Greenplum数据迁移及优化 朱贤文 成都文武信息11:20 -12:00 Oracle DBA的PostgreSQL转型之路 曾文旌 阿里云 28日上午：分会场2（PostgreSQL分布式架构实践）时间 主题 嘉宾 公司09:00 - 09:40 run your PG on ZFS 朱贤文 成都文武信息09:40 - 10:20 PGXZ在微信支付系统中的应用 李跃森 腾讯云10:20 - 10:40 中场休息10:40 - 11:20 Spark over OneProxy/PostgreSQL 楼方鑫 平民软件11:20 - 12:00 Greenplum 5.0及战略规划 陆公瑜 Pivotal 28日上午：分会场3（Not Only PostgreSQL）时间 主题 嘉宾 公司09:00 - 09:40 抽丝剥茧之MySQL疑难杂症排查 叶金荣 知数堂09:40 - 10:20 Hbase的最佳实践及优化 陈飚 Cloudera10:20 - 10:40 中场休息10:40 - 11:20 大数据实时流计算风云榜 陈旭 平安壹钱包11:20 - 12:00 固态硬盘的前世今生 钟勇 宝存科技 28日下午：分会场1（PostgreSQL特性分析）时间 主题 嘉宾 公司14:00 - 14:40 数据库多版本实现 唐成 杭州云徙科技14:40 - 15:20 PG自适应流复制技术实现 姜瑞海 山东瀚高15:20 - 15:40 中场休息15:40 - 16:20 sharding单元化（based on postgres_fdw)最佳实践 周正中 阿里云16:20 - 17:00 PostgreSQL优化器浅析 范孝剑 阿里云 28日下午：分会场2（PostgreSQL分布式及大数据实践）时间 主题 嘉宾 公司14:00 - 14:40 DeepGreen DB: 性能优化、开发方向 C.K. Tan Vitesse Data, Inc.14:40 - 15:20 Postgres-XC/XL/X2 Mason 华为15:20 - 15:40 中场休息15:40 - 16:20 Hadoop最新结构化存储利器Kudu介绍 陈飚 Cloudera16:20 - 17:00 阿里云ApsaraDB for Greenplum介绍 张广舟 阿里云 28日下午：分会场3（PostgreSQL + 技术研发）时间 主题 嘉宾 公司14:00 - 14:40 PostgreSQL数据库源码初探 陈刚 平安科技14:40 - 15:20 从PG实现FLASHBACK模块谈小白如何开始内核开发 兰海 武汉大学15:20 - 15:30 中场休息15:30 - 16:10 PostgreSQL流复制优化 张文杰 阿里云16:10 - 16:50 PostgreSQL窗口聚集函数执行优化 宋光旋 华东师范大学16:50 - 17:30 MongoDB分布式架构演进 张友东 阿里云 演讲嘉宾及主题简介平安科技 数据库技术部总监汪洋演讲主题：PG+金融的架构演进之路开源技术乃未来之势，并且随着多年的发展逐步趋于成熟，数据库领域也无例外。但有人对于在金融服务领域引入开源产品，特别是开源数据库产品，始终心存怀疑，担心对系统稳定、性能以及数据安全产生负面影响，以至影响企业声誉，甚或造成经济损失。本演讲通过平安科技2年来的实际经验，向大家讲述如何在一家财富全球500强中排名第41位的金融帝国中，展开帝国反击战，大胆引入且使用PostgreSQL开源数据库,并通过系统化的推进，实现PG+金融的完美结合！让PG不仅是开源世界中最强大的数据库，也同样成为服务金融行业的最强大开源数据库！阿里云高级技术专家周正中演讲主题：PostgreSQL 最佳实践本次分享的主题，将给大家介绍PostgreSQL的最佳实践，让它能够稳定的承载几十万甚至几百万的TPS；几十TB甚至几百TB的数据量；同时也会包含部分阿里云ApsaraDB for PostgreSQL的内核优化的分享。PostgreSQL是40几年陈的产品，算是关系数据库中的老大哥，但是它依旧保持着飞速的迭代和发展。这是为什么呢？通过本次分享，将给大家展示PostgreSQL的发展历程，每个时代的里程碑特性，背后的故事和应用场景，帮助大家更好的理解PostgreSQL的生态圈子，社区的设计理念，未来的Roadmap。Vitesse Data, Inc. 发起人C.K. Tan演讲主题：DeepGreen DB: 性能优化、开发方向DeepGreen DB 通过采用 JIT, bloomfilter, post-optimization 等技术，较 Greenplum DB 性能有了多倍的提高。我们简单介绍这些优化技术，并讨论这些技术的应用范围。之后，我们介绍在性能优化工作之外 DeepGreen DB 如何同 PostgreSQL ODS， Hadoop/S3 集成，从而为用户提供一个整体的数据湖解决方案平民软件创始人楼方鑫演讲主题：Spark over OneProxy/PostgreSQLSpark over Distributed PostgreSQL/MySQL Cluster通过中间层代理软件OneProxy将数据根据规则合理而均衡地分布到多台PostgreSQL数据库，通过特定的Spark Connector来让OneProxy作为Spark的外挂Partition Manager，连接Spark和PostgreSQL集群，进行在线数据的即时分析，是一个省事省心的方案，可以和Greenplum一较高下。腾讯云数据库架构师李跃森演讲主题：PGXZ在微信支付中的应用pgxz的动态扩容解决方案，在不影响业务的前提下做到系统动态扩容;数据倾斜解决方案，有效应对严重的数据倾斜问题;高效的内置集群分区表，高效处理分区表，性能是原生分区的1-2个数量级;海量数据高速排序算法实现，上亿条数据快速排序输出;数据高可用保证，两地三中心的自动容灾能力.运维能力分享：索引膨胀问题的应对，索引重建和替换;数据膨胀问题的应对，在线vacuum full贵州云飞科技研发部经理何祖文演讲主题：智慧物流与PostgreSQL中小型物流企业运输业务复杂性，货物难以追踪甚至有追不到的情况。针对这种特殊的情况，我们做出了物流运输平台（TMS平台）。主题主要介绍的运输模块数据主要使用PG来进行存储，货物车辆的追踪，历史线路的轨迹回放查询。其中运用到具体的pg技术会跟着业务穿插介绍。主要有空间数据存储(Point,line,Polygon…)，PostGIS几何函数的使用()，GIST索引针对历史轨迹的查询优化等。阿里云高级专家张广舟演讲主题：ApsaraDB for Greenplum介绍Greenplum作为MPP数据库仓库领域的佼佼者，开源后受到广泛关注。阿里云也已经正式推出了基于Greenplum的云产品-ApsaraDB for Greenplum。本分享为大家介绍ApsaraDB for Greenplum的架构、功能设计、内核定制等北京英资教育科技有限公司CEO孙鹏演讲主题：数据库设计中对JSON的使用JSON数据格式由于其简洁性，越来越多的得到了开发者的青睐，我将演示如何在数据库设计中使用JSON/JSONB的数据类型，探讨对||、array_to_json等操作符和函数的使用，以加快系统的开发效率，降低应用层的编码行数知数堂培训联合创始人叶金荣演讲主题：抽丝剥茧之MySQL疑难杂症排查从一个故障排查案例入手，分析MySQL常见故障处理手法平安科技数据库技术专家梁海安演讲主题：金融级PostgreSQL数据库监控与优化平安科技在2015年起引入开源PostgreSQL，经过两年的推广，已有上千个PG实例。本演讲通过平安科技2年来的实际运维经验，向大家讲述如何利用规范，自动化，精细运维等手段，在大幅增长的数据库规模同时保持数据库高可用平安科技数据库技术部分组经理王鹏冲演讲主题：从ACID的D看三种主流关系型数据库多数关系型数据库使用预写日志协议（Write-Ahead Logging protocol ，WAL）来处理事务日志，但是每种数据库在具体事务日志的设计策略上又不尽相同。本环节就主流的三种关系型数据库Oracle、PostgreSQL、Mysql在事务日志的处理方式上尝试进行研究探讨、对比分析，以期加深对不同数据库的设计思想的理解，并能给予日常实践工作一定的指导意义。平安科技数据库架构师陈刚演讲主题：PostgreSQL数据库源码初探如何能深入了解该数据库的技术原理，如何在数据库中定制所需要的功能和特性，乃至于改造数据库核心功能？最有效的方法就是基于源码层面进行学习和研究。本演讲将从初学者的角度介绍PostgreSQL的源码学习心得，并且介绍如何基于插件的开发来扩充数据库的功能。同时也倡导大家都参与到PostgreSQL源码学习的队伍之中。北京太阳塔信息科技CEO赵振平演讲主题：工业大数据初探工业大数据是互联网、大数据和工业产业结合的产物，是中国制造2025、工业互联网、工业4.0等国家战略在企业的发力点。我们主要探索工业大数据的提出、工业互联网涉及的相关技术，以及工业大数据应用。涉及的技术有：条形码、二维码、RFID、工业传感器、工业自动控制系统、工业物联网平安科技数据库架构师高金芳演讲主题：Postgresql“反向代理”RedisRedis是一个高性能的key-value数据库，它追求的是高效和简洁，在安全设计方面相对比较薄弱，没有用户的概念。本演讲将介绍使用PostgreSQL的外部表访问Redis，通过PostgreSQL权限管理方式对Redis进行后台管理武汉大学计算机学院兰海演讲主题：从flashback功能谈如何内核定制开发此次演讲将从一个具体的pg内核功能扩展上来，看如何进行内核的开发工作。Flashback是一系列闪回技术统称，包括了闪回查询，闪回表等。在商业产品oracle上有成熟和完善的闪回功能，而在开源的pg源码上没有有关的功能。本次主要在pg上实现闪回查询和闪回删除表。闪回查询能够在撤销段内搜素撤销的数据（“旧”数据），闪回删除表能够将已经删除的表“抢救回来”阿里云资深开发工程师范孝剑演讲主题：PostgreSQL优化器浅析在使用PostgreSQL数据库过程中，对SQL调优最常用的手段是使用explain查看执行计划，很多时候我们只关注了执行计划的结果而未深入了解执行计划是如何生成的。优化器作为数据库核心功能之一，也是数据库的“大脑”，理解优化器将有助于我们更好地优化SQL，本次分享主题将会为大家解开PostgreSQL优化器神秘的面纱。阿里云开发工程师张文杰演讲主题：PostgreSQL流复制优化虽然PG流复制功能越来越完善，但是仍然存在很多问题，例如主备延迟较大，主备断开，主备切换丢失数据，在一些场景下这些问题是不能忍受的。我们首先将简单介绍下目前PostgreSQL的流复制方案，然后以各种场景为例，具体去分析如何解决存在的问题，或者在一定程度上降低相应问题带来的风险。华东师范大学宋光旋演讲主题：PostgreSQL窗口聚集函数执行优化在目前互联网应用步入大数据时代的背景下，针对高吞吐和实时响应等需求，已有的Window(窗口)函数的处理性能已经出现了瓶颈。因而我们修改了PostgreSQL的现有执行框架，提出一些通用的和针对某些特定聚集函数的优化方法，大幅度提高PostgreSQL窗口函数的执行速度。探探科技张文升演讲主题：PostgreSQL和探探见证四亿次心动以开始聊天，眼缘不错，趣味相投，向陌生人尴尬的搭讪说再见。探探，就是这样一款带你找到生命中的那个人的简单APP。它简单，也不简单。探探有4000万＋用户，日活跃用户400万＋，架构非常简单：go ＋ PostgreSQL。目前已有415462275对一见钟情已在探探相遇，记录仍在持续刷新中。左划无爱，右滑喜欢，PostgreSQL在背后支撑着每一次的推荐和滑动。本次将有探探为大家分享PostgreSQL如何见证4亿次心动。杭州云徙科技云技术总监唐成演讲主题：数据库多版本实现多版本并发控制技术是目前主流数据库的主流技术。目前，多版本并发控制被很多数据库或存储引擎采用，如Oracle，MS SQL Server 2005+, PostgreSQL, MySQL（InnoDB）等等。新的数据库存储引擎，几乎毫无例外的使用多版本而不是单版本加锁的方法实现并发控制。本演讲详细讲解了MVCC的原理，以及innodb、Oracle、PostgreSQL中多版本实现的一些技术内幕飞象数据赖伟演讲主题：从Sybase迁移到PostgreSQL的那些事儿从介绍飞象数据的数据库迁移方法论开始，以真实的Sybase数据库迁移到PostgreSQL实际项目为案例，介绍Sybase数据库迁移到PostgreSQL数据库的工具、流程Cloudera陈飚演讲主题：HBase最佳实践和优化HBase的项目由Cloudera公司在约10年前贡献到开源社区，现在已经是全球最流行的NoSQL数据库之一，被很多厂商当为高可靠高性能键值查询场景的标准部署。作为Hadoop届的TOP1厂商，Cloudera有着世界上最广泛的大数据平台案例。在本次分享中Cloudera将介绍其客户生产环境中HBase的典型应用场景和案例，并结合多年大型集群部署和运维经验，介绍HBase在开发设计和参数配置时的最佳实践以及优化建议。","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL9.6 新特性汇总","slug":"20161001001513","date":"2016-09-30T16:15:13.000Z","updated":"2018-09-04T01:34:20.816Z","comments":true,"path":"20161001001513.html","link":"","permalink":"https://postgres.fun/20161001001513.html","excerpt":"","text":"昨天 PostgreSQL9.6 正式版已发行，新功能包括并行查询、同步复制改进、短语搜索、 性能和易用性方面的改进，由于时间关系，目前仅对 9.6 一部分特性进行研究，如下： PostgreSQL9.6：Parallel Sequential Scans 初体验 PostgreSQL9.6：Parallel Aggregates 初体验 PostgreSQL9.6：Partial index 索引支持 Index-Only Scan PostgreSQL9.6：新增等待事件（Wait Event）性能监控 PostgreSQL9.6：新增 pg_stat_progress_vacuum 视图监控 VACUUM PostgreSQL9.6：新增pg_blocking_pids函数准确定位 Blocking SQL PostgreSQL9.6：新增加“idle in transaction”超时空闲事务自动查杀功能 PostgreSQL9.6：新增内置默认角色（pg_signal_backend） PostgresSQL9.6：新增 pg_size_bytes() 函数 PostgreSQL9.6：新增 Bloom 索引类型支持任意列组合查询","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"MySQL 锁机制之一：初识行锁、表锁","slug":"20160828164656","date":"2016-08-28T08:46:56.000Z","updated":"2018-09-04T01:34:20.753Z","comments":true,"path":"20160828164656.html","link":"","permalink":"https://postgres.fun/20160828164656.html","excerpt":"","text":"今天看了 MySQL InnoDB 锁相关文档，内容挺多，锁的原理和机制比起 PostgreSQL 也稍显复杂，今天学习到行锁、表锁相关内容，了解到了一个奇特的特性，即“InnoDB 表根据无索引字段更新时，即使更新不同的数据行也会发生阻塞”。 InnoDB 行锁是通过索引上的索引项来实现的，也就是说InnoDB只有通过索引条件检索数据时才使用行级锁，否则将使用表级锁，举例如下： 一、不使用索引的场景创建测试表12345678910111213141516francs@localhost:francs&gt;create table test_lock1(id int4,name varchar(32));Query OK, 0 rows affected (0.17 sec) francs@localhost:francs&gt;insert into test_lock1(id,name) values (1,'a'),(2,'b'),(3,'c');Query OK, 3 rows affected (0.04 sec)Records: 3 Duplicates: 0 Warnings: 0 francs@localhost:francs&gt;select * from test_lock1;+------+------+| id | name |+------+------+| 1 | a|| 2 | b|| 3 | c|+------+------+3 rows in set (0.00 sec) 开启会话一12345678910francs@localhost:francs&gt;begin;Query OK, 0 rows affected (0.00 sec) francs@localhost:francs&gt;select * from test_lock1 where id=1 for update;+------+------+| id | name |+------+------+| 1 | a|+------+------+1 row in set (0.00 sec) 开启会话二12345francs@localhost:francs&gt;begin;Query OK, 0 rows affected (0.00 sec)francs@localhost:francs&gt;select * from test_lock1 where id=2 for update;此SQL处于等待状态 备注：更新表上不同的数据行也会产生等待，这很令人费解，PostgreSQL、Oracle 都不会出现这种情况。 开启另一会话查询 INNODB_TRX12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152root@localhost:information_schema&gt;select * from INNODB_TRXG* 1. row * trx_id: 511689 trx_state: LOCK WAIT trx_started: 2016-08-28 16:09:14 trx_requested_lock_id: 511689:42574:3:2 trx_wait_started: 2016-08-28 16:10:27 trx_weight: 2 trx_mysql_thread_id: 57 trx_query: select * from test_lock1 where id=2 for update trx_operation_state: starting index read trx_tables_in_use: 1 trx_tables_locked: 1 trx_lock_structs: 2 trx_lock_memory_bytes: 1096 trx_rows_locked: 2 trx_rows_modified: 0 trx_concurrency_tickets: 0 trx_isolation_level: REPEATABLE READ trx_unique_checks: 1 trx_foreign_key_checks: 1trx_last_foreign_key_error: NULLtrx_adaptive_hash_latched: 0trx_adaptive_hash_timeout: 0 trx_is_read_only: 0trx_autocommit_non_locking: 0* 2. row * trx_id: 511688 trx_state: RUNNING trx_started: 2016-08-28 16:07:15 trx_requested_lock_id: NULL trx_wait_started: NULL trx_weight: 2 trx_mysql_thread_id: 53 trx_query: NULL trx_operation_state: NULL trx_tables_in_use: 0 trx_tables_locked: 1 trx_lock_structs: 2 trx_lock_memory_bytes: 1096 trx_rows_locked: 4 trx_rows_modified: 0 trx_concurrency_tickets: 0 trx_isolation_level: REPEATABLE READ trx_unique_checks: 1 trx_foreign_key_checks: 1trx_last_foreign_key_error: NULLtrx_adaptive_hash_latched: 0trx_adaptive_hash_timeout: 0 trx_is_read_only: 0trx_autocommit_non_locking: 02 rows in set (0.01 sec) 备注：“trx_state”字段有 RUNNING, LOCK WAIT, ROLLING BACK or COMMITTING值， RUNNING表示运行中，LOCK WAIT 表示等待；从上面看出，事务511689处于等待状态。 二、使用索引场景给表 test_lock1 加上主键123francs@localhost:francs&gt;alter table test_lock1 add primary key (id);Query OK, 0 rows affected (0.12 sec)Records: 0 Duplicates: 0 Warnings: 0 开启会话一12345678910francs@localhost:francs&gt;begin;Query OK, 0 rows affected (0.00 sec) francs@localhost:francs&gt;select * from test_lock1 where id=1 for update;+------+------+| id | name |+------+------+| 1 | a|+------+------+1 row in set (0.00 sec) 开启会话二12345678910francs@localhost:francs&gt;begin;Query OK, 0 rows affected (0.00 sec) francs@localhost:francs&gt;select * from test_lock1 where id=2 for update;+----+------+| id | name |+----+------+| 2 | b|+----+------+1 row in set (0.00 sec) 备注：此事务没有等待，执行成功。 开启另一会话查询 INNODB_TRX12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152root@localhost:information_schema&gt;select * from INNODB_TRXG* 1. row * trx_id: 511707 trx_state: RUNNING trx_started: 2016-08-28 16:18:07 trx_requested_lock_id: NULL trx_wait_started: NULL trx_weight: 2 trx_mysql_thread_id: 57 trx_query: NULL trx_operation_state: NULL trx_tables_in_use: 0 trx_tables_locked: 1 trx_lock_structs: 2 trx_lock_memory_bytes: 1096 trx_rows_locked: 1 trx_rows_modified: 0 trx_concurrency_tickets: 0 trx_isolation_level: REPEATABLE READ trx_unique_checks: 1 trx_foreign_key_checks: 1trx_last_foreign_key_error: NULLtrx_adaptive_hash_latched: 0trx_adaptive_hash_timeout: 0 trx_is_read_only: 0trx_autocommit_non_locking: 0* 2. row * trx_id: 511706 trx_state: RUNNING trx_started: 2016-08-28 16:17:43 trx_requested_lock_id: NULL trx_wait_started: NULL trx_weight: 2 trx_mysql_thread_id: 53 trx_query: NULL trx_operation_state: NULL trx_tables_in_use: 0 trx_tables_locked: 1 trx_lock_structs: 2 trx_lock_memory_bytes: 1096 trx_rows_locked: 1 trx_rows_modified: 0 trx_concurrency_tickets: 0 trx_isolation_level: REPEATABLE READ trx_unique_checks: 1 trx_foreign_key_checks: 1trx_last_foreign_key_error: NULLtrx_adaptive_hash_latched: 0trx_adaptive_hash_timeout: 0 trx_is_read_only: 0trx_autocommit_non_locking: 02 rows in set (0.01 sec) 备注：无处于LOCK WAIT 状态的事务。 三、总结MySQL 通过索引项实现数据行加锁，具体原理机制现在还不是很清楚，后续学习补充。 四、参考 InnoDB Locking MySQL详见–锁","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL5.7：General Tablespaces 初体验","slug":"20160826114708","date":"2016-08-26T03:47:08.000Z","updated":"2018-09-04T01:34:20.691Z","comments":true,"path":"20160826114708.html","link":"","permalink":"https://postgres.fun/20160826114708.html","excerpt":"","text":"MySQL 5.7 版本 InnoDB 支持新的表空间类型 General Tablespaces ，表空间的物理文件位置可以位于 MySQL data 目录以外，语法如下： 1234CREATE TABLESPACE `tablespace_name`ADD DATAFILE 'file_name.ibd' [FILE_BLOCK_SIZE = n]下面做个简单演示： 创建表空间目录1[mysql@db1 mysql]$ mkdir -p /database/mysql/tbs 备注：我这里搭建了 MySQL 主从环境，在创建表空间前主从了点上都先创建表空间目录，如果从库不创建此目录，那么创建表空间时从库会报错，如下： 错误代码1234562016-08-26T03:29:38.779278Z 9 [ERROR] Slave SQL for channel '': Error 'Incorrect File Name '/database/mysql/tbs_test/tbs_test.ibd'.' on query. Default database: 'information_schema'. Query: 'CREATE TABLESPACE `tbs_test` ADD DATAFILE '/database/mysql/tbs_test/tbs_test.ibd' Engine=InnoDB', Error_code: 31212016-08-26T03:29:38.779688Z 9 [Warning] Slave: Incorrect File Name '/database/mysql/tbs_test/tbs_test.ibd'. Error_code: 31212016-08-26T03:29:38.779751Z 9 [Warning] Slave: The directory does not exist. Error_code: 31212016-08-26T03:29:38.779786Z 9 [Warning] Slave: Failed to create TABLESPACE tbs_test Error_code: 15282016-08-26T03:29:38.779818Z 9 [Warning] Slave: Table storage engine for 'tbs_test' doesnt have this option Error_code: 10312016-08-26T03:29:38.779851Z 9 [ERROR] Error running query, slave SQL thread aborted. Fix the problem, and restart the slave SQL thread with \"SLAVE START\". We stopped at log 'bin-log.000026'position 2339 备注：日志中已经提示了修复方法，只需在备节点上创建相应目录并重启备库实例即可。 创建表空间1234567891011121314151617root@localhost:mysql&gt;CREATE TABLESPACE `tbs1` ADD DATAFILE '/database/mysql/tbs/tbs1.ibd' Engine=InnoDB;Query OK, 0 rows affected (0.12 sec) root@localhost:information_schema&gt;select * from INNODB_SYS_TABLESPACES where name='tbs1'G* 1. row * SPACE: 42569 NAME: tbs1 FLAG: 2048 FILE_FORMAT: Any ROW_FORMAT: Any PAGE_SIZE: 16384ZIP_PAGE_SIZE: 0 SPACE_TYPE: GeneralFS_BLOCK_SIZE: 4096 FILE_SIZE: 655360ALLOCATED_SIZE: 6594561 row in set (0.00 sec) 创建表12root@localhost:francs&gt;CREATE TABLE t1 (id INT PRIMARY KEY) TABLESPACE tbs1 Engine=InnoDB;Query OK, 0 rows affected (0.06 sec) 备注：以上将表 t1 创建在 General Tablespaces，也可以将现有表 MOVE 到 General Tablespaces。 ALTER TABLE1234567891011root@localhost:francs&gt;select count(*) from test_1;+----------+| count(*) |+----------+| 10001 |+----------+1 row in set (0.14 sec) root@localhost:francs&gt;alter table test_1 tablespace tbs1;Query OK, 0 rows affected (1.16 sec)Records: 0 Duplicates: 0 Warnings: 0 关于 .isl文件General Tablespaces 创建后，会在 MySQL DATA 目录中新生成 .isl 文件，文件的内容为文本，记录了 General Tablespaces 对应的文件物理路径。123456[mysql@db1 data]$ ll /database/mysql/data/tbs_test.isl-rw-r----- 1 mysql mysql 37 Aug 26 11:29 /database/mysql/data/tbs_test.isl [mysql@db1 data]$ view tbs_test.isl /database/mysql/tbs_test/tbs_test.ibd 参考 InnoDB General Tablespaces CREATE TABLE Syntax .isl file","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL高可用之一：主从复制环境 MHA 部署","slug":"20160709173544","date":"2016-07-09T09:35:44.000Z","updated":"2018-09-04T01:34:20.628Z","comments":true,"path":"20160709173544.html","link":"","permalink":"https://postgres.fun/20160709173544.html","excerpt":"","text":"MHA（Master High Availability)目前在MySQL高可用方面是一个相对成熟的解决方案， MHA 官方宣称在故障切换时通常能在 30 秒内完成，并且最大的优势是：在故障切换过程中能够最大限度保证数据一致性。 今天先记录下 MHA 的部署过程，高可用部分可以结合 MHA 自带的脚本或 keepalived 管理 VIP ，以后再做这方面的测试。 一、安装前准备环境信息 主机名 IP 操作系统 角色 db1 192.168.2.37 RHEL6.2 MySQL MASTER + MHA NODE db2 192.168.2.38 RHEL6.2 MySQL SLAVE + MHA NODE db3 192.168.2.39 RHEL6.2 MHA MANAGER 备注：笔记本三台虚机，db1做主库，db2做备库，db3做 MHA MANAGER 结点。 前提条件db1、db2节点已配置半同步复制，详见 MySQL5.7: 半同步复制（Semisynchronous Replication）配置 db1、db2 上创建监控用户12345root@localhost:mysql&gt;grant all privileges on *.* to 'root'@'db2' identified by '111111';Query OK, 0 rows affected, 1 warning (0.01 sec) root@localhost:mysql&gt;grant all privileges on *.* to 'root'@'db3' identified by '111111';Query OK, 0 rows affected, 1 warning (0.01 sec) db2 设置只读12[mysql@db2 ~]$ mysql -uroot -p -e \"set global read_only=1\"Enter password: 下载https://code.google.com/p/mysql-master-ha/wiki/Downloads?tm=2 二、安装 MHA NODEdb1、db2、db3节点上编译安装 MHA NODE12345# yum install perl-DBD-MySQL perl-devel perl-CPAN -y# tar xvf mha4mysql-node-0.56.tar.gz # perl Makefile.PL# make# make install 备注：安装完 MHA NODE 结点后，在 /usr/local/bin 目录下存在以下文件。 MHA NODE 生成的脚本1234-r-xr-xr-x 1 root root7525 Jul 8 20:42 save_binary_logs-r-xr-xr-x 1 root root8261 Jul 8 20:42 purge_relay_logs-r-xr-xr-x 1 root root4807 Jul 8 20:42 filter_mysqlbinlog-r-xr-xr-x 1 root root 16367 Jul 8 20:42 apply_diff_relay_logs 备注，这几个脚本作用如下：save_binary_logs: 保存和复制主节点的 binary logapply_diff_relay_logs: 识别差异的 relay 日志事件并将差异应用于其它备节点purge_relay_logs: 清除 relay 日志 三、安装 MHA Manager1、db3 节点上编译安装 MHA MANAGER123456789# yum install perl-DBD-MySQL# yum install perl-Config-Tiny # yum install perl-Log-Dispatch # yum install perl-Parallel-ForkManager # yum install perl-Time-HiRes# tar xvf mha4mysql-manager-0.56.tar.gz# perl Makefile.PL# make# make install 备注：如果系统yum源找不到某个安装包，网上下载相应包安装即可。 2、配置信任关系db112# ssh-keygen -t rsa# ssh-copy-id -i /root/.ssh/id_rsa.pub root@db2 db212# ssh-keygen -t rsa# ssh-copy-id -i /root/.ssh/id_rsa.pub root@db1 db3123# ssh-keygen -t rsa# ssh-copy-id -i /root/.ssh/id_rsa.pub root@db1# ssh-copy-id -i /root/.ssh/id_rsa.pub root@db2 3、编辑 MHA MANAGER 配置文件创建 MHA MANAGER 目录12# mkdir -p /opt/masterha# mkdir -p /opt/masterha/app1 创建文件 /opt/masterha/app1.cnf1234567891011121314151617181920212223242526[server default]# mysql user and passworduser=rootpassword=111111ssh_user=rootmaster_pid_file=/opt/masterha/app1/master1.pidmanager_log=/opt/masterha/app1/manager.logmaster_binlog_dir=/database/mysql/data/binlogrepl_user=rep1repl_password=rep1abcd1243dping_interval=3master_ip_failover_script= '' # working directory on the managermanager_workdir=/opt/masterha/app1 # working directory on MySQL serversremote_workdir=/opt/masterha/app1 [server2]hostname=db1port=3306 [server1]hostname=db2port=3306 备注：主要参数解释：user: MySQL 数据库超级用户，这里配置成 rootpasword: MySQL 数据库 root 用户密码ssh_user：MHA MANAGER 和 MHA NODE 节点SSH通信时的操作系统用户manager_log： MHA MANAGER 日志文件master_binlog_dir: Master 主节点 binlog 目录位置repl_user：复制环境的用户名ping_interval：MHA MANAGER 节点 ping master 节点的时间间隔，默认三秒，如果三次 PING 不通，则判断主节点宕机。 更多参数详见：https://code.google.com/p/mysql-master-ha/wiki/Parameters#master_pid_file 4、Checking SSH connections123456789101112[root@db3 ~]# masterha_check_ssh --conf /opt/masterha/app1/app1.cnf Fri Jul 8 22:21:30 2016 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.Fri Jul 8 22:21:30 2016 - [info] Reading application default configuration from /opt/masterha/app1/app1.cnf..Fri Jul 8 22:21:30 2016 - [info] Reading server configuration from /opt/masterha/app1/app1.cnf..Fri Jul 8 22:21:30 2016 - [info] Starting SSH connection tests..Fri Jul 8 22:21:31 2016 - [debug] Fri Jul 8 22:21:30 2016 - [debug] Connecting via SSH from root@db1(192.168.2.37:22) to root@db2(192.168.2.38:22)..Fri Jul 8 22:21:31 2016 - [debug] ok.Fri Jul 8 22:21:31 2016 - [debug] Fri Jul 8 22:21:31 2016 - [debug] Connecting via SSH from root@db2(192.168.2.38:22) to root@db1(192.168.2.37:22)..Fri Jul 8 22:21:31 2016 - [debug] ok.Fri Jul 8 22:21:31 2016 - [info] All SSH connection tests passed successfully. 5、Checking Replication Configurationmasterha_check_repl 报错12345[root@db3 app1]# masterha_check_repl --conf=/opt/masterha/app1/app1.cnf Sat Jul 9 07:25:03 2016 - [info] Connecting to root@192.168.2.38(db2:22).. Can not exec \"mysqlbinlog\": No such file or directory at /usr/local/share/perl5/MHA/BinlogManager.pm line 106.mysqlbinlog version command failed with rc 1:0, please verify PATH, LD_LIBRARY_PATH, and client options at /usr/local/bin/apply_diff_relay_logs line 493 解决方法：db1、db2 添加软链接12ln -s /opt/mysql/bin/mysqlbinlog /usr/local/bin/mysqlbinlogln -s /opt/mysql/bin/mysql /usr/local/bin/mysql 备注：之后看到如下信息：1234567....省略Sat Jul 9 10:37:19 2016 - [info] Checking replication health on db2..Sat Jul 9 10:37:19 2016 - [info] ok.Sat Jul 9 10:37:19 2016 - [warning] master_ip_failover_script is not defined.Sat Jul 9 10:37:19 2016 - [warning] shutdown_script is not defined.Sat Jul 9 10:37:19 2016 - [info] Got exit code 0 (Not master dead).MySQL Replication Health is OK. 备注：说明 Replication 检测通过。 6、启动 MHA MANAGER12[root@db3 app1]# masterha_manager --conf=/opt/masterha/app1/app1.cnf &gt; /tmp/mha_manager.log 2&gt;&amp;1 &amp;[1] 4449 7、查看 MHA MANAGER状态12[root@db3 app1]# masterha_check_status --conf=/opt/masterha/app1/app1.cnf app1 (pid:4449) is running(0:PING_OK), master:db1 四、主从切换测试：自动方式1、db1 上关闭主库12[mysql@db1 bin]$ mysqladmin -uroot -p shutdownEnter password: 2、db3 上查看 MHA MANAGER 日志 /opt/masterha/app1/manager.log123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164Sat Jul 9 10:39:24 2016 - [warning] Got error on MySQL select ping: 2006 (MySQL server has gone away)Sat Jul 9 10:39:24 2016 - [info] Executing SSH check script: save_binary_logs --command=test --start_pos=4 --binlog_dir=/database/mysql/data/binlog --output_file=/opt/masterha/app1/save_binary_logs_test --manager_version=0.56 --binlog_prefix=bin-logSat Jul 9 10:39:24 2016 - [info] HealthCheck: SSH to db1 is reachable.Sat Jul 9 10:39:27 2016 - [warning] Got error on MySQL connect: 2013 (Lost connection to MySQL server at 'reading initial communication packet', system error: 111)Sat Jul 9 10:39:27 2016 - [warning] Connection failed 2 time(s)..Sat Jul 9 10:39:30 2016 - [warning] Got error on MySQL connect: 2013 (Lost connection to MySQL server at 'reading initial communication packet', system error: 111)Sat Jul 9 10:39:30 2016 - [warning] Connection failed 3 time(s)..Sat Jul 9 10:39:33 2016 - [warning] Got error on MySQL connect: 2013 (Lost connection to MySQL server at 'reading initial communication packet', system error: 111)Sat Jul 9 10:39:33 2016 - [warning] Connection failed 4 time(s)..Sat Jul 9 10:39:33 2016 - [warning] Master is not reachable from health checker!Sat Jul 9 10:39:33 2016 - [warning] Master db1(192.168.2.37:3306) is not reachable!Sat Jul 9 10:39:33 2016 - [warning] SSH is reachable.Sat Jul 9 10:39:33 2016 - [info] Connecting to a master server failed. Reading configuration file /etc/masterha_default.cnf and /opt/masterha/app1/app1.cnf again, and trying to connect to all servers to check server status..Sat Jul 9 10:39:33 2016 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.Sat Jul 9 10:39:33 2016 - [info] Reading application default configuration from /opt/masterha/app1/app1.cnf..Sat Jul 9 10:39:33 2016 - [info] Reading server configuration from /opt/masterha/app1/app1.cnf..Sat Jul 9 10:39:34 2016 - [info] GTID failover mode = 0Sat Jul 9 10:39:34 2016 - [info] Dead Servers:Sat Jul 9 10:39:34 2016 - [info] db1(192.168.2.37:3306)Sat Jul 9 10:39:34 2016 - [info] Alive Servers:Sat Jul 9 10:39:34 2016 - [info] db2(192.168.2.38:3306)Sat Jul 9 10:39:34 2016 - [info] Alive Slaves:Sat Jul 9 10:39:34 2016 - [info] db2(192.168.2.38:3306) Version=5.7.13-debug-log (oldest major version between slaves) log-bin:enabledSat Jul 9 10:39:34 2016 - [info] Replicating from 192.168.2.37(192.168.2.37:3306)Sat Jul 9 10:39:34 2016 - [info] Checking slave configurations..Sat Jul 9 10:39:34 2016 - [warning] relay_log_purge=0 is not set on slave db2(192.168.2.38:3306).Sat Jul 9 10:39:34 2016 - [info] Checking replication filtering settings..Sat Jul 9 10:39:34 2016 - [info] Replication filtering check ok.Sat Jul 9 10:39:34 2016 - [info] Master is down!Sat Jul 9 10:39:34 2016 - [info] Terminating monitoring script.Sat Jul 9 10:39:34 2016 - [info] Got exit code 20 (Master dead).Sat Jul 9 10:39:34 2016 - [info] MHA::MasterFailover version 0.56.Sat Jul 9 10:39:34 2016 - [info] Starting master failover.Sat Jul 9 10:39:34 2016 - [info]Sat Jul 9 10:39:34 2016 - [info] * Phase 1: Configuration Check Phase..Sat Jul 9 10:39:34 2016 - [info]Sat Jul 9 10:39:35 2016 - [info] GTID failover mode = 0Sat Jul 9 10:39:35 2016 - [info] Dead Servers:Sat Jul 9 10:39:35 2016 - [info] db1(192.168.2.37:3306)Sat Jul 9 10:39:35 2016 - [info] Checking master reachability via MySQL(double check)...Sat Jul 9 10:39:35 2016 - [info] ok.Sat Jul 9 10:39:35 2016 - [info] Alive Servers:Sat Jul 9 10:39:35 2016 - [info] db2(192.168.2.38:3306)Sat Jul 9 10:39:35 2016 - [info] Alive Slaves:Sat Jul 9 10:39:35 2016 - [info] db2(192.168.2.38:3306) Version=5.7.13-debug-log (oldest major version between slaves) log-bin:enabledSat Jul 9 10:39:35 2016 - [info] Replicating from 192.168.2.37(192.168.2.37:3306)Sat Jul 9 10:39:35 2016 - [info] Starting Non-GTID based failover.Sat Jul 9 10:39:35 2016 - [info]Sat Jul 9 10:39:35 2016 - [info] Phase 1: Configuration Check Phase completed.Sat Jul 9 10:39:35 2016 - [info]Sat Jul 9 10:39:35 2016 - [info] * Phase 2: Dead Master Shutdown Phase..Sat Jul 9 10:39:35 2016 - [info]Sat Jul 9 10:39:35 2016 - [info] Forcing shutdown so that applications never connect to the current master..Sat Jul 9 10:39:35 2016 - [warning] master_ip_failover_script is not set. Skipping invalidating dead master IP address.Sat Jul 9 10:39:35 2016 - [warning] shutdown_script is not set. Skipping explicit shutting down of the dead master.Sat Jul 9 10:39:36 2016 - [info] * Phase 2: Dead Master Shutdown Phase completed.Sat Jul 9 10:39:36 2016 - [info]Sat Jul 9 10:39:36 2016 - [info] * Phase 3: Master Recovery Phase..Sat Jul 9 10:39:36 2016 - [info]Sat Jul 9 10:39:36 2016 - [info] * Phase 3.1: Getting Latest Slaves Phase..Sat Jul 9 10:39:36 2016 - [info]Sat Jul 9 10:39:36 2016 - [info] The latest binary log file/position on all slaves is bin-log.000018:2659Sat Jul 9 10:39:36 2016 - [info] Latest slaves (Slaves that received relay log files to the latest):Sat Jul 9 10:39:36 2016 - [info] db2(192.168.2.38:3306) Version=5.7.13-debug-log (oldest major version between slaves) log-bin:enabledSat Jul 9 10:39:36 2016 - [info] Replicating from 192.168.2.37(192.168.2.37:3306)Sat Jul 9 10:39:36 2016 - [info] The oldest binary log file/position on all slaves is bin-log.000018:2659Sat Jul 9 10:39:36 2016 - [info] Oldest slaves:Sat Jul 9 10:39:36 2016 - [info] db2(192.168.2.38:3306) Version=5.7.13-debug-log (oldest major version between slaves) log-bin:enabledSat Jul 9 10:39:36 2016 - [info] Replicating from 192.168.2.37(192.168.2.37:3306)Sat Jul 9 10:39:36 2016 - [info]Sat Jul 9 10:39:36 2016 - [info] * Phase 3.2: Saving Dead Master's Binlog Phase..Sat Jul 9 10:39:36 2016 - [info] Sat Jul 9 10:39:36 2016 - [info] Fetching dead master's binary logs..Sat Jul 9 10:39:36 2016 - [info] Executing command on the dead master db1(192.168.2.37:3306): save_binary_logs --command=save --start_file=bin-log.000018 --start_pos=2659 --binlog_dir=/database/mysql/data/binlog --output_file=/opt/masterha/app1/saved_master_binlog_from_db1_3306_20160709103934.binlog --handle_raw_binlog=1 --disable_log_bin=0 --manager_version=0.56 Creating /opt/masterha/app1 if not exists.. ok.Concat binary/relay logs from bin-log.000018 pos 2659 to bin-log.000018 EOF into /opt/masterha/app1/saved_master_binlog_from_db1_3306_20160709103934.binlog ..Binlog Checksum enabled Dumping binlog format description event, from position 0 to 154.. ok. Dumping effective binlog data from /database/mysql/data/binlog/bin-log.000018 position 2659 to tail(2682).. ok.Binlog Checksum enabledConcat succeeded.Sat Jul 9 10:39:37 2016 - [info] scp from root@192.168.2.37:/opt/masterha/app1/saved_master_binlog_from_db1_3306_20160709103934.binlog to local:/opt/masterha/app1/saved_master_binlog_from_db1_3306_20160709103934.binlog succeeded.Sat Jul 9 10:39:37 2016 - [info] HealthCheck: SSH to db2 is reachable.Sat Jul 9 10:39:37 2016 - [info]Sat Jul 9 10:39:37 2016 - [info] * Phase 3.3: Determining New Master Phase..Sat Jul 9 10:39:37 2016 - [info]Sat Jul 9 10:39:37 2016 - [info] Finding the latest slave that has all relay logs for recovering other slaves..Sat Jul 9 10:39:37 2016 - [info] All slaves received relay logs to the same position. No need to resync each other.Sat Jul 9 10:39:37 2016 - [info] Searching new master from slaves..Sat Jul 9 10:39:37 2016 - [info] Candidate masters from the configuration file:Sat Jul 9 10:39:37 2016 - [info] Non-candidate masters:Sat Jul 9 10:39:37 2016 - [info] New master is db2(192.168.2.38:3306)Sat Jul 9 10:39:37 2016 - [info] Starting master failover..Sat Jul 9 10:39:37 2016 - [info]From:db1(192.168.2.37:3306) (current master)+--db2(192.168.2.38:3306) To:db2(192.168.2.38:3306) (new master)Sat Jul 9 10:39:37 2016 - [info]Sat Jul 9 10:39:37 2016 - [info] * Phase 3.3: New Master Diff Log Generation Phase..Sat Jul 9 10:39:37 2016 - [info]Sat Jul 9 10:39:37 2016 - [info] This server has all relay logs. No need to generate diff files from the latest slave.Sat Jul 9 10:39:37 2016 - [info] Sending binlog..Sat Jul 9 10:39:38 2016 - [info] scp from local:/opt/masterha/app1/saved_master_binlog_from_db1_3306_20160709103934.binlog to root@db2:/opt/masterha/app1/saved_master_binlog_from_db1_3306_20160709103934.binlog succeeded.Sat Jul 9 10:39:38 2016 - [info]Sat Jul 9 10:39:38 2016 - [info] * Phase 3.4: Master Log Apply Phase..Sat Jul 9 10:39:38 2016 - [info]Sat Jul 9 10:39:38 2016 - [info] *NOTICE: If any error happens from this phase, manual recovery is needed.Sat Jul 9 10:39:38 2016 - [info] Starting recovery on db2(192.168.2.38:3306)..Sat Jul 9 10:39:38 2016 - [info] Generating diffs succeeded.Sat Jul 9 10:39:38 2016 - [info] Waiting until all relay logs are applied.Sat Jul 9 10:39:38 2016 - [info] done.Sat Jul 9 10:39:38 2016 - [info] Getting slave status..Sat Jul 9 10:39:38 2016 - [info] This slave(db2)'s Exec_Master_Log_Pos equals to Read_Master_Log_Pos(bin-log.000018:2659). No need to recover from Exec_Master_Log_Pos.Sat Jul 9 10:39:38 2016 - [info] Connecting to the target slave host db2, running recover script..Sat Jul 9 10:39:38 2016 - [info] Executing command: apply_diff_relay_logs --command=apply --slave_user='root' --slave_host=db2 --slave_ip=192.168.2.38 --slave_port=3306 --apply_files=/opt/masterha/app1/saved_master_binlog_from_db1_3306_20160709103934.binlog --workdir=/opt/masterha/app1 --target_version=5.7.13-debug-log --timestamp=20160709103934 --handle_raw_binlog=1 --disable_log_bin=0 --manager_version=0.56 --slave_pass=xxxSat Jul 9 10:39:38 2016 - [info] MySQL client version is 5.7.13. Using --binary-mode.Applying differential binary/relay log files /opt/masterha/app1/saved_master_binlog_from_db1_3306_20160709103934.binlog on db2:3306. This may take long time...Applying log files succeeded.Sat Jul 9 10:39:38 2016 - [info] All relay logs were successfully applied.Sat Jul 9 10:39:38 2016 - [info] Getting new master's binlog name and position..Sat Jul 9 10:39:38 2016 - [info] bin-log.000021:1169Sat Jul 9 10:39:38 2016 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST='db2 or 192.168.2.38', MASTER_PORT=3306, MASTER_LOG_FILE='bin-log.000021', MASTER_LOG_POS=1169, MASTER_USER='rep1', MASTER_PASSWORD='xxx';Sat Jul 9 10:39:38 2016 - [warning] master_ip_failover_script is not set. Skipping taking over new master IP address.Sat Jul 9 10:39:38 2016 - [info] Setting read_only=0 on db2(192.168.2.38:3306)..Sat Jul 9 10:39:38 2016 - [info] ok.Sat Jul 9 10:39:38 2016 - [info] Finished master recovery successfully.Sat Jul 9 10:39:38 2016 - [info] * Phase 3: Master Recovery Phase completed.Sat Jul 9 10:39:38 2016 - [info]Sat Jul 9 10:39:38 2016 - [info] * Phase 4: Slaves Recovery Phase..Sat Jul 9 10:39:38 2016 - [info]Sat Jul 9 10:39:38 2016 - [info] * Phase 4.1: Starting Parallel Slave Diff Log Generation Phase..Sat Jul 9 10:39:38 2016 - [info]Sat Jul 9 10:39:38 2016 - [info] Generating relay diff files from the latest slave succeeded.Sat Jul 9 10:39:38 2016 - [info]Sat Jul 9 10:39:38 2016 - [info] * Phase 4.2: Starting Parallel Slave Log Apply Phase..Sat Jul 9 10:39:38 2016 - [info]Sat Jul 9 10:39:38 2016 - [info] All new slave servers recovered successfully.Sat Jul 9 10:39:38 2016 - [info]Sat Jul 9 10:39:38 2016 - [info] * Phase 5: New master cleanup phase..Sat Jul 9 10:39:38 2016 - [info]Sat Jul 9 10:39:38 2016 - [info] Resetting slave info on the new master..Sat Jul 9 10:39:38 2016 - [info] db2: Resetting slave info succeeded.Sat Jul 9 10:39:38 2016 - [info] Master failover to db2(192.168.2.38:3306) completed successfully.Sat Jul 9 10:39:38 2016 - [info] ----- Failover Report ----- app1: MySQL Master failover db1(192.168.2.37:3306) to db2(192.168.2.38:3306) succeeded Master db1(192.168.2.37:3306) is down! Check MHA Manager logs at db3:/opt/masterha/app1/manager.log for details. Started automated(non-interactive) failover.The latest slave db2(192.168.2.38:3306) has all relay logs for recovery.Selected db2(192.168.2.38:3306) as a new master.db2(192.168.2.38:3306): OK: Applying all logs succeeded.Generating relay diff files from the latest slave succeeded.db2(192.168.2.38:3306): Resetting slave info succeeded.Master failover to db2(192.168.2.38:3306) completed successfully. 备注： 日志中有一段“Failover Report” 看到由 db1 切换到 db2 成功，这个日志显示了MHA 切换过程中的详细步骤，应该仔细查看，切换完后发现 db3 节点的 masterha_manager 进程消失了，主从切换后需要将原主库修复成从库。 3、db2 上执行12root@localhost:mysql&gt;show slave statusGEmpty set (0.00 sec) 备注：可见 db2 上没有创建到 db1 的复制，此时 db2 已切换成主库。 4、启动原库库将操作以下db1 启动数据库1mysqld_safe --user=mysql --datadir=/database/mysql/data &amp; db3 节点上查看 manager.log 中最近的 CHANGE MASTER 日志1Sat Jul 9 10:39:38 2016 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST='db2 or 192.168.2.38', MASTER_PORT=3306, MASTER_LOG_FILE='bin-log.000021', MASTER_LOG_POS=1169, MASTER_USER='rep1', MASTER_PASSWORD='xxx'; db1节点执行主从复制: db2 上执行12345678910CHANGE MASTER TO MASTER_HOST='192.168.2.38', MASTER_PORT=3306, MASTER_LOG_FILE='bin-log.000026', MASTER_LOG_POS=1169, MASTER_USER='rep1', MASTER_PASSWORD='rep1abcd1243d'; root@localhost:mysql&gt;start slave;Query OK, 0 rows affected (0.01 sec) 备注： 这时新主库上的增量数据也会在 db1 上，从而保证从库和主库数据一致。 5、重新启动 MHA MANAGER 进程db3节点删除 .complete 文件1[root@db3 app1]# rm -f /opt/masterha/app1/app1.failover.complete 备注： MHA MANAGER 发生切换后，会在 manager_workdir 目录下产生 app1.failover.complete 文件，若要启动 MHA MANAGER，需要将此文件删除，否则会影响以后的切换。 masterha_check_repl1masterha_check_repl --conf=/opt/masterha/app1/app1.cnf 再次启动 MHA MANAGER1masterha_manager --conf=/opt/masterha/app1/app1.cnf &gt; /tmp/mha_manager.log 2&gt;&amp;1 &amp; 五、手动方式主从切换：使用 masterha_master_switch 脚本手工切换前需要关闭 manager 进程12[root@db3 app1]# masterha_stop --conf=/opt/masterha/app1/app1.cnf Stopped app1 successfully. 手工切换: db3 上操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[root@db3 app1]# masterha_master_switch --conf=/opt/masterha/app1/app1.cnf --master_state=alive --new_master_host=db1Sat Jul 9 11:52:51 2016 - [info] MHA::MasterRotate version 0.56.Sat Jul 9 11:52:51 2016 - [info] Starting online master switch..Sat Jul 9 11:52:51 2016 - [info] Sat Jul 9 11:52:51 2016 - [info] * Phase 1: Configuration Check Phase..Sat Jul 9 11:52:51 2016 - [info] Sat Jul 9 11:52:51 2016 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.Sat Jul 9 11:52:51 2016 - [info] Reading application default configuration from /opt/masterha/app1/app1.cnf..Sat Jul 9 11:52:51 2016 - [info] Reading server configuration from /opt/masterha/app1/app1.cnf..Sat Jul 9 11:52:52 2016 - [info] GTID failover mode = 0Sat Jul 9 11:52:52 2016 - [info] Current Alive Master: db2(192.168.2.38:3306)Sat Jul 9 11:52:52 2016 - [info] Alive Slaves:Sat Jul 9 11:52:52 2016 - [info] db1(192.168.2.37:3306) Version=5.7.13-debug-log (oldest major version between slaves) log-bin:enabledSat Jul 9 11:52:52 2016 - [info] Replicating from 192.168.2.38(192.168.2.38:3306) It is better to execute FLUSH NO_WRITE_TO_BINLOG TABLES on the master before switching. Is it ok to execute on db2(192.168.2.38:3306)? (YES/no): yesSat Jul 9 11:52:55 2016 - [info] Executing FLUSH NO_WRITE_TO_BINLOG TABLES. This may take long time..Sat Jul 9 11:52:55 2016 - [info] ok.Sat Jul 9 11:52:55 2016 - [info] Checking MHA is not monitoring or doing failover..Sat Jul 9 11:52:55 2016 - [info] Checking replication health on db1..Sat Jul 9 11:52:55 2016 - [info] ok.Sat Jul 9 11:52:55 2016 - [info] db1 can be new master.Sat Jul 9 11:52:55 2016 - [info]From:db2(192.168.2.38:3306) (current master)+--db1(192.168.2.37:3306) To:db1(192.168.2.37:3306) (new master) Starting master switch from db2(192.168.2.38:3306) to db1(192.168.2.37:3306)? (yes/NO): yesSat Jul 9 11:53:03 2016 - [info] Checking whether db1(192.168.2.37:3306) is ok for the new master..Sat Jul 9 11:53:03 2016 - [info] ok.Sat Jul 9 11:53:03 2016 - [info] Phase 1: Configuration Check Phase completed.Sat Jul 9 11:53:03 2016 - [info]Sat Jul 9 11:53:03 2016 - [info] * Phase 2: Rejecting updates Phase..Sat Jul 9 11:53:03 2016 - [info]master_ip_online_change_script is not defined. If you do not disable writes on the current master manually, applications keep writing on the current master. Is it ok to proceed? (yes/NO): yesSat Jul 9 11:53:49 2016 - [info] Locking all tables on the orig master to reject updates from everybody (including root):Sat Jul 9 11:53:49 2016 - [info] Executing FLUSH TABLES WITH READ LOCK..Sat Jul 9 11:53:49 2016 - [info] ok.Sat Jul 9 11:53:49 2016 - [info] Orig master binlog:pos is bin-log.000021:2549.Sat Jul 9 11:53:49 2016 - [info] Waiting to execute all relay logs on db1(192.168.2.37:3306)..Sat Jul 9 11:53:49 2016 - [info] master_pos_wait(bin-log.000021:2549) completed on db1(192.168.2.37:3306). Executed 0 events.Sat Jul 9 11:53:49 2016 - [info] done.Sat Jul 9 11:53:49 2016 - [info] Getting new master s binlog name and position..Sat Jul 9 11:53:49 2016 - [info] bin-log.000022:154Sat Jul 9 11:53:49 2016 - [info] All other slaves should start replication from here. Statement should be: CHANGE MASTER TO MASTER_HOST='db1 or 192.168.2.37', MASTER_PORT=3306, MASTER_LOG_FILE='bin-log.000022', MASTER_LOG_POS=154, MASTER_USER='rep1', MASTER_PASSWORD='xxx';Sat Jul 9 11:53:49 2016 - [info]Sat Jul 9 11:53:49 2016 - [info] * Switching slaves in parallel..Sat Jul 9 11:53:49 2016 - [info]Sat Jul 9 11:53:49 2016 - [info] Unlocking all tables on the orig master:Sat Jul 9 11:53:49 2016 - [info] Executing UNLOCK TABLES..Sat Jul 9 11:53:49 2016 - [info] ok.Sat Jul 9 11:53:49 2016 - [info] All new slave servers switched successfully.Sat Jul 9 11:53:49 2016 - [info]Sat Jul 9 11:53:49 2016 - [info] * Phase 5: New master cleanup phase..Sat Jul 9 11:53:49 2016 - [info]Sat Jul 9 11:53:49 2016 - [info] db1: Resetting slave info succeeded.Sat Jul 9 11:53:49 2016 - [info] Switching master to db1(192.168.2.37:3306) completed successfully. 备注：执行过程中会提示一些问题，可以加上参数”–interactive=0” 非交互形式进行手工切换。 六、参考 mysql-master-ha Architecture of MHA mysql-master-ha parameters MySQL学习笔记-MHA安装配置 MySQL高可用之MHA的搭建 MySQL5.7: 半同步复制（Semisynchronous Replication）配置","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL高可用","slug":"MySQL高可用","permalink":"https://postgres.fun/tags/MySQL高可用/"}]},{"title":"MySQL5.7: 半同步复制（Semisynchronous Replication）配置 ","slug":"20160707091555","date":"2016-07-07T01:15:55.000Z","updated":"2018-09-04T01:34:20.566Z","comments":true,"path":"20160707091555.html","link":"","permalink":"https://postgres.fun/20160707091555.html","excerpt":"","text":"关于异步复制、半同步复制、同步复制1、异步复制：主库上的事务不会等待从库的确认即返回客户端提交成功！ With asynchronous replication, the master writes events to its binary log and slaves request them when they are ready. There is no guarantee that any event will ever reach any slave._ 2、同步复制：主库上提交的事务向客户端返回成功之前，需要收到所有从库提交事务的确认信息。 With fully synchronous replication, when a master commits a transaction, all slaves also will have committed the transaction before the master returns to the session that performed the transaction. The drawback of this is that there might be a lot of delay to complete a transaction._ 3、半同步复制：异步复制和同步复制的折中，主库上提交事务时，需要等待至少一个从库发来的收到事件确认信息，才向客户端返回成功。 Semisynchronous replication falls between asynchronous and fully synchronous replication. The master waits only until at least one slave has received and logged the events. It does not wait for all slaves to acknowledge receipt, and it requires only receipt, not that the events have been fully executed and committed on the slave side._ 部署 MySQL5.7 半同步复制安装前提1、MySQL5.5 版本或更高2、主、备库的 have_dynamic_loading 系统变量值为 yes3、主、备异步复制已部署 主节点安装 rpl_semi_sync_master 插件12345678910111213141516root@localhost:mysql&gt;INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';Query OK, 0 rows affected (0.17 sec) root@localhost:mysql&gt;show variables like '%rpl%';+-------------------------------------------+------------+| Variable_name | Value |+-------------------------------------------+------------+| rpl_semi_sync_master_enabled| OFF|| rpl_semi_sync_master_timeout| 10000 || rpl_semi_sync_master_trace_level| 32 || rpl_semi_sync_master_wait_for_slave_count | 1 || rpl_semi_sync_master_wait_no_slave| ON || rpl_semi_sync_master_wait_point | AFTER_SYNC || rpl_stop_slave_timeout| 31536000 |+-------------------------------------------+------------+7 rows in set (0.03 sec) 备节点安装 rpl_semi_sync_slave 插件123456789101112root@localhost:mysql&gt;INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';Query OK, 0 rows affected (0.15 sec) root@localhost:mysql&gt;show variables like '%rpl%';+---------------------------------+----------+| Variable_name | Value |+---------------------------------+----------+| rpl_semi_sync_slave_enabled | OFF|| rpl_semi_sync_slave_trace_level | 32 || rpl_stop_slave_timeout| 31536000 |+---------------------------------+----------+3 rows in set (0.03 sec) 主节点配置增加以下123[mysqld]rpl_semi_sync_master_enabled=1rpl_semi_sync_master_timeout=10000 # 10 second 备注： rpl_semi_sync_master_enabled 参数控制主节点是否开启半同步复制；rpl_semi_sync_master_timeout 参数控制主节点等待备节点返回确认信息的超时时间，单位为毫秒，超过这个时间后半同步复制转变成异步复制，这里设置成 10 秒。 备节点配置增加以下12[mysqld]rpl_semi_sync_slave_enabled=1 备注：rpl_semi_sync_slave_enabled 参数控制备节点是否开启半同步复制； 之后重启主、备库。 测试1） 关闭从库12[mysql@db2 data]$ mysqladmin -uroot -p shutdownEnter password: 2） 主库测试 备注：关闭从库后，在主库上创建一张表延迟了 10 秒左右，刚好是参数 rpl_semi_sync_master_timeout 设置的值。 参考 Semisynchronous Replication Semisynchronous Replication Installation and Configuration MySQL半同步复制 MySQL：主从复制(Replication)搭建","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL Replication","slug":"MySQL-Replication","permalink":"https://postgres.fun/tags/MySQL-Replication/"}]},{"title":"MySQL5.6 升级到 MySQL5.7 后 HELP 表没有更新","slug":"20160706101355","date":"2016-07-06T02:13:55.000Z","updated":"2018-09-04T01:34:20.487Z","comments":true,"path":"20160706101355.html","link":"","permalink":"https://postgres.fun/20160706101355.html","excerpt":"","text":"上篇博客MySQL 5.6 升级到 MySQL 5.7 实践介绍了使用 in-place 方式升级 MySQL5.6 到 MySQL5.7，升级后发现 HELP 命令没有更新，如下： Help 命令123456789101112131415161718192021root@localhost:mysql&gt;h rename tableName: 'RENAME TABLE'Description:Syntax:RENAME TABLE tbl_name TO new_tbl_name [, tbl_name2 TO new_tbl_name2] ... This statement renames one or more tables. The rename operation is done atomically, which means that no othersession can access any of the tables while the rename is running. Forexample, if you have an existing table old_table, you can createanother table new_table that has the same structure but is empty, andthen replace the existing table with the empty one as follows (assumingthat backup_table does not already exist): URL: http://dev.mysql.com/doc/refman/5.6/en/rename-table.html Examples:CREATE TABLE new_table (...);RENAME TABLE old_table TO backup_table, new_table TO old_table; 备注：上面的链接依然是显示 5.6 版本，接着查看 HELP 系统表。 查看 Help_topic 系统表1234567891011121314151617181920212223242526root@localhost:mysql&gt;select * from help_topic where name='RENAME TABLE'G* 1. row * help_topic_id: 65 name: RENAME TABLEhelp_category_id: 40 description: Syntax:RENAME TABLE tbl_name TO new_tbl_name [, tbl_name2 TO new_tbl_name2] ... This statement renames one or more tables. The rename operation is done atomically, which means that no othersession can access any of the tables while the rename is running. Forexample, if you have an existing table old_table, you can createanother table new_table that has the same structure but is empty, andthen replace the existing table with the empty one as follows (assumingthat backup_table does not already exist): URL: http://dev.mysql.com/doc/refman/5.6/en/rename-table.html example: CREATE TABLE new_table (...);RENAME TABLE old_table TO backup_table, new_table TO old_table; url: http://dev.mysql.com/doc/refman/5.6/en/rename-table.html1 row in set (0.01 sec) 备注：系统表里显示的依然是 5.6 的链接，很是奇怪，查了下手册，有如下内容： 备注：原来 mysql_upgrade 不会更新 help 相关系统表的，解决方法：需要手工执行 $MYSQL_HOME/share/fill_help_tables.sql 脚本。 fill_help_tables.sql 脚本/opt/mysql/share/fill_help_tables.sql 脚本内容为刷新 help 四张系统表，部分内容如下图 导入 fill_help_tables.sql 脚本12[mysql@db1 ~]$ mysql -uroot -p -D mysql &lt; /opt/mysql/share/fill_help_tables.sqlEnter password: 备注：脚本没有报错。 测试12345678910111213141516171819root@localhost:mysql&gt;h rename tableName: 'RENAME TABLE'Description:Syntax:RENAME TABLE tbl_name TO new_tbl_name [, tbl_name2 TO new_tbl_name2] ... This statement renames one or more tables. The rename operation is doneatomically, which means that no other session can access any of thetables while the rename is running. For example, a table named old_table can be renamed to new_table asshown here: URL: http://dev.mysql.com/doc/refman/5.7/en/rename-table.html Examples:RENAME TABLE old_table TO new_table;备注：刷新 HELP 系统表后， URL 链接已经调整到 5.7 版本。 参考 MySQL 5.6 升级到 MySQL 5.7 实践 Server-Side Help Upgrading MySQL","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL Upgrade","slug":"MySQL-Upgrade","permalink":"https://postgres.fun/tags/MySQL-Upgrade/"}]},{"title":"MySQL 5.6 升级到 MySQL 5.7 实践","slug":"20160705142142","date":"2016-07-05T06:21:42.000Z","updated":"2018-09-04T01:34:20.440Z","comments":true,"path":"20160705142142.html","link":"","permalink":"https://postgres.fun/20160705142142.html","excerpt":"","text":"MySQL 5.7 在功能上有很大增强，例如支持 JSON 、GIS 等，最近打算熟悉下 MySQL 5.7，现有 MySQL 环境是笔记本虚机上的 MySQL 5.6.20 版本，先从版本升级开始，这里记录下升级的过程，不对之处大家帮忙指正。MySQL 手册上提到主要的两种升级方式，一种是 In-place 就地升级，一种是逻辑导入、导出升级。今天先测试 in-place 升级，因为对于大库来说， in-place 升级能大大减少停库时间。 Supported Upgrade Methods_In-place Upgrade: Involves shutting down the old MySQL version, replacing the old MySQL binaries or packages with the new ones, restarting MySQL on the existing data directory, and running mysql_upgrade.__Logical Upgrade: Involves exporting existing data from the old MySQL version using mysqldump, installing the new MySQL version, loading the dump file into the new MySQL version, and running mysql_upgrade. 现有环境MySQL版本： 5.6.20软件目录： /opt/mysql数据目录： /database/mysql/data 安装 MySQL5.7.13下载 MySQL 5.7.131[root@db1 soft_bak]# wget http://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.13.tar.gz 之后解压. 编译,如下：1[root@db1 mysql-5.7.13]# cmake . -DCMAKE_INSTALL_PREFIX=/opt/mysql -DMYSQL_DATADIR=/database/mysql/data -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci 备注：cmake 遇到的第一个问题是版本太低，提示要 2.8 版本升级下版本即可，之后编译报 boost 相关错误，如下 cmake 报错12345678910111213141516-- Could not find (the correct version of) boost.-- MySQL currently requires boost_1_59_0 CMake Error at cmake/boost.cmake:81 (MESSAGE): You can download it with -DDOWNLOAD_BOOST=1 -DWITH_BOOST=&lt;directory&gt; This CMake script will look for boost in &lt;directory&gt;. If it is not there, it will download and unpack it (in that directory) for you. If you are inside a firewall, you may need to use an http proxy: export http_proxy=http://example.com:80 Call Stack (most recent call first): cmake/boost.cmake:238 (COULD_NOT_FIND_BOOST) CMakeLists.txt:451 (INCLUDE) 备注：按照提示，加上参数： -DDOWNLOAD_BOOST=1 -DWITH_BOOST= 1234cmakecmake . -DCMAKE_INSTALL_PREFIX=/opt/mysql_5.7.13 -DMYSQL_DATADIR=/database/mysql/data -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci -DWITH_DEBUG=1 -DDOWNLOAD_BOOST=1 -DWITH_BOOST=/tmpmake make install 备注：这时 cmake 成功，但 make 编译很慢，不停地在 swap ，网上查了下说是内存不够，原有虚拟机 256MB 内存，后来内存扩到 1GB ，编译速度快了很多。 停库1[mysql@db1 ~]$ mysqladmin -uroot -p shutdown 软件备份12# mv /opt/mysql /opt/mysql_5.6.20# mv /opt/mysql_5.7.13 /opt/mysql 启库1[mysql@db1 ~]$ mysqld_safe --user=mysql --datadir=/database/mysql/data&amp; mysql_upgrade 升级系统表123456789101112131415161718192021222324252627[mysql@db1 ~]$ mysql_upgrade -uroot -pEnter password: Checking if update is needed.Checking server version.Running queries to upgrade MySQL server.Checking system database.mysql.columns_priv OKmysql.db OKmysql.engine_cost OKmysql.event OKmysql.func OKmysql.general_log OKmysql.gtid_executed OKmysql.help_category OKmysql.help_keyword OK...省略 Upgrading tablesRunning : ALTER TABLE `francs`.`tbl_access_log` UPGRADE PARTITIONINGstatus : OKRunning : ALTER TABLE `francs`.`tbl_access_log2` UPGRADE PARTITIONINGstatus : OKRunning : ALTER TABLE `francs`.`tbl_access_log_unix` UPGRADE PARTITIONINGstatus : OKRunning : ALTER TABLE `francs`.`user_info` UPGRADE PARTITIONINGstatus : OKUpgrade process completed successfully.Checking if update is needed. 备注：mysql_upgrade 检查所有库中的表与现有版本的兼容性并且会刷新系统表，这里执行没有报错，初步判断升级成功。 测试123456789101112131415161718192021[mysql@db1 ~]$ mysql -uroot -pEnter password: Welcome to the MySQL monitor. Commands end with ; or g.Your MySQL connection id is 4Server version: 5.7.13-debug-log Source distribution Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners. Type 'help;' or 'h' for help. Type 'c' to clear the current input statement. root@localhost:(none)&gt;select version();+------------------+| version() |+------------------+| 5.7.13-debug-log |+------------------+1 row in set (0.00 sec) 备注：升级到了最新版本 MySQL 5.7.13。 升级后遇到的问题升级后发现 HELP 中的内容依然是 5.6 版本的，没有更新到5.7版本，这个现象我在下一篇博客里详细介绍并提供解决方法，详见 MySQL5.6 升级到 MySQL5.7 后 HELP 表没有更新 参考 mysql_upgrade — Check and Upgrade MySQL Tables Upgrading MySQL MySQL5.7源码安装问题汇总","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL Upgrade","slug":"MySQL-Upgrade","permalink":"https://postgres.fun/tags/MySQL-Upgrade/"}]},{"title":"ITPUB 推出有奖讨论活动，赢取 Cookbook 第2版","slug":"20160701132434","date":"2016-07-01T05:24:34.000Z","updated":"2018-09-04T01:34:20.378Z","comments":true,"path":"20160701132434.html","link":"","permalink":"https://postgres.fun/20160701132434.html","excerpt":"","text":"ITpub 上推出的有奖活动：“有奖讨论：最先进的开源数据库PostgreSQL有哪些黑科技”，分享 PostgreSQL 使用经验将有机会赢取《PostgreSQL9 Administration Cookbook（第2版）中文版》图书一本，PGer 们赶快行动吧，详见http://www.itpub.net/thread-2062500-1-1.html，并且提供样章试读，详见本文末尾。 以下转自 ITPUB 帖子：http://www.itpub.net/thread-2062500-1-1.html 话题背景PostgreSQL 是一款功能丰富的开源数据库，它支持丰富的数据类型，除了常用的数据类型外，还支持网络地址、数组、JSON、几何类型等，以满足应用程序开发需求。此外，PostgreSQL 还支持局部索引、表达式索引、GIST/GIN 索引以及范围索引，支持物化视图、集合查询、窗口函数；同时，PostgreSQL 还支持众多丰富的外部扩展，外部扩展完善了PostgreSQL的功能，奠定了 PostgreSQL 强大的扩展能力基础。 PostgreSQL的境外知名用户有Skype、NTT、Saleforce的Heroku云数据库平台等诸多大型企业以及法国政府、NASA等。日本PostgreSQL使用非常活跃，大量制造业、游戏、电信运营商、企业ERP系统首选PostgreSQL。这两年来，PostgreSQL在国内亦迅猛发展，受到越来越多企业亲睐，平安科技、去哪儿网、斯凯网络、腾讯等众多企业已逐步使用或规模部署，在阿里云及青云也专门提供了基于PostgreSQL的云数据库服务。 讨论话题 个人工作或者学习过程中是否接触过 PostgreSQL 数据库？介绍对 PostgreSQL 的了解。 通过哪些途径了解并学习 PostgreSQL？使用或学习 PostgreSQL 过程中遇到最大的困难是什么？ 您所在的公司生产环境是否使用 PostgreSQL 数据库？是否可以分享数据库选型 PostgreSQL 的原因以及生产环境 PostgreSQL 的实践经验？ 对于数据库管理员，您认为哪些事情最重要，按重要程度列出并说明原因。 活动时间活动时间: 2016年7月1日―7月15日奖励设置：活动结束后，我们将会选取5个亮点回复，送《PostgreSQL9 Administration Cookbook（第2版）中文版》图书一本。 样章试读PostgreSQL 9 Administration Cookbook （第2版）中文版试读样章.pdf(819.79 KB, 下载次数: 1)","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Book","slug":"PostgreSQL-Book","permalink":"https://postgres.fun/tags/PostgreSQL-Book/"}]},{"title":"《PostgreSQL 9 Administration Cookbook（第2版）》中文版出版","slug":"20160612205500","date":"2016-06-12T12:55:00.000Z","updated":"2018-12-14T07:40:29.677Z","comments":true,"path":"20160612205500.html","link":"","permalink":"https://postgres.fun/20160612205500.html","excerpt":"","text":"经过漫长的时间，终于和黄坚将《PostgreSQL 9 Administration Cookbook （第2版）》翻译完成，翻译过程中占用了大量的业余时间，感谢我的妻子长期以来对我翻译工作的支持，她的理解与支持是我的动力！ 语言：中文出版日期：2016年5月作者：Simon Riggs、Gianni Ciolli、Hannu Krosing、Gabriele Bartolini译者：黄坚、谭峰 主要内容PostgreSQL 是一款功能丰富的开源数据库，它支持丰富的数据类型，除了常用的数据类型外，还支持网络地址、数组、JSON、几何类型等，以满足应用程序开发需求。此外，PostgreSQL 还支持局部索引、表达式索引、GIST/GIN 索引以及范围索引，支持物化视图、集合查询、窗口函数；最重要的是，PostgreSQL 还支持众多丰富的外部扩展，外部扩展完善了PostgreSQL的功能，奠定了 PostgreSQL 强大的扩展能力基础。本书前六章介绍了PostgreSQL 数据库基础，后六章由浅入深地介绍了PostgreSQL 日常维护技巧、性能分析、备份恢复和重量级的复制技术。通过此书读者能学习到新的PostgreSQL数据库维护技巧，适合对PostgreSQL 有一定基础，同时想进一步了解PostgreSQL 的工程师，尤其适合于有志于成为PostgreSQL DBA 的朋友，这是一本经典的不可多得的PostgreSQL 管理类书籍。 章节目录本书共12章，如下 第一章 迈出第一步 第二章 浏览数据库 第三章 配置 第四章 服务控制 第五章 表和数据 第六章 安全 第七章 数据库管理 第八章 监控和诊断 第九章 常规维护 第十章 性能和并发 第十一章 备份和恢复 第十二章 复制和升级","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Book","slug":"PostgreSQL-Book","permalink":"https://postgres.fun/tags/PostgreSQL-Book/"}]},{"title":"PostgreSQL9.6：新增 Bloom 索引支持任意列组合查询 ","slug":"20160611193953","date":"2016-06-11T11:39:53.000Z","updated":"2018-09-04T01:34:20.253Z","comments":true,"path":"20160611193953.html","link":"","permalink":"https://postgres.fun/20160611193953.html","excerpt":"","text":"PostgreSQL 9.6 版本新增 bloom 索引类型，支持任意列的组合查询，我们通过例子演示，假如有以下表结构 测试准备创建测试表12345678create table test_bloom ( i1 int4, i2 int4, i3 int4, i4 int4, i5 int4, i6 int4); 如果应用场景涉及以下查询 SQL，我们可能需要在表 test_bloom 创建四个索引，分别为 index1(i1),index2(i2),index3(i3,i4)，index4(i5,i6)，一张表上的索引越多插入维护成本越大。1234select * from test_bloom where i1=xxx;select * from test_bloom where i2=xxx;select * from test_bloom where i3=xxx and i4=xxx;select * from test_bloom where i5=xxx and i6=xxx; 9.6 版本开始支持 bloom 索引，这时只需要创建一个索引就可以了，接着测试。 插入测试数据1234567891011121314151617insert into test_bloom(i1,i2,i3,i4,i5,i6)select random()*1000000, random()*1000000, random()*1000000, random()*1000000, random()*1000000, random()*1000000from generate_series(1,2000000); francs=&gt; select * from test_bloom limit 3; i1 | i2 | i3 | i4 | i5 | i6 --------+--------+--------+--------+--------+--------315075 | 990584 | 21876 | 778755 | 163148 | 70877851679 | 329673 | 86038 | 650005 | 37895 | 298422184483 | 460258 | 237388 | 302736 | 294007 | 366822(3 rows) 创建 Bloom 模块12francs=# create extension bloom;CREATE EXTENSION 创建 bloom 索引12francs=&gt; create index bloom_idx1 on test_bloom using bloom (i1,i2,i3,i4,i5,i6);CREATE INDEX 备注：在(i1,i2,i3,i4,i5,i6)列上创建了组合 bloom 索引。 查询测试123456789101112131415francs=&gt; set enable_seqscan=off;SET francs=&gt; explain analyze select * from test_bloom where i1=315075 and i2=990584; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------Bitmap Heap Scan on test_bloom (cost=35692.00..35696.02 rows=1 width=24) (actual time=46.595..52.600 rows=1 loops=1) Recheck Cond: ((i1 = 315075) AND (i2 = 990584)) Rows Removed by Index Recheck: 4355 Heap Blocks: exact=3657 -&gt; Bitmap Index Scan on bloom_idx1 (cost=0.00..35692.00 rows=1 width=0) (actual time=45.709..45.709 rows=4356 loops=1) Index Cond: ((i1 = 315075) AND (i2 = 990584))Planning time: 0.190 msExecution time: 52.657 ms(8 rows) 备注：根据 i1,i2 字段查询，执行时间需要 52 ms 左右，速度有些慢。 123456789101112francs=&gt; explain analyze select * from test_bloom where i3=21876 and i5=163148; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------Bitmap Heap Scan on test_bloom (cost=35692.00..35696.02 rows=1 width=24) (actual time=43.754..44.412 rows=1 loops=1) Recheck Cond: ((i3 = 21876) AND (i5 = 163148)) Rows Removed by Index Recheck: 438 Heap Blocks: exact=428 -&gt; Bitmap Index Scan on bloom_idx1 (cost=0.00..35692.00 rows=1 width=0) (actual time=43.674..43.674 rows=439 loops=1) Index Cond: ((i3 = 21876) AND (i5 = 163148))Planning time: 0.156 msExecution time: 44.460 ms(8 rows) 备注：根据 i3,i5 字段查询，执行时间需要 44 ms 左右，速度依然有些慢。 1234567891011francs=&gt; create index btree_i3_i5 on test_bloom using btree (i3,i5);CREATE INDEXfrancs=&gt; explain analyze select * from test_bloom where i3=21876 and i5=163148; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------Index Scan using btree_i3_i5 on test_bloom (cost=0.43..8.45 rows=1 width=24) (actual time=0.033..0.035 rows=1 loops=1) Index Cond: ((i3 = 21876) AND (i5 = 163148))Planning time: 0.181 msExecution time: 0.078 ms(4 rows) 备注：在 i3,i5 列上创建 btree 组合索引，执行时间只需 0.078 ms ，从以上测试来看 bloom 索引的查询速度比 btree 慢很多。 Bloom 索引选项 bloom indexes accept the following parameters in the WITH clause. lengthLength of signature in uint16 type values col1 ― col16Number of bits for corresponding column 备注：这两参数依然不太明白具体含义，其中 length 以 16为单位， 默认值为5，个人推测根据表的数据分布调整这两个参数也许能够提高 bloom 索引查询效率。 举例如下： 1create index bloom_idx2 on test_bloom using bloom (i1,i2,i3,i4,i5,i6) with ( length=30,col1=5,col2=5,col3=5,col4=5,col5=5,col6=5); 参考 WAITING FOR 9.6 – BLOOM INDEX CONTRIB MODULE Bloom Extension 布隆过滤器(Bloom Filter)详解 PostgreSQL 9.6 黑科技 bloom 算法索引，一个索引支撑任意列组合查询","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgresSQL9.6：新增 Pg_size_bytes() 函数 ","slug":"20160608160918","date":"2016-06-08T08:09:18.000Z","updated":"2018-09-04T01:34:20.190Z","comments":true,"path":"20160608160918.html","link":"","permalink":"https://postgres.fun/20160608160918.html","excerpt":"","text":"Add pg_size_bytes() function to convert human-readable size strings to numbers (Pavel Stehule, Vitaly Burovoy, Dean Rasheed) pg_size_bytes() 函数将物理占用空间大小转换成以 bytes 为单位，输入参数支持 “bytes”、”kB”、”MB”、”GB”、”TB” 单位。 Pg_size_bytes() 举例1234567891011121314151617francs=&gt; select pg_size_bytes('1 kB');pg_size_bytes--------------- 1024(1 row) francs=&gt; select pg_size_bytes('1 MB');pg_size_bytes---------------1048576(1 row) francs=&gt; select pg_size_bytes('1 gb');pg_size_bytes--------------- 1073741824(1 row) 备注：根据上面演示很好理解这个函数的作用，利用这个函数可以查询一个库中占用空间大于指定值的对象列表，如下： 查询占用空间大于 100 MB 的表123SELECT oid::regclass,pg_size_pretty(pg_total_relation_size(oid))FROM pg_classWHERE pg_total_relation_size(oid) &gt; pg_size_bytes('100 MB') order by pg_total_relation_size(oid) desc ;","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.6：新增内置默认角色（pg_signal_backend） ","slug":"20160608120545","date":"2016-06-08T04:05:45.000Z","updated":"2018-09-04T01:34:20.128Z","comments":true,"path":"20160608120545.html","link":"","permalink":"https://postgres.fun/20160608120545.html","excerpt":"","text":"PostgreSQL9.6 版本开始新增默认角色，默认角色通常具有一些超级用户才有的权限，9.6 版本新增的默认角色为 pg_signal_backend，这个角色具有 cancel query、terminate 其它会话的权限，也就是之前需要超级用户干的活现在可以通过权限下放让普通用户来做，从这个程度来说，细化了权限管理，增强了数据库的安全性。 新增角色 备注：这个角色具有调用 pg_cancel_backend()、pg_terminate_backend() 函数的权限，下面做个简单测试。 测试创建三个测试用户123456789101112[pg96@db1 ~]$ psqlpsql (9.6beta1)Type \"help\" for help. postgres=# create role admin with login;CREATE ROLE postgres=# create role user1 with login;CREATE ROLE postgres=# create role user2 with login;CREATE ROLE 会话一: user1 用户登陆 francs 库123456789[pg96@db1 ~]$ psql francs user1psql (9.6beta1)Type \"help\" for help. francs=&gt; select pg_backend_pid();pg_backend_pid---------------- 13308(1 row) 会话二：admin 用户尝试 kill 会话 13308123456[pg96@db1 ~]$ psql francs adminpsql (9.6beta1)Type \"help\" for help. francs=&gt; select pg_terminate_backend(13308);ERROR: must be a member of the role whose process is being terminated or member of pg_signal_backend 备注：提示没有权限。 会话三：给 admin 用户赋权12postgres=# grant pg_signal_backend to admin;GRANT ROLE 备注：将9.6新增的默角色 pg_signal_backend 赋给用户 admin 。 再次回到会话二：admin 用户尝试 kill 会话 1330812345francs=&gt; select pg_terminate_backend(13308);pg_terminate_backend----------------------t(1 row) 备注：会话 kill 成功，赋予后，普通用户 admin 具有 pg_terminate_backend 函数的调用权限，kill 其它会话。 参考 pg_signal_backend 角色","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.6：新增“idle in transaction”超时空闲事务自动查杀功能","slug":"20160601163329","date":"2016-06-01T08:33:29.000Z","updated":"2018-09-04T01:34:20.065Z","comments":true,"path":"20160601163329.html","link":"","permalink":"https://postgres.fun/20160601163329.html","excerpt":"","text":"熟悉 PostgreSQL 的朋友应该知道 “idle in transaction” 进程，引发 idle in transaction 的原因很多，例如应用代码中忘记关闭已开启的事务，或者系统中存在僵死进程等，曾经看到过某个库中的 idle in transaction 进程存在一年有余，这类进程严重危害了数据库的安全，例如它会阻止 VACUUM 进程回收记录，造成表数据膨胀，同时它有可能引起整个 PostgreSQL 数据库 Transaction ID Wraparound 的风险。 Allow sessions to be terminated automatically if they sit too long in an idle-in-transaction state (Vik Fearing)This behavior is enabled and controlled by the new configuration parameter idle_in_transaction_session_timeout. It can be useful to prevent forgotten transactions from holding onto locks or preventing vacuum cleanup for very long periods. 以上出自 PostgreSQL9.6 Beta1 发行说明，这段指出9.6版本 PostgreSQL 支持自动查杀超过指定时间的 idle in transaction 空闲事务连接，下面演示下。 修改参数修改 postgresql.conf 以下参数1idle_in_transaction_session_timeout = 20000 备注：参数单位为毫秒，这里设置 idle in transaction 超时空闲事务时间为 20 秒。 重载配置文件12[pg96@db1 pg_root]$ pg_ctl reloadserver signaled 备注：此参数修改后对当前连接依然生效，应用不需要重连即能生效。 模拟测试开启会话一：模拟一个事务123456789101112[pg96@db1 ~]$ psql francs francspsql (9.6beta1)Type \"help\" for help. francs=&gt; begin;BEGIN francs=&gt; select 1;?column?---------- 1(1 row) 事务中，不提交也不回滚。 开启会话二：监控123456789101112131415161718192021222324postgres=# select * from pg_stat_activity where pid&lt;&gt;pg_backend_pid();-[ RECORD 1 ]----+------------------------------datid | 16386datname | francspid | 7776usesysid | 16384usename | francsapplication_name | psqlclient_addr |client_hostname |client_port | -1backend_start | 2016-06-01 16:03:12.557328+08xact_start | 2016-06-01 16:03:16.921353+08query_start | 2016-06-01 16:03:18.754706+08state_change | 2016-06-01 16:03:18.755422+08wait_event_type |wait_event |state | idle in transactionbackend_xid |backend_xmin |query | select 1; postgres=# select * from pg_stat_activity where pid&lt;&gt;pg_backend_pid();(0 rows) 备注：开始还能监控到这个 “idle in transaction” 的事务，大概过了 20秒后，这个事务查询不到了。 再回到会话一123456789francs=&gt; select 1;?column?---------- 1FATAL: terminating connection due to idle-in-transaction timeoutserver closed the connection unexpectedly This probably means the server terminated abnormallybefore or while processing the request.The connection to the server was lost. Attempting reset: Succeeded. 备注：回到会话一执行 select 1 测试命令，发现连接被断开了，报错代码很明显，idle-in-transaction 超时了。 数据库日志12016-06-01 16:03:38.756 CST,\"francs\",\"francs\",7776,\"[local]\",574e96c0.1e60,1,\"idle in transaction\",2016-06-01 16:03:12 CST,2/5887,0,FATAL,25P03,\"terminating connection due to idle-in-transaction timeout\",,,,,,,,,\"psql\" 备注：数据库日志里清晰地记录了 7796 进程的连接由于空闲事务超时被断开连接。 参考 idle_in_transaction_session_timeout (integer) Preventing Transaction ID Wraparound Failures","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.6：新增pg_blocking_pids函数准确定位 Blocking SQL","slug":"20160529172020","date":"2016-05-29T09:20:20.000Z","updated":"2018-09-04T01:34:20.018Z","comments":true,"path":"20160529172020.html","link":"","permalink":"https://postgres.fun/20160529172020.html","excerpt":"","text":"PosttgreSQL 的SQL被锁情况在数据库维护过程中非常常见，之前博客PostgreSQL 锁分析 演示了 PostgreSQL 锁的一些场景，在开始本文的介绍之前特做以下说明，假如会话A堵住会话B，我们称会话B为 blocked 会话，会话A为 blokcing 会话，后续介绍时都用这两个词；当数据库出现锁时，如果对应用有影响，DBA应该在最短的时间内找到 blocking 会话并快速处理，在 9.6 版本前查找 blocking SQL 通常需要查询 pg_stat_activity、 pg_locks 等一系列视图，增加了故障分析的时间，9.6 版本新增 pg_blocking_pids() 函数，能够快速找到 blocking SQL，下面模拟一个简单的场景介绍这个函数的使用。 模拟测试创建测试表12345francs=&gt; create table test_lock(id int4,name text);CREATE TABLE francs=&gt; insert into test_lock values(1,'a'),(2,'b'),(3,'c');INSERT 0 3 会话一12345678francs=&gt; select pg_backend_pid();pg_backend_pid---------------- 22814francs=&gt; begin; BEGINfrancs=&gt; update test_lock set name='cc' where id=3;UPDATE 1 备注：会话一在事务里更新 ID=3 的记录，并不提交。 会话二1234567francs=&gt; select pg_backend_pid();pg_backend_pid---------------- 22845(1 row) francs=&gt; delete from test_lock where id=3; 备注：会话二删除ID=3的记录，此时由于这条记录之前被UPDATE并没有提交，这句DELETE仍然处于等待状态。 查找受阻的会话监控会话备注：从图中看到之前操作的两条 SQL，为什么 22845 会话处于等待状态呢，运行 pg_blocking_pids 函数可以找到 blocking 会话，如下： 查找 blocking SQL12345postgres=# select pg_blocking_pids(22845);pg_blocking_pids------------------&#123;22814&#125;(1 row) 备注：22814 正是 blocking SQL， 22845 为 blocked SQL。 总结这篇博客仅模拟了一个简单场景，并通过 pg_blocking_pids 函数查找 blocking SQL，真实生产环境锁的案例远比这复杂，具体情况具体分析。 参考 PostgreSQL 锁分析","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"PostgreSQL9.6：新增 pg_stat_progress_vacuum 视图监控 VACUUM","slug":"20160527165605","date":"2016-05-27T08:56:05.000Z","updated":"2018-09-04T01:34:19.956Z","comments":true,"path":"20160527165605.html","link":"","permalink":"https://postgres.fun/20160527165605.html","excerpt":"","text":"关于 Pg_stat_progress_vacuum 备注：pg_stat_progress_vacuum 视图的每一行对应一个运行中的VACUUM后台进程， 重要字段 phase 表示 VACUUM 进程的过程，VACUUM 阶段详见以下： Vacuum 的几个阶段 备注：接下来通过一个简单的实验查看VACUUM后台进程的阶段。 模拟测试创建测试表12345francs=&gt; create table test_big2(id int4,name character varying(64),create_time timestamp(0) without time zone default clock_timestamp());CREATE TABLEfrancs=&gt; insert into test_big2(id,name) select n, n||'_vacuum' from generate_series(1,5000000) n;INSERT 0 5000000 备注：创建一张 500 万记录的测试表。 会话一： VACUUM 表 test_big212francs=&gt; vacuum analyze test_big2;VACUUM 过程中 会话二：监控 VACUUM123456789101112131415161718192021222324252627282930postgres=# select * from pg_stat_progress_vacuum ;(0 rows)postgres=# \\watch 1Fri May 27 16:36:05 2016 (every 1s)-[ RECORD 1 ]------+--------------pid | 19427datid | 16386datname | francsrelid | 16445phase | scanning heapheap_blks_total | 34478heap_blks_scanned | 1712heap_blks_vacuumed | 0index_vacuum_count | 0max_dead_tuples | 291num_dead_tuples | 0省略。。。Fri May 27 16:36:33 2016 (every 1s)-[ RECORD 1 ]------+--------------pid | 19427datid | 16386datname | francsrelid | 16445phase | scanning heapheap_blks_total | 34478heap_blks_scanned | 31612heap_blks_vacuumed | 0index_vacuum_count | 0max_dead_tuples | 291num_dead_tuples | 0 备注：只看到 VACUUM 进程处于 scanning heap 状态，并且 heap_blks_scanned 一直在涨直到VAUUM结束，通过这个字段和 heap_blks_total 可以估算 VACUUM 进程的进度。 会话一：删除一部分记录再执行 VACUUM12345678francs=&gt; create index idx_test_big2_id on test_big2 using btree (id);CREATE INDEXfrancs=&gt; delete from test_big2 where id &gt; 4000000;DELETE 1000000francs=&gt; vacuum analyze test_big2;VACUUM 进程执行过程中，表越大，VACUUM耗时越长。 会话二：监控 VACUUM1234567891011121314151617181920212223242526272829postgres=# select * from pg_stat_progress_vacuum ;(0 rows)postgres=# \\watch 2Fri May 27 16:39:46 2016 (every 2s)-[ RECORD 1 ]------+------------------pid | 19427datid | 16386datname | francsrelid | 16445phase | vacuuming indexesheap_blks_total | 34478heap_blks_scanned | 34478heap_blks_vacuumed | 0index_vacuum_count | 0max_dead_tuples | 10033098num_dead_tuples | 1000000Fri May 27 16:39:49 2016 (every 2s)-[ RECORD 1 ]------+--------------------pid | 19427datid | 16386datname | francsrelid | 16445phase | cleaning up indexesheap_blks_total | 34478heap_blks_scanned | 34478heap_blks_vacuumed | 34478index_vacuum_count | 1max_dead_tuples | 10033098num_dead_tuples | 1000000 备注：删除索引后执行VACUUM过程中，可以查看到 phase 开始处于 vacuuming indexes 阶段，之后进入 cleaning up indexes 阶段，并且 num_dead_tuples 为 100 万。 对于数据量较大的生产库，如果开启了 autovacuum，通常大表的 VACUUM 周期较长，之前几乎无法判断表级别的 VACUM 进程进度，9.6 版本pg_stat_progress_vacuum 视图的出现可以监控表级别 VACUUM 进程的进度，对于 VACUUM 的监控提供了较大的方便。 参考 pg_stat_progress_vacuum 视图 VACUUM 几个阶段","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.6：新增等待事件（Wait Event）性能监控","slug":"20160522151435","date":"2016-05-22T07:14:35.000Z","updated":"2018-09-04T01:34:19.893Z","comments":true,"path":"20160522151435.html","link":"","permalink":"https://postgres.fun/20160522151435.html","excerpt":"","text":"等待事件（Wait event） 是Oracle性能分析一项重要的参考指标，PostgreSQL 9.6 版本之前不支持 Wait event，9.6 版本之后终于支持 Wait event，这在锁性能监控方面前进了一大步。之前版本 PostgreSQL 的 pg_stat_activity 视图的 waiting 字段判断会话是否等待锁资源（通俗地讲， waiting 值为true表示申请不到锁资源处于等待状态），但是并不会给出具体的锁的信息，9.6 版本之后 pg_stat_activity 视图的 waiting 字段被 wait_event_type 和 wait_event 字段取代，这两个字段分别代表等待事件的类型、等待事件名称，我们先来看以下示例。 模拟等待事件创建测试表12345678910111213141516francs=&gt; drop table test_wait;DROP TABLE francs=&gt; create table test_wait(id int4 ,name character varying(32));CREATE TABLE francs=&gt; insert into test_wait values(1,'a'),(2,'b'),(3,'c');INSERT 0 3 francs=&gt; select * from test_wait;id | name----+------ 1 | a 2 | b 3 | c(3 rows) 会话一：事务中执行一条ID=3的 UPDATE 语句，不提交1234567891011francs=&gt; begin;BEGINfrancs=&gt; select pg_backend_pid();pg_backend_pid---------------- 11261(1 row) francs=&gt; update test_wait set name='ccc' where id=3;UPDATE 1 备注：注意此时不提交事务。 会话二：删除ID=3的记录1234567francs=&gt; select pg_backend_pid();pg_backend_pid---------------- 11273(1 row) francs=&gt; delete from test_wait where id=3; 备注：此时这条 DELETE 语句处于等待状态。 会话三：查看 pg_stat_activity 视图备注：这时 pg_stat_activity 视图的 wait_event_type 字段值为 Lock，wait_event 值为 transactionid，表示等待事务完成。 回到会话二：执行 ALTER TABLE DDL终止之前的 DELETE 语句，执行一条 ALTER TABLE 加字段的 DDL，看看情况1francs=&gt; alter table test_wait add column create_time date; 备注：此时这条 alter table DDL处于等待状态 回到会话三：查看 pg_stat_activity 视图备注：这时 wait_event_type 值仍然为 Lock，wait_event 值变成了 relation，表示申请 relation 级别的锁，这里是指表级锁；通过以上两个测试大概对等待事件有了基本的了解，等待事件种类很多，不同的等待事件具有不同的含义， PostgreSQL9.6 手册提供详细的等待事件表，如下： wait_event_type 主要分类四类： LWLockNamed：表示backend后台进程等待某种特定的轻量级锁； LWLockTranche：表示backend后台进程等待一组相关轻量级锁； Lock：表示backend后台进程等待重量级的锁，通常是指 relation、tuple、page、transactionid 等子类型锁； BufferPin：表示server process 后台进程等待 buffer pin，手册上解释为 Waiting to acquire a pin on a buffer，比较难理解，以后想想如何模拟此场景。 详见: PostgreSQL9.6 wait_event 通过 pgbench 监控等待事件监控等待事件的 SQL123456SELECT pid, wait_event_type, wait_event FROM pg_stat_activity WHERE pid &lt;&gt; pg_backend_pid() and wait_event is NOT NULL;pid | wait_event_type | wait_event------+-----------------+---------------2540 | Lock | relation11273| Lock | transactionid(2 rows) 备注：这条SQL用户监控数据等待事件情况，下面通过 pgbench 压力测试示例监控等待事件情况。 创建测试表12345francs=&gt; create table test_wait_big(id int4 primary key, name character varying(32));CREATE TABLE francs=&gt; insert into test_wait_big(id)select n from generate_series(1,3000000) n;INSERT 0 3000000 编写 update_1.sql ，用于 pgbench1234[pg96@db1 load_test]$ cat update_1.sqlset v_id random(1,1000000) update test_wait_big set name=hashtext('random()*1000000') where id=:v_id; 备注： 9.6 版本的 random 函数用法不同，使用内置 random 函数替换 setrandom。 压力测试12[pg96@db1 load_test]$ nohup pgbench -c 10 -T 90 -d francs -U francs -n N -M prepared -f update_1.sql &gt; update_1.out &amp;[1] 11856 查询等待事件情况，每隔1秒运行以下监控SQL图一图二图三 备注: 压力测试过程中，监控到了 buffer_content、extend、XidGenLock 等待事件。 参考 PostgreSQL9.6 pg_stat_activity PostgreSQL9.6 Wait_Event Alexander Korotkov: Monitoring Wait Events in PostgreSQL 9.6 PostgreSQL 锁浅析","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Opitimize","slug":"Opitimize","permalink":"https://postgres.fun/tags/Opitimize/"}]},{"title":"PostgreSQL9.6：Partial Index 索引支持 Index-Only Scan ","slug":"20160521214115","date":"2016-05-21T13:41:15.000Z","updated":"2018-09-04T01:34:19.831Z","comments":true,"path":"20160521214115.html","link":"","permalink":"https://postgres.fun/20160521214115.html","excerpt":"","text":"An index-only scan is now allowed if the query mentions such columns only in WHERE clauses that match the index predicate. Index-only scan 最早出现于 PostgreSQL9.2 版本， 9.6版之前局部索引（Partial index）不支持 Index-only scan，9.6 版开始 Partial index 将支持 Index-only scan，下面演示下。 测试准备创建测试表并导入数据123456789101112131415[pg96@db1 ~]$ psqlpsql (9.6beta1)Type \"help\" for help. francs=&gt; create table test_2(id int4,flag char(1));CREATE TABLE francs=&gt; insert into test_2 select n,'Y' from generate_series(1,10000) n;INSERT 0 10000 francs=&gt; insert into test_2 select n,'N' from generate_series(10001,11000) n;INSERT 0 1000创建 Partial indexfrancs=&gt; create index idx_partial_test_2_id on test_2 using btree (id) where flag='N';CREATE INDEX 备注：索引字段为 id， Where 条件为 flag 字段。 执行计划123456francs=&gt; explain select id from test_2 where id=10001 and flag='N'; QUERY PLAN -----------------------------------------------------------------------------------------Index Only Scan using idx_partial_test_2_id on test_2(cost=0.28..4.29 rows=1 width=4)Index Cond: (id = 10001)(2 rows) 备注：从执行计划中看出，走了 Index-only scan。 下面的执行计划没有走 Index-only scan123456francs=&gt; explain select flag from test_2 where id=10001 and flag='N'; QUERY PLAN ------------------------------------------------------------------------------------Index Scan using idx_partial_test_2_id on test_2(cost=0.28..4.29 rows=1 width=2)Index Cond: (id = 10001)(2 rows) 备注：为什么这条SQL没有走 Index-only scan，留给读者思考。 总结虽然只是将 Index-only scan 适应范围扩展到了局部索引（Partial index）， 这对局部索引应用场景的性能优化具有较大的促进作用。 参考 Index-Only Scans PostgreSQL 9.2 Beta: Test Index-only scans","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Parallel Query","slug":"Parallel-Query","permalink":"https://postgres.fun/tags/Parallel-Query/"}]},{"title":"PostgreSQL9.6：Parallel Aggregates 初体验 ","slug":"20160519163016","date":"2016-05-19T08:30:16.000Z","updated":"2018-09-04T01:34:19.768Z","comments":true,"path":"20160519163016.html","link":"","permalink":"https://postgres.fun/20160519163016.html","excerpt":"","text":"PostgreSQL9.6 新版本不仅支持并行的 sequential scans，而且支持并行的 aggregates 操作，下面演示下。 测试准备测试环境笔记本 thinkpas x250 虚拟机 ， CPU 双核双线程。 创建测试表123456789[pg96@db1 ~]$ psql francs francspsql (9.6beta1)Type \"help\" for help. francs=&gt; create table test_big1(id serial, name character varying(64),create_time timestamp(0) without time zone);CREATE TABLE francs=&gt; insert into test_big1(id,name)select n, n||'_test' from generate_series(1,5000000)n;INSERT 0 5000000 不开启并行1234567891011francs=&gt; set max_parallel_degree =0;SET francs=&gt; explain analyze select sum(id) from test_big1; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------Aggregate (cost=91435.00..91435.01 rows=1 width=8) (actual time=3569.480..3569.480 rows=1 loops=1)-&gt; Seq Scan on test_big1(cost=0.00..78935.00 rows=5000000 width=4) (actual time=7.109..2252.965 rows=5000000 loops=1)Planning time: 14.424 msExecution time: 3596.088 ms(4 rows) 备注：不开启并行，查询时间 3596 ms。 开启并行开启并行112345678910111213141516francs=&gt; set max_parallel_degree =1;SET francs=&gt; explain analyze select sum(id) from test_big1; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------Finalize Aggregate (cost=66699.82..66699.83 rows=1 width=8) (actual time=3206.424..3206.424 rows=1 loops=1)-&gt; Gather (cost=66699.71..66699.82 rows=1 width=8) (actual time=3196.270..3196.285 rows=2 loops=1)Workers Planned: 1Workers Launched: 1-&gt; Partial Aggregate (cost=65699.71..65699.72 rows=1 width=8) (actual time=3157.948..3157.949 rows=1 loops=2)-&gt; Parallel Seq Scan on test_big1(cost=0.00..58346.76 rows=2941176 width=4) (actual time=4.789..2128.432 rows=2500000 loops=2)Planning time: 27.492 msExecution time: 3229.083 ms(8 rows) 备注：开启并行1，查询时间 3229 ms。 测试结果设置不同 max_parallel_degree， 多次测试结果如下： max_parallel_degree 最优Execution time(ms) 0 3596 1 3229 2 2665 3 2882 4 2879 5 2549 6 2656 7 2538 8 3064 备注：每次更改 max_parallel_degree 参数时，测试三次取最短查询时间，从图中看出，max_parallel_degree 设置值从0增加到8的过程中，本次测试的 SQL 的执行时间是降低的，当增加到9时，执行时间反而增加。 参考 WAITING FOR 9.6 – SUPPORT PARALLEL AGGREGATION. PostgreSQL9.6：Parallel Sequential Scans 初体验","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Parallel Query","slug":"Parallel-Query","permalink":"https://postgres.fun/tags/Parallel-Query/"}]},{"title":"PostgreSQL9.6：Parallel Sequential Scans 初体验 ","slug":"20160517211517","date":"2016-05-17T13:15:17.000Z","updated":"2018-09-04T01:34:19.706Z","comments":true,"path":"20160517211517.html","link":"","permalink":"https://postgres.fun/20160517211517.html","excerpt":"","text":"Oracle 支持强大的并行功能，创建索引，表分析，数据加载时可以开启并行，这项功能让很多数据库产品垂涎， 作为开源数据库 PostgreSQL 在并行方面也在努力尝试，很早之前 PostgreSQL 几乎不支持任何并行的作业，到了 9.5 版本 PostgreSQL 支持并行的 vacuum，到了 9.6 后， PostgreSQL 支持并行的顺序扫描，这是令人振奋的消息，因为这极大的提升了 PostgreSQL 统计分析SQL的性能，由于硬件环境限制，今天简单体验下，以下实验在笔记本虚拟机上进行。 Max_parallel_degree (integer) 参数 这个参数配置决定了每个 parallel query 并行操作允许的最大后台进程数，这个值的设置受后台进程数参数 max_worker_processes 限制。 PostgreSQL9.6 Beta1 测试设置 max_parallel_degree12pg96@db1 ~]$ grep &quot;max_parallel_degree&quot; $PGDATA/postgresql.confmax_parallel_degree = 4 # max number of worker processes per node 创建测试表123456789[pg96@db1 ~]$ psql francs francspsql (9.6beta1)Type \"help\" for help. francs=&gt; create table test_big1(id serial, name character varying(64),create_time timestamp(0) without time zone);CREATE TABLE francs=&gt; insert into test_big1(id,name)select n, n||'_test' from generate_series(1,5000000)n;INSERT 0 5000000 执行计划1234567891011francs=&gt; explain analyze select count(*) from test_big1;-------------------------------------------------------------------------------------------------------------------------------------Finalize Aggregate (cost=45560.42..45560.43 rows=1 width=8) (actual time=4236.468..4236.469 rows=1 loops=1)-&gt; Gather (cost=45560.00..45560.41 rows=4 width=8) (actual time=4232.517..4232.556 rows=5 loops=1)Workers Planned: 4Workers Launched: 4-&gt; Partial Aggregate (cost=44560.00..44560.01 rows=1 width=8) (actual time=4182.972..4182.973 rows=1 loops=5)-&gt; Parallel Seq Scan on test_big1(cost=0.00..41435.00 rows=1250000 width=0) (actual time=0.034..2450.966 rows=1000000 loops=5)Planning time: 112.309 msExecution time: 4236.920 ms(8 rows) 备注：执行多次，执行时间大概都在4秒多点，从执行计划中看到走了并行顺序扫描“Parallel Seq Scan on test_big1”，再细看“Workers Launched: 4”，表示开启了四个并行进程。 查看并行顺序扫描进程备注：图中可看到出现了四个 parallel worker 进程。 PostgreSQL9.5 测试测试之前先把 PostgreSQL 9.6 的数据库关了，在确保相等情况下进行测试。 创建测试表123456789101112131415161718[pg95@db1 ~]$ psql fdb fdbpsql (9.5alpha1)Type \"help\" for help. fdb=&gt; create table test_big1(id serial, name character varying(64),create_time timestamp(0) without time zone);CREATE TABLE fdb=&gt; insert into test_big1(id,name)select n, n||'_test' from generate_series(1,5000000)n;INSERT 0 5000000 fdb=&gt; explain analyze select count(*) from test_big1; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------Aggregate (cost=91435.00..91435.01 rows=1 width=0) (actual time=8389.093..8389.094 rows=1 loops=1)-&gt; Seq Scan on test_big1(cost=0.00..78935.00 rows=5000000 width=0) (actual time=9.958..4781.116 rows=5000000 loops=1)Planning time: 2.436 msExecution time: 8391.758 ms(4 rows) 备注：多次执行，时间在 8 秒左右。 总结由于硬件关系原因，本测试只在笔记本虚拟机上测试，在这个全表扫描测试场景下， PostgreSQL 9.6 是 PostgreSQL9.5 性能的两倍，今天仅是初步体验并行扫描，可以预测如果在X86服务器上测试，这个性能倍数会高一些，后续测试并行扫描的其它场景；个人认为 PostgreSQL 对并行顺序扫描的支持，在统计分析性能方面的提升前进了一大步。 参考 Robert Haas: PostgreSQL 9.6 with Parallel Query vs. TPC-H max_parallel_degree (integer)","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Parallel Query","slug":"Parallel-Query","permalink":"https://postgres.fun/tags/Parallel-Query/"}]},{"title":"DTCC 2016 中国数据库技术大会","slug":"20160505113150","date":"2016-05-05T03:31:50.000Z","updated":"2018-09-04T01:34:19.643Z","comments":true,"path":"20160505113150.html","link":"","permalink":"https://postgres.fun/20160505113150.html","excerpt":"","text":"会议时间：2016年5月12日-14 日 北京国际会议中心详见： http://dtcc.it168.com/ 这两届的 DTCC 数据库大会陆续有了 PostgreSQL 数据库的影子，看了下 2016 DTCC 数据库技术大会，有三个关于 PostgreSQL 的议题，如下： 第一个议题是德哥带来的“从Oracle DBA 到 PostgreSQL 布道者”的分享 第二个议题是来自 AppAnnie 的DBA 关启盼带来的“在AWS构建高可用扩展PostgreSQL集群”的分享。 第三个议题是来自去哪儿网DBA李海龙带来的“Optimize slow query in PostgreSQL”的分享。 期待以后 DTCC 数据库技术大会能有更多关于 PostgreSQL 的议题分享。","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"【社区大礼】PostgreSQL 9.4.4 中文文档发布","slug":"20160429160605","date":"2016-04-29T08:06:05.000Z","updated":"2018-09-04T01:34:19.581Z","comments":true,"path":"20160429160605.html","link":"","permalink":"https://postgres.fun/20160429160605.html","excerpt":"","text":"在PG中文社区文档翻译组的努力下，尤其是山东瀚高的韩悦悦、南京陈华军等各位社区志愿者的长时间的辛勤工作下，PostgreSQL 9.4.4版本的中文文档已全部翻译完成并上线，让我们向他们表示十二分的敬意，谢谢你们，志愿者是最可爱的人！ 《PostgreSQL9.4.4中文手册》是在《PostgreSQL9.3.1中文手册》 的基础上翻译而成，山东瀚高的韩悦悦和另一名同事完成了绝大部分的翻译工作。详细请参考:PostgreSQL9.4中文手册的翻译。 如果发现中文手册中的问题请向Github源码仓库或PostgreSQL中文手册翻译小组QQ群(309292849)反馈，也可直接向Github源码仓库提交PR。 在线手册http://www.postgres.cn/docs/9.4/ 离线下载猛戳这里！ Githubhttps://github.com/postgres-cn/pgdoc-cn/releases/tag/v9.4.4-1.0","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"2016 象行中国（南京站）及南京数据库用户交流会","slug":"20160414100352","date":"2016-04-14T02:03:52.000Z","updated":"2018-09-04T01:34:19.518Z","comments":true,"path":"20160414100352.html","link":"","permalink":"https://postgres.fun/20160414100352.html","excerpt":"","text":"活动详情活动主题：2016年象行中国（南京站）及南京数据库用户交流会活动时间：2016年4月23日活动地址：雨花台区软件大道180号南京大数据产业基地3#-101招商中心活动主办：PostgreSQL中国用户会南京分会、南京数据库用户组活动协办：中兴软创科技股份有限公司、南京大数据产业协会活动负责人：陈华军(18912964546)、周佳明(15295752646)、刘泉(13813976998) 日程安排上午9:20 ~ 9:30: 会议开始，主持人介绍9:30 ~10:10: 陈华军：PostgreSQL流复制高可用原理与实践10:10~10:20: 中间休息10:20~11:00: 卢加俊：Redis常见集群的高可用及运维的一些坑11:00~11:40: 王永：SaaS应用下PostgreSQL的问题总结 下午13:30~14:10: 德哥：PG物联网应用场景分析14:10~14:20: 中间休息14:20~15:00: 王强：百台数据库自动化运维基础15:00~15:40: 王伟","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"【转】XTTS，减少数据库迁移时业务停机时间的利器","slug":"20160321145801","date":"2016-03-21T06:58:01.000Z","updated":"2018-09-04T01:34:19.456Z","comments":true,"path":"20160321145801.html","link":"","permalink":"https://postgres.fun/20160321145801.html","excerpt":"","text":"对于跨平台（例如 AIX、HP-Unix 迁移到 Linux）数据库迁移，使用导出导入已不能满足大数据库的迁移需求， XTTS 正是为了解决这一问题存在。 XTTS 可以提供跨平台的增量迁移，从而大大减少停机时间，详见新炬网络最近推出一篇关于 XTTS 迁移方案的文章，介绍很得详细，非常受用。XTTS，减少数据库迁移时业务停机时间的利器 文中给出了 XTTS 的主要步骤： 一般情况下XTTS有以下几个主要步骤A、将源端数据库表空间设置为READ ONLY模式。B、传输数据文件到目标系统。C、转换数据文件为目标系统的字节序。D、在源端导出元数据，并在目标端导入。E、将目标端的数据库表空间设置为READ WRITE 增强版XTTS的主要步骤将变成如下A、将源端数据文件传输到目标系统。B、转换数据文件为目标系统的字节序。C、在源端创建增量备份，并传输到目标端。D、在目标端恢复增量备份。E、重复多次操作C和D步骤。F、将源端数据库表空间设置为READ ONLY模式。G、最后一次执行C和D步骤。H、在源端导出元数据，并在目标端导入。I、将目标端的数据库表空间设置为READ WRITE。 参考XTTS，减少数据库迁移时业务停机时间的利器","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"【译】PostgreSQL优于其他开源数据库的特性：Part II ","slug":"20151211151702","date":"2015-12-11T07:17:02.000Z","updated":"2018-09-04T01:34:19.393Z","comments":true,"path":"20151211151702.html","link":"","permalink":"https://postgres.fun/20151211151702.html","excerpt":"","text":"接着上一篇，和同事共同翻译这篇 PostgreSQL 推广文章的第二部分。 【译】PostgreSQL优于其他开源数据库的特性：Part II 译者朱智武 浙江移动DCOS工程师谭峰（francs） 浙江移动PG数据库专家 作者原文作者：Lisa Smith作者博客：https://www.compose.io/articles/author/lisa-smith/ PostgreSQL的宣传口号声称它是“全世界最先进的开源数据库”。在本系列Part I我们介绍了存储数据，包括数据模型、数据结构、类型、大小限制，给出了一些PostgreSQL为何如此声称的理由。在Part II，我们将介绍数据操作和检索，包括索引、虚拟表特性和查询能力。 索引PostgreSQL提供其他开源数据库所不具备的索引功能。PostgreSQL除了标准索引类型之外，还支持局部、表达式、GiST、GIN索引。我们来看上述这些特殊索引。 局部索引当你仅仅想为一张表的子集添加索引就可以创建局部索引（Partial Indexes），比如某列的值符合一个特定条件的所有行。这个有利特性让你保持合理的索引大小，并达成提高性能和减少磁盘空间的目标。局部索引的一个关键是被索引的列可以与提供子集约束条件的列不同。比如，你可能只想索引那些支付客户的帐号而不包括为内部测试而创建的帐号。 说明重要的一点，有时候MySQL的局部索引（Partial Indexes有时也被翻译为部分索引）术语用来指截取被索引的列值至一定数量的字节数，而不是基于一个条件去限制被索引行的数量。我们这里描述的局部索引MySQL不支持。 表达式索引创建表达式索引用来索引通过函数预计算得到的一个列。这些新值在查询时被索引和对待如同常量，而不是查询每次运行时需要重新计算。举一个例子，如果你有一个网页点击日志，采集他们接收的任何格式URL点击，你可能想创建一个基于小写的标准URL的索引（PostgreSQL是大小写敏感的，compose.io和Compose.io会被认为是不同的结果）： GIST和GINGiST（Generalized Search Tree）允许联合B树、R树和用户自定义索引类型来创建拥有先进查询能力的定制索引。GiST在PostGIS（从2015年1月以来我们所有PostgreSQL部署的标配）和OpenFTS（一个开源全文搜索引擎）中使用。PostgreSQL也支持SP-GiST，它允许使数据检索异常快速的分区查找索引的创建。 GIN（Generalized Inverted Index）可以索引复杂数据类型。复杂数据类型允许你以不同方式联合其他数据类型来创建完全定制化的数据类型。查看本系列的Part I以概览复杂数据类型。创建GiST和GIN索引的语法是，CREATE INDEX .. ON .. USING GIST|GIN ..。简单！在PostgreSQL 9.5（译者注：目前处于beta 2），BRIN（Block Range Index）将被支持。BRIN允许基于被索引的列将大表打散为一系列范围。这意味着查询计划只需要扫描查询所限定的某一个范围。此外，范围索引所需要的磁盘空间大小比标准B树索引要小很多。 对比我们关注的其他SQL数据库在表达式索引上正在缩小差距。在MySQL 5.7.6，生成列（Generated Column）开始被支持，可以用作表达式索引。对于MariaDB，虚拟列（Virtual Column，也成为生成列或计算列）在版本5.2中开始支持，但仅支持使用内置函数创建列（无法使用用户自定义函数）。Firebird的2.0版本，使用计算列（Computed Column）的表达式索引开始被支持。然而，这些数据库不支持局部、GiST或GIN索引。 当创建索引并希望去分析它们的性能时，别忘记去阅读mySidewalk的Matt Barr书写的技术文章Simple Index Checking with PostgreSQL。 虚拟表特性虚拟表在很多查询中是必需的。我们对比过的所有SQL数据库提供一些虚拟表功能，PostgreSQL提供了更多。 通用表表达式和递归PostgreSQL通过WITH子句支持通用表表达式（Common Table Expression，CTE）。我们在技术文章PostgreSQL – Series Random and With中展示过该特性。通用表表达式使你在查询语句以内联方式创建虚拟表，逻辑上表达一系列操作的顺序，这相比在其他地方使用子查询创建虚拟表更容易阅读和保证质量。PostgreSQL中的通用表表达式可以递归使用。这个方便的功能使你单步遍历一个层次结构，语句重复自我引用直到没有数据被返回。这是一个递归通用表表达式的例子，在一个话题分类中标识了层级、话题、父子关系： MySQL和MariaDB不使用WITH子句，所以，并不正式支持通用表表达式。这些数据库中可以使用子查询创建衍生表，然而它们并不允许递归。Firebird这方面比MySQL和MariaDB好，与PostgreSQL一样支持使用WITH子句的通用表表达式并提供递归功能。 物化视图物化视图是另一项PostgreSQL支持的实用的虚拟表特性。物化视图就像普通视图那样代表一个经常使用的查询结果集，只是结果集像一个普通表那样存储在磁盘上。物化视图也可以添加索引，不像普通视图每次请求时重新生成，物化视图是及时的快照。它们只在特定时刻刷新。这可以极大地加快使用物化视图的查询的执行速度。无需在查询中使用普通视图或做复杂表关联或运行聚合函数，使用一个包含所需数据在磁盘的物化视图可以提高效率。当你在一个物化视图中更新数据，可以按需使用REFRESH命令。这是一个物化视图的例子，生成聚合收益数据： Firebird、MySQL和MariaDB并不支持物化视图，但可以使用一种变通方案，创建一张普通表并使用存储过程或者触发器更新它。 查询能力PostgreSQL的查询功能是丰富的。前面章节讨论了WITH子句，现在来看SELECT语句中使用的另外两个可选特性。 集合查询PostgreSQL提供UNION、INTERSECT和EXCEPT子句用于SELECT语句之间的交互。UNION将第二个SELECT语句的结果附加到第一个。INTERSECT返回两个SELECT语句均有的行。EXCEPT返回第一个SELECT语句有而第二个SELECT语句没有的行。我们看一个使用EXCEPT的例子，该语句返回客户联系信息除非客户一周内已经收到并回复邮件。 MySQL、MariaDB和Firebird都支持UNION，但都不支持INTERSECT和EXCEPT。然而，通过查询中的关联以及EXISTS条件，可以获取与PostgreSQL相同的结果集。当然，这会使查询变得更为复杂。 窗口函数窗口函数基于结果集的部分行（一个子集一个窗口）运行聚合函数，极其有用。实质上，它遍历与当前行有关的分区中的所有行，运行该函数。常用函数包括ROW_NUMBER()、RANK()、DENSE_RANK()和PERCENT_RANK()。关键词OVER，与PARTITION BY和ORDER BY一起，指示使用一个窗口函数。举一个例子，在下面的章节“函数及其他”，我们使用一个窗口函数ROW_NUMBER() OVER来确定一系列数值的中位数。注意WINDOW子句并不是必需的，只是用来创建和命名窗口以帮助保持条理。 Firebird、MySQL和MariaDB现阶段不支持窗口函数，虽然窗口函数几年前就在Firebird 3的支持计划中宣布。 横向子查询（Lateral Subquery）在FROM子句中关键词LATERAL可以作用于子查询，允许子查询和之前创建的其他表或虚拟表之间做交叉引用。查询语句如此可以更为简化。它的工作方式是每一行与交叉引用的表作衡量，这意味着查询语句执行的速度加快。这里是一个例子，我们想要一个学生列表以了解他们最近是否阅读面向技术的话题： MySQL、Firebird和MariaDB现阶段不支持横向子查询（Lateral Subquery）。同样地，存在变通方案，但是查询语句将变得更为复杂。 另一件事需要说明，MySQL和MariaDB不支持完全外连接，但一个使用UNION ALL的变通方案可以用来合并两张表的所有行。 函数及其他PostgreSQL提供健壮的内置操作符和函数，包括那些支持本系列Part I里特定数据类型，但你可以创建自己的操作符和函数（包括聚合函数），如同定制的存储过程和触发器。我们无法提及所有这些细节，因为内容过多，但我们可以看函数相关的两个简单例子。 PostgreSQL支持4种用户自定义函数：查询语言、过程语言、C语言和内部语言。每一种都可以传入和返回基础和复杂类型。注意在PostgreSQL中CREATE FUNCTION命令不仅可以创建函数也可以创建存储过程。 让我们看一个例子，创建一个返回复杂类型的函数： 这是一个实用的定制函数，用来找到一个数值序列中的中位数： 我们用来对比的其他开源SQL数据库也允许创建自己的函数、存储过程和触发器，但它们没有PostgreSQL提供的那么丰富的数据类型和自定义选项。额外的，在PostgreSQL你可以创建自己的操作符。其他数据库并不支持用户自定义操作符。 语言扩展PostgreSQL拥有大量的语言扩展，一些是发行版的一部分，更多的是第三方。 在Compose，我们仅支持可信任的PostgreSQL语言扩展，以保证你的部署是安全的。我们在二月重新支持PL/Perl，并在八月支持PL/v8，一个基于Javascript的过程语言。这些语言扩展，比基于SQL的PL/pgSQL语言（Compose的部署同样可以使用）拥有更多内置函数，使你可以创建复杂脚本来操作和处理服务器上的数据。 更多PostgreSQL刚刚宣布了9.5版本的Beta 1发行版（译者注：目前已发布9.5 Beta 2）。我们已经着手于它，学习所有新特性以便使9.5版本一旦稳定就第一时间提供出来。在过去几个月我们查看了9.5带来的几个特性，比如前面提及的BRIN索引。 阅读 PostgreSQL’s Future Is Looking Up-sert和Beyond Upsert - Coming in PostgreSQL 9.5以预览即将到来的9.5。 总结PostgreSQL有丰富的内置特性和大量的方式可以定制或扩展来满足需求。另外，它是可靠和成熟的，这是一个值得任何企业致力于的数据库解决方案。即便如此，它仍对刚起步的开发项目保持易用性和高效性。 我们仅仅涉及了少数PostgreSQL不同于其他开源SQL数据库的功能，还有更多的其他功能未涉及（在9.5版本还将带来更多）。我们希望这两篇文章能提供一个为什么选择PostgreSQL的坚实概述。 阅读原文 What PostgreSQL has over other open source SQL databases: Part II","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"【译】PostgreSQL优于其他开源数据库的特性：Part I","slug":"20151209093052","date":"2015-12-09T01:30:52.000Z","updated":"2018-09-04T01:34:19.331Z","comments":true,"path":"20151209093052.html","link":"","permalink":"https://postgres.fun/20151209093052.html","excerpt":"","text":"和同事 DCOS 工程师共同翻译了一篇稍有争议的文章，这篇译文主要从功能特性方面介绍 PG ，观点中肯，难度适中，由于这篇文章在描述 PG 功能特性时拿其它开源数据库相比，在翻译之前有考虑过这样可能不太妥当，但这篇文章突出了 PG 功能特性，仍然是一篇好文，并且此文只做技术讨论，并没有带着情绪去写，详见： 转自我们部门的公众号”三墩IT人” 【译】PostgreSQL优于其他开源数据库的特性：Part I 译者谭峰（francs） 浙江移动PG数据库专家朱智武 浙江移动DCOS工程师 作者原文作者：Lisa Smith作者博客：https://www.compose.io/articles/author/lisa-smith/ 你可能会问自己 “为什么选择PostgreSQL ?” 开源数据库我们有好几种选择(本文参考 MySQL， MariaDB 和 Firebird )， 那么 PostgreSQL 具有哪些其它开源数据库不具备的特性呢？ PostgreSQL 宣称它是“世界上最先进的开源数据库。” 我们将会给出 PostgreSQL 这么宣称的原因。 本系列 Part I，我们将看看数据存储 – 数据模型， 结构， 数据类型， 和大小限制. Part II 我们将更关注数据操作和检索。 数据模型PostgreSQL 不仅仅是关系型，它也是对象关系型，这使它一定程度优于其他一些开源数据库，例如 MySQL，MariaDB 和 Firebird。 一个对象-关系数据库的一个基本特征是支持用户自定义对象和它的属性，包括数据类型、函数、操作符，域和索引。这使得PostgreSQL非常灵活和健壮，除此之外，复杂的数据结构可以被创建，存储和检索，下面的例子可以看到标准RDBMS不支持的嵌套和复合结构。 数据类型和结构PostgreSQL 有着广泛的被支持数据类型列表，除了 numeric， floating-point， string， boolean 和你能想到的数据类型(并且支持各种选项)，PostgreSQL 还引以为傲地支持 uuid， monetary， enumerated， geometric， binary， network address，bit string， text search， xml， json， array， composite 和 range 数据类型，以及一些内部对象标识和日志位置类型。公平地说，MySQL，MariaDB 和 Firebird 在不同程度上支持上面部分数据类型，但仅仅 PostgreSQL 支持以上全部数据类型。 让我们仔细看看其中几个数据类型： 网络地址类型PostgreSQL提供用于存储不同网络地址的类型， CIDR (Classless Internet Domain Routing) 数据类型适合IPv4和IPv6网络地址，CIDR的一些例子： 192.168.100.128/25 10.1.2.3/32 2001:4f8:3:ba:2e0:81ff:fe22:d1f1/128 ::ffff:1.2.3.0/128 也可用于网络地址存储的是INET数据类型， 用于IPv4和IPv6主机，子网是可选的，MACADDR 数据类型用于存储硬件标识的 MAC 地址，例如 08-00-2b-01-02-03。 MySQL和MariaDB 提供一些 INET 函数用于网络地址转换，但不直接提供用于存储网络地址的数据类型， Firebird 也没有网络地址类型。 多维数组因为PostgreSQL是对象关系数据库，数组的元素可以存储大多数现有的数据类型，通过将方括号附加到使用数组类型的字段后就能定义数组，可以指定数组大小，但不是必需的。让我们通过一个假日野餐菜单展示数组的使用：(译者注：建表脚本有错误，创建表会报错，作者大概只是展示数组数据类型的使用。) MySQL， MariaDB， 和 Firebird 不具备这种能力，如果想把类似这样的数组存储在传统的关系数据库中，替代的解决方法是为数组值每一行创建单独的表。 几何类型地理数据正迅速成为很多应用程序的核心需求， PostgreSQL一直支持各种各样的几何数据类型，如点、线、圆、多边形。路径数据类型就是其中之一。路径中包含多个点序列，可以开放(开始和结束点是没有连接的)或封闭(开始和结束点连接)。让我们用一个徒步旅行的例子作为一个路径，在这个例子中我的徒步旅行路线是循环的，开始点和结束点相连，所以我的路径是闭环的。坐标内的圆括号意味着一个封闭的路径而方括号表示开放的路径。 PostGIS 扩展增强了 PostgreSQL 现有的几何数据特性，例如额外的空间类型，函数，操作符和索引，它支持位置特性以及栅格和矢量数据数据。它还提供了与各种第三方开源和专有的地理空间处理工具的互操作性，例如映射和呈现数据.今年一月份我们为Compose PostgreSQL部署提供了PostGIS：为所有Compose PostgreSQL部署的PostGIS。 注意在MySQL 5.7.8 和MariaDB 5.3.3，才添加了支持OpenGIS地理信息标准的数据类型扩展， 这个版本的 MySQL和之后的 MariaDB版本提供了和 类似PostgreSQL方便使用的几何数据类型的数据类型存储。 然而，在MySQL和MariaDB，数据值必须先使用简单的命令转换为几何格式之后才能插入到表中，Firebird目前并不提供地理数据类型。 JSON 支持PostgreSQL 的 JSON 支持在 SQL 数据库中支持非结构化数据，当数据结构由于处在开发中需要灵活性或数据对象包含了未知的字段时是有用的。 JSON数据类型强制检查 JSON 有效性，这让你可以使用专门的 JSON 操作符和 PostgreSQL提供的内置函数用于查询和操作数据。也可用JSONB类型 – JSON 的二进制形式，与 JSON 不同的是它删除了数据中的空格，保存对象的顺序不一样，存储层面做了优化，只有最后一个重复的键值保留。JSONB通常是首选的格式因为它需要更少的空间存储对象，可以被索引，处理速度更快，因为它不需要被解析，要了解更多，请查看： Is PostgreSQL Your Next JSON Database? 在MySQL 5.7.8 和 MariaDB 10.0.1 支持JSON对象， 虽然目前在这些数据库中有不同的函数和运算符支持 JSON，它们的索引方式与 PostgreSQL 的 JSONB不同。 Firebird 目前仅支持文本对象的 JSON。 创建新的数据类型如果 PostgreSQL 提供的数据类型列表还不够，您还可以使用 CREATE TYPE 命令创建新的数据类型，例如复合类型，枚举，范围等。 这里是一个创建并且使用新创建的复合类型的例子。 MySQL，MariaDB，和Firebird不提供这种强大的功能，因为它们不是面向对象的。 Data sizePostgreSQL可以处理大量的数据。下面列出了当前的大小限制: 在 Compose 平台我们会自动部署扩展，所以您不必担心数据增长。但是，正如每位 DBA 知道的，最好警惕容量上的限制，我们建议您在创建表和索引时遵从常规性的指导。 相比之下， MySQL和MariaDB行大小限制为65535字节，Firebird 宣称最大行大小为 64KB ，通常数据大小被操作系统文件大小限制。因为PostgreSQL可以将表数据存储在多个小文件，它可以绕过这个限制 – 不过需要注意的是太多的文件可能对性能造成负面影响。然而，MySQL和MariaDB 确实比 PostgreSQL 单表支持更多的列(最多4096列，与数据类型有关)和更大的单表大小，但在罕见的情况下，现有的PostgreSQL限制需要被超过。 数据完整性PostgreSQL毫无疑问符合 ANSI-SQL:2008 标准， 完全遵从 ACID (Atomicity， Consistency， Isolation and Durability) ，并且它因稳定性和事务完整性而闻名。它支持的主键，约束，外键，唯一约束，非空约束，以及其它数据完整性特性确保只有合法的数据被存储。 MySQL 和 MariaDB 使用合InnoDB / XtraDB存储引擎可兼容更多的 SQL 标准，他们现在为SQL模式提供一个STRICT选项，SQL模式决定了使用的数据检查方法。然而，基于使用的模式，非法和截断的数据可能会被插入或更新时创建。这些数据库现在都不支持检查约束，外键约束也存在许多附加说明。此外，数据的完整性可能会大大取决于所选择的存储引擎。 MySQL ，MariaDB 长期侧重于速度和效率甚于遵从完整性和遵从性。 总结PostgreSQL有很多功能。使用一个对象-关系模型，它支持复杂的结构和内置的丰富用户定义的数据类型，它提供了广阔的数据容量和可信的数据完整性，你可能不需要我这里回顾的所有高级特性，但由于数据需求发展很快，拥有所有这些毫无疑问具有明显的好处。 如果PostgreSQL不能完全满足你的需求，或者你更倾向于更多选型， 那么看看我们在 Compose 平台提供的NoSQL数据库或其他开源SQL数据库，它们每个都有自己的优势，Compose 坚信选择合适的数据库为当务之急，作为解决方案，有时候这也意味着需要选择多个数据库。 准备好了看更多关于 PostgreSQL 的内容吗？在本系列的 Part II ，我们将看看PostgreSQL的数据操作和检索，包括虚拟表，查询功能，索引和语言扩展。 阅读原文 What PostgreSQL has over other open source SQL databases: Part I","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"一张牛图：高效运行","slug":"20151205122801","date":"2015-12-05T04:28:01.000Z","updated":"2018-09-04T01:34:19.268Z","comments":true,"path":"20151205122801.html","link":"","permalink":"https://postgres.fun/20151205122801.html","excerpt":"","text":"一张牛图，来自于成都文武信息技术有限公司 CEO 朱贤文的分享主题：《PostgreSQL存储安全和存储性能规划》，详见以下： PDF 新浪微盘上放了一份，下载地址：http://vdisk.weibo.com/s/qA5_qJd6kIStc/1449286997","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Oracle 12C 新特性：CDB 与 PDB","slug":"20151202155200","date":"2015-12-02T07:52:00.000Z","updated":"2018-09-04T01:34:19.206Z","comments":true,"path":"20151202155200.html","link":"","permalink":"https://postgres.fun/20151202155200.html","excerpt":"","text":"今天抽空了解了下 Oracle12C 的新特性之一： CDB 与 PDB，Oracle 12C 开始引入多租户的概念，允许 CDB(数据库容器)上支持多个 PDB(可挺拔数据库)，结构如下： Components of a CDBA CDB includes the following components: Root The root, namedCDB$ROOT, stores Oracle-supplied metadata and common users. An example of metadata is the source code for Oracle-supplied PL/SQL packages. A common user is a database user known in every container. A CDB has exactly one root. Seed The seed, namedPDB$SEED, is a template that you can use to create new PDBs. You cannot add objects to or modify objects in the seed. A CDB has exactly one seed. PDBs A PDB appears to users and applications as if it were a non-CDB. For example, a PDB can contain the data and code required to support a specific application. A PDB is fully backward compatible with Oracle Database releases before Oracle Database 12c. 这篇文章介绍得比较详细：http://www.cnblogs.com/kerrycode/p/3386917.html， 参考Overview of Managing a Multitenant Environment","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"2015 Postgre 大象会圆满举行","slug":"20151122092304","date":"2015-11-22T01:23:04.000Z","updated":"2018-09-04T01:34:19.143Z","comments":true,"path":"20151122092304.html","link":"","permalink":"https://postgres.fun/20151122092304.html","excerpt":"","text":"记得第一届 PG 中国用户会 2011 在广州举行，走过了五个年头； 今年用户大会在北京丽亭华苑酒店举行， 是过去五年中规模最大，参会人数最多的一次，记得三年前同样在北京举行的 PG 大会，到场人数才 70 人左右，今年估计至少有 300 人参会，看看下面这张图，座无虚席。 为了这次大会的举行，大会的组织者和志愿者都付出了巨大的努力，特别是大会期间的两天，深夜的时候我看到大会组织者群里仍在讨论第二天大会事务，看得出这次大会的组织者为这次大会很用心。 这次参会的嘉宾阵容强大，特邀美国，俄罗斯，日本数据库方面专家助阵，今年我代表浙江移动三墩IT人参加 PG 大会，分享了《浙江移动 Postgres-XC 应用实践》 的议题 贴几张 PPT 国外嘉宾现场交流互动环节 最后，全场 PGer 的合影留念。 这两天北京大雪，天气不是很好，但依然挡不住 PGer 们参加技术交流会的热情，今年 PG 大会举办得很成功，感谢这次大会的组织者和志愿者们，你们是这次大会最可爱的人。 我分享的议题《浙江移动 Postgres-XC 应用实践》http://pan.baidu.com/s/1sj9rRL7","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL 2015年欧洲大会“会报”","slug":"20151106134746","date":"2015-11-06T05:47:46.000Z","updated":"2018-09-04T01:34:19.081Z","comments":true,"path":"20151106134746.html","link":"","permalink":"https://postgres.fun/20151106134746.html","excerpt":"","text":"PostgreSQL 2015 欧洲大会在美丽的奥地利维也纳举行，面向全球的 PG DBA及开发者，为期四天，参会人员来自38个国家地区，参会总人数约400人，其中有4人来自中国。感谢 Henry Ren 的这篇文章。 详见：PostgreSQL 2015年欧洲大会“会报”","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"What's New in MySQL 5.7?","slug":"20151029151514","date":"2015-10-29T07:15:14.000Z","updated":"2018-12-04T00:29:59.232Z","comments":true,"path":"20151029151514.html","link":"","permalink":"https://postgres.fun/20151029151514.html","excerpt":"","text":"MySQL 5.7 GA 版本已发行，简单看了新特性介绍，改变挺大的，其中新增对 JSON 的支持，这篇文章介绍得比较详细：http://mysqlserverteam.com/whats-new-in-mysql-5-7-generally-available/","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"PostgreSQL：浅谈 Fsync 参数","slug":"20151029120019","date":"2015-10-29T04:00:19.000Z","updated":"2018-09-04T01:34:18.971Z","comments":true,"path":"20151029120019.html","link":"","permalink":"https://postgres.fun/20151029120019.html","excerpt":"","text":"这两天有朋友在做性能测试，咨询 PG 的 fsync 参数对性能的影响。这个参数比较特殊，通常情况都需要打开，今天有空研究下这个参数的影响。 关于 Fsync 参数 如果 fsync 参数设置成 on ，PostgreSQL 服务通过调用系统fsync()或其它方式确保更新已经物理写到磁盘，这样就保证了数据库集群将在操作系统或者硬件崩溃的情况下恢复到一个一致的状态。虽然关闭这个参数有一定提升，通常情况下需要打开这个参数，除非您能经受掉电或硬件故障带来的数据丢失，否则不要关闭这参数，下面做两方面测试：1) 关闭 sync 参数观察对 SELECT/UPDATE 压力测试的影响；2）关闭参数，冷关闭主机测试数据库是否能正常启动。 场景一: Fsync = on测试模型：一张 1000 万的表12345drop table if exists test_sync;create table test_sync(id int8,name text,creat_time timestamp(0) without time zone default clock_timestamp());insert into test_sync(id,name) select n,n||'_test' from generate_series(1,10000000) n;alter table test_sync add primary key(id);vacuum analyze test_sync; 设置 fsync 参数123456789pg95@pg95:~/script/load_test&gt; psqlpsql (9.5alpha2)Type \"help\" for help. postgres=# show fsync;fsync-------on(1 row) 主键查询1234[pg95@pg95 load_test]$ cat select_1.sqlsetrandom v_id 1 10000000 select name from test_sync where id=:v_id; pgbench1pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f select_1.sql &gt; select_1.out 2&gt;&amp;1 &amp; pgbench 结果:12345678910transaction type: Custom queryscaling factor: 1query mode: preparednumber of clients: 64number of threads: 1duration: 120 snumber of transactions actually processed: 2031338latency average: 3.781 mstps = 16909.975656 (including connections establishing)tps = 16930.166433 (excluding connections establishing) 主键更新1234[pg95@pg95 load_test]$ cat update_1.sqlsetrandom v_id 1 10000000 update test_sync set name='off' where id=:v_id; pgbench1pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f update_1.sql &gt; update_1.out 2&gt;&amp;1 &amp; pgbench 结果:12345678910transaction type: Custom queryscaling factor: 1query mode: preparednumber of clients: 64number of threads: 1duration: 120 snumber of transactions actually processed: 713242latency average: 10.768 mstps = 5939.386731 (including connections establishing)tps = 5946.560775 (excluding connections establishing) 场景二: Fsync = off设置 fsync 参数123456789pg95@pg95:~/script/load_test&gt; psqlpsql (9.5alpha2)Type \"help\" for help. postgres=# show fsync;fsync-------off(1 row) 为了测试准确重新刷以下脚本12345drop table if exists test_sync;create table test_sync(id int8,name text,creat_time timestamp(0) without time zone default clock_timestamp());insert into test_sync(id,name) select n,n||'_test' from generate_series(1,10000000) n;alter table test_sync add primary key(id);vacuum analyze test_sync; 主键查询1234[pg95@pg95 load_test]$ cat select_1.sqlsetrandom v_id 1 10000000 select name from test_sync where id=:v_id; pgbench1pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f select_1.sql &gt; select_1.out 2&gt;&amp;1 &amp; pgbench 结果:12345678910transaction type: Custom queryscaling factor: 1query mode: preparednumber of clients: 64number of threads: 1duration: 120 snumber of transactions actually processed: 2006681latency average: 3.827 mstps = 16707.303006 (including connections establishing)tps = 16727.661063 (excluding connections establishing) 主键更新1234[pg95@pg95 load_test]$ cat update_1.sqlsetrandom v_id 1 10000000 update test_sync set name='off' where id=:v_id; pgbench1pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f update_1.sql &gt; update_1.out 2&gt;&amp;1 &amp; pgbench 结果:12345678910transaction type: Custom queryscaling factor: 1query mode: preparednumber of clients: 64number of threads: 1duration: 120 snumber of transactions actually processed: 1505343latency average: 5.102 mstps = 12530.405426 (including connections establishing)tps = 12545.757516 (excluding connections establishing) Fsync = off 时模拟数据库异常接下来测试当 fsync=off 的情况下数据库异常宕掉的情况，做测试前做好数据库备份。 跑 update 脚本，给数据库施加压力 12pg95@pg95 :~/script/load_test&gt; pgbench -c 64 -T 300 -n -N -M prepared -d francs -U francs -f update_1.sql &gt; update_1.out 2&gt;&amp;1 &amp;[1] 52424 异常关闭数据库，强制关闭数据库主机，模拟掉电。 再次启动数据库，发现数据库无法启动，数据库日志如下： 数据库日志12345672015-10-28 16:51:59.942 CST,,,10141,,56308caf.279d,1,,2015-10-28 16:51:59 CST,,0,LOG,00000,\"ending log output to stderr\",,\"Future log output will go to log destination \"\"csvlog\"\".\",,,,,,,\"\"2015-10-28 16:51:59.944 CST,,,10143,,56308caf.279f,1,,2015-10-28 16:51:59 CST,,0,LOG,00000,\"database system was interrupted; last known up at 2015-10-29 00:37:41 CST\",,,,,,,,,\"\"2015-10-28 16:51:59.944 CST,,,10143,,56308caf.279f,2,,2015-10-28 16:51:59 CST,,0,LOG,00000,\"invalid primary checkpoint record\",,,,,,,,,\"\"2015-10-28 16:51:59.944 CST,,,10143,,56308caf.279f,3,,2015-10-28 16:51:59 CST,,0,LOG,00000,\"invalid secondary checkpoint record\",,,,,,,,,\"\"2015-10-28 16:51:59.944 CST,,,10143,,56308caf.279f,4,,2015-10-28 16:51:59 CST,,0,PANIC,XX000,\"could not locate a valid checkpoint record\",,,,,,,,,\"\"2015-10-28 16:51:59.944 CST,,,10141,,56308caf.279d,2,,2015-10-28 16:51:59 CST,,0,LOG,00000,\"startup process (PID 10143) was terminated by signal 6: Aborted\",,,,,,,,,\"\"2015-10-28 16:51:59.944 CST,,,10141,,56308caf.279d,3,,2015-10-28 16:51:59 CST,,0,LOG,00000,\"aborting startup due to startup process failure\",,,,,,,,,\"\" 备注：数据库果然无法启动，这时你可能通过 pg_resetxlog 救活数据库，当然会丢失一部分事务；或者如果有备份，从备份中恢复数据库。 总结 把上面的测试结果汇总，如下图，可见，关闭 fsync 对 SELECT 无影响， 而 UPDATE 性能有较大提升，这个场景提升了 111%；当然关闭 fsync 参数的代价是巨大的，当数据库主机遭受操作系统故障或硬件故障时，数据库很有可能无法启动，并丢失数据，建议生产库不要关闭这参数。 fsync SELECT 场景TPS UPDATE 场景TPS on 16930 5946 off 16727 12545 参考Fsync (boolean)","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"重大利好：GreenPlum 已开源","slug":"20151028105944","date":"2015-10-28T02:59:44.000Z","updated":"2018-09-04T01:34:18.909Z","comments":true,"path":"20151028105944.html","link":"","permalink":"https://postgres.fun/20151028105944.html","excerpt":"","text":"Greenplum 已开源，宣称世界上第一个开源的大规模并行数据仓库，绝对是令人振奋的消息，对于数据仓库的应用多一重量级选择。详见：http://greenplum.org/ The Greenplum Database is an advanced, fully featured, open source data warehouse. It provides powerful and rapid analytics on petabyte scale data volumes. Uniquely geared toward big data analytics, Greenplum Database is powered by the world’s most advanced cost-based query optimizer delivering high analytical query performance on large data volumes. The Greenplum project is released under the Apache 2 license. We want to thank all our current community contributors and are interested in all new potential contributions. For the Greenplum Database community no contribution is too small, we encourage all types of contributions. 项目地址：https://github.com/greenplum-db/gpdb","categories":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://postgres.fun/categories/GreenPlum/"}],"tags":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://postgres.fun/tags/GreenPlum/"}]},{"title":"MySQL：高可用方案选型参考(转自 叶金荣 ) ","slug":"20151026095028","date":"2015-10-26T01:50:28.000Z","updated":"2018-09-04T01:34:18.846Z","comments":true,"path":"20151026095028.html","link":"","permalink":"https://postgres.fun/20151026095028.html","excerpt":"","text":"老叶的这篇文章列举了 MySQL 各种高可用方案，非常精彩。原文：http://imysql.com/2015/09/14/solutions-of-mysql-ha.shtml 可选MySQL高可用方案MySQL的各种高可用方案，大多是基于以下几种基础来部署的： 基于主从复制； 基于Galera协议； 基于NDB引擎； 基于中间件/proxy； 基于共享存储； 基于主机高可用； 在这些可选项中，最常见的就是基于主从复制的方案，其次是基于Galera的方案，我们重点说说这两种方案。其余几种方案在生产上用的并不多，我们只简单说下。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"2015 Postgres 全国用户大会 -- PG大象会","slug":"20151022171739","date":"2015-10-22T09:17:39.000Z","updated":"2018-09-04T01:34:18.784Z","comments":true,"path":"20151022171739.html","link":"","permalink":"https://postgres.fun/20151022171739.html","excerpt":"","text":"自2011年以来，PG大象会（Postgres Conference China）已经举办到第5届。大会由中国Postgres用户会（China Postgres User Group，简称：CPUG）主办，是一场汇聚各界PostgreSQL大拿、交流最新业界技术动态和真实应用案例的盛宴。大象会的成功举办推动了PostgreSQL数据库在中国的发展，也起到了联系PG中国社区和国外社区的作用。今年“PG大象会”将于2015年11月20至21日在北京丽亭华苑酒店举行，我们期待您的参与！ 大会嘉宾：会议报名规模预计将超过400人，嘉宾阵容强大，特邀欧洲，俄罗斯，日本数据库方面专家助阵，国内顶级PostgreSQL数据库专家也悉数到场。 Postgres-XC项目的发起人铃木市一(SUZUKI Koichi) Postgres-XL的项目发起人Mason Sharp pgpool的作者石井达夫(Tatsuo Ishii) PG-Strom的作者海外浩平(Kaigai Kohei) Greenplum研发总监姚延栋 周正中(德哥), PostgreSQL中国用户会创始人之一 汪洋，平安科技数据库技术部经理 …… 嘉宾们所属的单位也都在业内享负盛名，包括： 国外企业：Pivotal（美国EMC旗下）、EnterpriseDB（美国）、NTT（日本）、SAROSS（日本）、NEC（日本）、Postgres Pro（俄罗斯）等； 国内企业：上容数据库、山东瀚高、阿里云、沃趣科技、平安科技、浙江移动、华为、中兴、Qunar去哪儿网、共致开源、太阳塔科技等； 国内知名大学：武汉大学、上海师范大学 今年我们预计将有 200 位以上行业嘉宾、技术专家及Postgres用户将参加本次会议，当中包括国内外知名企业，如：Pivotal（美国）、EnterpriseDB（美国）、NTT（日本）、SAROSS（日本）、NEC（日本）、Postgres Pro（俄罗斯）、上容数据库、山东瀚高、阿里云、沃趣科技、平安科技、中国移动、华为、中兴、去哪网、武汉大学、华东师范大学及多位BAT的众多专家 及 全国各地的PostgreSQL忠实用户。 大会日程安排11月20日，上午，主会场 09:00 - 09:10 [10] 欢迎致辞 Welcome PGer（萧少聪，2015年PG大象会负责人，阿里） 09:10 - 09:20 [10] 中国PG发展报告2015（李元佳，2015年PG中国用户会主席，华为） 09:20 - 09:35 [15] 上容数据库替代Oracle的国产化之路（田军，上容数据库） 09:35 - 10:10 [30+5] 高校PG人才培养的总结与思考（彭煜玮，武汉大学，《PostgreSQL 数据库内核分析》作者） 10:10 - 10:20 [10] 会间休息 10:20 - 10:55 [30+5] 平安数据库的PG之路（汪洋，平安科技） 10:55 - 11:30 [30+5] 浙江移动 PostgreSQL-XC 应用实践（谭峰，浙江移动，《PostgreSQL 9 Admin Cookbook》译者之一） 11:30 - 11:45 拍照留念 11月20日，下午，专场一 13:00 - 13:35 [30+5] PostgreSQL在平安的本土化（梁海安，平安科技） 13:35 - 14:10 [30+5] 上容旋极空间编码数据库引擎(李雨德，上容数据库) 14:10 - 14:55 [40+5] 一位PGer的安全修养（周正中(德哥)，阿里云） 14:55 - 15:15 [20] 会间休息 15:15 - 15:50 [30+5] Qunar的PostgreSQL HA探索及日常运维（李海龙，Qunar去哪儿网） 15:50 - 16:25 [30+5] 新型BI架构思考及PG中的实现（黄坚，湖南红手指，《pgpool-II中文手册》译者，《PostgreSQL 9 Admin Cookbook》译者之一） 16:25 - 17:00 [30+5] PostgreSQL存储过程加密(权宗亮，共致开源) 11月20日，下午，专场二 13:00 - 13:45 [40+5] PostgreSQL在实时大数据分析中的应用（金华峰(Aida)，莲子数据） 13:45 - 14:20 [30+5] PG索引自动化推荐案例分析（窦贤明(执白)，阿里云） 14:20 - 14:40 [20] 会间休息 14:40 - 15:15 [30+5] 最新硬件上的PostgreSQL性能优化（叶涛，华为） 15:15 - 15:50 [30+5] PostgreSQL存储安全和存储性能规划（朱贤文，成都文武信息技术） 15:50 - 16:25 [30+5] 通过MADLib实现PG的In-DB分析(王伟珣，Pivotal) 16:25 - 17:00 [30+5] Greenplum架构分析及开源历程（姚延栋，Pivotal） 11月21日，上午，PG高可用及容灾技术专场 09:00 - 09:45 [40+5] PostgreSQL实例恢复与热备份技术内幕（唐成，沃趣科技，《PostgreSQL修炼之道:从小工到专家》作者） 09:45 - 10:20 [30+5] 基于逻辑日志的PostgreSQL数据同步方案（曾文旌(义从)，阿里云） 10:20 - 10:30 [10] 会间休息 10:30 - 11:05 [30+5] PostgreSQL的Window分析函数源码优化（王晓玲，华东师范大学） 11:05 - 11:20 [45+10] PostgreSQL数据库视图优化（李海翔，Oracle，《数据库查询优化器的艺术》作者） 11月21日，上午，PG扩展性技术专场 09:00 - 09:55 [45+10] PostgreSQL cluster and scaling out (Koichi Suzuki, NTT DATA) 09:55 - 10:50 [45+10] Postgres-XL【待定】 (Mason Sharp) 10:50 - 11:05 [15] 会间休息 11:05 - 12:00 [45+10] How to manage a herd of elephants: introducing new features of pgpool-II (Tatsuo Ishii, SRAOSS INC) 11月21日，下午，PG数据库选型及迁移 13:00 - 13:35 [30+5] 数据库稳定性，选型与去IOE（赵振平，太阳塔科技，畅销书《Oracle数据库精讲与疑难解析》作者） 13:35 - 14:10 [30+5] 异构数据向Postgres的实时迁移（卢健，山东瀚高） 14:10 - 14:45 [30+5] Postgres“去O”实战（刘泉，中兴软创） 14:45 - 15:05 [20+5] O2PG如何在企业落地（萧少聪(铁庵)，阿里云） 11月21日，下午，PG扩展性技术专场 13:00 - 13:55 [45+10] Distributed transaction manager for PostgreSQL (Alexander Korotkov, Компания Postgres Professional) 13:55 - 14:50 [45+10] GPGPU Accelerates PostgreSQL, Unlock the power of multi-thousands cores（Kaigai Kohei, NEC） 11月21日，下午，主会场 15:15 - 15:45 [30] 【神秘嘉宾主题分享】 15:45 - 16:30 [45] 中国PG社区生态发展圆桌论坛与全球社区Leader现场交流 16:30 - 16:45 [15] Q &amp; A 16:45 - 17:00 [15] 谢幕及2016社区发展号召 报名链接http://postgres.cn/news/viewone/1/75","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"Postgres-XC：分片表两表关联性能测试","slug":"20151021085301","date":"2015-10-21T00:53:01.000Z","updated":"2018-09-04T01:34:18.721Z","comments":true,"path":"20151021085301.html","link":"","permalink":"https://postgres.fun/20151021085301.html","excerpt":"","text":"Postgres-XC 主要特性在于它的分片扩展功能，之前博客介绍过 Postgres-XC 的复制表和分片表模式，这篇博客选取了业务场景的一条两表关联 SQL， 分别测试在复制表模式和分片表模式下的性能。 测试环境硬件环境：3台虚拟机软件版本：Postgres-XC 1.2 PGXC 环境备注：两个协调节点，两个数据节点。 业务场景测试 SQL 如下：12345678select a.* from tbl_operate a join tbl_info b on a.applyid = b.idwhere a.syskey = 'BOSS'and a.operationid = '7'and a.targetid = 'zhuhua1'and (a.state in ('DataSaved', 'DataProc') or b.state in ('MainBillDeptAdminApprove', 'MainBillDeptApprove') or a.applyid = '8ace4a9e506c7af101508354dddd4d95'); 备注：此条 SQL 为业务场景中的一条 SQL，其中 tbl_operate 记录数 1534437， tbl_info 表记录数 1699246, 目前采用的是复制表模式，两张表的 id 为主键。 注意关联字段为a.applyid = b.id。 场景一：tbl_operate 和 tbl_info 都为复制表场景一的执行计划。123456QUERY PLAN ----------------------------------------------------------------------------------------------------------------------Data Node Scan on \"__REMOTE_FQS_QUERY__\" (cost=0.00..0.00 rows=0 width=0) (actual time=3.314..3.331 rows=3 loops=1)Node/s: datanode2Total runtime: 3.376 ms(3 rows) 备注：两张表都是复制表模式下，执行时间为 3.376 ms。 将两张表修改成分片表12alter table tbl_operate distribute by hash(id);alter table tbl_info distribute by hash(id); 备注： 此条命令会涉及到数据节点数据重分布，会锁表，命令执行过程中 coor 节点上先是有个 copy 进程，之后有个 REINDEX 进程，或许这是 Postgres-XC 修改表分片方式的内部过程。 场景二 两表都 hash(id) 分片12345678910111213--------------------------------------------------------------------------------------------------------------------------------------Hash Join (cost=0.01..0.07 rows=1 width=4670) (actual time=31.442..769.754 rows=3 loops=1)Hash Cond: ((b.id)::text = (a.applyid)::text)Join Filter: (((a.state)::text = ANY ('&#123;DataSaved,DataProc&#125;'::text[])) OR ((b.state)::text = ANY ('&#123;MainBillDeptAdminApprove,MainBillDeptApprove&#125;'::text[])) OR ((a.applyid)::text = '8ace4a9e506c7af101508354dddd4d95'::text))Rows Removed by Join Filter: 25-&gt; Data Node Scan on tbl_info \"_REMOTE_TABLE_QUERY_\" (cost=0.00..0.00 rows=1000 width=208) (actual time=0.573..484.255 rows=337473 loops=1)Node/s: datanode2, datanode3-&gt; Hash (cost=0.00..0.00 rows=1000 width=4670) (actual time=2.590..2.590 rows=28 loops=1)Buckets: 1024 Batches: 8 Memory Usage: 3kB-&gt; Data Node Scan on tbl_operate \"_REMOTE_TABLE_QUERY__1\" (cost=0.00..0.00 rows=1000 width=4670) (actual time=1.626..1.850 rows=28 loops=1)Node/s: datanode2, datanode3Total runtime: 776.020 ms(11 rows) 备注：将两张表都改成 HASH 分片后，执行时间需要 776.020 ms，效率降低 230 倍左右，执行计划也复杂得多。 场景三 tbl_operate: 复制表 , tbl_info: hash(id)123456 QUERY PLAN ----------------------------------------------------------------------------------------------------------------------Data Node Scan on \"__REMOTE_FQS_QUERY__\" (cost=0.00..0.00 rows=0 width=0) (actual time=3.303..5.061 rows=3 loops=1)Node/s: datanode2, datanode3Total runtime: 5.106 ms(3 rows) 备注：执行时间 5.106 ms。 场景四 tbl_operate: hash(id) , tbl_info: 复制表123456 QUERY PLAN ----------------------------------------------------------------------------------------------------------------------Data Node Scan on \"__REMOTE_FQS_QUERY__\" (cost=0.00..0.00 rows=0 width=0) (actual time=3.065..3.218 rows=3 loops=1)Node/s: datanode2, datanode3Total runtime: 3.263 ms(3 rows) 备注：执行时间 3.263 ms，之前的业务场景 SQL 关联字段有一个是非分区键，如果关联字段都是分片字段，情况如何呢？接着测试。 关联字段都为分片字段创建测试表1234567create table t1(id int4,name character varying(32),create_time timestamp(0) without time zone default clock_timestamp() ) distribute by hash(name);create unique index idx_t1_name on t1 using btree(name);insert into t1(id,name) select n,n||'_a' from generate_series(1,100000) n; create table t2 as select name from t1;create unique index idx_t2_name on t2 using btree(name);alter table t2 add column flag boolean default 't'; 分区键关联SQL123select t1.id,t1.create_time,t2.name,t2.flagfrom t1,t2where t1.name=t2.name and t1.name='1_a'; 备注： 关联字段 name 分别是 t1，t2 表的分片字段。 分片表执行计划123456789francs=&gt; explain analyze select t1.id,t1.create_time,t2.name,t2.flagfrom t1,t2where t1.name=t2.name and t1.name='2_a'; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------Data Node Scan on \"__REMOTE_FQS_QUERY__\" (cost=0.00..0.00 rows=0 width=0) (actual time=1.243..1.244 rows=1 loops=1)Node/s: datanode2, datanode3Total runtime: 1.293 ms(3 rows) 备注：执行时间 1.293 ms，根据执行计划可以看到扫描了两个数据节点。 修改成复制表12alter table t1 distribute by replication;alter table t2 distribute by replication; 复制表执行计划123456789francs=&gt; explain analyze select t1.id,t1.create_time,t2.name,t2.flagfrom t1,t2where t1.name=t2.name and t1.name='2_a'; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------Data Node Scan on \"__REMOTE_FQS_QUERY__\" (cost=0.00..0.00 rows=0 width=0) (actual time=0.909..0.910 rows=1 loops=1)Node/s: datanode2Total runtime: 0.941 ms(3 rows) 备注：执行时间 0.941 ms，公扫描 datanode2 节点，性能比分片情况稍降低。 总结Postgres-XC 环境下，两表关联的业务场景，如果关联字段正好是两表的分片字段，性能会比复制表稍降低，如果关联字段不是分片字段，性能会比复制表大辐度降低， 分片表的使用场景需谨慎。 参考 Postgres-XC : Data Replication or Distribution ? Postgres-XC：修改表的 Distribute 分布方式 High Availability in Postgres-XC","categories":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/categories/Postgres-XC/"}],"tags":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/tags/Postgres-XC/"}]},{"title":"2015 PostgreSQL走进高校暨“象行中国”武汉站活动","slug":"20151013111952","date":"2015-10-13T03:19:52.000Z","updated":"2018-09-04T01:34:18.643Z","comments":true,"path":"20151013111952.html","link":"","permalink":"https://postgres.fun/20151013111952.html","excerpt":"","text":"很高兴PostgreSQL能走进高校，这次活动是在武汉大学举行，活动链接：http://totemdb.whu.edu.cn/personal/ywpeng/pg2015/ 本次活动由PostgreSQL中国用户会和武汉大学珞珈图腾数据库实验室主办，旨在为高校数据库教学与科研人员、数据库方向学生以及PostgreSQL企业人士提供一个交流平台。借此次交流机会，希望一方面让高校数据库教学和科研人员了解PostgreSQL在实际应用中所发挥的作用以及企业对PostgreSQL及数据库研究的需求，更好地改进高校数据库的教学；另一方面让在校各类学生更加了解PostgreSQL及其应用，激发学习、研究PostgreSQL的兴趣，为社区增加更多活力。本次活动将邀请国内知名IT企业数据库人员、PostgreSQL社区代表、高校数据库教学与科研人员参加交流活动，话题精彩，期待各位朋友的参与。 活动信息时间： 2015年10月24日地点： 武汉大学计算机学院，湖北武汉珞珈山主办： PostgreSQL中国用户会、武汉大学珞珈图腾数据库实验室活动网页：http://totemdb.whu.edu.cn/ywpeng/pg2015/联系人：彭煜玮报名方式：ywpeng@whu.edu.cn（邮件），whupyw（微信），15623500651（手机） 活动议程10月24日签到：8:30 - 9:0009:00 - 09:30 欢迎辞 Welcome09:30 - 10:00 PostgreSQL@武汉大学10:00 - 10:45 主题演讲10:45 - 11:00 — Break —11:00 - 11:45 主题演讲11:45 - 12:00 照集体照12:00 - 14:00 休息14:00 - 17:00 与本地高校及CCF武汉分部交流 嘉宾彭智勇，武汉大学教授，CCF武汉分部主席李元佳，PostgreSQL中国用户会、华为周正中，阿里云唐成，沃趣科技朱贤文，成都文武信息技术有限公司田军，湖南上容信息技术有限公司彭煜玮，武汉大学 武汉大学珞珈图腾数据库实验室","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"Postgres-XC：修改表的 Distribute 分布方式","slug":"20150928143058","date":"2015-09-28T06:30:58.000Z","updated":"2018-09-04T01:34:18.580Z","comments":true,"path":"20150928143058.html","link":"","permalink":"https://postgres.fun/20150928143058.html","excerpt":"","text":"Postgres-XC 里的表可以分为复制表和分片表，之前博客有介绍过，Postgres-XC : Data Replication Or Distribution ? 那么表的位置属性是否可以更改呢？ 例如复制表是否可以修改成分片表？ 在测试之前先来回顾下复制表和分片表。 关于复制表和分片表创建复制表和分片表1234567891011复制表francs=&gt; create table test_rep(id1 int4, id2 int4, name text) distribute by replication;CREATE TABLE HASH 分片表francs=&gt; create table test_hash(id1 int4, id2 int4, name text) distribute by hash(id2);CREATE TABLE modulo 分片表francs=&gt; create table test_modulo(id1 int4, id2 int4, name text) distribute by modulo(id1);CREATE TABLE 备注：如何查看表是复制表和分片表呢？有两种方式。 方式一：\\d+ 元子命令12345678910francs=&gt; \\d+ test_repTable \"francs.test_rep\"Column | Type | Modifiers | Storage | Stats target | Description--------+---------+-----------+----------+--------------+-------------id1 | integer | | plain| |id2 | integer | | plain| |name | text| | extended | |Has OIDs: noDistribute By: REPLICATIONLocation Nodes: ALL DATANODES 备注：查看 Distribute By 属性，可以看到 test_rep 为复制表。 方式二：查看 pgxc_class 系统表备注：pgxc_class 系统表存储的是 pgxc 表的位置信息，为 Postgres-XC 新增，主要字段含义如下： pcrelid: pgxc： 表的 OID pclocatortype： 表的属性，R 表示复制表，H 表示 hash 分片，M 表示 modulo 分片 pcattnum： 表的分片字段位置 pchashalgorithm：是否使用 hash 分片算法 nodeoids： 数据结点列表 修改表的分片方式创建一张复制表 test_rep2 并插入测试数据123456789101112131415francs=&gt; create table test_rep2(id1 int4, id2 int4, name text) distribute by replication;CREATE TABLE francs=&gt; insert into test_rep2(id1,name) select n, n|| 'a' from generate_series(1,100) n;INSERT 0 100 francs=&gt; select pcrelid::regclass, * from pgxc_class where pcrelid::regclass ='test_rep2'::regclass; -[ RECORD 1 ]---+------------ pcrelid | test_rep2 pcrelid | 26561 pclocatortype | R pcattnum | 0 pchashalgorithm | 0 pchashbuckets | 0 nodeoids | 16388 16389 修改复制表 test_rep2 Distribute 属性123456789101112francs=&gt; alter table test_rep2 distribute by hash(id1);ALTER TABLE francs=&gt; select pcrelid::regclass, * from pgxc_class where pcrelid::regclass ='test_rep2'::regclass; -[ RECORD 1 ]---+------------ pcrelid | test_rep2 pcrelid | 26561 pclocatortype | H pcattnum | 1 pchashalgorithm | 1 pchashbuckets | 4096 nodeoids | 16388 16389 备注：可见 test_rep2 表成功地修改成了 hash 分片表，有一点需要注意，修改表的分片方式意味影着表的数据重新分布，如果是大表将非常耗时，且影响表上的 DML 操作。 参考 Postgres-XC : Data Replication or Distribution ? pgxc_class","categories":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/categories/Postgres-XC/"}],"tags":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/tags/Postgres-XC/"}]},{"title":"象行中国 2015 PostgreSQL 技术交流会 - 杭州站","slug":"20150923105729","date":"2015-09-23T02:57:29.000Z","updated":"2018-09-04T01:34:18.487Z","comments":true,"path":"20150923105729.html","link":"","permalink":"https://postgres.fun/20150923105729.html","excerpt":"","text":"象行中国 2015 PostgreSQL 技术交流会是由 PostgreSQL 全国用户会主办，在全国主要城市(北京、西安、上海、杭州、南京、长沙、广州、深圳、成都) 举行的开源数据库PostgreSQL线下技术交流活动, 杭州站的活动由杭州 PostgreSQL 本地用户会联合杭州魔品科技有限公司在杭州联合大厦举行，在这次活动中分享了主题为 “Keepalived + PostgreSQL 流复制实现高可用”的话题，和大家介绍了 PostgreSQL 高可用方案中比较重要的流复制模式：同步复制和异步复制。 同步流复制 异步流复制 PostgreSQL 本身不提供任何高可用方案，常见的高可用方案大部分是使用第三方插件结合 PostgreSQL 的流复制方案制定的，因此理解同步复制和异步复制的技术差异对于制定高可用方案有着至关重要的作用。同时，还介绍了高可用方案中故障转移的难点： 此次高可用方案的详细架构图： 杭州沃趣科技有限公司首席数据库架构师唐成和大家做了主题为“使用sysbench对PostgreSQL、MySQL、Oracle进行性能测试”的技术分享 阿里云 RDS PG 内核专家和大家介绍了“阿里云RDSPG的那些事”，主要和大家分享了在维护 RDS PG 的过程中的经验总结。 “PostgreSQL在莲子分析中的应用” 活动结束后，杭州站的 PGer 合影留念。 我分享的“Keepalived + PostgreSQL流复制实现高可用”议题已上传到百度云http://pan.baidu.com/s/1ntvi153","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL 9.4 Associate Certification 考试通过了","slug":"20150902150944","date":"2015-09-02T07:09:44.000Z","updated":"2018-09-04T01:34:18.424Z","comments":true,"path":"20150902150944.html","link":"","permalink":"https://postgres.fun/20150902150944.html","excerpt":"","text":"最近想考个 PostgreSQL 认证，找遍了所有资料发现只有 EDB 提供 PostgreSQL 认证， EDB 提供 PostgreSQLCertification 认证和 Postgres Plus Certification 认证，前者是 PostgreSQL 认证，后者是 EDB 认证， 我这次考的是 PostgreSQL 9.4 Associate Certification (简称 PAC )认证，花了 200 $，国内目前考这个证的人比较少，网上资料也少， 好在网名叫 “小桥河西” EnterpriseDB认证通关攻略 这篇文章介绍得比较详细， 几乎介绍了考试的方方面面，建议打算考 PostgreSQL 认证的朋友都先好好读读这篇文章。 PostgreSQL 9.4 Associate Certification 认证共 50 道题目，有一定难度，大部分单选题，也有多选题，考试时允许查资料，有些题目很明显的陷进，得看清题目。 值得一提的是，50 道题目须在 60 分钟内完成，时间有点紧，资料别查太久，另外 EDB 考试系统很慢，进入下一个页面感觉要 10 秒；70 %通过，好在最后以 86 分成绩过了， 啥也不说，上证书照片，下次有空时考个 PPC (PostgreSQL Professional Certification)。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：Synchronous_commit 参数性能测试","slug":"20150828163628","date":"2015-08-28T08:36:28.000Z","updated":"2018-09-04T01:34:18.361Z","comments":true,"path":"20150828163628.html","link":"","permalink":"https://postgres.fun/20150828163628.html","excerpt":"","text":"synchronous_commit 参数是 PostgreSQL WAL 日志文件类的重要参数，当设置成 on 时，表示事务提交需等待 WAL 刷到磁盘后才返回成功信息，这是最大限度数据保护模式；当设置成 off 时，事务提交不需等待 WAL 刷到磁盘就返回成功信息。而这个理解来看，设置成 on 会带来性能开销，那么这个开销大概有多少呢？ 一直想做这个测试，今天有空，测试下。 环境信息硬件：华为服务器 RH5885 V3PostgreSQL版本：9.5alpha2操作系统： SUSE Linux 测试模型创建测试表并插入一千万条数据，如下：12345drop table if exists test_sync; create table test_sync(id int8,name text,creat_time timestamp(0) without time zone default clock_timestamp()); insert into test_sync(id,name) select n,n||'_test' from generate_series(1,10000000) n; alter table test_sync add primary key(id); vacuum analyze test_sync; 测试场景 Synchronous_commit = off主键查询： select_1.sql1234[pg93@db2 load_test]$ cat select_1.sql setrandom v_id 1 10000000 select name from test_sync where id=:v_id; pgbench12pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f select_1.sql &gt; select_1.out 2&gt;&amp;1 &amp; pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f select_1.sql &gt; select_2.out 2&gt;&amp;1 &amp; pgbench 结果123456789101112131415161718192021222324pg95@db1:~/script/load_test&gt; tail -f select_1.out transaction type: Custom query scaling factor: 1 query mode: prepared number of clients: 64 number of threads: 1 duration: 120 s number of transactions actually processed: 1891643 latency average: 4.060 ms tps = 15741.393851 (including connections establishing) tps = 15761.220964 (excluding connections establishing) pg95@db1:~/script/load_test&gt; tail -f select_2.out transaction type: Custom query scaling factor: 1 query mode: prepared number of clients: 64 number of threads: 1 duration: 120 s number of transactions actually processed: 1867651 latency average: 4.112 ms tps = 15546.474928 (including connections establishing) tps = 15565.536436 (excluding connections establishing)备注：合计 TPS 为： 15741+15546=31287， 开了两个 pgbench 进程，机器性能依然没压到极限。 主键更新： updaet_1.sql1234[pg93@db2 load_test]$ cat update_1.sql setrandom v_id 1 10000000 update test_sync set name='off' where id=:v_id; pgbench12pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f update_1.sql &gt; update_1.out 2&gt;&amp;1 &amp; pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f update_1.sql &gt; update_2.out 2&gt;&amp;1 &amp; pgbench 结果:1234567891011121314151617181920212223pg95@db1:~/script/load_test&gt; tail -f update_1.out transaction type: Custom query scaling factor: 1 query mode: prepared number of clients: 64 number of threads: 1 duration: 120 s number of transactions actually processed: 533015 latency average: 14.409 ms tps = 4436.334568 (including connections establishing) tps = 4441.801624 (excluding connections establishing) pg95@db1:~/script/load_test&gt; tail -f update_2.out transaction type: Custom query scaling factor: 1 query mode: prepared number of clients: 64 number of threads: 1 duration: 120 s number of transactions actually processed: 538226 latency average: 14.269 ms tps = 4480.139660 (including connections establishing) tps = 4485.694991 (excluding connections establishing) 备注：合计 TPS 为： 4436+4480=8916 测试场景: Synchronous_commit = on为了测试准确，重新刷一次脚本12345drop table if exists test_sync; create table test_sync(id int8,name text,creat_time timestamp(0) without time zone default clock_timestamp()); insert into test_sync(id,name) select n,n||'_test' from generate_series(1,10000000) n; alter table test_sync add primary key(id); vacuum analyze test_sync; 主键查询： select_1.sql1234[pg93@db2 load_test]$ cat select_1.sql setrandom v_id 1 10000000 select name from test_sync where id=:v_id; pgbench12pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f select_1.sql &gt; select_1.out 2&gt;&amp;1 &amp; pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f select_1.sql &gt; select_2.out 2&gt;&amp;1 &amp; pgbench 结果1234567891011121314151617181920212223pg95@db1:~/script/load_test&gt; tail -f select_1.out transaction type: Custom query scaling factor: 1 query mode: prepared number of clients: 64 number of threads: 1 duration: 120 s number of transactions actually processed: 1907432 latency average: 4.026 ms tps = 15872.562421 (including connections establishing) tps = 15892.146509 (excluding connections establishing) pg95@db1:~/script/load_test&gt; tail -f select_2.out transaction type: Custom query scaling factor: 1 query mode: prepared number of clients: 64 number of threads: 1 duration: 120 s number of transactions actually processed: 1913928 latency average: 4.013 ms tps = 15926.439118 (including connections establishing) tps = 15946.082816 (excluding connections establishing) 备注：合计 TPS 为： 15872+15926=31798 主键更新： updaet_1.sql1234[pg93@db2 load_test]$ cat update_1.sql setrandom v_id 1 10000000 update test_sync set name='off' where id=:v_id; pgbench12pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f update_1.sql &gt; update_1.out 2&gt;&amp;1 &amp; pgbench -c 64 -T 120 -n -N -M prepared -d francs -U francs -f update_1.sql &gt; update_2.out 2&gt;&amp;1 &amp; pgbench 结果1234567891011121314151617181920212223pg95@db1:~/script/load_test&gt; tail -f update_1.out transaction type: Custom query scaling factor: 1 query mode: prepared number of clients: 64 number of threads: 1 duration: 120 s number of transactions actually processed: 337821 latency average: 22.734 ms tps = 2803.250510 (including connections establishing) tps = 2806.689839 (excluding connections establishing) pg95@db1:~/script/load_test&gt; tail -f update_2.out transaction type: Custom query scaling factor: 1 query mode: prepared number of clients: 64 number of threads: 1 duration: 120 s number of transactions actually processed: 340240 latency average: 22.572 ms tps = 2822.794480 (including connections establishing) tps = 2826.240282 (excluding connections establishing) 备注：合计 TPS 为： 2822+2826=5648 测试总结 测试场景 主键查询 主键更新 synchronous_commit = off 31287 8916 synchronous_commit = on 31798 5648 备注：从结果看出，SELECT 几乎不受 synchronous_commit 参数影响，而 UPDATE 在 synchronous_commit= on 的场景下要比 synchronous_commit = off 场景下的 TPS 低 36.7% 左右。 参考 Pgpool 流复制模式压力测试 Synchronous_commit","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：本博客访问情况","slug":"20150826155103","date":"2015-08-26T07:51:03.000Z","updated":"2018-09-04T01:34:17.799Z","comments":true,"path":"20150826155103.html","link":"","permalink":"https://postgres.fun/20150826155103.html","excerpt":"","text":"拉出了博客最近一周访问情况。 最近一周访问量备注：每天访问量在 1000 左右。 访问地域备注： 根据访问地域统计，海外最近一周访问量占比 32.2%, 依然是此博客的访问主要地域，在博客中我一直尽量少使用英文词汇，目的是主推国内用户，降低海外用户的访问比重。后面排名依次是北京，广东，浙江，这在一定程度上或许能反映 PostgreSQL 在各省的应用情况。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5 新特性汇总","slug":"20150822095620","date":"2015-08-22T01:56:20.000Z","updated":"2018-09-04T01:34:17.736Z","comments":true,"path":"20150822095620.html","link":"","permalink":"https://postgres.fun/20150822095620.html","excerpt":"","text":"PostgreSQL 9.5 正式版还没发行，但 Alpha 2 版已经发行， 主要特性包括 IMPORT FOREIGN SCHEMA，Row-Level Security Policies，BRIN 索引，JSONB 数据类型操作的增强，以及 UPSERT 和 pg_rewind 等，详细如下： PostgreSQL9.5 新特性 PostgreSQL9.5：Foreign Table Inheritance PostgreSQL9.5：Row-Level Security Policies PostgreSQL9.5：IMPORT FOREIGN SCHEMA PostgreSQL9.5：JSONB 数据类型: 支持元素修改，新增，删除 PostgreSQL9.5：INSERT ON CONFLICT UPDATE, otherwise known as “UPSERT” PostgreSQL9.5：pg_rewind 快速恢复备节点 PostgreSQL9.5：BRIN ( Block Range INdexes) 索引 PostgreSQL9.5: Allow CREATE/ALTER DATABASE to manipulate datistemplate and datallowconn PostgreSQL9.5：archive_mode = always 支持备节点接收主节点的归档文件 PostgreSQL9.5：新增参数设置神器：pg_file_settings 视图 PostgreSQL9.5：SQL 新增 TABLESAMPLE 数据取样功能 PostgreSQL9.5：Parallel VACUUMing PostgreSQL9.5：新增行级锁 SKIP LOCKED 属性 PostgreSQL9.5：ALTER TABLE .. SET LOGGED / UNLOGGED","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5：BRIN ( Block Range INdexes) 索引","slug":"20150821173411","date":"2015-08-21T09:34:11.000Z","updated":"2018-09-04T01:34:17.658Z","comments":true,"path":"20150821173411.html","link":"","permalink":"https://postgres.fun/20150821173411.html","excerpt":"","text":"BRIN ( Block Range INdexes) 是 9.5 新的索引类型，相比 btree 索引占用空间更少，号称是对于非常巨大的表能提高性能，目前没有服务器测试巨大的表，依然在虚拟机下简单测试下 BRIN 索引。 环境准备创建测试表,并插入数据1234567891011121314CREATE TABLE orders ( id int, order_date timestamptz, item text); INSERT INTO orders (order_date, item) SELECT x, 'dfiojdso' FROM generate_series('2015-07-01 00:00:00', '2015-08-01 00:00:00','2 seconds'::interval) a(x); fdb=&gt; select count(*) from orders; count --------- 1339201 (1 row) 备注：这张表才133 万记录，不算大表。 创建 BTREE 索引创建 BTREE 索引：花费 14 秒123fdb=&gt; create index idx_orders_time on orders using btree (order_date); CREATE INDEX Time: 14084.416 ms BTREE 索引大小: 29 MB12345fdb=&gt; \\di+ idx_orders_time List of relations Schema | Name | Type | Owner | Table | Size | Description --------+-----------------+-------+-------+--------+-------+------------- fdb | idx_orders_time | index | fdb | orders | 29 MB | Btree 索引扫描： 2.757 ms1234567891011121314fdb=&gt; explain analyze select count(*) from orders WHERE order_date BETWEEN '2015-07-11 00:00:00' and '2015-07-11 01:00:00'; QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------- Aggregate (cost=74.99..75.00 rows=1 width=0) (actual time=1.866..1.866 rows=1 loops=1) -&gt; Index Only Scan using idx_orders_time on orders (cost=0.43..70.49 rows=1803 width=0) (actual time=0.037..1.213 rows=1801 loops=1) Index Cond: ((order_date &gt;= '2015-07-11 00:00:00+08'::timestamp with time zone) AND (order_date &lt;= '2015-07-11 01:00:00+08' ::timestamp with time zone)) Heap Fetches: 1801 Planning time: 0.308 ms Execution time: 1.908 ms (6 rows) Time: 2.757 ms 创建 BRIN索引创建 BRIN 索引: 920 ms123fdb=&gt; create index brin_orders_time on orders using BRIN (order_date); CREATE INDEX Time: 920.145 ms BRIN 索引扫描： 7.816 ms ms123456789101112131415161718fdb=&gt; explain analyze select count(*) from orders WHERE order_date BETWEEN '2015-07-11 00:00:00' and '2015-07-11 01:00:00'; QUERY PLAN ------------------------------------------------------------------------------------------------------------- Aggregate (cost=4334.24..4334.25 rows=1 width=0) (actual time=6.948..6.949 rows=1 loops=1) -&gt; Bitmap Heap Scan on orders (cost=30.48..4329.74 rows=1803 width=0) (actual time=4.761..6.295 rows=1801 loops=1) Recheck Cond: ((order_date &gt;= '2015-07-11 00:00:00+08'::timestamp with time zone) AND (order_date &lt;= '2015-07-11 01:00:00+0 8'::timestamp with time zone)) Rows Removed by Index Recheck: 19959 Heap Blocks: lossy=128 -&gt; Bitmap Index Scan on idx_orders_time (cost=0.00..30.03 rows=1803 width=0) (actual time=0.090..0.090 rows=1280 loops=1) Index Cond: ((order_date &gt;= '2015-07-11 00:00:00+08'::timestamp with time zone) AND (order_date &lt;= '2015-07-11 01:00: 00+08'::timestamp with time zone)) Planning time: 0.210 ms Execution time: 7.010 ms (9 rows) Time: 7.816 ms BRIN 索引大小: 48 kB fdb=&gt; \\di+ brin_orders_time List of relations Schema | Name | Type | Owner | Table | Size | Description --------+-----------------+-------+-------+--------+-------+------------- fdb | brin_orders_time | index | fdb | orders | 48 kB | (1 row) BTREE、BRIN 索引测试统计以下是在 orders 表，表记录数1339201，做的 Btree 和 BRIN 索引测试统计： 索引类型 索引创建时间 索引大小 时间范围扫描 Btree 14084 ms 29 MB 2.757 ms BRIN 920 ms 48 kB 7.816 ms 备注：从这项测试来看， BRIN 索引比 Btree 索引小，创建时间也短，但在查询效率上没有 Btree 索引效率高，有条件建议在大表上再测试 BRIN 性能。 参考 Waiting for 9.5 – BRIN: Block Range Indexes. BRIN INDEX","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"象行中国2015 Let's Postgres","slug":"20150819105036","date":"2015-08-19T02:50:36.000Z","updated":"2018-09-04T01:34:17.596Z","comments":true,"path":"20150819105036.html","link":"","permalink":"https://postgres.fun/20150819105036.html","excerpt":"","text":"本次活动是由Postgres中国用户会主办的，在全国主要城市(北京、西安、上海、杭州、南京、长沙、广州、深圳、成都)举行的Postgres线下技 术交流活动。各城市都有精彩的Postgres话题分享，议题包括 PostGIS, 性能优化，PostgreSQL-XC,PostgreSQL-X2, Greenplum，高可用，压力测试，阿里云 RDS FOR PG， PG 分布式方案探讨，PG 逻辑复制等，精彩不容错过。 活动主题： Postgres的技术分享及交流 时间： 2015年9月 地点： 北京、西安、上海、杭州、南京、长沙、广州、深圳、成都 主办： Postgres中国用户会 会议官方网页：http://letspostgres.eventdove.com/ 负责人：赵振平(Postgres用户会全国活动负责人) 18911070380 报名方式：本页面 或者 开源中国 媒体赞助： 开源中国 oschina.net 视频直播赞助: 微吼 征招赞助商：如有意赞助当地活动礼品，请联系会议负责人 用户会信息 官方微信 PostgresChina QQ群_100910388 _官网 postgres.cn小广告： 诚招UI设计志愿者帮用户会网站改版，有意请联系会议负责人 北京站 时间：2015年9月12日（星期六）9:20 - 17:00 地点：北京市海淀区科学院南路2号融科资讯中心C座南楼17层 赞助商：Pivotal 容纳人数: 70-80 费用： 免费 直播视频：http://e.vhall.com/webinar/watch/331177325 负责人： 萧少聪 （北京分会长） 联系方式：18600036141 议程： 09:20-09:50registration(签到)09:55-10:00Opening(Ray)10:00-10:30PostgreSQL前世今生2015(萧少聪)10:30-11:15OpenStreetMap,ArcGIS,Postgre(AndreasScherbaum)11:15-12:00HAWQ-HadoopSQLEngine(Lei)13:30-14:15PostGIS互联网+地图(田生军)14:15-15:00NOSQL(NoOnlySQL)inPostgreSQL(萧少聪)15:00-15:30Break15:30-16:15GPDBGeospatial(刘奎恩)16:15-17:00OpeningGreemplum(YanDong)17:00Takepicture 嘉宾简介 Andreas Scherbaum: Advisory Consultant at EMC Deutschland GmbH. EMC Deutschland GmbH,PostgreSQL Project, PostgreSQL User Group Europe 刘奎恩: Principle Software Engineer at Pivotal Software, Inc. Lei Chang: Director of Engineering, R&amp;D, at Pivotal Software, Inc. Yan Dong: Director of Engineering, R&amp;D, at Pivotal Software, Inc. 田生军: 8年GIS行业从业经验，专注于空间数据渲染和数据管理。2013年开始在百度全景任高级研发工程师，负责百度全景数据库的运维和开发。 萧少聪（Postgres中国用户会）:Postgres中国用户会发起人之一，长期活跃于国内各开源社区，阿里云RDS for PostgreSQL&amp;PPAS产品专家 西安站 时间:2015年9月12日（星期六）13:00 - 17:00 地点:西安市高新区昆明池路金辉悦府 赞助商:金辉悦府 费用:免费 直播视频：http://e.vhall.com/webinar/watch/916884662 负责人: 符强 联系方式：15891768426 议程: 13:30-14:00自我介绍14:00-14:30Postgres中国用户会情况介绍(符强)14:30-15:15Postgres性能调优经验分享(肖玲峰)15:15-15:30休息15:30-16:15数据库与大数据(韩峰) 嘉宾简介 符强 肖玲峰 韩峰 杭州站 时间:2015年9月19日（星期六）13:00 - 17:45 地点:杭州市西湖区天目山路335号福地创业园5楼-福云咖啡 费用:场地费AA，70左右 直播视频： http://e.vhall.com/webinar/watch/497934102 负责人: 周正中 （杭州分会长&amp;用户会CTO） 联系方式：15397136813（digoal） digoal@126.com 议程: 13:00-13:30签到13:30-14:00PostgreSQL在莲子分析中的应用（aida）14:15-14:45阿里云RDSPG的那些事（康贤/明虚/执白/义从/digoal）15:00-15:30PG-XC触发器实现(潘志铭)15:45-16:15Keepalived+PostgreSQL流复制方式实现高可用(谭峰)16:30-17:00使用sysbench对PostgreSQL、MySQL、Oracle进行性能测试(唐成)17:15-17:45自由交流晚宴，AA 嘉宾简介 周正中：多年数据库平台工作经验，熟悉Oracle、PostgreSQL、EnterpriseDB、GreenPlum、mongoDB等多种数据库平台，熟悉OS、存储等，热衷开源以及分享。 谭峰： PostgreSQL 中文社区版主， 译者之一，热衷于博客分享技术心得，致力于 PostgreSQL 中文技术推广，曾任杭州斯凯网络科技有限公司 PostgreSQL DBA 一职，主要负责公司魔品助手，安卓支付，冒泡社区，冒泡市场等业务线数据库维护，超过 5 年的 PostgreSQL 数据库运维经验，现就职于中国移动通信集团浙江有限公司。 唐成：杭州沃趣科技有限公司首席数据库架构师，《PostgreSQL修炼之道：从小工到专 家》书作者，活跃于PostgreSQL中国社区，历任阿里巴巴Oracle高级数据库专家，曾为阿里巴巴的PostgrSQL数据库的布道者，曾任网易 杭州研究院开发专家，主导了网易云计算中的云硬盘产品（类似amazon EBS）的设计和开发。目前对数据库技术、对分布式存储及云计算中虚拟化技术很感兴趣。 上海站 时间: 2015年9月12日（星期六）13:00 - 17:45 地点: 上海市徐汇区龙吴路195号天华信息软件园4号楼二楼（地铁三号线漕溪路站） 直播视频：http://e.vhall.com/webinar/watch/927060559 费用: 免费 负责人: 胡怡文（上海分会会长） 联系方式：18958088285 议程: 13:00开始签到13:30-14:15Postgres与数据库高可用（胡怡文）14:15-15:00Postgres与ERP(openERP)（马成龙）15:15-16:00Postgres的回归测试(周正中)16:00-16:45Greenplum数据库技术介绍(王伟","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL9.5：Allow CREATE/ALTER DATABASE to manipulate datistemplate and datallowconn","slug":"20150816151847","date":"2015-08-16T07:18:47.000Z","updated":"2018-09-04T01:34:17.533Z","comments":true,"path":"20150816151847.html","link":"","permalink":"https://postgres.fun/20150816151847.html","excerpt":"","text":"pg_database 系统表展现了数据库信息，例如数据库名称，字符集，是否是模板库(datistemplate),是否允许连接(datallowconn)等信息。系统表在绝大多数情况下是不建议修改的，因为万一带来不可控的影响就悲剧了。 datistemplate 和 datallowconn 属性在之前版不支持数据库命令操作，如果要修改只能通过更新 pg_database 系统表的形式，9.5 版本则支持 ALTER DATABASE 命令设置这两个属性，下面演示下。 pg_database 系统表12345678910111213141516171819202122232425[pg95@db2 ~]$ psql psql (9.5alpha1) Type \"help\" for help. postgres=# \\d pg_database Table \"pg_catalog.pg_database\" Column | Type | Modifiers ---------------+-----------+----------- datname | name | not null datdba | oid | not null encoding | integer | not null datcollate | name | not null datctype | name | not null datistemplate | boolean | not null datallowconn | boolean | not null datconnlimit | integer | not null datlastsysoid | oid | not null datfrozenxid | xid | not null datminmxid | xid | not null dattablespace | oid | not null datacl | aclitem[] | Indexes: \"pg_database_datname_index\" UNIQUE, btree (datname), tablespace \"pg_global\" \"pg_database_oid_index\" UNIQUE, btree (oid), tablespace \"pg_global\" Tablespace: \"pg_global\" 查看数据库 datistemplate, datallowconn 属性12345678postgres=# select datname,encoding,datistemplate,datallowconn from pg_database; datname | encoding | datistemplate | datallowconn -----------+----------+---------------+-------------- template1 | 6 | t | t template0 | 6 | t | f postgres | 6 | f | t fdb | 6 | f | t (4 rows) 设置数据库 datistemplate 属性修改 fdb 数据库 datistemplate 属性，将它设置成 t1234567891011postgres=# alter database fdb IS_TEMPLATE true; ALTER DATABASE postgres=# select datname,encoding,datistemplate,datallowconn from pg_database; datname | encoding | datistemplate | datallowconn -----------+----------+---------------+-------------- template1 | 6 | t | t template0 | 6 | t | f postgres | 6 | f | t fdb | 6 | t | t (4 rows) 备注： fdb 库的 datistemplate 值变成 t 了。 设置数据库 datallowconn 属性修改 fdb 数据库 datallowconn 属性，将它设置成 f1234567891011postgres=# alter database fdb ALLOW_CONNECTIONS false; ALTER DATABASE postgres=# select datname,encoding,datistemplate,datallowconn from pg_database; datname | encoding | datistemplate | datallowconn -----------+----------+---------------+-------------- template1 | 6 | t | t template0 | 6 | t | f postgres | 6 | f | t fdb | 6 | t | f (4 rows) 备注： fdb 库的 datallowconn 值变成 f 了。 另开一会话：尝试连接 fdb 库12[pg95@db2 ~]$ psql fdb fdb psql: FATAL:database \"fdb\" is not currently accepting connections 备注：报错，不允许连接 fdb 库；这个特性在数据库某些维护场景下特别有用。之前的方法一般是通过更改防火墙或数据库的 pg_hba.conf 策略来达到这个目标。 参考 ALTER DATABASE pg_database","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5：archive_mode = always 支持备节点接收主节点的归档文件","slug":"20150812152632","date":"2015-08-12T07:26:32.000Z","updated":"2018-09-04T01:34:17.470Z","comments":true,"path":"20150812152632.html","link":"","permalink":"https://postgres.fun/20150812152632.html","excerpt":"","text":"Add new archive_mode value always to allow standbys to always archive received WAL files PostgreSQL 流复制架构中备节点不支持接收来自主节点的 WAL 日志文件，如果备节点要取归档文件时，一般有两种解决方案，第一种是主节点将 WAL 文件归档到共享存储上(NFS 也可以)，主，备节点都能读取归档的日志文件; 另一种是将主节点已归档的日志文件 copy 到备节点上。 9.5 版本新增 archive_mode = always，允许备库接收主库的 WAL 日志文件。之前版本 archive_mode 参数仅允许 on 或 off，接着做个小实验: 环境准备主节点 192.168.2.38/1931 主机名 db2 归档目录 /archive/pg95/主节点 192.168.2.37/1931 主机名 db1 归档目录 /archive/pg95/ insert.sh 脚本1234567#!/bin/bash while true do psql -c \"insert into test_pitr(create_time) values (now());\" sleep 1 done 备注：写个脚本定时插入数据，让数据库处于写的状态。 执行脚本1[pg95@db2 tf]$./insert.sh &gt; /dev/null 2&gt;&amp;1 &amp; 配置归档参数修改备节点 archive_mode 参数：db1 上执行12[pg95@db1 pg95]$ grep \"archive_mode =\" $PGDATA/postgresql.conf archive_mode = always # enables archiving; off, on, or always 删除归档文件: db1 上执行12[pg95@db1 pg_root]$ cd /archive/pg95 [pg95@db1 pg95]$ rm -rf * 查看归档命令: db2 上执行12345[pg95@db2 tf]$ psql -c \"show archive_command\" archive_command ------------------------ cp %p /archive/pg95/%f (1 row) 切换 WAL: db2 上执行12345postgres=# select pg_switch_xlog(); pg_switch_xlog ---------------- 1/2F147068 (1 row) 查看归档情况查看归档日志, db2 上执行12[pg95@db2 pg95]$ ls -alrt | tail -n 1 -rw------- 1 pg95 pg95 16777216 Aug 12 14:21 00000006000000010000002F 查看归档日志, db1 上执行123[pg95@db1 pg95]$ ll total 16M -rw------- 1 pg95 pg95 16M Aug 12 14:21 00000006000000010000002F 备注：可以看出备节点归档目录新增加的 WAL 文件和主节点归档目录产生的 WAL 一样。 附 archive_mode (enum) When archive_mode is enabled, completed WAL segments are sent to archive storage by setting archive_command. In addition to off, to disable, there are two modes: on, and always. During normal operation, there is no difference between the two modes, but when set to always the WAL archiver is enabled also during archive recovery or standby mode. In always mode, all files restored from the archive or streamed with streaming replication will be archived (again). See Section 25.2.9 for details. archive_mode and archive_command are separate variables so that archive_command can be changed without leaving archiving mode. This parameter can only be set at server start. archive_mode cannot be enabled when wal_level is set to minimal. 参考 Postgres 9.5 feature highlight: archive_mode = always Continuous archiving in standby PostgreSQL：关于 archive_command 归档命令 PostgreSQL：“ FATAL: requested WAL segment 0000000800002A0000000000 has already been removed”","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5：新增参数设置神器：pg_file_settings 视图","slug":"20150811115245","date":"2015-08-11T03:52:45.000Z","updated":"2018-09-04T01:34:17.408Z","comments":true,"path":"20150811115245.html","link":"","permalink":"https://postgres.fun/20150811115245.html","excerpt":"","text":"9.5 版本新增 pg_file_settings 视图，可谓参数设置的神器，为啥这么说呢？ 因为 postgresql.conf 参数值调整后，有些 reload 后就生效了，有些需要重启服务才生效，如果你设置的参数值是非法的， pg_ctl reload 命令也不报错，这时很让人尴尬，reload 后还得连到数据库里去 show 参数值，确认参数值是否生效，9.5 版本新增 pg_file_settings 视图，让这项工作容易很多。 关于 pg_file_settings 视图pg_file_settings 视图12345678910111213141516171819postgres=# \\d pg_file_settings View \"pg_catalog.pg_file_settings\" Column | Type | Modifiers ------------+---------+----------- sourcefile | text | sourceline | integer | seqno | integer | name | text | setting | text | applied | boolean | error | text | postgres=# select * from pg_file_settings limit 3; sourcefile | sourceline | seqno |name | setting | applied | error ----------------------------------------+------------+-------+------------------+---------+---------+------- /database/pg95/pg_root/postgresql.conf | 59 | 1 | listen_addresses | * | t | /database/pg95/pg_root/postgresql.conf | 63 | 2 | port | 1931 | t | /database/pg95/pg_root/postgresql.conf | 64 | 3 | max_connections | 100 | t | (3 rows) sourcefile: 配置文件名称 sourceline：配置参数位于配置文件的行数 seqno： 配置参数的序列号 name: 参数名称 setting： 参数当前设置值 applied： 参数设置成功与否标志，设置成功为 t error: 如果非空，表示此参数被 applied 时的报错信息 接下来做两项测试： 测试一: 设置 log_statement 参数成非法值; 测试二: 设置需要重启服务才生效的参数。 测试一: 设置 log_statement 参数成非法值设置 log_statement 参数成非法值12[pg95@db2 pg_root]$ grep \"log_statement =\" postgresql.conf log_statement = 'ddd' # none, ddl, mod, all reload 配置12[pg95@db2 pg_root]$ pg_ctl reload server signaled 备注： postgresql.conf 参数调整后，不管设置成功与否, pg_ctl reload 参数是不输出相关信息的，9.5 版本之前要确认参数是否成功有两种方法，一种是查看相关 pg_log , 例如 查看报错日志123[pg95@db2 pg_log]$ grep \"log_statement\" postgresql-2015-08-11_000000.csv 2015-08-11 10:36:36.177 CST,,,31634,,55c83dc1.7b92,9,,2015-08-10 13:59:29 CST,,0,LOG,22023,\"invalid value for parameter \"\"log_statement\"\": \"\"ddd\"\"\",,\"Available values: none, ddl, mod, all.\",,,,,,,\"\" 2015-08-11 10:43:25.063 CST,,,31634,,55c83dc1.7b92,12,,2015-08-10 13:59:29 CST,,0,LOG,22023,\"invalid value for parameter \"\"log_statement\"\": \"\"ddd\"\"\",,\"Available values: none, ddl, mod, all.\",,,,,,,\"\" 另一种方式是连接到数据库中查看12345[pg95@db2 pg_log]$ psql -c \"show log_statement\" log_statement --------------- none (1 row) 备注：可见 log_statement 参数设置无效。 9.5 版本之后，可以通过 pg_file_settings 视图查看。 查看 pg_file_settings 中的错误信息123456789postgres=# select * from pg_file_settings where error is not null; -[ RECORD 1 ]-------------------------------------- sourcefile | /database/pg95/pg_root/postgresql.conf sourceline | 441 seqno | 32 name | log_statement setting | ddd applied | f error | setting could not be applied 备注：错误信息简单明了。 测试二: 设置需要重启服务才生效的参数修改 max_connections 参数12[pg95@db2 pg_root]$ grep \"max_connections =\" postgresql.conf max_connections = 100 # (change requires restart) 备注: max_connections 参数默认为 100，这个参数设置后需要重启才生效，我们把它设置成 200。 12345[pg95@db2 pg_root]$ grep \"max_connections =\" postgresql.conf max_connections = 200 # (change requires restart) [pg95@db2 pg_root]$ pg_ctl reload server signaled 数据库查看123456789postgres=# select * from pg_file_settings where name='max_connections'; -[ RECORD 1 ]-------------------------------------- sourcefile | /database/pg95/pg_root/postgresql.conf sourceline | 64 seqno | 3 name | max_connections setting | 200 applied | f error | setting could not be applied 备注：max_connections 参数的 ERROR 信息很明显了，新增这个视图将让参数设置这项工作变得容易 。 参考 pg_file_settings","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5：SQL 新增 TABLESAMPLE 数据取样功能","slug":"20150809183622","date":"2015-08-09T10:36:22.000Z","updated":"2018-09-04T01:34:17.330Z","comments":true,"path":"20150809183622.html","link":"","permalink":"https://postgres.fun/20150809183622.html","excerpt":"","text":"查看文档中发现 9.5 版本新增数据取样功能，支持查询返回取样数据，这个功能吸引了我的注意，数据取样在很多应用场景都有用到。 TABLESAMPLE 取样方式TABLESAMPLE 取样方式有两种： SYSTEM: 数据块级的数据取样，后面接取样参数，数据抽取返回以数据块为单位，理论上表上每个数据块被取样的机率是一样的。 BERNOULLI: BERNOULLI 取样方式会扫描整张表，后面接取样参数，并且返回指定百分比抽样数据，理论上每条数据被取样的机率是一样的。 环境准备创建测试表并插入 100 万数据12345678910111213141516[pg95@db2 tf]$ psql fdb fdb psql (9.5alpha1) Type \"help\" for help. fdb=&gt; create table test_sample(id int4,message text, create_time timestamp(6) without time zone default clock_timestamp()); CREATE TABLE fdb=&gt; insert into test_sample(id,message) select n, md5(random()::text) from generate_series(1,1000000) n; INSERT 0 1000000 fdb=&gt; select * from test_sample limit 3; id |message | create_time ----+----------------------------------+---------------------------- 1 | 049559e35471b27b713c66cacfc518ac | 2015-08-09 17:51:52.732066 2 | 3dba81d93b91769071fd2bff5b9d6373 | 2015-08-09 17:51:52.732432 3 | 38746c72d7d5ce80ed0c16311c93ee77 | 2015-08-09 17:51:52.732449 SYSTEM 取样: 取样因子 0.011234567fdb=&gt; explain analyze select * from test_sample TABLESAMPLE SYSTEM(0.01); QUERY PLAN ---------------------------------------------------------------------------------------------------------------------- Sample Scan (system) on test_sample (cost=0.00..1.00 rows=100 width=45) (actual time=0.101..0.180 rows=107 loops=1) Planning time: 0.100 ms Execution time: 0.263 ms (3 rows) 备注：为什么返回是 107 条记录呢，往下看。 查看表占用 page12345678910fdb=&gt; select relname,relpages from pg_class where relname='test_sample'; relname | relpages -------------+---------- test_sample | 9346 fdb=&gt; select ceil(1000000/9346::numeric); ceil ------ 107 (1 row) 备注: test_sample 表占用 9346 个数据块，每个数据块数据为 107 条。 查看表 ctid,123456789fdb=&gt; select ctid, * from test_sample TABLESAMPLE SYSTEM(0.01); ctid |id |message | create_time ------------+--------+----------------------------------+---------------------------- (8887,1) | 950910 | e36fe0340ca717af13e50b9cef83441c | 2015-08-09 17:52:37.724168 (8887,2) | 950911 | 0fe2c556544556f9c89c6a51dc2b96eb | 2015-08-09 17:52:37.724281 ... 省略输出 (8887,107) | 951016 | 9ab7e88b5a2199f203f3668095d50d1d | 2015-08-09 17:52:37.725332 (107 rows) 备注：从上看出返回的数据都位于数据块 8887 中。 SYSTEM 方式不可以返回少于一个块的数据123456fdb=&gt; explain analyze select * from test_sample TABLESAMPLE SYSTEM(0.0001); QUERY PLAN -------------------------------------------------------------------------------------------------------------------- Sample Scan (system) on test_sample (cost=0.00..0.01 rows=1 width=45) (actual time=0.466..0.542 rows=107 loops=1) Planning time: 0.868 ms Execution time: 0.682 ms 备注：取样因子设置成 0.0001,依然返回了一个 page 的数据，接着看 bernoulli 取样方式。 system 方式随机返回不同 page 的数据1234567891011121314151617fdb=&gt; select ctid, * from test_sample TABLESAMPLE system (0.01) order by id desc limit 1 ; ctid |id |message | create_time ------------+--------+----------------------------------+---------------------------- (2939,107) | 314580 | 6469df3a7ec83782b61e5c125862e02c | 2015-08-09 17:52:00.637544 (1 row) fdb=&gt; select ctid, * from test_sample TABLESAMPLE system (0.01) order by id desc limit 1 ; ctid |id |message | create_time ------------+--------+----------------------------------+--------------------------- (5587,107) | 597916 | 41a2769073b791cdbc7c972d6efdd9dc | 2015-08-09 17:52:10.77434 (1 row) fdb=&gt; select ctid, * from test_sample TABLESAMPLE system (0.01) order by id desc limit 1 ; ctid |id |message | create_time -----------+-------+----------------------------------+---------------------------- (123,107) | 13268 | ba85b1b26ba94d9a686f00637dfa1879 | 2015-08-09 17:51:52.957365 (1 row) 备注：根据 ctid 看出返回的 page 是随机的。 BERNOULLI 取样: 取样因子 0.0112345678910111213141516171819fdb=&gt; explain analyze select * from test_sample TABLESAMPLE bernoulli(0.01) ; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------ Sample Scan (bernoulli) on test_sample (cost=0.00..9347.00 rows=100 width=45) (actual time=4.658..340.127 rows=106 loops=1) Planning time: 0.092 ms Execution time: 340.409 ms (3 rows) fdb=&gt; select ctid, * from test_sample TABLESAMPLE bernoulli(0.01) ; ctid |id |message | create_time ------------+--------+----------------------------------+---------------------------- (89,73) | 9596 | 3389376ab71d7e89642953508e1791a1 | 2015-08-09 17:51:52.890982 (91,27) | 9764 | 5f23f8feb3cdc2c87856b17a7ad4db09 | 2015-08-09 17:51:52.89356 (177,49) | 18988 | 8d58679ada864ca245d1b3fe90afbdc6 | 2015-08-09 17:51:53.049145 ... 省略输出 (9152,14) | 979278 | 01b9d63735955da6432af360376daa6a | 2015-08-09 17:52:39.444338 (9227,94) | 987383 | 73ca23c5950d85a615ce8dcd300e2930 | 2015-08-09 17:52:40.680049 (98 rows) 备注: bernoulli 取样方式执行时间要 340 ms 左右，速度比 system 方式慢了很多，从 ctid 可看出返回的数据位于不同的数据块。 关注 bernoulli 的取样因子123456789101112131415fdb=&gt; explain analyze select ctid, * from test_sample TABLESAMPLE bernoulli(1); QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------- Sample Scan (bernoulli) on test_sample (cost=0.00..9446.00 rows=10000 width=51) (actual time=0.055..359.777 rows=10047 loops=1) Planning time: 0.137 ms Execution time: 367.271 ms (3 rows) fdb=&gt; explain analyze select ctid, * from test_sample TABLESAMPLE bernoulli(2); QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------- Sample Scan (bernoulli) on test_sample (cost=0.00..9546.00 rows=20000 width=51) (actual time=0.033..325.595 rows=19863 loops=1) Planning time: 0.071 ms Execution time: 336.098 ms (3 rows) 备注：bernoulliy 方式取样因子为 1 时返回约 10047 条数据，取样因子为 2 时返回约为 19863 条数据，取样因子正好是数据的百分比。 参考 SELECT Waiting for 9.5 – TABLESAMPLE, SQL Standard and extensible","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5：Parallel VACUUMing","slug":"20150809121428","date":"2015-08-09T04:14:28.000Z","updated":"2018-09-04T01:34:17.267Z","comments":true,"path":"20150809121428.html","link":"","permalink":"https://postgres.fun/20150809121428.html","excerpt":"","text":"9.5 版本的 vacuumdb 支持并行功能，类似 pg_dump 和 pg_restore 的 -j 参数，做整库 vacuum 时能提高速度，同时资源消耗也会大些，当然之前版也可以通过其它方法实现并行 vacuum 功能，例如开启多个 vacuum 脚本同时 vacuum 不同的表。本文简单演示下 vacuumdb 并行。 并行 Vaccuum1vacuumdb -j4 dbname 环境准备创建一批测试表1234for a in &#123;10..19&#125; do psql fdb fdb -c \"create table test_$a as select n ,random() from generate_series(1, 1000000) n;\" done 查看创建的表12345678910111213141516171819[pg95@db2 tf]$ psql fdb fdb psql (9.5alpha1) Type \"help\" for help. fdb=&gt; \\dt+ test_* List of relations Schema | Name | Type | Owner | Size | Description --------+-------------+-------+-------+-------+------------- fdb | test_10 | table | fdb | 19 MB | fdb | test_11 | table | fdb | 19 MB | fdb | test_12 | table | fdb | 19 MB | fdb | test_13 | table | fdb | 19 MB | fdb | test_14 | table | fdb | 19 MB | fdb | test_15 | table | fdb | 19 MB | fdb | test_16 | table | fdb | 19 MB | fdb | test_17 | table | fdb | 19 MB | fdb | test_18 | table | fdb | 19 MB | fdb | test_19 | table | fdb | 19 MB | (10 rows) 开启并行 Vacuumdb12345678910111213141516[pg95@db2 tf]$ vacuumdb -e -j4 fdb vacuumdb: vacuuming database \"fdb\" SELECT c.relname, ns.nspname FROM pg_class c, pg_namespace ns WHERE relkind IN ('r', 'm') AND c.relnamespace = ns.oid ORDER BY c.relpages DESC; VACUUM fdb.test_10; VACUUM fdb.test_11; VACUUM fdb.test_12; VACUUM fdb.test_13; VACUUM fdb.test_14; VACUUM fdb.test_15; VACUUM fdb.test_16; VACUUM fdb.test_17; VACUUM fdb.test_18; VACUUM fdb.test_19; ... 备注：开启 4 个并行 vacuum 进程，输出中有一段 SQL，取出的表列表根据表大小排序。 查看 Vacuum 进程12345[pg95@db2 pg95]$ ps -ef | grep VACUUM pg95 25144 17848 2 11:58 ? 00:00:00 postgres: postgres fdb [local] VACUUM pg95 25145 17848 1 11:58 ? 00:00:00 postgres: postgres fdb [local] VACUUM pg95 25146 17848 2 11:58 ? 00:00:00 postgres: postgres fdb [local] VACUUM pg95 25147 17848 2 11:58 ? 00:00:00 postgres: postgres fdb [local] VACUUM 查看 Vacuum 会话12345678[pg95@db2 pg95]$ psql fdb -c \"select pid, datname,usename,query_start,client_addr,query ,waiting from pg_stat_activity where state='active' and pid &lt;&gt; pg_backend_pid() order by pid;\" pid | datname | usename |query_start | client_addr |query | waiting -------+---------+----------+-------------------------------+-------------+---------------------+--------- 25144 | fdb | postgres | 2015-08-09 11:58:05.160335+08 | | VACUUM fdb.test_12; | f 25145 | fdb | postgres | 2015-08-09 11:58:34.790037+08 | | VACUUM fdb.test_14; | f 25146 | fdb | postgres | 2015-08-09 11:58:34.790115+08 | | VACUUM fdb.test_16; | f 25147 | fdb | postgres | 2015-08-09 11:58:51.504252+08 | | VACUUM fdb.test_19; | f (4 rows) 参考 Vacuumdb Waiting for 9.5 – vacuumdb: enable parallel mode PostgreSQL9.3Beta1：pg_dump 新增并行参数 (Parallel pg_dump)","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：Pgcli 客户端工具使用","slug":"20150809091945","date":"2015-08-09T01:19:45.000Z","updated":"2018-09-04T01:34:17.205Z","comments":true,"path":"20150809091945.html","link":"","permalink":"https://postgres.fun/20150809091945.html","excerpt":"","text":"pgcli 是一个连接 PostgreSQL 的命令行客户端工作，提供自动补全和语法高亮功能。项目地址：https://github.com/dbcli/pgcli 环境信息操作系统： RHEL 6.2PostgreSQL: 9.5alpha1 安装 Pgcli下载1[root@db1 soft_bak]# git clone https://github.com/dbcli/pgcli 安装 python-pip 模块1[root@db1 ~]# yum install python-pip 安装 python-dev 模块1[root@db1 ~]# yum install python-devel 安装 pgcli123[root@db1 pgcli]# cd /opt/soft_bak/pgcli/ [root@db1 pgcli]# source /home/pg95/.bash_profile [root@db1 pgcli]# pip install pgcli 备注：没报错即安装成功。 测试 Pgcli使用 pgcli 报错，如下：1234567891011[pg95@db1 ~]$ pgcli fdb fdb Traceback (most recent call last): File \"/usr/bin/pgcli\", line 5, in &lt;module&gt; from pkg_resources import load_entry_point File \"/usr/lib/python2.6/site-packages/pkg_resources.py\", line 2655, in &lt;module&gt; working_set.require(__requires__) File \"/usr/lib/python2.6/site-packages/pkg_resources.py\", line 648, in require needed = self.resolve(parse_requirements(requirements)) File \"/usr/lib/python2.6/site-packages/pkg_resources.py\", line 546, in resolve raise DistributionNotFound(req) pkg_resources.DistributionNotFound: prompt-toolkit==0.45 解决方法：安装 setuptools12[root@db1 soft_bak]# wget https://bitbucket.org/pypa/setuptools/raw/bootstrap/ez_setup.py -O - | python [root@db1 setuptools-18.1]# python ez_setup.py 再次测试备注：看起来很酷，更多功能请查看项目 github 地址。 参考 https://github.com/dbcli/pgcli http://pgcli.com/","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Keepalived + PostgreSQL 流复制方式实现高可用 HA","slug":"20150808152332","date":"2015-08-08T07:23:32.000Z","updated":"2018-09-04T01:34:17.142Z","comments":true,"path":"20150808152332.html","link":"","permalink":"https://postgres.fun/20150808152332.html","excerpt":"","text":"HA 是数据库领域一个永恒的议题，同时也是最复杂的方案之一，PostgreSQL 本身并不提供任何高可用方案， 本文通过 Keepalived + PostgreSQL 流复制方式实现高可用 HA，故障切换逻辑和部分脚本参考德哥 sky_postgresql_cluster (https://github.com/digoal/sky_postgresql_cluster) 的 HA 项目，本文仅分享一种 PostgreSQL的高可用方案，脚本的切换逻辑可根据实际情况调整，如果您对此方案有更好的建议或补充，欢迎探讨。 Keepalived + PostgreSQL 流复制方式架构图如下： Keepalived + PostgreSQL 流复制高可用方案详见 https://github.com/francs/PostgreSQL-Keepalived-HA","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"},{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/tags/PG高可用性/"}]},{"title":"PostgreSQL9.5：新增行级锁 SKIP LOCKED 属性","slug":"20150805165321","date":"2015-08-05T08:53:21.000Z","updated":"2018-09-04T01:34:17.080Z","comments":true,"path":"20150805165321.html","link":"","permalink":"https://postgres.fun/20150805165321.html","excerpt":"","text":"PostgreSQL9.5 新增行级锁 SKIP LOCKED 属性,文档中描述如下： Add new SELECT option SKIP LOCKED to skip locked rows (Thomas Munro)This does not throw an error for locked rows like NOWAIT does. 我们知道可以通过 select .. from table where .. for update 语句锁住要更新的记录，在此事务提交前，之后的事务根据不知道这些行被锁住了。9.5 版本新增 SKIP LOCKED 属性，可以跳过这些被锁定的行，使 SQL 不报错，演示下。 环境准备创建测试表，如下：123456789[pg95@db2 ~]$ psql fdb fdb psql (9.5alpha1) Type \"help\" for help. fdb=&gt; create table test_5(id serial, name character varying(64)); CREATE TABLE fdb=&gt; insert into test_5(id,name) select n , n|| 'a' from generate_series(1,100) n; INSERT 0 100 事务A: 更新 id=100 的记录12345678fdb=&gt; begin; BEGIN fdb=&gt; select * from test_5 order by id desc limit 1 for update; id | name -----+------ 100 | 100a (1 row) 备注：事务 A 未提交。 事务B: 使用 Nowait 属性1234567fdb=&gt; begin; BEGIN fdb=&gt; select * from test_5 order by id desc limit 1 for update nowait; ERROR:could not obtain lock on row in relation \"test_5\" fdb=&gt; \\q 备注：事务 B 无法获取到 id=100 的行锁。 事务C: 使用 Skip Locked 属性12345fdb=&gt; select * from test_5 order by id desc limit 1 for update skip locked; id | name ----+------ 99 | 99a (1 row) 备注：使用 skip locked 属性则跳过了被锁的记录。 参考 SELECT COMMAND Waiting for 9.5 – Implement SKIP LOCKED for row-level locks","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"How large is your PostgreSQL database ？","slug":"20150805142056","date":"2015-08-05T06:20:56.000Z","updated":"2018-09-04T01:34:17.001Z","comments":true,"path":"20150805142056.html","link":"","permalink":"https://postgres.fun/20150805142056.html","excerpt":"","text":"最近有朋友问到 PostgreSQL 数据库最多能支持多大数据量，其实数据量仅是衡量数据库的一个标准之一，主要看应用场景，如果是日志库，只写的应用，数据量达到 10 TB 也正常，甚至更大。 在 PG 官网找到一份关于 PG 数据量大小的问卷调查，只是年代久远，是一份 2004 年的调查，从问卷调查结果来看，当时已经有 124 名投票者投了大于 4TB 的库。 How large is your PostgreSQL database? Survey ResultsThe current results of our How large is your PostgreSQL database? survey are: Answer Responses Percentage 0 - 100MB 732 38% 100 - 500MB 326 17% 500MB - 1GB 197 10% 1GB - 500GB 442 23% 500GB - 1TB 44 2% 1TB - 4TB 36 1% More than 4TB 124 6% 说得更近点，前东家的 PostgreSQL 环境，最大的日志库大于 10 TB ，应用场景为应用只写，数据仓库 ETL 抽取会读; 最大的业务库为订单库，订单库保留最近三个月左右的订单，整库数据量在 5TB 左右，订单库业务场景为读写场景，并且业务较繁忙。而且这两套应用都是跑在主，备单实例上，没有用到分片方案。 上面的的这些说明希望能对有这方面疑惑的朋友提供些帮助。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5：ALTER TABLE .. SET LOGGED / UNLOGGED","slug":"20150803162552","date":"2015-08-03T08:25:52.000Z","updated":"2018-09-04T01:34:16.939Z","comments":true,"path":"20150803162552.html","link":"","permalink":"https://postgres.fun/20150803162552.html","excerpt":"","text":"9.5 版本之前 PG 已经支持 UNLOGGED 表，UNLOGGED 由于不会记录 WAL 日志，在加载速度上会快些，当实例异常 crash 时会丢失数据，9.5 版本新特性支持日志表和非日志表的切换。以下在流复制环境测试。 主库: 创建一张 Unlogged 表123456789101112131415161718192021[pg95@db2 ~]$ psql fdb fdb psql (9.5alpha1) Type \"help\" for help. fdb=&gt; select pg_is_in_recovery(); pg_is_in_recovery ------------------- f (1 row) fdb=&gt; create unlogged table test_log(id serial primary key,message text); CREATE TABLE fdb=&gt; insert into test_log (id,message) select generate_series(1,1000),'message'; INSERT 0 1000 fdb=&gt; select relname,relpersistence from pg_class where relname='test_log'; relname | relpersistence ----------+---------------- test_log | u (1 row) 备库操作: 查询 Unlogged 表123456789101112[pg95@db1 ~]$ psql fdb fdb psql (9.5alpha1) Type \"help\" for help. fdb=&gt; select pg_is_in_recovery(); pg_is_in_recovery ------------------- t (1 row) fdb=&gt; select count(*) from test_log; ERROR:cannot access temporary or unlogged relations during recovery 备注：备库上不允许查询 unlogged 表，unlogged 表不会产生 WAL ，数据也不会复制到备节点。 主库操作: 将表 test_log 修改成 logged 表12345678fdb=&gt; alter table test_log set logged; ALTER TABLE fdb=&gt; select relname,relpersistence from pg_class where relname='test_log'; relname | relpersistence ----------+---------------- test_log | p (1 row) 备库操作: 查询成功123456789101112131415[pg95@db1 ~]$ psql fdb fdb psql (9.5alpha1) Type \"help\" for help. fdb=&gt; select pg_is_in_recovery(); pg_is_in_recovery ------------------- t (1 row) fdb=&gt; select count(*) from test_log; count ------- 1000 (1 row) 参考 PostgreSQL 9.1 新特性之一: 日志表的使用 CREATE TABLE","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5：Pg_rewind 快速恢复备节点","slug":"20150802155117","date":"2015-08-02T07:51:17.000Z","updated":"2018-09-04T01:34:16.876Z","comments":true,"path":"20150802155117.html","link":"","permalink":"https://postgres.fun/20150802155117.html","excerpt":"","text":"了解 PG 的朋友应该知道 PG 的主备切换并不容易，步骤较严谨，在激活备节点前需主动关闭主节点，否则再想以备节点角色拉起主节点会比较困难，之前博客介绍过主备切换，PostgreSQL HOT-Standby 的主备切换 ，PG 9.5 版本已经将 pg_rewind 加入到源码，当主备发生切换时，可以将原来主库通过同步模式恢复，避免重做备库。这样对于较大的库来说，节省了大量重做备库时间。 pg_rewind 会将目标库的数据文件，配置文件复制到本地目录，由于 pg_rewind 不会读取所有未发生变化的数据块，所以速度比重做备库要快很多， 环境准备流复制环境192.168.2.37/1931 主节点(主机名 db1)192.168.2.38/1931 备节点(主机名 db2)备注：流复制环境参考 PostgreSQL：使用 pg_basebackup 搭建流复制环境 ， 本文略。 pg_rewind 前提条件 full_page_writes wal_log_hints 设置成 on 或者 PG 在初始化时开启 checksums 功能 主备切换备节点 recovery.conf 配置: db2 上操作1234[pg95@db2 pg_root]$ grep ^[a-z] recovery.conf recovery_target_timeline = 'latest' standby_mode = on primary_conninfo = 'host=192.168.2.37 port=1931 user=repuser' # e.g. 'host=localhost port=5432' 激活备节点: db2 上操作12345[pg95@db2 pg_root]$ pg_ctl promote -D $PGDATA server promoting [pg95@db2 pg_root]$ pg_controldata | grep cluster Database cluster state: in production 备节点激活后，创建一张测试表并插入数据123456789[pg95@db2 pg_root]$ psql psql (9.5alpha1) Type \"help\" for help. postgres=# create table test_2(id int4); CREATE TABLE postgres=# insert into test_2(id) select n from generate_series(1,10000) n; INSERT 0 10000 停原来主节点: db1 上操作123456[pg95@db1 ~]$ pg_controldata | grep cluster Database cluster state: in production [pg95@db1 ~]$ pg_ctl stop -m fast -D $PGDATA waiting for server to shut down....... done server stopped 备注：停完原主库后，千万不能立即以备节点形式拉起老库，否则在执行 pg_rewind 时会报，”target server must be shut down cleanly” 错误。 pg_rewind: db1 操作1234567[pg95@db1 pg_root]$ pg_ctl stop -m fast -D $PGDATA waiting for server to shut down......... done server stopped [pg95@db1 pg_root]$ pg_rewind --target-pgdata $PGDATA --source-server='host=192.168.2.38 port=1931 user=postgres dbname=postgres' -P connected to server target server needs to use either data checksums or \"wal_log_hints = on\" 备注：执行 pg_rewind 抛出以上错误，错误内容很明显。 pg_rewind 代码分析12345678910364 /* 365 * Target cluster need to use checksums or hint bit wal-logging, this to 366 * prevent from data corruption that could occur because of hint bits. 367 */ 368 if (ControlFile_target.data_checksum_version != PG_DATA_CHECKSUM_VERSION &amp;&amp; 369 !ControlFile_target.wal_log_hints) 370 &#123; 371 pg_fatal(\"target server needs to use either data checksums or \"wal_log_hints = on\"n\"); 372 &#125; 373 备注：数据库在 initdb 时需要开启 checksums 或者设置 “wal_log_hints = on”， 接着设置主，备节点的 wal_log_hints 参数并重启数据库。 再次 pg_rewind, db1 上操作1234567891011[pg95@db1 pg_root]$ pg_rewind --target-pgdata $PGDATA --source-server='host=192.168.2.38 port=1931 user=postgres dbname=postgres' -P connected to server The servers diverged at WAL position 0/1300CEB0 on timeline 5. Rewinding from last common checkpoint at 0/1200008C on timeline 5 reading source file list reading target file list reading WAL in target need to copy 59 MB (total source directory size is 76 MB) 61185/61185 kB (100%) copied creating backup label and updating control file Done! 备注：pg_rewind 成功。 调整 recovery.conf 文件: db1 操作12[pg95@db1 ~]$ cd $PGDATA [pg95@db1 pg_root]$ mv recovery.done recovery.conf 备注：注意是否需要修改 primary_conninfo 配置。 1234[pg95@db1 pg_root]$ grep ^[a-z] recovery.conf recovery_target_timeline = 'latest' standby_mode = on primary_conninfo = 'host=192.168.2.38 port=1931 user=repuser' # e.g. 'host=localhost port=5432' 启动原主库， db1 上操作12345[pg95@db1 pg_root]$ pg_ctl start -D $PGDATA server starting [pg95@db1 pg_root]$ pg_controldata | grep cluster Database cluster state: in archive recovery 数据验证, db1 上操作123456789[pg95@db1 pg_root]$ psql psql (9.5alpha1) Type \"help\" for help. postgres=# select count(*) from test_2; count ------- 10000 (1 row) 备注：pg_rewind 成功，原主库现在是以备库角色启动，而且数据表 test_2 也同步过来了。 pg_rewind 原理123456789The basic idea is to copy everything from the new cluster to the old cluster, except for the blocks that we know to be the same. 1)Scan the WAL log of the old cluster, starting from the last checkpoint before the point where the new cluster's timeline history forked off from the old cluster. For each WAL record, make a note of the data blocks that were touched. This yields a list of all the data blocks that were changed in the old cluster, after the new cluster forked off. 2)Copy all those changed blocks from the new cluster to the old cluster. 3)Copy all other files like clog, conf files etc. from the new cluster to old cluster. Everything except the relation files. 4) Apply the WAL from the new cluster, starting from the checkpoint created at failover. (Strictly speaking, pg_rewind doesn't apply the WAL, it just creates a backup label file indicating that when PostgreSQL is started, it will start replay from that checkpoint and apply all the required WAL.) 参考 PostgreSQL HOT-Standby 的主备切换 PostgreSQL：使用 pg_basebackup 搭建流复制环境 pg_rewind","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"pg_rewind","slug":"pg-rewind","permalink":"https://postgres.fun/tags/pg-rewind/"}]},{"title":"PostgreSQL9.5：INSERT ON CONFLICT UPDATE, otherwise known as \"UPSERT\"","slug":"20150802105522","date":"2015-08-02T02:55:22.000Z","updated":"2018-09-04T01:34:16.814Z","comments":true,"path":"20150802105522.html","link":"","permalink":"https://postgres.fun/20150802105522.html","excerpt":"","text":"9.5 版本支持 “UPSERT” 特性， 这个特性支持 INSERT 语句定义 ON CONFLICT DO UPDATE/IGNORE 属性，当插入 SQL 违反约束的情况下定义动作，而不抛出错误，这个特性之前有不少开发人员咨询过。 环境准备定义一张用户登陆表1234567891011121314fdb=&gt; create table user_logins (username character varying (64) primary key,logins numeric(10,0)); CREATE TABLE fdb=&gt; insert into user_logins values ('francs',1); INSERT 0 1 fdb=&gt; insert into user_logins values ('matiler',2); INSERT 0 1 fdb=&gt; select * from user_logins ; username | logins ----------+-------- francs | 1 matiler | 2 增加两个用户登陆信息: 违反主键约束123fdb=&gt; INSERT INTO user_logins (username, logins) VALUES ('tutu',1),('francs',1); ERROR:duplicate key value violates unique constraint \"user_logins_pkey\" DETAIL: Key (username)=(francs) already exists. 定义 ON CONFLICT 属性1234567891011INSERT INTO user_logins (username, logins) VALUES ('tutu',1),('francs',1) ON CONFLICT (username) DO UPDATE SET logins = user_logins.logins + EXCLUDED.logins; fdb=&gt; select * from user_logins ; username | logins ----------+-------- matiler | 2 tutu | 1 francs | 2 备注：定义 ON CONFLICT 属性后，已有的用户只需更新 logins 值。EXCLUDED 为试图插入的值。 ON CONFLICT 语法 [ ON CONFLICT [ conflict_target ] conflict_action ] 备注：conflict_target 指冲突类型，例如 unique 约束或用户自定义约束。conflict_action 定义当冲突发生时采取的动作，例如 ON CONFLICT DO NOTHING 和 ON CONFLICT DO UPDATE。 再来看个例子：一张 city 表12345678910111213fdb=&gt; create table citys(city_name character varying(64) primary key ); CREATE TABLE fdb=&gt; insert into citys values('Hanzhoug'),('beijing'),('shanghai'); INSERT 0 3 fdb=&gt; select * from citys ; city_name ----------- Hanzhoug beijing shanghai (3 rows) 再插入两条数据：违反唯一约束123fdb=&gt; insert into citys values('Hanzhoug'),('shenzhen'); ERROR:duplicate key value violates unique constraint \"citys_pkey\" DETAIL: Key (city_name)=(Hanzhoug) already exists. 定义 On conflict do nothing 属性1234567891011fdb=&gt; insert into citys values('Hanzhoug'),('shenzhen') on conflict do nothing; INSERT 0 1 fdb=&gt; select * from citys ; city_name ----------- Hanzhoug beijing shanghai shenzhen (4 rows) 备注：可见没有违反主键约束的数据可以插入，违反了主键约束的数据不能插入，但也不报错，这在一些日志表的使用场很有用，当然上面这个例子并没有指定 conflict_target ，假如表上有多个约束，则应该指定 conflict_target。 指定 conflict_target12fdb=&gt; insert into citys values('Hanzhoug'),('suzhou') on conflict (city_name) do nothing; INSERT 0 1 ON CONFLICT 属性也能指定 WHERE12INSERT INTO distributors (did, dname) VALUES (10, 'Conrad International') ON CONFLICT (did) WHERE is_active DO NOTHING; 参考 INSERT COMMAND","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5：JSONB 数据类型: 支持元素修改，新增，删除","slug":"20150801174302","date":"2015-08-01T09:43:02.000Z","updated":"2018-09-04T01:34:16.736Z","comments":true,"path":"20150801174302.html","link":"","permalink":"https://postgres.fun/20150801174302.html","excerpt":"","text":"9.4 版本中 JSONB 数据类型可以通过函数和操作符获取元素值，但不能修改其元素值，很多有 JSON 需求场景的朋友都非常期待这一功能，好在 9.5 版支持 JSONB 元素修改，可以说 JSONB 数据类型在功能上获得了较大提升。 JSONB 元素修改有两种方法，详见以下。 通过 || 操作符通过 jsonb || jsonb (concatenate / overwrite) 操作符可以覆盖元素值，例如。 样例 jsonb 数据1234postgres=# select '&#123;\"name\":\"francs\",\"age\":\"31\"&#125;'::jsonb; jsonb --------------------------------- &#123;\"age\": \"31\", \"name\": \"francs\"&#125; 修改 age 元素值1234postgres=# select '&#123;\"name\":\"francs\",\"age\":\"31\"&#125;'::jsonb || '&#123;\"age\":\"32\"&#125;'::jsonb; ?column? --------------------------------- &#123;\"age\": \"32\", \"name\": \"francs\"&#125; 通过 jsonb_set() 函数 9.5 版本支持 jsonb_set() 函数修改元素值。 jsonb_set 函数语法1jsonb_set(target jsonb, path text[], new_value jsonb[, create_missing boolean]) jsonb_set 函数使用修改 age 元素值为 321234567891011121314151617postgres=# select jsonb_set('&#123;\"name\":\"francs\",\"age\":\"31\"&#125;'::jsonb,'&#123;age&#125;','\"32\"'::jsonb,false); jsonb_set --------------------------------- &#123;\"age\": \"32\", \"name\": \"francs\"&#125; create_missing 值为 true：如果元素值不存在，则添加 postgres=# select jsonb_set('&#123;\"name\":\"francs\",\"age\":\"31\"&#125;'::jsonb,'&#123;sex&#125;','\"male\"'::jsonb,true); jsonb_set ------------------------------------------------ &#123;\"age\": \"31\", \"sex\": \"male\", \"name\": \"francs\"&#125; (1 row) create_missing 值为 false：如果元素值不存在，不添加 postgres=# select jsonb_set('&#123;\"name\":\"francs\",\"age\":\"31\"&#125;'::jsonb,'&#123;sex&#125;','\"male\"'::jsonb,false); jsonb_set --------------------------------- &#123;\"age\": \"31\", \"name\": \"francs\"&#125; (1 row) 仔细看下面这个操作的区别123456789101112postgres=# SELECT '&#123;\"name\": \"Jane\", \"contact\": &#123;\"fax\": \"0000\",\"phone\": \"01234 567890\", \"mobile\": \"07890 123456\"&#125;&#125;'::jsonb || '&#123;\"contact\": &#123;\"fax\": \"01987 654321\"&#125;&#125;'::jsonb; ?column? ------------------------------------------------------ &#123;\"name\": \"Jane\", \"contact\": &#123;\"fax\": \"01987 654321\"&#125;&#125; (1 row) postgres=# SELECT jsonb_set('&#123;\"name\": \"Jane\", \"contact\": &#123;\"fax\": \"0000\",\"phone\": \"01234 567890\", \"mobile\": \"07890 123456\"&#125;&#125;'::jsonb,'&#123;contact,fax&#125;', '\"1111\"'::jsonb); jsonb_set ------------------------------------------------------------------------------------------------- &#123;\"name\": \"Jane\", \"contact\": &#123;\"fax\": \"1111\", \"phone\": \"01234 567890\", \"mobile\": \"07890 123456\"&#125;&#125; (1 row) 备注: 如果是嵌套元素，|| 操作会从上层替换整个嵌套元素值。 Jsonb: 添加元素 || 操作符同时也支持连接 jsonb 数据类型 ，例如，连接 sex 元素值1234postgres=# select '&#123;\"name\":\"francs\",\"age\":\"31\"&#125;'::jsonb || '&#123;\"sex\":\"male\"&#125;'::jsonb; ?column? ------------------------------------------------ &#123;\"age\": \"31\", \"sex\": \"male\", \"name\": \"francs\"&#125; Jsonb: 删除元素 删除元素值也有两种方法，一种是通过操作符 - ，另一种通过指定路径删除。 通过操作符删除元素12345678postgres=# SELECT '&#123;\"name\": \"James\", \"email\": \"james@localhost\"&#125;'::jsonb - 'email'; ?column? ------------------- &#123;\"name\": \"James\"&#125; postgres=# SELECT '[\"red\",\"green\",\"blue\"]'::jsonb - 0; ?column? ------------------- [\"green\", \"blue\"] 指定路径删除元素也可以指定路径删除元素，适合 jsonb 数据有嵌套元素并且内容较多的场景，语法如下：1jsonb #- text[] / int (remove key / array element in path) 示例如下： 123456789101112postgres=# SELECT '&#123;\"name\": \"James\", \"contact\": &#123;\"phone\": \"01234 567890\", \"fax\": \"01987 543210\"&#125;&#125;'::jsonb #- '&#123;contact,fax&#125;'::text[]; ?column? --------------------------------------------------------- &#123;\"name\": \"James\", \"contact\": &#123;\"phone\": \"01234 567890\"&#125;&#125; (1 row) postgres=# SELECT '&#123;\"name\": \"James\", \"aliases\": [\"Jamie\",\"The Jamester\",\"J Man\"]&#125;'::jsonb #- '&#123;aliases,1&#125;'::text[]; ?column? -------------------------------------------------- &#123;\"name\": \"James\", \"aliases\": [\"Jamie\", \"J Man\"]&#125; (1 row) jsonb_pretty 函数 jsonb_pretty 函数用来格式化 jsonb 数据类型输出，如下。1234567891011postgres=# SELECT jsonb_pretty('&#123;\"name\": \"James\", \"contact\": &#123;\"phone\": \"01234 567890\", \"fax\": \"01987 543210\"&#125;&#125;'::jsonb); jsonb_pretty --------------------------------- &#123; + \"name\": \"James\", + \"contact\": &#123; + \"fax\": \"01987 543210\", + \"phone\": \"01234 567890\"+ &#125; + &#125; (1 row) 参考 PostgreSQL9.3Beta1：JSON 功能增强 PostgreSQL 9.4: 新增 JSONB 数据类型 PostgreSQL9.4: JSONB 性能测试 JSON Functions and Operators","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"JSON/JSONB","slug":"JSON-JSONB","permalink":"https://postgres.fun/tags/JSON-JSONB/"}]},{"title":"PostgreSQL9.5：Foreign Table Inheritance","slug":"20150731113241","date":"2015-07-31T03:32:41.000Z","updated":"2018-09-04T01:34:16.673Z","comments":true,"path":"20150731113241.html","link":"","permalink":"https://postgres.fun/20150731113241.html","excerpt":"","text":"上篇 blog 介绍了外部表模式的导入，详见 PostgreSQL9.5: IMPORT FOREIGN SCHEMA ，9.5 版本还支持外部表继承本地表，也支持本地表继承外部表，这篇博客演示外部表继承本地表。 环境准备外部表 test_112345678910111213141516[pg95@db1 ~]$ psql fdb fdb psql (9.5alpha1) Type \"help\" for help. fdb=&gt; \\dE test_1 List of relations Schema | Name | Type | Owner --------+--------+---------------+------- fdb | test_1 | foreign table | fdb (1 row) fdb=&gt; select count(*) from test_1; count --------- 3000000 (1 row) 备注： fdb 库中已有表名为 test_1 的外部表。 外部表继承本地表12345678910111213141516171819fdb=&gt; create table test_master ( like test_1 including all ) ; CREATE TABLE fdb=&gt; ALTER TABLE test_1 INHERIT test_master; ALTER TABLE fdb=&gt; \\d+ test_master Table \"fdb.test_master\" Column | Type | Modifiers | Storage | Stats target | Description --------+---------+-----------+----------+--------------+------------- id | integer | | plain | | name | text | | extended | | Child tables: test_1 fdb=&gt; select count(*) from test_master; count --------- 3000000 (1 row) 备注： 创建本地表 test_master，并让 test_master 继承外部表 test_1。 在本地表 test_master 插入一条数据123456789101112131415161718192021fdb=&gt; insert into test_master(id,name) values (1,'master'); INSERT 0 1 fdb=&gt; select * from test_master where id=1; id | name ----+-------- 1 | master 1 | 1a (2 rows) fdb=&gt; select count(*) from test_master; count --------- 3000001 fdb=&gt; select tableoid::regclass, * from test_master where id=1; tableoid | id | name -------------+----+-------- test_master | 1 | master test_1 | 1 | 1a (2 rows) 执行计划1234567891011fdb=&gt; explain analyze select tableoid::regclass, * from test_master where id=1; QUERY PLAN --------------------------------------------------------------------------------------------------------------------- Result (cost=0.00..128.71 rows=7 width=40) (actual time=0.034..0.816 rows=2 loops=1) -&gt; Append (cost=0.00..128.71 rows=7 width=40) (actual time=0.031..0.810 rows=2 loops=1) -&gt; Seq Scan on test_master (cost=0.00..2.59 rows=1 width=40) (actual time=0.029..0.031 rows=1 loops=1) Filter: (id = 1) -&gt; Foreign Scan on test_1 (cost=100.00..126.12 rows=6 width=40) (actual time=0.772..0.773 rows=1 loops=1) Planning time: 0.252 ms Execution time: 1.520 ms (7 rows) 备注：外部表的继承用法为 PostgreSQL 的分片方案提供了思路，有兴趣的同学可以看看下面这篇通过外部表扩展的博客。 参考 PostgreSQL9.5: IMPORT FOREIGN SCHEMA","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5：Row-Level Security Policies","slug":"20150731085800","date":"2015-07-31T00:58:00.000Z","updated":"2018-09-04T01:34:16.611Z","comments":true,"path":"20150731085800.html","link":"","permalink":"https://postgres.fun/20150731085800.html","excerpt":"","text":"“Row-Level Security (RLS) support” 是 9.5 版本的主要特性之一，提供了基于行的安全策略，限制数据库用户的查看表数据权限， 先来看以下例子。 创建测试表创建测试表，插入测试数据1234567891011121314151617fdb=&gt; create table test_row(id serial primary key, username text, log_event text, create_time timestamp(0) without time zone default clock_timestamp()); CREATE TABLE fdb=&gt; insert into test_row(username,log_event) values('user1','user1:aaa'); INSERT 0 1 fdb=&gt; insert into test_row(username,log_event) values('user1','user1:aadsfdfa'); INSERT 0 1 fdb=&gt; insert into test_row(username,log_event) values('user2','user2:aadsfdfa'); INSERT 0 1 fdb=&gt; insert into test_row(username,log_event) values('user2','user2:test'); INSERT 0 1 fdb=&gt; insert into test_row(username,log_event) values('user3','user3:test3'); INSERT 0 1 fdb=&gt; insert into test_row(username,log_event) values('user3','user3:test3333'); INSERT 0 1 fdb=&gt; insert into test_row(username,log_event) values('user4','user4:test3333'); INSERT 0 1 创建测试用户创建 user1,user2,user3 测试用户12345678910111213141516[pg95@db1 ~]$ psql fdb psql (9.5alpha1) Type \"help\" for help. fdb=# create role user1 with login; CREATE ROLE fdb=# create role user2 with login; CREATE ROLE fdb=# create role user3 with login; CREATE ROLE fdb=&gt; grant select on test_row to user1,user2,user3; GRANT fdb=&gt; grant usage on schema fdb to user1,user2,user3; GRANT 以 User1 登陆可以查询全部数据1234567891011121314fdb=&gt; \\c fdb user1 You are now connected to database \"fdb\" as user \"user1\". fdb=&gt; select * from fdb.test_row; id | username |log_event | create_time ----+----------+----------------+--------------------- 1 | user1 | user1:aaa | 2015-07-30 14:48:49 2 | user1 | user1:aadsfdfa | 2015-07-30 14:48:54 3 | user2 | user2:aadsfdfa | 2015-07-30 14:48:59 4 | user2 | user2:test | 2015-07-30 14:49:06 5 | user3 | user3:test3 | 2015-07-30 14:49:15 6 | user3 | user3:test3333 | 2015-07-30 14:49:24 7 | user4 | user4:test3333 | 2015-07-30 14:49:29 (7 rows) 备注：之前版本只要给数据库用户赋予 SELECT 权限，那么用户可以查看全表数据。 给表添加 Policy123456789101112131415161718192021222324[pg95@db1 ~]$ psql fdb fdb psql (9.5alpha1) Type \"help\" for help. fdb=&gt; CREATE POLICY policy_test_row ON test_row fdb-&gt; FOR SELECT fdb-&gt; TO PUBLIC fdb-&gt; USING (username = current_user); CREATE POLICY fdb=&gt; select relname,relrowsecurity from pg_class where relname='test_row'; relname | relrowsecurity ----------+---------------- test_row | f (1 row) fdb=&gt; ALTER TABLE test_row ENABLE ROW LEVEL SECURITY; ALTER TABLE fdb=&gt; select relname,relrowsecurity from pg_class where relname='test_row'; relname | relrowsecurity ----------+---------------- test_row | t (1 row) 备注：给表 test_row 添加 policy ，限制数据库登陆用户仅允许查看当前用户的日志记录。 验证用户的数据权限user1 用户登陆123456789fdb=&gt; \\c fdb user1 You are now connected to database \"fdb\" as user \"user1\". fdb=&gt; select * from fdb.test_row; id | username |log_event | create_time ----+----------+----------------+--------------------- 1 | user1 | user1:aaa | 2015-07-30 14:48:49 2 | user1 | user1:aadsfdfa | 2015-07-30 14:48:54 (2 rows) user2 用户登陆123456789fdb=&gt; \\c fdb user2 You are now connected to database \"fdb\" as user \"user2\". fdb=&gt; select * from fdb.test_row; id | username |log_event | create_time ----+----------+----------------+--------------------- 3 | user2 | user2:aadsfdfa | 2015-07-30 14:48:59 4 | user2 | user2:test | 2015-07-30 14:49:06 (2 rows) 备注：user1 用户仅能查看 username 值为 ‘user1’ 的记录，user2 用户仅能查看 username 值为 ‘user2’ 的记录。 参考 Waiting for 9.5 – Row-Level Security Policies (RLS) CREATE POLICY","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.5：IMPORT FOREIGN SCHEMA","slug":"20150730115943","date":"2015-07-30T03:59:43.000Z","updated":"2018-09-04T01:34:16.548Z","comments":true,"path":"20150730115943.html","link":"","permalink":"https://postgres.fun/20150730115943.html","excerpt":"","text":"PostgreSQL 9.5 Alpha 版都已经出来近一个月了，新增了不少重量级的功能，最近比较忙，一直没空来测， 挤点时间，测着玩下。以后博客更新不会像之前频繁，好在现在了解 PG 或者对 PG 有兴趣的人越来越多，而且阿里云也上了 PG 的 RDS, 还希望大家多写博客分享，推动 PG 的中文化。 之前版本 PG 在定义外部表时需要一张一张表定义，例如：12345CREATE FOREIGN TABLE ft_test_1 ( id integer , name text ) SERVER pgsql_srv OPTIONS (schema_name 'francs', table_name 'test_1'); 假如源库有多张表，那么这样定义比较费力而且容易出错，记得之前做一个 MySQL 转 PG 的项目，MySQL 库不大， 100 来 GB 左右，表总数在 40 张左右，不包含分区表，当初这个项目的数据迁移是通过 mysql_fdw 外部表来做的，虽然只有 40 来张表，脚本量却很大，实施起来也费劲，好在当初经过多次测试，在项目组配合下最终完成了这个项目。 闲话少说，PostgreSQL 9.5 版本可以通过一个命令定义源库所有外部表，如下：12345IMPORT FOREIGN SCHEMA remote_schema [ &#123; LIMIT TO | EXCEPT &#125; ( table_name [, ...] ) ] FROM SERVER server_name INTO local_schema [ OPTIONS ( option 'value' [, ... ] ) ] 备注：接着实验演示。 环境准备创建建用户1CREATE ROLE fdb LOGIN ENCRYPTED PASSWORD 'fdb' nosuperuser noinherit nocreatedb nocreaterole ; 创建表空间12mkdir -p /database/pg95/pg_tbs/tbs_fdb_card create tablespace tbs_fdb_card owner postgres LOCATION '/database/pg95/pg_tbs/tbs_fdb_card'; 创建数据库1CREATE DATABASE fdb WITH OWNER = postgres TEMPLATE = template0 ENCODING = 'UTF8' TABLESPACE = tbs_fdb_card; 赋权1grant all on database fdb to fdb with grant option; 创建模式12\\c fdb fdb create schema fdb; 服务器上另一个版本为 pg9.3.3 实例12345678910111213141516171819202122232425[pg95@db1 ~]$ psql -h 127.0.0.1 -p 1921 francs francs psql (9.5alpha1, server 9.3.3) Type \"help\" for help. francs=&gt; \\dt+ List of relations Schema | Name | Type | Owner | Size | Description --------+----------------+-------+--------+------------+------------- francs | events | table | francs | 8192 bytes | francs | invoice_items | table | francs | 8192 bytes | francs | mkt_bean_shell | table | francs | 16 kB | francs | order_sn | table | francs | 0 bytes | 订单号表 francs | tbl_user_json | table | francs | 26 MB | francs | test_1 | table | francs | 3960 kB | francs | test_2 | table | francs | 16 kB | francs | test_3 | table | francs | 16 kB | francs | test_4 | table | francs | 16 kB | francs | test_bytea | table | francs | 16 kB | francs | test_count | table | francs | 0 bytes | francs | test_dup | table | francs | 8192 bytes | francs | test_time1 | table | francs | 8192 bytes | francs | test_time2 | table | francs | 8192 bytes | francs | test_trash | table | francs | 5912 kB | francs | user_ini | table | francs | 12 MB | (16 rows) 备注：目标在 pg 9.5 实例上创建到 pg9.3.3 的外部表。 创建外部表创建外部服务器1234567891011121314fdb=# create extension postgres_fdw ; CREATE EXTENSION fdb=# grant usage on foreign data wrapper postgres_fdw to fdb; GRANT fdb=# \\c fdb fdb You are now connected to database \"fdb\" as user \"fdb\". fdb=&gt; CREATE SERVER pgsql_srv FOREIGN DATA WRAPPER postgres_fdw OPTIONS (host '127.0.0.1', port '1921', dbname 'francs'); CREATE SERVER fdb=&gt; CREATE USER MAPPING FOR fdb SERVER pgsql_srv OPTIONS (user 'francs', password 'francs'); CREATE USER MAPPING 9.5 之前版本: 创建外部表1234567891011121314151617CREATE FOREIGN TABLE ft_test_1 ( id integer , name text ) SERVER pgsql_srv OPTIONS (schema_name 'francs', table_name 'test_1'); fdb=&gt; select * from ft_test_1 limit 1; id | name ----+------ 1 | 1a (1 row) fdb=&gt; select count(*) from ft_test_1; count -------- 100000 (1 row) 9.5 版本: 一次性创建源库所有外部表123456789101112131415161718192021222324252627fdb=&gt; drop foreign table ft_test_1; DROP FOREIGN TABLE fdb=&gt; IMPORT FOREIGN SCHEMA francs FROM SERVER pgsql_srv INTO fdb; IMPORT FOREIGN SCHEMA fdb=&gt; \\dE List of relations Schema | Name | Type | Owner --------+----------------+---------------+------- fdb | events | foreign table | fdb fdb | invoice_items | foreign table | fdb fdb | mkt_bean_shell | foreign table | fdb fdb | order_sn | foreign table | fdb fdb | tbl_user_json | foreign table | fdb fdb | test_1 | foreign table | fdb fdb | test_2 | foreign table | fdb fdb | test_3 | foreign table | fdb fdb | test_4 | foreign table | fdb fdb | test_bytea | foreign table | fdb fdb | test_count | foreign table | fdb fdb | test_dup | foreign table | fdb fdb | test_time1 | foreign table | fdb fdb | test_time2 | foreign table | fdb fdb | test_trash | foreign table | fdb fdb | user_ini | foreign table | fdb (16 rows) 备注：太棒了！ IMPORT FOREIGN SCHEMA 更多功能5.1 过滤功能123IMPORT FOREIGN SCHEMA public EXCEPT (reports, audit) FROM SERVER dest_server INTO remote; 5.2 指定某些表123IMPORT FOREIGN SCHEMA public LIMIT TO (customers, purchases) FROM SERVER dest_server INTO remote; 备注：这两个功能暂未测试，有兴趣的朋友可以测试下。 参考 PostgreSQL 外部表汇总 Waiting-for-9-5-implement-import-foreign-schema/ IMPORT FOREIGN SCHEMA","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.5 Alpha 1 Released","slug":"20150703141534","date":"2015-07-03T06:15:34.000Z","updated":"2018-09-04T01:34:16.470Z","comments":true,"path":"20150703141534.html","link":"","permalink":"https://postgres.fun/20150703141534.html","excerpt":"","text":"PostgreSQL 9.5 Alpha 1 版发布，这速度比我预期要快，在正式发布前有些特性会做调整，但主要包括以下特性： Major FeaturesAmong new major features available for testing in this alpha are: Block-Range Indexes (BRIN) which enable compact indexing of very large tables Large speed improvements in in-memory sorting and hashing Automated management of transaction log size INSERT ON CONFLICT UPDATE, otherwise known as “UPSERT” Grouping Sets, CUBE and ROLLUP analytic operations Row-Level Security (RLS) support More JSONB data manipulation functions and operators Added the pg_rewind tool and other high availability improvements to replication Multiple enhancements to Foreign Data Wrappers, including IMPORT FOREIGN SCHEMA Large increases in multi-core and large memory scalability 详见： http://www.postgresql.org/about/news/1595/ 有空时研究下 9.5 的新特性。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"阿里云 RDS For PostgreSQL 正式发布","slug":"20150602102033","date":"2015-06-02T02:20:33.000Z","updated":"2018-09-04T01:34:16.407Z","comments":true,"path":"20150602102033.html","link":"","permalink":"https://postgres.fun/20150602102033.html","excerpt":"","text":"阿里云 RDS For PostgreSQL 昨天正式发布，这是件可喜可贺的事情，用户在云数据库上又多了个选择，链接地址：RDS邀您6.5折体验PostgreSQL 通读全文， RDS For PostgreSQL 支持的功能有 JSON，PostGIS 空间信息，全文检索等，支持的插件也丰富，例如 PL/pgSQL，PL/V8, HSTORE, postgres_fdw , pg_stat_statements 等，对于大多数互联网中小企业来说是个福音。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：Pldebugger 模块安装：支持函数调试","slug":"20150402101746","date":"2015-04-02T02:17:46.000Z","updated":"2018-09-04T01:34:16.345Z","comments":true,"path":"20150402101746.html","link":"","permalink":"https://postgres.fun/20150402101746.html","excerpt":"","text":"习惯了 Oracle 的朋友，非常熟悉使 plsql developer 工具的存储过程调试功能，目前 PostgreSQL 本身不支持函数调试，但可以通过安装外部模块实现这一重要功能，本文以 PostgreSQL 9.4beta3 为例子，演示 pldebugger 模块的安装和使用。 安装 Pldebugger下载 pldebugger12cd /opt/pgsql_9.4beta3/share/contrib/ git clone git://git.postgresql.org/git/pldebugger.git 安装 pldebugger1234567891011121314151617[root@db1 pldebugger]# source /home/pg94/.bash_profile root@db1-&gt; USE_PGXS=1 make gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I/opt/pgsql_9.4beta3/lib/pgxs/src/makefiles/../../src/pl/plpgsql/src -I. -I./ -I/opt/pgsql_9.4beta3/include/server -I/opt/pgsql_9.4beta3/include/internal -D_GNU_SOURCE -c -o plpgsql_debugger.o plpgsql_debugger.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I. -I./ -I/opt/pgsql_9.4beta3/include/server -I/opt/pgsql_9.4beta3/include/internal -D_GNU_SOURCE -c -o plugin_debugger.o plugin_debugger.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I. -I./ -I/opt/pgsql_9.4beta3/include/server -I/opt/pgsql_9.4beta3/include/internal -D_GNU_SOURCE -c -o dbgcomm.o dbgcomm.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I. -I./ -I/opt/pgsql_9.4beta3/include/server -I/opt/pgsql_9.4beta3/include/internal -D_GNU_SOURCE -c -o pldbgapi.o pldbgapi.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fpic -shared -o plugin_debugger.so plpgsql_debugger.o plugin_debugger.o dbgcomm.o pldbgapi.o -L/opt/pgsql_9.4beta3/lib -Wl,--as-needed -Wl,-rpath,'/opt/pgsql_9.4beta3/lib',--enable-new-dtags root@db1-&gt; USE_PGXS=1 make install /bin/mkdir -p '/opt/pgsql_9.4beta3/lib' /bin/mkdir -p '/opt/pgsql_9.4beta3/share/extension' /bin/mkdir -p '/opt/pgsql_9.4beta3/share/extension' /bin/mkdir -p '/opt/pgsql_9.4beta3/share/doc/extension' /usr/bin/install -c -m 755 plugin_debugger.so '/opt/pgsql_9.4beta3/lib/plugin_debugger.so' /usr/bin/install -c -m 644 pldbgapi.control '/opt/pgsql_9.4beta3/share/extension/' /usr/bin/install -c -m 644 pldbgapi--1.0.sql pldbgapi--unpackaged--1.0.sql '/opt/pgsql_9.4beta3/share/extension/' /usr/bin/install -c -m 644 README.pldebugger '/opt/pgsql_9.4beta3/share/doc/extension/' 修改 postgresql.conf1shared_preload_libraries = '$libdir/plugin_debugger' 备注：此参数修改后需要重启数据库生效。 重启数据库1pg94@db1-&gt; pg_ctl restart -m fast 创建 Pldbgapi 扩展123456pg94@db1-&gt; psql francs psql (9.4beta3) Type \"help\" for help. francs=# create extension pldbgapi ; CREATE EXTENSION 函数调试测试123456789CREATE OR REPLACE FUNCTION func_add(i_a int4, i_b int4 ) RETURNS int4 AS $$ DECLARE v_add int4 ; begin RAISE NOTICE '% + %', i_a, i_b; v_add:=i_a+i_b; return v_add; end; $$ LANGUAGE 'plpgsql'; 备注：接下来打开本地 pgAdmin 工具，连接到相应库，找到对应函数，单击右键，出现如图： 之后出现调试界面，如下图： 备注： 太棒了！","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"MySQL：热备份 XtraBackup 工具初步使用","slug":"20150306164839","date":"2015-03-06T08:48:39.000Z","updated":"2018-09-04T01:34:16.282Z","comments":true,"path":"20150306164839.html","link":"","permalink":"https://postgres.fun/20150306164839.html","excerpt":"","text":"XtraBackup 是开源的 MySQL 执备份工具，支持 Percona Server, MySQL, MariaDB ，这篇 blog 介绍 XtraBackup 的安装和简单使用。 一 XtraBackup 工具安装下载http://www.percona.com/downloads/XtraBackup/XtraBackup-2.2.9/source/tarball/percona-xtrabackup-2.2.9.tar.gz 安装相关包1[root@db1 ~]# yum install cmake gcc gcc-c++ libaio libaio-devel automake autoconf bzr bison libtool ncurses5-devel libgcrypt libgcrypt-devel python-sphinx 安装 xtrabackup1234[root@db1 soft_bak]# tar zxvf percona-xtrabackup-2.2.9.tar.gz [root@db1 soft_bak]# cd percona-xtrabackup-2.2.9 [root@db1 percona-xtrabackup-2.2.9]# cmake -DBUILD_CONFIG=xtrabackup_release &amp;&amp; make -j4 [root@db1 percona-xtrabackup-2.2.9]# make install 备注： 默认装到了 /usr/local/xtrabackup/ 目录。 查看 xtrabackup 相关备份脚本123456[root@db1 xtrabackup]# ll /usr/local/xtrabackup/bin total 62104 -rwxr-xr-x 1 root root 185707 Mar 5 20:01 innobackupex -rwxr-xr-x 1 root root 4077080 Mar 6 08:52 xbcrypt -rwxr-xr-x 1 root root 4124543 Mar 6 08:52 xbstream -rwxr-xr-x 1 root root 55200546 Mar 6 09:15 xtrabackup 问题1： Can’t locate Time/HiRes.pm123[mysql@db1 ~]$ innobackupex --help Can't locate Time/HiRes.pm in @INC (@INC contains: /usr/local/lib/perl5 /usr/local/share/perl5 /usr/lib/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib/perl5 /usr/share/perl5 .) at /usr/local/xtrabackup/bin/innobackupex line 24. BEGIN failed--compilation aborted at /usr/local/xtrabackup/bin/innobackupex line 24. 备注：网上查了下，需要安装 perl-Time-HiRes 模块。 二 XtraBackup 使用创建备份用户123CREATE USER 'bk_user'@'localhost' IDENTIFIED BY 'bk_user'; GRANT RELOAD, LOCK TABLES, REPLICATION CLIENT ON *.* to 'bk_user'@'localhost'; flush privileges; 备注：RELOAD, LOCK TABLES, REPLICATION CLIENT 是备份用户需要的最小权限。 全量备份123456[mysql@db1 tf]$ innobackupex --host=127.0.0.1 --port=3306 --defaults-file=/opt/mysql/my.cnf --user=bk_user --password=bk_user backup ...最后几行日志 innobackupex: Backup created in directory '/home/mysql/script/tf/backup/2015-03-06_14-58-08' innobackupex: MySQL binlog position: filename 'bin-log.000003', position 1036 150306 14:58:13 innobackupex: Connection to database server closed 150306 14:58:13 innobackupex: completed OK 查看备份目录 [mysql@db1 tf]$ cd backup/2015-03-06_14-58-08/ [mysql@db1 2015-03-06_14-58-08]$ ll total 77M -rw-rw-r-- 1 mysql mysql 295 Mar 6 14:58 backup-my.cnf drwxrwxr-x 2 mysql mysql 4.0K Mar 6 14:58 binlog drwx------ 2 mysql mysql 4.0K Mar 6 14:58 francs -rw-rw---- 1 mysql mysql 76M Mar 6 14:58 ibdata1 drwxrwxr-x 2 mysql mysql 4.0K Mar 6 14:58 log_bk drwx------ 2 mysql mysql 4.0K Mar 6 14:58 mysql drwxrwxr-x 2 mysql mysql 4.0K Mar 6 14:58 performance_schema drwxrwxr-x 2 mysql mysql 4.0K Mar 6 14:58 test -rw-rw-r-- 1 mysql mysql 20 Mar 6 14:58 xtrabackup_binlog_info -rw-rw---- 1 mysql mysql 91 Mar 6 14:58 xtrabackup_checkpoints -rw-rw-r-- 1 mysql mysql 613 Mar 6 14:58 xtrabackup_info -rw-rw---- 1 mysql mysql 2.5K Mar 6 14:58 xtrabackup_logfile 三 参考 The innobackupex Script About Percona XtraBackup","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL + Keepalived 实现 HA","slug":"20150304185955","date":"2015-03-04T10:59:55.000Z","updated":"2018-09-04T01:34:16.220Z","comments":true,"path":"20150304185955.html","link":"","permalink":"https://postgres.fun/20150304185955.html","excerpt":"","text":"今天学习了 MySQL 高可用的相关方案， MySQL HA 方案很多，这里仅介绍使用 Keepalived 和 MySQL 本身的主从复制实现高可用。 一 环境信息主库 192.168.2.37/3306 主库VIP 192.168.2.137备库 192.168.2.38/3306 备库VIP: 192.168.2.138版本 5.6.20系统: RHEL 6.2备注： 主备节点 mysql 安装略，主备节点都分配单独 VIP，此 HA 方案应用程序不同时连两个节点，例如，应用程序仅连主库，当主库发生故障时，才切换到连备库。 二 主从复制搭建创建复制用户12345root@localhost:(none)&gt;create user 'rep1'@'192.168.2.%' identified by 'rep1abcd1243d'; Query OK, 0 rows affected (0.02 sec) root@localhost:(none)&gt;GRANT REPLICATION SLAVE ON *.* TO 'rep1'@'192.168.2.%'; Query OK, 0 rows affected (0.01 sec) 备注: 创建新用户并赋予 REPLICATION SLAVE 权限，这个用户用来接收主节点的 binary log. 主节点 my.cnf1234567891011121314basedir = /opt/mysql datadir = /database/mysql/data port = 3306 server_id = 1 log_bin = \"/database/mysql/data/binlog/bin-log\" binlog_format = \"STATEMENT\" master-info-repository=TABLE log_output = 'FILE,TABLE' slow_query_log = on long_query_time = 1 max_connections = 800 max_user_connections = 100 sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES prompt=\"u@h:d&gt;\" 备节点 my.cnf1234567891011121314151617basedir = /opt/mysql datadir = /database/mysql/data port = 3306 server_id = 2 report-host=192.168.2.38(db2) log_bin = \"/database/mysql/data/binlog/bin-log\" binlog_format = \"STATEMENT\" master-info-repository=TABLE relay_log_info_repository=TABLE relay_log_recovery=on log_output = 'FILE,TABLE' slow_query_log = on long_query_time = 1 max_connections = 1000 max_user_connections = 100 sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES prompt=\"u@h:d&gt;\" 主库上执行123456789CHANGE MASTER TO MASTER_HOST='192.168.2.38', MASTER_PORT=3306, MASTER_USER='rep1', MASTER_PASSWORD='rep1abcd1243d', MASTER_LOG_FILE='bin-log.000023', MASTER_LOG_POS=2298; mysql&gt; start slave; 备库上执行123456789CHANGE MASTER TO MASTER_HOST='192.168.2.37', MASTER_PORT=3306, MASTER_USER='rep1', MASTER_PASSWORD='rep1abcd1243d', MASTER_LOG_FILE='bin-log.000023', MASTER_LOG_POS=552; mysql&gt; start slave; 备注：MASTER_LOG_FILE 和 MASTER_LOG_POS 参数根据实际情况调整下， 其它主从复制相关步骤略，具体参考 MySQL：主从复制(Replication)搭建 三 安装 Keepalived下载1wget http://www.keepalived.org/software/keepalived-1.2.13.tar.gz 安装 OpenSSL , popt 包1yum -y install openssl openssl-devel popt 解压安装12345[root@db1 soft_bak]# tar xvf keepalived-1.2.13.tar.gz # ./configure --prefix=/usr/local/keepalived --sysconf=/etc # make # make install 遇到的错误123[root@db1 ~]# /etc/init.d/keepalived start Starting keepalived: /bin/bash: keepalived: command not found [FAILED] 解决方法1234567[root@db1 etc]# cp /usr/local/keepalived/sbin/keepalived /usr/sbin [root@db1 etc]# service keepalived restart Stopping keepalived: [OK ] Starting keepalived: [OK ] [root@db1 etc]# chkconfig keepalived off 四 配置 Keepalived主库 keepalived 配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@db1 ~]# cat /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs &#123; notification_email &#123; xxx@163.com &#125; smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id MYSQL_BALANCE &#125; vrrp_script check_mysql_alived &#123; script \"/usr/local/bin/mysql_moniter.sh\" interval 5 &#125; vrrp_instance VI_1 &#123; state BACKUP nopreempt # 主VIP 为非抢占模式 interface eth0 virtual_router_id 61 priority 100 # 注意这里将优先级设置比备节点高 advert_int 1 authentication &#123; auth_type PASS auth_pass 7BwhNLYpF6Ihs &#125; track_script &#123; check_mysql_alived &#125; virtual_ipaddress &#123; 192.168.2.137 &#125; # smtp_alert #notify_master /usr/local/bin/active_standby.sh &#125; vrrp_instance VI_2 &#123; state BACKUP preempt # 备VIP 为抢占模式 interface eth0 virtual_router_id 62 priority 90 # 注意这里将优先级设置比备节点低 advert_int 1 authentication &#123; auth_type PASS auth_pass 7BwhNLYpF6Ihs &#125; track_script &#123; check_mysql_alived &#125; virtual_ipaddress &#123; 192.168.2.138 &#125; # smtp_alert #notify_master /usr/local/bin/active_standby.sh &#125; 备库 keepalived 配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758[root@db2 ~]# cat /etc/keepalived/keepalived.conf ! Configuration File for keepalived global_defs &#123; notification_email &#123; xxx@163.com &#125; smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id MYSQL_BALANCE &#125; vrrp_script check_mysql_alived &#123; script \"/usr/local/bin/mysql_moniter.sh\" interval 5 &#125; vrrp_instance VI_1 &#123; state BACKUP nopreempt interface eth0 virtual_router_id 61 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 7BwhNLYpF6Ihs &#125; track_script &#123; check_mysql_alived &#125; virtual_ipaddress &#123; 192.168.2.137 &#125; # smtp_alert #notify_master /usr/local/bin/active_standby.sh &#125; vrrp_instance VI_2 &#123; state BACKUP preempt interface eth0 virtual_router_id 62 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 7BwhNLYpF6Ihs &#125; track_script &#123; check_mysql_alived &#125; virtual_ipaddress &#123; 192.168.2.138 &#125; # smtp_alert #notify_master /usr/local/bin/active_standby.sh &#125; /usr/local/bin/mysql_moniter.sh 脚本1234567891011#!/bin/bash # check mysqld status MYSQL_PORT=3306 nmap localhost -p $&#123;MYSQL_PORT&#125; | grep \"$&#123;MYSQL_PORT&#125;/tcp open\" if [ $? -ne 0 ]; then exit 1 else exit 0 fi 备注： 这里通过检测 mysql 服务端口方式判断 mysqld 服务是否正常。 五 HA 测试1) 停主库12[root@db1 ~]# /etc/init.d/mysqld stop Shutting down MySQL............ [OK ] 备注： 发现主库 VIP 192.168.2.137 漂到备库上了，切换正常。 2) 停主 Keepalived 服务12[root@db1 ~]# /etc/init.d/keepalived stop Stopping keepalived: 备注： 发现主库 VIP 192.168.2.137 漂到备库上了，切换正常。 3) 关主库主机1[root@db1 ~]# shutdown -h now 备注： 发现主库 VIP 192.168.2.137 漂到备库上了，切换正常。 六 总结 此文通过 keepalived + MySQL 主从实现了 HA， 但这种方法并不严谨，因为在切换前并没有主备延迟判断，当备库落后主库很长时间后，显然不适合切换了，建议将主备延迟监控起来。如果主备延迟超过阀值，则发出告警。 相关脚本需后续完善。 七 参考 Mysql + keepalived 实现双主热备读写分离 MySQL：主从复制(Replication)搭建","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL高可用","slug":"MySQL高可用","permalink":"https://postgres.fun/tags/MySQL高可用/"}]},{"title":"MySQL：主从切换","slug":"20150302160150","date":"2015-03-02T08:01:50.000Z","updated":"2018-09-04T01:34:16.157Z","comments":true,"path":"20150302160150.html","link":"","permalink":"https://postgres.fun/20150302160150.html","excerpt":"","text":"MySQL：主从复制(Replication)搭建 介绍了 MySQL 主从配置过程，这篇 blog 介绍手工主从切换过程。 一 环境信息主库 192.168.2.37/3306 主机名 db1备库 192.168.2.38/3306 主机名 db2版本 5.6.20备注： 主从节点 mysql 安装略，Replication 安装略。 二 主从切换主，备节点都要创建 Replication 用户12create user 'rep1'@'192.168.2.%' identified by 'rep1abcd1243d'; GRANT REPLICATION SLAVE ON *.* TO 'rep1'@'192.168.2.%'; 查询从库状态1root@localhost:francs&gt;show slave statusG 查询主库状态1234567root@localhost:francs&gt;show slave hosts; +-----------+-------------------+------+-----------+--------------------------------------+ | Server_id | Host | Port | Master_id | Slave_UUID | +-----------+-------------------+------+-----------+--------------------------------------+ | 2 | 192.168.2.38(db2) | 3306 | 1 | ad397a06-7c56-11e4-b2fb-000c29dcb3b5 | +-----------+-------------------+------+-----------+--------------------------------------+ 1 row in set (0.00 sec) 从库: 停止 IO_THREAD 线程1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859root@localhost:francs&gt;stop slave IO_THREAD; root@localhost:francs&gt;show slave statusG * 1. row * Slave_IO_State: Master_Host: 192.168.2.37 Master_User: rep1 Master_Port: 3306 Connect_Retry: 60 Master_Log_File: bin-log.000001 Read_Master_Log_Pos: 362 Relay_Log_File: db2-relay-bin.000002 Relay_Log_Pos: 523 Relay_Master_Log_File: bin-log.000001 Slave_IO_Running: No Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 362 Relay_Log_Space: 694 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: NULL Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: 0c130d47-22bb-11e4-aaaa-000c2986ac80 Master_Info_File: mysql.slave_master_info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 1 row in set (0.00 sec) 激活从库(从库上操作)1234567891011root@localhost:francs&gt;stop slave; root@localhost:francs&gt;reset master; root@localhost:francs&gt;reset slave all; root@localhost:francs&gt;show binary logs; +----------------+-----------+ | Log_name | File_size | +----------------+-----------+ | bin-log.000001 | 120 | +----------------+-----------+ 1 row in set (0.00 sec) 备注：reset slave all 命令会删除从库的 replication 参数，之后 show slave statusG 的信息返回为空。 将原来主库变为从库12345678910 CHANGE MASTER TO MASTER_HOST='192.168.2.38', MASTER_PORT=3306, MASTER_USER='rep1', MASTER_PASSWORD='rep1abcd1243d', MASTER_LOG_FILE='bin-log.000001', MASTER_LOG_POS=120; root@localhost:francs&gt;start slave; root@localhost:francs&gt; show slave statusG 备注：这步执行之后，发现 db1 日志文件报了以下错误，提示 db1 连接不上 db2。 db1 日志报错12015-03-02 14:24:14 26198 [ERROR] Slave I/O: error connecting to master 'rep1@192.168.2.38:3306' - retry-time: 60 retries: 8, Error_code: 1045 解决过程,连接测试1[mysql@db1 ~]$ mysql -h 192.168.2.38 -P 3306 -urep1 备注：居然不需要密码能直接能连上。 测试匿名用户1[mysql@db1 ~]$ mysql -h 192.168.2.38 -P 3306 -urep2 备注：依然不需要密码。 原因分析1234567root@localhost:francs&gt;select Host,User,Password from mysql.user where User=''; +-----------+------+----------+ | Host | User | Password | +-----------+------+----------+ | localhost | | | | db1 | | | +-----------+------+----------+ 备注: 原来 db2 节点上存在 User 为空的的两行，表示匿名用户可以连接数据库， 删除这两行，之后 flush privileges; 再连接测试1[mysql@db1 ~]$ mysql -h 192.168.2.38 -P 3306 -urep1 备注：这次连接需要密码了。之后再次观看 db1 同步日志，不再报错。 三 数据验证主从切换后db2 为主节点， db1 为备节点，在 db2 节点上插入一条数据测试同步是否正常。 db2 上执行12345678910root@localhost:francs&gt;insert into test_sr(id) values(30); Query OK, 1 row affected (0.03 sec) root@localhost:francs&gt;select * from test_sr order by id desc limit 1; +------+---------------------+ | id | create_time | +------+---------------------+ | 30 | 2015-03-02 15:19:53 | +------+---------------------+ 1 row in set (0.00 sec) db1 上验证1234567root@localhost:francs&gt;select * from test_sr order by id desc limit 1; +------+---------------------+ | id | create_time | +------+---------------------+ | 30 | 2015-03-02 15:19:53 | +------+---------------------+ 1 row in set (0.00 sec) 备注：数据同步正常，以上是对 MySQL 主备切换的初次了解，后续再补充。 四 参考 MySQL：主从复制(Replication)搭建 RESET SLAVE RESET MASTER mysql主从切换","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL Replication","slug":"MySQL-Replication","permalink":"https://postgres.fun/tags/MySQL-Replication/"}]},{"title":"PostgreSQL：如何显示元子命令的 SQL 脚本？","slug":"20150228113427","date":"2015-02-28T03:34:27.000Z","updated":"2018-09-04T01:34:15.610Z","comments":true,"path":"20150228113427.html","link":"","permalink":"https://postgres.fun/20150228113427.html","excerpt":"","text":"PostgreSQL 提供以反斜线 \\ 打头的命令，可以方便地列出数据库信息，反斜线命令也可翻译为元子命令 (meta command)。 例一: 显示表空间列表1234567891011postgres=# \\db List of tablespaces Name | Owner | Location -------------------+----------+------------------------------------------- pg_default | postgres | pg_global | postgres | tbs_cac_charge | postgres | /opt/pgdata/pg94/pg_tbs/tbs_cac_charge tbs_francs | postgres | /opt/pgdata/pg94/pg_tbs/tbs_francs tbs_norland | postgres | /opt/pgdata/pg94/pg_tbs/tbs_norland tbs_skypcsuit_log | postgres | /opt/pgdata/pg94/pg_tbs/tbs_skypcsuit_log (6 rows) 例二: 显示 Table 列表123456789101112131415francs=&gt; \\dt List of relations Schema | Name | Type | Owner --------+----------------------------+-------+-------- francs | db_list | table | francs francs | net_list | table | francs francs | tbl_log_pc_app_bag_install | table | francs francs | tbl_store | table | francs francs | test_1 | table | francs francs | test_2 | table | francs francs | test_cha | table | francs francs | test_search1 | table | francs francs | user1 | table | francs francs | users_normal | table | francs (10 rows) 备注： 反斜线 命令可以显示数据库表，索引，序列等多项信息，不一一举例，那么反斜线命令底层调用的是哪些 SQL 脚本呢？可以通过以下方法得到。 获取元子命令的 SQL 脚本psql 使用 -E 选项可以获取元子命令的 SQL 脚本，如下： 备注: psql 的 -E 参数显示出了 “\\db” 元子命令的 SQL 脚本，如有兴趣研究各种反斜线命令脚本，那么对 PostgreSQL 系统表，视图的熟悉会更进一步。尽管这篇 blog 仅列出一项简单功能，方便有需要的朋友查阅。 参考1234-E --echo-hidden Echo the actual queries generated by d and other backslash commands. You can use this to study psql's internal operations. This is equivalent to setting the variable ECHO_HIDDEN from within psql.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"psql","slug":"psql","permalink":"https://postgres.fun/tags/psql/"}]},{"title":"PostgreSQL：使用 Pg_upgrade 升级大版本","slug":"20150209170706","date":"2015-02-09T09:07:06.000Z","updated":"2018-09-04T01:34:15.548Z","comments":true,"path":"20150209170706.html","link":"","permalink":"https://postgres.fun/20150209170706.html","excerpt":"","text":"pg_upgrade 是 PostgreSQL 提供的一个大版本升级工具，例如将 PG 从版本 9.0 升级到 9.1, 可以跨多个大版本升级， 当然也可以用 pg_dump ，之后再 pg_restore 到新版本 PG 软件的方法， 当数据库比较大时，比如 1 TB 以上，pg_dump/pg_restore 的方法太费时了，这里介绍使用 pg_upgrade 将 PostgreSQL 版本从 9.1.14 升级到 9.4.1 的步骤，跨越了 3 个大版本。 安装 PostgreSQL 9.4.1 软件下载 postgresql-9.4.11wget -c https://ftp.postgresql.org/pub/source/v9.4.1/postgresql-9.4.1.tar.bz2 解压,编译,123tar jxvf postgresql-9.4.1.tar.bz2 cd postgresql-9.4.1 ./configure --prefix=/opt/pgsql_9.4.1 --with-pgport=1922 --with-wal-blocksize=16 安装12gmake world gmake install-world 初始化： initdb1/opt/pgsql_9.4.1/bin/initdb -E UTF8 -D /database/pg91_9.4/pg_root --locale=C -U postgres -W 使用 Pg_upgrade 升级大版本停老库123[pg92@db1 pg_root]$ pg_ctl stop -m fast -D /database/pg91/pg_root waiting for server to shut down.... done server stopped pg_upgrade 前检查12345678910111213141516171819202122[pg91@db1 pg91_9.4]$ /opt/pgsql_9.4.1/bin/pg_upgrade -c --link -b /opt/pgsql_9.1.14/bin -B /opt/pgsql_9.4.1/bin -d /database/pg91/pg_root -D /database/pg91_9.4/pg_root *failure* Consult the last few lines of \"pg_upgrade_server.log\" for the probable cause of the failure. Performing Consistency Checks on Old Live Server ------------------------------------------------ Checking cluster versions ok Checking database user is a superuser ok Checking for prepared transactions ok Checking for reg* system OID user data types ok Checking for contrib/isn with bigint-passing mismatch ok Checking for invalid \"line\" user columns ok Checking for presence of required libraries fatal Your installation references loadable libraries that are missing from the new installation. You can add these libraries to the new installation, or remove the functions using them from the old installation. A list of problem libraries is in the file: loadable_libraries.txt Failure, exiting 备注： -b, -B 分别表示老版本 PG bin 目录，新版本 PG bin目录， -d, -D 分别表示老版本PG 数据目录，新版本 PG 数据目录， -c 表示仅检查，并不会做任何更改， 根据提示查看文件 loadable_libraries.txt 。 报错信息123[pg91@db1 pg91_9.4]$ cat loadable_libraries.txt Could not load library \"$libdir/pg_tokenize\" ERROR:could not access file \"$libdir/pg_tokenize\": No such file or directory 备注：说明新版软件 9.4 没有安装 pg_tokenize 插件，这个插件是在安装中文全文索引 nlpbamboo 时需要安装的。 解决方法: 安装 nlpbamboo 相关插件12345[root@db1 pg_tokenize]#export PGHOME=/opt/pgsql_9.4.1 [root@db1 pg_tokenize]#export PATH=$PGHOME/bin:$PATH:. [root@db1 pg_tokenize]#export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib [root@db1 pg_tokenize]# make [root@db1 pg_tokenize]# make install 再次编译通过123456789101112131415161718[pg91@db1 ~]$ /opt/pgsql_9.4.1/bin/pg_upgrade -c --link -b /opt/pgsql_9.1.14/bin -B /opt/pgsql_9.4.1/bin -d /database/pg91/pg_root -D /database/pg91_9.4/pg_root *failure* Consult the last few lines of \"pg_upgrade_server.log\" for the probable cause of the failure. Performing Consistency Checks on Old Live Server ------------------------------------------------ Checking cluster versions ok Checking database user is a superuser ok Checking for prepared transactions ok Checking for reg* system OID user data types ok Checking for contrib/isn with bigint-passing mismatch ok Checking for invalid \"line\" user columns ok Checking for presence of required libraries ok Checking database user is a superuser ok Checking for prepared transactions ok *Clusters are compatible* pg_upgrade 升级1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[pg91@db1 ~]$ /opt/pgsql_9.4.1/bin/pg_upgrade --link -b /opt/pgsql_9.1.14/bin -B /opt/pgsql_9.4.1/bin -d /database/pg91/pg_root -D /database/pg91_9.4/pg_root Performing Consistency Checks ----------------------------- Checking cluster versions ok Checking database user is a superuser ok Checking for prepared transactions ok Checking for reg* system OID user data types ok Checking for contrib/isn with bigint-passing mismatch ok Checking for invalid \"line\" user columns ok Creating dump of global objects ok Creating dump of database schemas ok Checking for presence of required libraries ok Checking database user is a superuser ok Checking for prepared transactions ok If pg_upgrade fails after this point, you must re-initdb the new cluster before continuing. Performing Upgrade ------------------ Analyzing all rows in the new cluster ok Freezing all rows on the new cluster ok Deleting files from new pg_clog ok Copying old pg_clog to new server ok Setting next transaction ID and epoch for new cluster ok Deleting files from new pg_multixact/offsets ok Setting oldest multixact ID on new cluster ok Resetting WAL archives ok Setting frozenxid and minmxid counters in new cluster ok Restoring global objects in the new cluster ok Adding support functions to new cluster ok Restoring database schemas in the new cluster ok Setting minmxid counter in new cluster ok Creating newly-required TOAST tables ok Removing support functions from new cluster ok Adding \".old\" suffix to old global/pg_control ok If you want to start the old cluster, you will need to remove the \".old\" suffix from /database/pg91/pg_root/global/pg_control.old. Because \"link\" mode was used, the old cluster cannot be safely started once the new cluster has been started. Linking user relation files ok Setting next OID for new cluster ok Sync data directory to disk ok Creating script to analyze new cluster ok Creating script to delete old cluster ok Upgrade Complete ---------------- Optimizer statistics are not transferred by pg_upgrade so, once you start the new server, consider running: analyze_new_cluster.sh Running this script will delete the old cluster's data files: delete_old_cluster.sh 备注：这里使用了 –link 模式, 升级完成后提示运行分析脚本 analyze_new_cluster.sh。 用新版本软件起新库12345[pg91@db1 ~]$ /opt/pgsql_9.4.1/bin/pg_ctl --version pg_ctl (PostgreSQL) 9.4.1 [pg91@db1 ~]$ /opt/pgsql_9.4.1/bin/pg_ctl start -D /database/pg91_9.4/pg_root server starting 备注：能正常启动。 版本验证1234567891011121314151617181920212223[pg91@db1 ~]$ /opt/pgsql_9.4.1/bin/psql psql (9.4.1) Type \"help\" for help. postgres=# select version(); version ------------------------------------------------------------------------------------------------------- PostgreSQL 9.4.1 on i686-pc-linux-gnu, compiled by gcc (GCC) 4.4.6 20110731 (Red Hat 4.4.6-3), 32-bit (1 row) postgres=# select * from test_1 limit 3; id | name ----+------ 1 | a_1 2 | a_2 3 | a_3 (3 rows) postgres=# select count(*) from test_1; count ------- 100 (1 row) 备注：升级成功。 大版本升级后的操作运行分析脚本1[pg91@db1 ~]$ ./analyze_new_cluster.sh 备注： 这个脚本其实就一条 vacuumdb 命令，收集新库统计信息。 删除老版本软件1234[pg91@db1 ~]$ cat delete_old_cluster.sh #!/bin/sh rm -rf /database/pg91/pg_root 执行脚本 [pg91@db1 ~]$ ./delete_old_cluster.sh 备注：新库一切正常后，删除老版本软件。 修改 postgresql.conf, pg_hba.conf 等配置文件，根据生产需要，调整这两个文件的配置，在生产环境升级时，为了减少停机维护时间，可以事先写好这两个文件。 注意事项 此篇 blog 仅演示 pg_upgrade 的基本使用，生产环境下操作请自行做好备份; 此篇 blog 使用了 pg_upgrade 的 –link 模式，新版本软件共享老版本软件数据目录，用新版本软件启动数据目录后，再次用老版本软件启动目录会有问题。 如果老版本软件安装了相关插件，使用 pg_upgrade 升级前，新版本软件也需要安装相关插件。 参考 pg_upgrade PostgreSQL : 如何升级小版本？","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 中文全文检索(之三)：给 SCWS 中文词库添加新词","slug":"20150112195237","date":"2015-01-12T11:52:37.000Z","updated":"2018-09-04T01:34:15.485Z","comments":true,"path":"20150112195237.html","link":"","permalink":"https://postgres.fun/20150112195237.html","excerpt":"","text":"前两篇 blog 分别介绍了 zhparser 和 nlpbamboo 中文全文检索的部署和使用，但从分词效果来看都不十分理想。例如 “每家乐” 分词后“每”字会消失，”久久超市” 分词后变成”超市”，这会导致中文检索结果不匹配。 Zhparser 中文检索1234567891011francs=&gt; select to_tsvector('testzhcfg','每家乐'); to_tsvector ------------- '家乐':1 (1 row) francs=&gt; select to_tsvector('testzhcfg','久久超市'); to_tsvector ------------- '超市':1 (1 row) 备注：zhparser 用的是 SCWS ( Simple Chinese Word Segmentation ) 简易中文分词系统，是非常不错的中文分词解决方案，但有些词分词难免不尽如意，接下来介绍给 SCWS 添加新词的方法 。 词库 XDB 文件12[root@db ~]# ll /opt/pgsql_9.4beta3/share/tsearch_data/dict.utf8.xdb -rw-r--r-- 1 root root 20720423 Jan 11 17:41 /opt/pgsql_9.4beta3/share/tsearch_data/dict.utf8.xdb 备注：安装完 zhparser 插件后，会在 $PGHOME/share/tsearch_data 目录下生成 dict.utf8.xdb 词库文件，此文件格式不是文本，不能被编辑，好在 SCWS 提供词库导入导出工具。 下载 XDB导入导出工具http://www.ftphp.com/scws/down/phptool_for_scws_xdb.zip备注：解压后生成 dump_xdb_file.php 和 make_xdb_file.php 两个重要的工具，分别用来将词库文件导出到文本文件以及从文本文件导入到词库。 导出词库1234[root@db phptool]# php dump_xdb_file.php Usage: dump_xdb_file.php &lt;xdb file&gt; [output file] [root@db tsearch_data]# php /opt/soft_bak/phptool/dump_xdb_file.php dict.utf8.xdb xdb1.txt 备注：将词库文件 dict.utf8.xdb 导出到 xdb1.txt 文本文件。 修改 xdb1.txt，添加以下1234# WORD TF IDF ATTR 家乐 13.76 8.79 n 久久 14.47 5.42 n 久久超市 13.65 9.08 n 备注：文本文件有四个字段组成，用空格或制表符分隔， TF-IDF（term frequency– inverse document frequency），大概是加权作用，具体也不大清楚如何计算，不过有以下链接可以计算。 新词生词的TF/IDF计算器http://www.xunsearch.com/scws/demo/get_tfidf.php 重新生成 XDB 文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768[root@db tsearch_data]# php /opt/soft_bak/phptool/make_xdb_file.php dict.utf8.xdb xdb1.txtINFO: Loading text file data ... OK, Total words=550871Inserting [00/64] ... 11332 Records saved.Inserting [01/64] ... 11439 Records saved.Inserting [02/64] ... 13655 Records saved.Inserting [03/64] ... 10138 Records saved.Inserting [04/64] ... 7911 Records saved.Inserting [05/64] ... 3726 Records saved.Inserting [06/64] ... 9940 Records saved.Inserting [07/64] ... 3244 Records saved.Inserting [08/64] ... 4799 Records saved.Inserting [09/64] ... 15721 Records saved.Inserting [10/64] ... 6042 Records saved.Inserting [11/64] ... 4555 Records saved.Inserting [12/64] ... 4709 Records saved.Inserting [13/64] ... 5637 Records saved.Inserting [14/64] ... 5762 Records saved.Inserting [15/64] ... 4655 Records saved.Inserting [16/64] ... 2053 Records saved.Inserting [17/64] ... 855 Records saved.Inserting [18/64] ... 9271 Records saved.Inserting [19/64] ... 12463 Records saved.Inserting [20/64] ... 6059 Records saved.Inserting [21/64] ... 8448 Records saved.Inserting [22/64] ... 9868 Records saved.Inserting [23/64] ... 7665 Records saved.Inserting [24/64] ... 7427 Records saved.Inserting [25/64] ... 6301 Records saved.Inserting [26/64] ... 3365 Records saved.Inserting [27/64] ... 6683 Records saved.Inserting [28/64] ... 40398 Records saved.Inserting [29/64] ... 13978 Records saved.Inserting [30/64] ... 17539 Records saved.Inserting [31/64] ... 8222 Records saved.Inserting [32/64] ... 6732 Records saved.Inserting [33/64] ... 14588 Records saved.Inserting [34/64] ... 9572 Records saved.Inserting [35/64] ... 8870 Records saved.Inserting [36/64] ... 8987 Records saved.Inserting [37/64] ... 7027 Records saved.Inserting [38/64] ... 6839 Records saved.Inserting [39/64] ... 7160 Records saved.Inserting [40/64] ... 5034 Records saved.Inserting [41/64] ... 6856 Records saved.Inserting [42/64] ... 15446 Records saved.Inserting [43/64] ... 9458 Records saved.Inserting [44/64] ... 6026 Records saved.Inserting [45/64] ... 7510 Records saved.Inserting [46/64] ... 6006 Records saved.Inserting [47/64] ... 10465 Records saved.Inserting [48/64] ... 12255 Records saved.Inserting [49/64] ... 10853 Records saved.Inserting [50/64] ... 15622 Records saved.Inserting [51/64] ... 5880 Records saved.Inserting [52/64] ... 10766 Records saved.Inserting [53/64] ... 14227 Records saved.Inserting [54/64] ... 4791 Records saved.Inserting [55/64] ... 3618 Records saved.Inserting [56/64] ... 4721 Records saved.Inserting [57/64] ... 2279 Records saved.Inserting [58/64] ... 5041 Records saved.Inserting [59/64] ... 12033 Records saved.Inserting [60/64] ... 11009 Records saved.Inserting [61/64] ... 7840 Records saved.Inserting [62/64] ... 6409 Records saved.Inserting [63/64] ... 3091 Records saved.INFO: optimizing ...DONE! 中文检索测试123456789101112131415161718192021pg94@db-&gt; psql francs francspsql (9.4beta3)Type \"help\" for help.francs=&gt; select to_tsvector('testzhcfg','每家乐'); to_tsvector------------- '每家乐':1(1 row)francs=&gt; select to_tsvector('testzhcfg','久久超市'); to_tsvector------------------- '久久':1 '超市':2(1 row)francs=&gt; select to_tsvector('testzhcfg','久久'); to_tsvector------------- '久久':1(1 row) 备注：’久久超市’ 可以正常分词了，但需要退出之前会话，重新连接数据库才生效。 参考 SCWS 中文分词 SCWS中文分词，向xdb词库添加新词 PG 中文全文检索(之一)：Zhparser 安装及使用 PG 中文全文检索(之二)：Nlpbamboo 安装及使用","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"全文检索","slug":"全文检索","permalink":"https://postgres.fun/tags/全文检索/"}]},{"title":"PostgreSQL 中文全文检索(之二)：Nlpbamboo 安装及使用","slug":"20150109210327","date":"2015-01-09T13:03:27.000Z","updated":"2018-09-04T01:34:15.423Z","comments":true,"path":"20150109210327.html","link":"","permalink":"https://postgres.fun/20150109210327.html","excerpt":"","text":"上篇 blog 介绍了 PG 中文全文检索(之一): zhparser 安装及使用 ， 这篇 blog 准备介绍 nlpbamboo，虽然这个项目已经不更新了，也准备安装测试下。 安装 Cmake[pg94@db1 soft_bak]# yum install cmake 安装 CRF下载CRFhttps://code.google.com/p/crfpp/downloads/detail?name=CRF%2B%2B-0.57.tar.gz&amp;can=2&amp;q= 解压安装12345[pg94@db1 CRF++-0.57]# tar xvf CRF++-0.57.tar.gz [pg94@db1 CRF++-0.57]# cd CRF++-0.57 [pg94@db1 CRF++-0.57]# ./configure [pg94@db1 CRF++-0.57]# make [pg94@db1 CRF++-0.57]# make install 安装 Nlpbamboo下载 bamboohttps://code.google.com/p/nlpbamboo/downloads/list 解压安装1234567[pg94@db1 soft_bak]# tar xvf nlpbamboo-1.1.2.tar.bz2 [pg94@db1 soft_bak]# cd nlpbamboo-1.1.2 [pg94@db1 nlpbamboo-1.1.2]# mkdir build [pg94@db1 build]# cd build/ [pg94@db1 build]# cmake .. -DCMAKE_BUILD_TYPE=release [pg94@db1 build]# make all [pg94@db1 build]# make install 备注：根据日志， 安装目录为 /opt/bamboo/。 下载分词数据库文件https://code.google.com/p/nlpbamboo/downloads/list 解压 index.tar.bz2 到 /opt/bamboo 目录123456789101112[pg94@db1 bamboo]# cd /opt/bamboo/ [pg94@db1 bamboo]# tar jxvf index.tar.bz2 [pg94@db1 bamboo]# ll total 24 drwxr-xr-x 2 root root 4096 Jan 7 11:20 bin drwxr-xr-x 2 root root 4096 Jan 7 11:20 etc drwxr-xr-x 4 root root 4096 Jan 7 11:20 exts drwxr-sr-x 2 28852 users 4096 Apr 1 2009 index drwxr-xr-x 2 root root 4096 Jan 7 11:20 processor drwxr-xr-x 2 root root 4096 Jan 7 11:20 template [pg94@db1 bamboo]# 创建中文索引停止词（干扰词）1[pg94@db1 bamboo]# touch /opt/pgsql_9.1.14/share/tsearch_data/chinese_utf8.stop 编译安装12345678910111213141516[pg94@db1 bamboo]# cd /opt/bamboo/exts/postgres/pg_tokenize [pg94@db1 pg_tokenize]# source /home/pg93/.bash_profile pg94@db1-&gt; make pg94@db1-&gt; make install /bin/mkdir -p '/opt/pgsql_9.4beta3/lib' /bin/mkdir -p '/opt/pgsql_9.4beta3/share/contrib' /usr/bin/install -c -m 755 pg_tokenize.so '/opt/pgsql_9.4beta3/lib/pg_tokenize.so' /usr/bin/install -c -m 644 uninstall_pg_tokenize.sql pg_tokenize.sql '/opt/pgsql_9.4beta3/share/contrib/' pg94@db1-&gt; cd /opt/bamboo/exts/postgres/chinese_parser pg94@db1-&gt; make pg94@db1-&gt; make install /bin/mkdir -p '/opt/pgsql_9.4beta3/lib' /bin/mkdir -p '/opt/pgsql_9.4beta3/share/contrib' /usr/bin/install -c -m 755 chinese_parser.so '/opt/pgsql_9.4beta3/lib/chinese_parser.so' /usr/bin/install -c -m 644 uninstall_chinese_parser.sql chinese_parser.sql '/opt/pgsql_9.4beta3/share/contrib/' 导入分词模块1234567891011121314pg94@db1-&gt; cd /opt/pgsql_9.4beta3/share/contrib pg94@db1-&gt; psql -h 127.0.0.1 -d francs -f pg_tokenize.sql SET CREATE FUNCTION pg94@db1-&gt; psql -h 127.0.0.1 -d francs -f chinese_parser.sql SET CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE TEXT SEARCH PARSER CREATE TEXT SEARCH CONFIGURATION CREATE TEXT SEARCH DICTIONARY ALTER TEXT SEARCH CONFIGURATION 功能测试1234567891011121314151617181920212223242526272829[pg94@db1 contrib]$ psql db_test psql (9.3.3) Type \"help\" for help. francs=&gt; \\dF chinesecfg List of text search configurations Schema | Name | Description ------------+------------+------------- pg_catalog | chinesecfg | (1 row) db_test=# select tokenize('山坑休闲钓鱼山庄'); tokenize ---------------------- 山坑 休闲 钓鱼 山庄 (1 row) db_test=# SELECT to_tsvector('chinesecfg', '山坑休闲钓鱼山庄'); to_tsvector ------------------------------------- '休闲':2 '山坑':1 '山庄':4 '钓鱼':3 (1 row) francs=&gt; SELECT to_tsvector('chinesecfg', '咬不得生煎'); to_tsvector --------------------- '咬不得':1 '生煎':2 (1 row) 性能测试测试环境IBM BladeCenter HS228 核8GCentOS release 6.5PostgreSQL 9.4beta3cmake 2.8.12.2CRF++-0.57nlpbamboo-1.1.2 测试表: 300 万数据12345francs=&gt; select count(*) from tbl_store ; count --------- 2994433 (1 row) 创建 GIN 索引123francs=&gt;create index idx_gin_tbl_store_name on tbl_store using gin(to_tsvector('chinesecfg',name)); CREATE INDEX Time: 147735.647 ms sql1234567891011francs=&gt; explain analyze select name,type,city from tbl_store where to_tsvector('chinesecfg',name) @@ to_tsquery('chinesecfg','易买得长江店'); QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------- Bitmap Heap Scan on tbl_store (cost=68.00..72.02 rows=1 width=44) (actual time=0.431..0.434 rows=2 loops=1) Recheck Cond: (to_tsvector('chinesecfg'::regconfig, (name)::text) @@ '''易'' &amp; ''买'' &amp; ''得'' &amp; ''长江店'''::tsquery) Heap Blocks: exact=2 -&gt; Bitmap Index Scan on idx_gin_tbl_store_name (cost=0.00..68.00 rows=1 width=0) (actual time=0.418..0.418 rows=2 loops=1) Index Cond: (to_tsvector('chinesecfg'::regconfig, (name)::text) @@ '''易'' &amp; ''买'' &amp; ''得'' &amp; ''长江店'''::tsquery) Planning time: 0.361 ms Execution time: 0.493 ms (7 rows) 备注：性能非常不错。 常见错误以下总结了 nlpbamboo 安装过程中的典型错误，如下： 错误一: 导入 pg_tokenize.sql 报错12[pg94@db1 contrib]$ psql -h 127.0.0.1 -p 1921 -d francs -U postgres -f pg_tokenize.sql psql:pg_tokenize.sql:2: ERROR:could not load library \"/opt/pgsql_9.4beta3/lib/pg_tokenize.so\": /opt/pgsql_9.4beta3/lib/pg_tokenize.so: undefined symbol: DirectFunctionCall1 解决方法: 修改 Makefile 的 SHLIB_LINK 这行123456789101112131415cd /opt/bamboo/exts/postgres/pg_tokenize [pg94@db1 pg_tokenize]# cat Makefile MODULE_big = pg_tokenize OBJS = pg_tokenize.o DATA_built = pg_tokenize.sql DATA = uninstall_pg_tokenize.sql PG_CONFIG = pg_config PGXS := $(shell $(PG_CONFIG) --pgxs) PG_CPPFLAGS += -DNDEBUG -I/usr/include/bamboo #SHLIB_LINK = -L/usr/lib -lbamboo -lstdc++ SHLIB_LINK = -L/usr/lib -I /opt/soft_bak/postgresql-9.4beta3/src/include -lbamboo -lstdc++ include $(PGXS) 备注：之后重新编译安装即可。 错误二: 导入 pg_tokenize.sql 报错123pg94@db1-&gt; psql -h 127.0.0.1 -d francs -f pg_tokenize.sql SET psql:pg_tokenize.sql:2: ERROR:can not init tokenize 解决方法 解压 index.tar.bz2 到 /opt/bamboo 目录。 [pg94@db1 bamboo]# ll total 24 drwxr-xr-x 2 root root 4096 Jan 7 12:13 bin drwxr-xr-x 2 root root 4096 Jan 7 12:13 etc drwxr-xr-x 4 root root 4096 Jan 7 12:13 exts drwxr-sr-x 2 28852 users 4096 Apr 1 2009 index drwxr-xr-x 2 root root 4096 Jan 7 12:13 processor drwxr-xr-x 2 root root 4096 Jan 7 12:13 template 参考 PG 中文全文检索(之一)：Zhparser 安装及使用 PostgreSQL的中文全文检索(二) 借助 NlpBamboo 中文分词打造 PostgreSQL 的全文检索 http://code.google.com/p/nlpbamboo/","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"全文检索","slug":"全文检索","permalink":"https://postgres.fun/tags/全文检索/"}]},{"title":"PostgreSQL 中文全文检索(之一)：Zhparser 安装及使用","slug":"20150106175650","date":"2015-01-06T09:56:50.000Z","updated":"2018-09-04T01:34:15.360Z","comments":true,"path":"20150106175650.html","link":"","permalink":"https://postgres.fun/20150106175650.html","excerpt":"","text":"zhparser 是一个支持中文全文检索的 extension，基于Simple Chinese Word Segmentation(SCWS) 上开发的，作者是 amutu，这篇 blog 介绍 zhparser 的使用。 安装 Scws下载 scwshttp://www.xunsearch.com/scws/down/scws-1.2.2.tar.bz2 解压并安装1234[root@db1 soft_bak]# tar xvf scws-1.2.2.tar.bz2 [root@db1 soft_bak]# cd scws-1.2.2 [root@db1 scws-1.2.2]# ./configure [root@db1 scws-1.2.2]# make install 查看 scws 安装目录12[root@db1 scws-1.2.2]# ls /usr/local/include/scws/ charset.h crc32.h darray.h pool.h rule.h scws.h version.h xdb.h xdict.h xtree.h 安装 Zhparser下载 zhparserhttps://github.com/amutu/zhparser/archive/master.zip 编译和安装zhparser12[root@db1 ~]# source /home/pg94/.bash_profile [root@db1 scws-1.2.2]# SCWS_HOME=/usr/local make &amp;&amp; make install 备注：编译前需先配置环境变量。 查看 zhparser 相关文件1234pg94@db1-&gt; ll /opt/pgsql_9.4beta3/share/extension/zhparser* -rw-r--r-- 1 root root 606 Jan 5 20:52 /opt/pgsql_9.4beta3/share/extension/zhparser--1.0.sql -rw-r--r-- 1 root root 133 Jan 5 20:52 /opt/pgsql_9.4beta3/share/extension/zhparser.control -rw-r--r-- 1 root root 471 Jan 5 20:52 /opt/pgsql_9.4beta3/share/extension/zhparser--unpackaged--1.0.sql 使用 Zhparser创建 extension1234567891011121314pg94@db1-&gt; psql francs psql (9.4beta3) Type \"help\" for help. francs=# create extension zhparser; CREATE EXTENSION --make test configuration using parser francs=# CREATE TEXT SEARCH CONFIGURATION testzhcfg (PARSER = zhparser); CREATE TEXT SEARCH CONFIGURATION --add token mapping francs=# ALTER TEXT SEARCH CONFIGURATION testzhcfg ADD MAPPING FOR n,v,a,i,e,l WITH simple; ALTER TEXT SEARCH CONFIGURATION 中文分词测试测试中文分词 to_tsvector123456789101112131415161718192021francs=&gt; select to_tsvector('testzhcfg','南京市长江大桥'); to_tsvector ------------------------- '南京市':1 '长江大桥':2 (1 row) francs=&gt; select to_tsvector('testzhcfg','山坑休闲钓鱼山庄'); to_tsvector ------------------------------------- '休闲':2 '山坑':1 '山庄':4 '钓鱼':3 (1 row) francs=&gt; SELECT to_tsvector('testzhcfg','“今年保障房新开工数量虽然有所下调，但实际的年度在建规模以及竣工规模会超以往年份，相对应的对资金的需求也会创历&gt;史纪录。”陈国强说。在他看来，与2011年相比，2012年的保障房建设在资金配套上的压力将更为严峻。'); to_tsvector -------------------------------------------------------------------------------------------------------------------------------------'2011':27 '2012':29 '上':35 '下调':7 '严峻':37 '会':14 '会创':20 '保障':1,30 '压力':36 '史':21 '国强':24 '在建':10 '实际':8 '对应':17 '年份':16 ' 年度':9 '开工':4 '房':2 '房建':31 '数量':5 '新':3 '有所':6 '相比':28 '看来':26 '竣工':12 '纪录':22 '规模':11,13 '设在':32 '说':25 '资金':18,33 '超 ':15 '配套':34 '陈':23 '需求':19 (1 row) 备注： 从上面几个例子来看，分词效果较好。 测试 to_tsquery12345francs=&gt; SELECT to_tsquery('testzhcfg', '保障房资金压力'); to_tsquery --------------------------------- '保障' &amp; '房' &amp; '资金' &amp; '压力' (1 row) 性能测试测试环境IBM BladeCenter HS228 核8GCentOS release 6.5PostgreSQL 9.4beta3zhparser 1.0 测试表， 300 万左右数据12345francs=&gt; select count(*) from tbl_store ; count --------- 2994433 (1 row) 普通的 like123456789francs=&gt; explain analyze select name,type,city from tbl_store where name like '%美猴王批发%'; QUERY PLAN ------------------------------------------------------------------------------------------------------------- Seq Scan on tbl_store (cost=0.00..106601.41 rows=282 width=44) (actual time=0.018..870.666 rows=2 loops=1) Filter: ((name)::text ~~ '%美猴王批发%'::text) Rows Removed by Filter: 2994431 Planning time: 0.964 ms Execution time: 870.706 ms (5 rows) 备注：两边带有 % 的 like 走全表扫，效率很低。 创建 GIN 索引123francs=&gt; create index idx_gin_tbl_store_name on tbl_store using gin(to_tsvector('testzhcfg',name)); CREATE INDEX Time: 48434.917 ms PG 中文全文检索1234567891011francs=&gt; explain analyze select name,type,city from tbl_store where to_tsvector('testzhcfg',name) @@ to_tsquery('testzhcfg','美猴王批发'); QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------- Bitmap Heap Scan on tbl_store (cost=28.58..322.30 rows=75 width=44) (actual time=0.352..0.356 rows=2 loops=1) Recheck Cond: (to_tsvector('testzhcfg'::regconfig, (name)::text) @@ '''美猴王'' &amp; ''批发'''::tsquery) Heap Blocks: exact=2 -&gt; Bitmap Index Scan on idx_gin_tbl_store_name (cost=0.00..28.56 rows=75 width=0) (actual time=0.342..0.342 rows=2 loops=1) Index Cond: (to_tsvector('testzhcfg'::regconfig, (name)::text) @@ '''美猴王'' &amp; ''批发'''::tsquery) Planning time: 0.168 ms Execution time: 0.398 ms (7 rows) 备注： 查询时间 0.398 ms ，效率非常高。 参考 PostgreSQL的中文全文检索(二) https://github.com/amutu/zhparser Full Text Search CREATE TEXT SEARCH CONFIGURATION SCWS 中文分词","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"全文检索","slug":"全文检索","permalink":"https://postgres.fun/tags/全文检索/"}]},{"title":"PostgreSQL 9.4 Released!","slug":"20141219100650","date":"2014-12-19T02:06:50.000Z","updated":"2018-09-04T01:34:15.298Z","comments":true,"path":"20141219100650.html","link":"","permalink":"https://postgres.fun/20141219100650.html","excerpt":"","text":"PostgreSQL 9.4 Released! 18th December 2014The PostgreSQL Global Development Group is pleased to announce the availability of PostgreSQL 9.4!This release includes highly performant native JSON support, a new API for reading and manipulating the PostgreSQL replication stream, numerous performance improvements, and much more. 发行说明详见: PostgreSQL 9.4 Increases Flexibility, Scalability and Performance PostgreSQL 9.4 新特性9.4 新特性包括 jsonb, 逻辑复制 ( logical decoding ) , Replication slots, delayed standbys 等，详见：PostgreSQL9.4：新特性汇总 2014 PostgreSQL 杭州交流2014 PostgreSQL 杭州交流小聚分享的话题PostgreSQL9.4 新特性分享.pptx","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"MySQL：执行 SQL 脚本的几种方式","slug":"20141219093840","date":"2014-12-19T01:38:40.000Z","updated":"2018-09-04T01:34:15.235Z","comments":true,"path":"20141219093840.html","link":"","permalink":"https://postgres.fun/20141219093840.html","excerpt":"","text":"调用 SQL 脚本属于数据库维护过程中再普通不过的工作了，这里介绍几种调用 SQL 脚本的方法。 方法一: 通过 Linux 重定向一个 sql 脚本123[mysql@db1 tf]$ cat 1.sql select now(); select user(); 调用 1.sql 脚本12345[mysql@db1 tf]$ mysql &lt; 1.sql now() 2014-12-18 17:27:47 user() root@localhost 设置输出格式1234567891011[mysql@db1 tf]$ mysql -t &lt; 1.sql +---------------------+ | now() | +---------------------+ | 2014-12-18 17:27:57 | +---------------------+ +----------------+ | user() | +----------------+ | root@localhost | +----------------+ 备注： -t 表示以 table 格式输出。如果脚本在执行过程中遇到错误则中止; 如果想让出错后的 sql 继续执行，则需加上 –force 参数。 方法二: 通过 “. “ 或者 “souce “ 命令调用123456789101112131415161718192021222324252627282930313233343536373839404142[mysql@db1 ~]$ mysql Welcome to the MySQL monitor. Commands end with ; or g. Your MySQL connection id is 1579 Server version: 5.6.20-log Source distribution Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or 'h' for help. Type 'c' to clear the current input statement. root@localhost:(none)&gt;. /home/mysql/script/tf/1.sql +---------------------+ | now() | +---------------------+ | 2014-12-18 17:33:37 | +---------------------+ 1 row in set (0.00 sec) +----------------+ | user() | +----------------+ | root@localhost | +----------------+ 1 row in set (0.00 sec) root@localhost:(none)&gt;source /home/mysql/script/tf/1.sql +---------------------+ | now() | +---------------------+ | 2014-12-18 17:34:13 | +---------------------+ 1 row in set (0.00 sec) +----------------+ | user() | +----------------+ | root@localhost | +----------------+ 1 row in set (0.00 sec) 方法三: 使用 -e 参数1234567891011[mysql@db1 ~]$ mysql -e \"select now();select user();\" +---------------------+ | now() | +---------------------+ | 2014-12-18 17:34:58 | +---------------------+ +----------------+ | user() | +----------------+ | root@localhost | +----------------+ 输出日志1234[mysql@db1 tf]$ cat insert.sql insert into test_call(id,name) values(1,'a'); insert into test_call(id,name) values(2,'a'); insert into test_call(id,name) values(3,'a'); 无日志调用方式1[mysql@db1 tf]$ mysql francs &lt; insert.sql 将日志打出 [mysql@db1 tf]$ mysql -vvv francs &lt; insert.sql -------------- insert into test_call(id,name) values(1,&apos;a&apos;) -------------- Query OK, 1 row affected (0.00 sec) -------------- insert into test_call(id,name) values(2,&apos;a&apos;) -------------- Query OK, 1 row affected (0.01 sec) -------------- insert into test_call(id,name) values(3,&apos;a&apos;) -------------- Query OK, 1 row affected (0.01 sec) Bye 参考 Using mysql in Batch Mode mysql Options","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：延迟的主从复制 ( Delayed Replication ) ","slug":"20141212151645","date":"2014-12-12T07:16:45.000Z","updated":"2018-09-04T01:34:15.173Z","comments":true,"path":"20141212151645.html","link":"","permalink":"https://postgres.fun/20141212151645.html","excerpt":"","text":"MySQL 5.6 已经支持延迟的流复制， 可设置备节点的延迟时间， 延迟复制是有意义的，例如防止主节点数据误删，查看数据库历史状态等， PostgreSQL 在即将发行的 9.4 版本也支持延迟复制，有兴趣的朋友可以查看之前写的 blog: PostgreSQL9.4 : 支持备库延迟复制(delayed standbys) ， 这里简单介绍 MySQL 延迟复制的配置。 MySQL 的主从搭建本文略。 CHANGE MASTER TO 命令指定 MASTER_DELAY 选项即可设置，如下： 一 语法1CHANGE MASTER TO MASTER_DELAY = 30; 备注：设置备节点延迟的时间，单位秒。 二 延迟复制配置从节点设置12345678root@localhost:mysql&gt;stop slave; Query OK, 0 rows affected (0.21 sec) root@localhost:mysql&gt;CHANGE MASTER TO MASTER_DELAY = 30; Query OK, 0 rows affected (0.17 sec) root@localhost:mysql&gt;start slave; Query OK, 0 rows affected (0.27 sec) 查看从节点状态123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869root@localhost:mysql&gt;select * from slave_relay_log_infoG * 1. row * Number_of_lines: 7 Relay_log_name: ./db2-relay-bin.000007 Relay_log_pos: 281 Master_log_name: bin-log.000041 Master_log_pos: 361 Sql_delay: 30 Number_of_workers: 0 Id: 1 1 row in set (0.00 sec) root@localhost:mysql&gt;show slave statusG * 1. row * Slave_IO_State: Waiting for master to send event Master_Host: 192.168.2.37 Master_User: rep1 Master_Port: 3306 Connect_Retry: 60 Master_Log_File: bin-log.000040 Read_Master_Log_Pos: 604 Relay_Log_File: db2-relay-bin.000002 Relay_Log_Pos: 281 Relay_Master_Log_File: bin-log.000040 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 604 Relay_Log_Space: 452 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: 0c130d47-22bb-11e4-aaaa-000c2986ac80 Master_Info_File: mysql.slave_master_info SQL_Delay: 30 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 1 row in set (0.00 sec) 主节点创建表测试12345root@localhost:mysql&gt;create table test_delay(id int4 primary key, create_time datetime default current_timestamp); Query OK, 0 rows affected (0.59 sec) root@localhost:mysql&gt;insert into test_delay (id) values(1); Query OK, 1 row affected (0.07 sec) 备注：在主节点上创建一张测试表并插入一条测试数据。 备节点查询12345root@localhost:mysql&gt;select * from test_delay; Empty set (0.00 sec) root@localhost:mysql&gt;select * from test_delay; Empty set (0.00 sec) 这时在备节点上还查不到表。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465root@localhost:mysql&gt;show slave statusG * 1. row * Slave_IO_State: Waiting for master to send event Master_Host: 192.168.2.37 Master_User: rep1 Master_Port: 3306 Connect_Retry: 60 Master_Log_File: bin-log.000040 Read_Master_Log_Pos: 1013 Relay_Log_File: db2-relay-bin.000002 Relay_Log_Pos: 281 Relay_Master_Log_File: bin-log.000040 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 604 Relay_Log_Space: 861 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 15 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: 0c130d47-22bb-11e4-aaaa-000c2986ac80 Master_Info_File: mysql.slave_master_info SQL_Delay: 30 SQL_Remaining_Delay: 14 Slave_SQL_Running_State: Waiting until MASTER_DELAY seconds after master executed event Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 1 row in set (0.00 sec) root@localhost:mysql&gt;select * from test_delay; +----+---------------------+ | id | create_time | +----+---------------------+ | 1 | 2014-12-12 14:38:47 | +----+---------------------+ 1 row in set (0.00 sec) 备注: 发现备节点在 30 秒后才能查到新建的表，这里有三个字段显示延迟的信息： SQL_Delay: 显示已设置的主备延迟的时间，单位秒; SQL_Remaining_Delay: 显示剩余的主备延迟时间，单位秒; Slave_SQL_Running_State: 表示 SQL thread 状态; 三 参考 Delayed Replication CHANGE MASTER TO Syntax PostgreSQL9.4 : 支持备库延迟复制(delayed standbys)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL Replication","slug":"MySQL-Replication","permalink":"https://postgres.fun/tags/MySQL-Replication/"}]},{"title":"MySQL：初识 Relay log，Master info log ， Relay log info log","slug":"20141210093328","date":"2014-12-10T01:33:28.000Z","updated":"2018-09-04T01:34:15.110Z","comments":true,"path":"20141210093328.html","link":"","permalink":"https://postgres.fun/20141210093328.html","excerpt":"","text":"在 MySQL 主从复制环( Replication )境中，备节点会产生几种文件，如下： 一 Relay log filesrelay log 和 binary log 日志类似， 记录了数据库的变化，由一系列文件组成, relaoy log 记录了主节点发来的数据库变化信息，并且由 I/O thread 写入。之后 SQL thread 在备节点上执行 relay log 文件里的事件。 查看 relay log12345678910111213141516171819[mysql@db2 data]$ ll total 173M -rw-rw---- 1 mysql mysql 56 Dec 5 16:14 auto.cnf drwxrwxr-x 2 mysql mysql 4.0K Dec 9 16:00 binlog -rw-r----- 1 mysql root 120K Dec 9 15:59 db2.err -rw-rw---- 1 mysql mysql 6 Dec 9 15:05 db2.pid -rw-rw---- 1 mysql mysql 332 Dec 9 16:00 db2-relay-bin.000064 -rw-rw---- 1 mysql mysql 236 Dec 9 16:00 db2-relay-bin.000065 -rw-rw---- 1 mysql mysql 46 Dec 9 16:00 db2-relay-bin.index -rw-rw---- 1 mysql mysql 2.3K Dec 9 16:00 db2-slow.log drwx------ 2 mysql mysql 4.0K Dec 4 14:34 francs -rw-rw---- 1 mysql mysql 76M Dec 9 16:00 ibdata1 -rw-rw---- 1 mysql mysql 48M Dec 9 16:00 ib_logfile0 -rw-rw---- 1 mysql mysql 48M Aug 13 15:08 ib_logfile1 drwxrwxr-x 2 mysql mysql 4.0K Aug 19 10:21 log_bk drwx------ 2 mysql mysql 4.0K Aug 13 15:08 mysql -rw-rw---- 1 mysql mysql 0 Aug 19 11:09 on.index drwx------ 2 mysql mysql 4.0K Aug 13 15:24 performance_schema drwx------ 2 mysql mysql 4.0K Aug 13 15:08 test 备注: db2-relay-bin 打头的几个文件就是 relay log，其中 db2-relay-bin.index 为索引文件; 默认在主数据目录下，格式为 host_name-relay-bin，不清楚为什么 MySQL 要将 relay log 和 binary log 分开。 二 Master info log master info log 文件记录了备节点的连接信息，例如用户名，密码等。同时包括主节点信息，目前版本这个文件可以写到 MySQL 系统表中，但需要设置以下参数: 设置备节点 my.cnf1master-info-repository=TABLE 备注：之后重启 mysql 服务 查看 slave_master_info 表1234567891011121314151617181920212223242526root@localhost:mysql&gt;select * from slave_master_infoG * 1. row * Number_of_lines: 23 Master_log_name: bin-log.000040 Master_log_pos: 120 Host: 192.168.2.37 User_name: rep1 User_password: rep1abcd1243d Port: 3306 Connect_retry: 60 Enabled_ssl: 0 Ssl_ca: Ssl_capath: Ssl_cert: Ssl_cipher: Ssl_key: Ssl_verify_server_cert: 0 Heartbeat: 1800 Bind: Ignored_server_ids: 0 Uuid: 0c130d47-22bb-11e4-aaaa-000c2986ac80 Retry_count: 86400 Ssl_crl: Ssl_crlpath: Enabled_auto_position: 0 1 row in set (0.13 sec) 备注：这里密码 User_password 显示为明文，感觉不是很合适，应该加密下。 三 Relay log info log relay log info log 文件记录了备节点应用 relay log 文件的进度情况，目前版本这个文件可以写到 MySQL 系统表中，但需要设置以下参数 设置备节点 my.cnf1relay_log_info_repository=TABLE 备注：之后重启 mysql 服务 查看 slave_relay_log_info 表 root@localhost:mysql&gt;select * from slave_relay_log_infoG * 1. row * Number_of_lines: 7 Relay_log_name: ./db2-relay-bin.000064 Relay_log_pos: 281 Master_log_name: bin-log.000040 Master_log_pos: 120 Sql_delay: 0 Number_of_workers: 0 Id: 1 1 row in set (0.00 sec) 四 参考 Replication Relay and Status Logs The Slave Relay Log","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：Replication 实现原理","slug":"20141208155857","date":"2014-12-08T07:58:57.000Z","updated":"2018-09-04T01:34:15.048Z","comments":true,"path":"20141208155857.html","link":"","permalink":"https://postgres.fun/20141208155857.html","excerpt":"","text":"之前介绍了主从搭建，具体参考 MySQL：主从复制(Replication)搭建 ，接下来简单介绍 MySQL 主从复制原理。 MySQL 主从复制基于 binary log，这个文件类似于 Oracle 的 redo 和 PostgreSQL WAL 日志文件，用来记录数据库的变化( update, insert, delete 等)，但 SELECT 一般不记录到日志里。 MySQL 主从复制的三个线程 Binlog dump thread这个线程位于主节点上，用来发送变化的 binary log 到备节点。 Slave I/O thread这个线程位于备节点上， 作用是与主节点保持连接，并且接收主节点发送过来的变化的 binary log 到本地。 Slave SQL thread这个线程位于备节点，作用是读取备节点的 relay log 并且在备节点上重做日志里的事件。 主节点查看: Binlog dump thread1234567891011root@localhost:francs&gt;show processlistG * 2. row * Id: 1493 User: rep1 Host: db2:20976 db: NULL Command: Binlog Dump Time: 1491 State: Master has sent all binlog to slave; waiting for binlog to be updated Info: NULL 2 rows in set (0.00 sec) 备注：主节点状态信息具体查看：Replication Master Thread States 备节点查看: Slave I/O thread 和 Slave SQL thread123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657root@localhost:francs&gt;show slave statusG * 1. row * Slave_IO_State: Waiting for master to send event Master_Host: 192.168.2.37 Master_User: rep1 Master_Port: 3306 Connect_Retry: 60 Master_Log_File: bin-log.000038 Read_Master_Log_Pos: 604 Relay_Log_File: db2-relay-bin.000054 Relay_Log_Pos: 281 Relay_Master_Log_File: bin-log.000038 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 604 Relay_Log_Space: 855 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: 0c130d47-22bb-11e4-aaaa-000c2986ac80 Master_Info_File: /database/mysql/data/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 1 row in set (0.00 sec) 备注： 查看 Slave_IO_Running: Yes 和 Slave_SQL_Running: Yes 两项, yes 表示线程正常。Slave_IO_State 表示 I/O THREAD 状态，详细信息参考 Replication Slave I/O Thread States, Slave_SQL_Running_State 表示 SQL thread 状态，详细信息参考 Replication Slave SQL Thread States。 备节点的 I/O, SQL 两个线程也通过 show processlist 查看 root@localhost:francs&gt;show processlistG * 2. row * Id: 7 User: system user Host: db: NULL Command: Connect Time: 2055 State: Waiting for master to send event Info: NULL * 3. row * Id: 8 User: system user Host: db: NULL Command: Connect Time: 2038 State: Slave has read all relay log; waiting for the slave I/O thread to update it Info: NULL 3 rows in set (0.00 sec) 四 参考 Replication Implementation Details 主从复制(Replication)搭建","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL Replication","slug":"MySQL-Replication","permalink":"https://postgres.fun/tags/MySQL-Replication/"}]},{"title":"MySQL：动态开启慢查询日志(Slow Query Log)","slug":"20141206210210","date":"2014-12-06T13:02:10.000Z","updated":"2018-09-04T01:34:14.985Z","comments":true,"path":"20141206210210.html","link":"","permalink":"https://postgres.fun/20141206210210.html","excerpt":"","text":"MySQL 提供单独的日志文件记录慢查询，这个文件默认文件名为 host_name-slow.log, 但可以通过初始化参数进行配置，这里记录下临时开启慢查询的方法。 一 在日志文件中开启慢查询查看 slow_query_log 参数1234567root@localhost:francs&gt;show variables like '%slow_query_log%'; +---------------------+-----------------------------------+ | Variable_name | Value | +---------------------+-----------------------------------+ | slow_query_log | OFF | | slow_query_log_file | /database/mysql/data/db1-slow.log | +---------------------+-----------------------------------+ 备注： slow_query_log 参数控制慢查询功能是否打开，值为on (1)/off(0), slow_query_log_file 参数为数据库慢查询日志文件名。 查看 long_query_time123456root@localhost:francs&gt;show variables like '%long_query_time%'; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 备注： SQL 语句运行超过 long_query_time 设定的值时，语句将被记录到慢查询日志中 ，此参数单位为秒。 设置慢查询12345root@localhost:francs&gt;set global slow_query_log=on; Query OK, 0 rows affected (0.05 sec) root@localhost:francs&gt;set global long_query_time=1; Query OK, 0 rows affected (0.00 sec) 验证12345678910111213141516root@localhost:francs&gt;show variables like '%slow_query_log%'; +---------------------+-----------------------------------+ | Variable_name | Value | +---------------------+-----------------------------------+ | slow_query_log | ON | | slow_query_log_file | /database/mysql/data/db1-slow.log | +---------------------+-----------------------------------+ 2 rows in set (0.00 sec) root@localhost:francs&gt;show variables like '%long_query_time%'; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 1 row in set (0.00 sec) 备注： 发现 long_query_time 的值没做相应的改变，网上查了下资料说是退出当前会话，重连数据库时生效。退出当前会话并重连123456789101112131415161718192021[mysql@db1 ~]$ mysql Welcome to the MySQL monitor. Commands end with ; or g. Your MySQL connection id is 47 Server version: 5.6.20 Source distribution Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or 'h' for help. Type 'c' to clear the current input statement. root@localhost:(none)&gt;show variables like '%long_query_time%'; +-----------------+----------+ | Variable_name | Value | +-----------------+----------+ | long_query_time | 1.000000 | +-----------------+----------+ 1 row in set (0.00 sec)备注：重新登陆后， long_query_time 才生效。 验证 ，开启session1 执行以下：1234567root@localhost:(none)&gt;select sleep(1); +----------+ | sleep(1) | +----------+ | 0 | +----------+ 1 row in set (1.00 sec) 查看慢查询日志1234567[mysql@db1 data]$ tail -f db1-slow.log .... # Time: 140818 10:50:17 # User@Host: root[root] @ localhost [] Id: 47 # Query_time: 1.000651 Lock_time: 0.000000 Rows_sent: 1 Rows_examined: 0 SET timestamp=1408330217; select sleep(1); 备注：慢查询日志也可以记录到数据库表中。 二 在数据库表中开启慢查询设置 log_output12345678910root@localhost:mysql&gt;show variables like '%log_output%'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | log_output | FILE | +---------------+-------+ 1 row in set (0.00 sec) root@localhost:mysql&gt;set global log_output='FILE,TABLE'; Query OK, 0 rows affected (0.00 sec) 备注： log_output 参数设定日志文件的输出，可选值为 TABLE, FILE ,NONE; “TABLE” 意思为设定日志分别记录到 mysql 库的 general_log 和 slow_log 表中; “FILE” 意思为记录日志到操作系统的文件中， “NONE” 意思为取消日志记录。 验证，执行以下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748root@localhost:mysql&gt;show variables like '%log_output%'; +---------------+------------+ | Variable_name | Value | +---------------+------------+ | log_output | FILE,TABLE | +---------------+------------+ 1 row in set (0.00 sec) root@localhost:mysql&gt;select count(*) from slow_log; +----------+ | count(*) | +----------+ | 0 | +----------+ 1 row in set (0.00 sec) root@localhost:mysql&gt;select sleep(1); +----------+ | sleep(1) | +----------+ | 0 | +----------+ 1 row in set (1.00 sec) root@localhost:mysql&gt;select count(*) from slow_log; +----------+ | count(*) | +----------+ | 1 | +----------+ 1 row in set (0.00 sec) root@localhost:mysql&gt;select * from slow_logG * 1. row * start_time: 2014-08-18 11:21:39 user_host: root[root] @ localhost [] query_time: 00:00:01 lock_time: 00:00:00 rows_sent: 1 rows_examined: 0 db: mysql last_insert_id: 0 insert_id: 0 server_id: 0 sql_text: select sleep(1) thread_id: 47 1 row in set (0.00 sec) 备注： slow_log 表中果然有了一条 sleep(1) 慢查询日志，也可以将慢查询日志参数设置到 my.cnf 配置文件中，重启数据库重效。 三 参考 The Slow Query Log","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：Show Slave Hosts","slug":"20141206193336","date":"2014-12-06T11:33:36.000Z","updated":"2018-09-04T01:34:14.922Z","comments":true,"path":"20141206193336.html","link":"","permalink":"https://postgres.fun/20141206193336.html","excerpt":"","text":"上一节 MySQL：主从复制(Replication)搭建 介绍了主从搭建， 接下来看下 MySQL: show slave hosts 命令。 主库上执行12345678root@localhost:mysql&gt;show slave hostsG * 1. row * Server_id: 2 Host: Port: 3306 Master_id: 1 Slave_UUID: ad397a06-7c56-11e4-b2fb-000c29dcb3b5 1 row in set (0.00 sec) 备注： show slave hosts 命令显示备库信息，每一行显示一个备库信息，但这里 HOST 字段显示为空，需要设置备库的 report-host 参数才能显示 HOST 值。 修改备库 /etc/my.cnf修改备库 /etc/my.cnf ，添加以下:1report-host=192.168.2.38(db2) 重启备库123[root@db2 init.d]# /etc/init.d/mysqld restart Shutting down MySQL.... [OK ] Starting MySQL................ [OK ] 主库上验证12345678root@localhost:mysql&gt;show slave hostsG * 1. row * Server_id: 2 Host: 192.168.2.38(db2) Port: 3306 Master_id: 1 Slave_UUID: ad397a06-7c56-11e4-b2fb-000c29dcb3b5 1 row in set (0.00 sec) 备注：字段意思很清楚了。 详见： SHOW SLAVE HOSTS 参考 MySQL：主从复制(Replication)搭建 SHOW SLAVE HOSTS","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL Replication","slug":"MySQL-Replication","permalink":"https://postgres.fun/tags/MySQL-Replication/"}]},{"title":"MySQL：主从复制(Replication)搭建","slug":"20141206141543","date":"2014-12-06T06:15:43.000Z","updated":"2018-09-04T01:34:14.860Z","comments":true,"path":"20141206141543.html","link":"","permalink":"https://postgres.fun/20141206141543.html","excerpt":"","text":"今天阅读了 MySQL 复制相关的内容，MySQL 的复制比较灵活，手册上介绍说可以实现基于实例级，数据库级，表级别的复制， 今天实验的是实例级主从复制。 一 环境信息主库 192.168.2.37/3306备库 192.168.2.38/3306版本 5.6.20备注： 主备节点 mysql 安装略。 二 主节点配置修改主库 /etc/my.cnf 增加以下参数12server_id = 1 log_bin = \"/database/mysql/data/binlog/bin-log\" 重启 MySQL1[root@db1 ~]# service mysql restart 创建复制用户12345root@localhost:(none)&gt;create user 'rep1'@'192.168.2.%' identified by 'rep1abcd1243d'; Query OK, 0 rows affected (0.02 sec) root@localhost:(none)&gt;GRANT REPLICATION SLAVE ON *.* TO 'rep1'@'192.168.2.%'; Query OK, 0 rows affected (0.01 sec) 备注: 创建新用户并赋予 REPLICATION SLAVE 权限，这个用户用来读取主节点的 binary log. 开启 session1: 锁定数据库所有表12root@localhost:(none)&gt;flush tables with read lock; Query OK, 0 rows affected (0.08 sec) 开启 sesson2: 查看 binary 信息1234567root@localhost:(none)&gt;show master status; +----------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +----------------+----------+--------------+------------------+-------------------+ | bin-log.000023 | 552 | | | | +----------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 备注：记住这些信息，备节点配置时要用到。 复制数据并传到从库对应目录这里使用 tar 将目录 /database/mysql/data 拷贝到备节点。 主节点解锁12root@localhost:(none)&gt;unlock tables; Query OK, 0 rows affected (0.00 sec) 三 从节点配置修改备库 /etc/my.cnf 增加以下参数1server_id = 2 重启 MySQL1[root@db2 ~]# service mysql restart 设置主节点信息12345678910CHANGE MASTER TO MASTER_HOST='192.168.2.37', MASTER_PORT=3306, MASTER_USER='rep1', MASTER_PASSWORD='rep1abcd1243d', MASTER_LOG_FILE='bin-log.000023', MASTER_LOG_POS=552; mysql&gt; start slave; Query OK, 0 rows affected, 1 warning (0.00 sec) 发现日志报错1232014-09-15 15:23:08 8699 [Note] Slave I/O thread: connected to master 'rep1@192.168.2.37:3306',replication started in log 'bin-log.000023' at position 552 2014-09-15 15:23:08 8699 [ERROR] Slave I/O: Fatal error: The slave I/O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work. Error_code: 1593 2014-09-15 15:23:08 8699 [Note] Slave I/O thread exiting, read up to log 'bin-log.000023', position 552 备注：这是因为在复制主节点数据目录 /database/mysql/data 时，将文件 auto.cnf 也复制过来了，这时只需要修改备节点的 auto.cnf 文件，参照格式修改 server-uuid 即可。 修改后的 auto.cnf123[mysql@db2 data]$ cat auto.cnf [auto] server-uuid=0c130d48-22bb-11e4-aaaa-000c2986ac80 备注：修改后重启数据库，日志没有出现异常信息。 查看备节点详细信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657mysql&gt; show slave statusG * 1. row * Slave_IO_State: Waiting for master to send event Master_Host: 192.168.2.37 Master_User: rep1 Master_Port: 3306 Connect_Retry: 60 Master_Log_File: bin-log.000023 Read_Master_Log_Pos: 552 Relay_Log_File: db2-relay-bin.000003 Relay_Log_Pos: 281 Relay_Master_Log_File: bin-log.000023 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 552 Relay_Log_Space: 452 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: 0c130d47-22bb-11e4-aaaa-000c2986ac80 Master_Info_File: /database/mysql/data/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for the slave I/O thread to update it Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 1 row in set (0.01 sec) 四 验证主节点创建表123456789101112root@localhost:francs&gt;show tables; +------------------+ | Tables_in_francs | +------------------+ | test_1 | | test_2 | | test_3 | | test_4 | +------------------+ root@localhost:francs&gt;create table test_5 as select * from test_1; Query OK, 10001 rows affected (0.38 sec) Records: 10001 Duplicates: 0 Warnings: 0 备节点验证1234567891011mysql&gt; show tables; +------------------+ | Tables_in_francs | +------------------+ | test_1 | | test_2 | | test_3 | | test_4 | | test_5 | +------------------+ 5 rows in set (0.00 sec) 备注： 验证成功。 五 参考 How to Set Up Replication MySQL：主从切换 六 相关链接 MySQL5.7: 半同步复制（Semisynchronous Replication）配置","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL Replication","slug":"MySQL-Replication","permalink":"https://postgres.fun/tags/MySQL-Replication/"}]},{"title":"MySQL：my.cnf 读取优先级","slug":"20141205175218","date":"2014-12-05T09:52:18.000Z","updated":"2018-09-04T01:34:14.797Z","comments":true,"path":"20141205175218.html","link":"","permalink":"https://postgres.fun/20141205175218.html","excerpt":"","text":"之前对 MySQL 的配置文件选取优先级一直感到困惑，因为之前一直用 /etc/my.cnf 作为配置文件，今天读取手册，刚好看到配置文件my.cnf 读取优先级的问题，记录下备忘。 手册中的解释 On Unix, Linux and Mac OS X, MySQL programs read startup options from the following files, in the specified order (top items are used first). File Name Purpose /etc/my.cnf Global options /etc/mysql/my.cnf Global options SYSCONFDIR/my.cnf Global options $MYSQL_HOME/my.cnf Server-specific options defaults-extra-file The file specified with –defaults-extra-file=path, if any ~/.my.cnf User-specific options ~/.mylogin.cnf Login path options 备注：今天将配置文 $MYSQL_HOME/my.cnf， /etc/my.cnf 各设置一份，发现参数还是以 $MYSQL_HOME/my.cnf 为准，并没有按文档中的优先顺序，感觉 mysql 服务将配置文件从上到下依次都了一遍，然后将之前读的配置覆盖了，很奇怪。 参考 4.2.6 Using Option Files","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：分区表之二：LIST 分区","slug":"20141205143605","date":"2014-12-05T06:36:05.000Z","updated":"2018-09-04T01:34:14.735Z","comments":true,"path":"20141205143605.html","link":"","permalink":"https://postgres.fun/20141205143605.html","excerpt":"","text":"上篇 blog 介绍了 MySQL 中的 RANGE 分区， MySQL: 分区表之一: RANGE 分区, 接下来简单了解下 LIST 分区，例子如下： 创建表1234567891011create table user_info ( user_id int8 primary key, user_name varchar(64), create_time timestamp ) PARTITION BY LIST ( mod(user_id,4)) ( PARTITION user_info_p0 VALUES IN (0), PARTITION user_info_p1 VALUES IN (1), PARTITION user_info_p2 VALUES IN (2), PARTITION user_info_p3 VALUES IN (3) ); 批量插入数据存储过程12345678910111213DELIMITER // CREATE PROCEDURE pro_ins_user_info() BEGIN DECLARE i INT; SET i = 1; WHILE i &lt;= 10000 DO insert into user_info(user_id,user_name,create_time ) values(i,CONCAT('user_',i),current_timestamp); SET i = i + 1; END WHILE; END // DELIMITER ; 执行函数12root@localhost:francs&gt;call pro_ins_user_info() ; Query OK, 1 row affected (17.19 sec) 查看分区表数据分布情况12345678910root@localhost:francs&gt; select table_schema,table_name,partition_name,PARTITION_METHOD from information_schema.partitions where table_name='user_info'; +--------------+------------+----------------+------------------+ | table_schema | table_name | partition_name | PARTITION_METHOD | +--------------+------------+----------------+------------------+ | francs | user_info | user_info_p0 | LIST | | francs | user_info | user_info_p1 | LIST | | francs | user_info | user_info_p2 | LIST | | francs | user_info | user_info_p3 | LIST | +--------------+------------+----------------+------------------+ 4 rows in set (0.07 sec) 查看各分区数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071root@localhost:francs&gt;select count(*) from user_info PARTITION (user_info_p0); +----------+ | count(*) | +----------+ | 2500 | +----------+ 1 row in set (0.01 sec) root@localhost:francs&gt;select count(*) from user_info PARTITION (user_info_p1); +----------+ | count(*) | +----------+ | 2500 | +----------+ 1 row in set (0.00 sec) root@localhost:francs&gt;select count(*) from user_info PARTITION (user_info_p2); +----------+ | count(*) | +----------+ | 2500 | +----------+ 1 row in set (0.00 sec) root@localhost:francs&gt;select count(*) from user_info PARTITION (user_info_p3); +----------+ | count(*) | +----------+ | 2500 | +----------+ 1 row in set (0.01 sec) root@localhost:francs&gt;select * from user_info PARTITION (user_info_p0) limit 3; +---------+-----------+---------------------+ | user_id | user_name | create_time | +---------+-----------+---------------------+ | 4 | user_4 | 2014-12-03 13:38:11 | | 8 | user_8 | 2014-12-03 13:38:11 | | 12 | user_12 | 2014-12-03 13:38:11 | +---------+-----------+---------------------+ 3 rows in set (0.00 sec) root@localhost:francs&gt;select * from user_info PARTITION (user_info_p1) limit 3; +---------+-----------+---------------------+ | user_id | user_name | create_time | +---------+-----------+---------------------+ | 1 | user_1 | 2014-12-03 13:38:11 | | 5 | user_5 | 2014-12-03 13:38:11 | | 9 | user_9 | 2014-12-03 13:38:11 | +---------+-----------+---------------------+ 3 rows in set (0.00 sec) root@localhost:francs&gt;select * from user_info PARTITION (user_info_p2) limit 3; +---------+-----------+---------------------+ | user_id | user_name | create_time | +---------+-----------+---------------------+ | 2 | user_2 | 2014-12-03 13:38:11 | | 6 | user_6 | 2014-12-03 13:38:11 | | 10 | user_10 | 2014-12-03 13:38:11 | +---------+-----------+---------------------+ 3 rows in set (0.00 sec) root@localhost:francs&gt;select * from user_info PARTITION (user_info_p3) limit 3; +---------+-----------+---------------------+ | user_id | user_name | create_time | +---------+-----------+---------------------+ | 3 | user_3 | 2014-12-03 13:38:11 | | 7 | user_7 | 2014-12-03 13:38:11 | | 11 | user_11 | 2014-12-03 13:38:11 | +---------+-----------+---------------------+ 3 rows in set (0.00 sec) 查看执行计划根据分区键 user_idd 查询1234567891011121314francs@localhost:francs&gt;explain partitions select * from user_info where user_id=1G * 1. row * id: 1 select_type: SIMPLE table: user_info partitions: user_info_p1 type: const possible_keys: PRIMARY key: PRIMARY key_len: 8 ref: const rows: 1 Extra: NULL 1 row in set (0.06 sec) 根据非分区键 user_name 查询1234567891011121314francs@localhost:francs&gt;explain partitions select * from user_info where user_name='user_1'G * 1. row * id: 1 select_type: SIMPLE table: user_info partitions: user_info_p0,user_info_p1,user_info_p2,user_info_p3 type: ALL possible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 10000 Extra: Using where 1 row in set (0.00 sec) 备注：根据分区键查询走了分区 user_info_p1，而根据非分区键查询走了所有分区。 参考 LIST Partitioning LIST COLUMNS partitioning MySQL: 分区表之一: RANGE 分区","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：分区表之一：RANGE 分区","slug":"20141205141942","date":"2014-12-05T06:19:42.000Z","updated":"2018-09-04T01:34:14.672Z","comments":true,"path":"20141205141942.html","link":"","permalink":"https://postgres.fun/20141205141942.html","excerpt":"","text":"MySQL 作为开源数据库已经支持分区表，而 PostgreSQL 在分区表方面本身做得不是很好，虽然可以通过继承实现表分区功能。接下来打算了解 MySQL 分区表相关的内容。 MySQL 分区表有 RANGE, LIST, HASH, KEY 四种类型, 本文介绍 RANGE 分区类型。 一 查看是否支持分区 首先查看所安装的 MySQL 数据库是否分区，有两种方法，第一种是通过 “show plugins;” 命令查看，另一种是通过以下 SQL 查看：1234root@localhost:(none)&gt;use information_schema select plugin_name,plugin_version,plugin_status ,plugin_type from plugins where plugin_type='STORAGE ENGINE'; 如图 备注：如果以上结果没有 partition 为 active 的信息，则当前 MySQL 数据库不支持分区。 二 RANGE 分区举例: 通过 RANGE COLUMNS 分区方式建表1234567891011create table tbl_access_log ( id bigint, name varchar(64), create_time datetime ) PARTITION BY RANGE COLUMNS (create_time) ( PARTITION p201410 values less than ('2014-11-01 00:00:00'), PARTITION p201411 values less than ('2014-12-01 00:00:00'), PARTITION p201412 values less than ('2015-01-01 00:00:00'), PARTITION pmax values less than maxvalue ); 查看分区12345678910root@localhost:information_schema&gt;select table_schema,table_name,partition_name,PARTITION_METHOD from information_schema.partitions where table_name='tbl_access_log'; +--------------+----------------+----------------+------------------+ | table_schema | table_name | partition_name | PARTITION_METHOD | +--------------+----------------+----------------+------------------+ | francs | tbl_access_log | p201410 | RANGE COLUMNS | | francs | tbl_access_log | p201411 | RANGE COLUMNS | | francs | tbl_access_log | p201412 | RANGE COLUMNS | | francs | tbl_access_log | pmax | RANGE COLUMNS | +--------------+----------------+----------------+------------------+ 4 rows in set (0.01 sec) 插入数据测试12345678910111213141516171819202122root@localhost:francs&gt;insert into tbl_access_log values(1,'a','2014-10-01 12:00:00'); Query OK, 1 row affected (0.06 sec) root@localhost:francs&gt;insert into tbl_access_log values(1,'a','2014-10-01 13:00:00'); Query OK, 1 row affected (0.06 sec) root@localhost:francs&gt;insert into tbl_access_log values(1,'a','2014-11-01 13:00:00'); Query OK, 1 row affected (0.00 sec) root@localhost:francs&gt;insert into tbl_access_log values(1,'a','2015-01-01 13:00:00'); Query OK, 1 row affected (0.00 sec) root@localhost:francs&gt;select * from tbl_access_log; +------+------+---------------------+ | id | name | create_time | +------+------+---------------------+ | 1 | a | 2014-10-01 12:00:00 | | 1 | a | 2014-10-01 13:00:00 | | 1 | a | 2014-11-01 13:00:00 | | 1 | a | 2015-01-01 13:00:00 | +------+------+---------------------+ 4 rows in set (0.00 sec) 查询指定分区123456789101112131415161718192021222324root@localhost:francs&gt;select * from tbl_access_log PARTITION ( p201410); +------+------+---------------------+ | id | name | create_time | +------+------+---------------------+ | 1 | a | 2014-10-01 12:00:00 | | 1 | a | 2014-10-01 13:00:00 | +------+------+---------------------+ 2 rows in set (0.00 sec) root@localhost:francs&gt;select * from tbl_access_log PARTITION (p201411); +------+------+---------------------+ | id | name | create_time | +------+------+---------------------+ | 1 | a | 2014-11-01 13:00:00 | +------+------+---------------------+ 1 row in set (0.00 sec) root@localhost:francs&gt;select * from tbl_access_log PARTITION (pmax); +------+------+---------------------+ | id | name | create_time | +------+------+---------------------+ | 1 | a | 2015-01-01 13:00:00 | +------+------+---------------------+ 1 row in set (0.00 sec) 三 RANGE 分区举例: 通过 RANGE 分区方式建表1234567891011create table tbl_access_log_unix ( id bigint, name varchar(64), create_time TIMESTAMP ) PARTITION BY RANGE ( UNIX_TIMESTAMP(create_time) ) ( PARTITION p201410 values less than (UNIX_TIMESTAMP('2014-11-01 00:00:00')), PARTITION p201411 values less than (UNIX_TIMESTAMP('2014-12-01 00:00:00')), PARTITION p201412 values less than (UNIX_TIMESTAMP('2015-01-01 00:00:00')), PARTITION pmax values less than (maxvalue) ); 查看分区类型12345678910root@localhost:information_schema&gt;select table_schema,table_name,partition_name,PARTITION_METHOD from partitions where table_name='tbl_access_log_unix'; +--------------+---------------------+----------------+------------------+ | table_schema | table_name | partition_name | PARTITION_METHOD | +--------------+---------------------+----------------+------------------+ | francs | tbl_access_log_unix | p201410 | RANGE | | francs | tbl_access_log_unix | p201411 | RANGE | | francs | tbl_access_log_unix | p201412 | RANGE | | francs | tbl_access_log_unix | pmax | RANGE | +--------------+---------------------+----------------+------------------+ 4 rows in set (0.02 sec) 备注：以上介绍了两种 RANGE 分区例子。第一种是 RANGE COLUMNS partitioning 方式，第二种是 RANGE partitioning 方式，两种方式不同，主要区别是RANGE COLUMNS partitioning 方式可以基于多个字段分区，同时支持整型以外的字段分区。 四 参考 RANGE Partitioning RANGE COLUMNS partitioning","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：查看版本的方法","slug":"20141204135647","date":"2014-12-04T05:56:47.000Z","updated":"2018-09-04T01:34:14.610Z","comments":true,"path":"20141204135647.html","link":"","permalink":"https://postgres.fun/20141204135647.html","excerpt":"","text":"MySQL 查看版本的方法很多，目前找到以下几种，如下： 方法一: 使用 mysql 命令查看12[mysql@db1 data]$ mysql --version mysql Ver 14.14 Distrib 5.6.20, for Linux (i686) using EditLine wrapper 方法二: 登录的时候查看123456789101112[mysql@db1 data]$ mysql Welcome to the MySQL monitor. Commands end with ; or g. Your MySQL connection id is 805 Server version: 5.6.20-log Source distribution Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or 'h' for help. Type 'c' to clear the current input statement. 备注：接连接数据库，开始的提示信息有版本信息 方法三: 使用函数查看1234567root@localhost:(none)&gt;select version(); +------------+ | version() | +------------+ | 5.6.20-log | +------------+ 1 row in set (0.00 sec) 方法四: Status 命令1234567891011121314151617181920212223root@localhost:(none)&gt;status -------------- mysql Ver 14.14 Distrib 5.6.20, for Linux (i686) using EditLine wrapper Connection id: 805 Current database: Current user: root@localhost SSL: Not in use Current pager: stdout Using outfile: '' Using delimiter: ; Server version: 5.6.20-log Source distribution Protocol version: 10 Connection: Localhost via UNIX socket Server characterset: utf8 Db characterset: utf8 Client characterset: utf8 Conn. characterset: utf8 UNIX socket: /tmp/mysql.sock Uptime: 20 hours 36 min 32 sec Threads: 3 Questions: 336 Slow queries: 0 Opens: 93 Flush tables: 1 Open tables: 86 Queries per second avg: 0.004 ------------","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"Redis：常用命令","slug":"20141204112943","date":"2014-12-04T03:29:43.000Z","updated":"2018-09-04T01:34:14.547Z","comments":true,"path":"20141204112943.html","link":"","permalink":"https://postgres.fun/20141204112943.html","excerpt":"","text":"以下简单记录 redis 常用命令，持续补充。 set : 设置 key 值123[redis@db1 redis]$ redis-cli 127.0.0.1:6379&gt; set a 1 OK get: 获取 key 的值12127.0.0.1:6379&gt; get a \"1\" help 帮助命令123456127.0.0.1:6379&gt; help set SET key value [EX seconds] [PX milliseconds] [NX|XX] summary: Set the string value of a key since: 1.0.0 group: string set : 设置 key 过期时间1234567EX seconds -- Set the specified expire time, in seconds. 127.0.0.1:6379&gt; set a 1 EX 5 OK 127.0.0.1:6379&gt; get a \"1\" 127.0.0.1:6379&gt; get a (nil) 备注： 以上设置 expire 时间为 5 秒。 del: 删除 key 值12345678127.0.0.1:6379&gt; set a 1 OK 127.0.0.1:6379&gt; get a \"1\" 127.0.0.1:6379&gt; del a (integer) 1 127.0.0.1:6379&gt; get a (nil) TTL: 查看 key 值剩余生存时间，单位 秒123456127.0.0.1:6379&gt; set a 1 EX 5 OK 127.0.0.1:6379&gt; ttl a (integer) 4 127.0.0.1:6379&gt; ttl a (integer) 3 备注： 两个特殊的返回值: -2 表示 key 不存在， -1 表示 key 存在但没有设置过期时间。 PERSIST: 删除 key 的过期时间12345678910127.0.0.1:6379&gt; set a 1 OK 127.0.0.1:6379&gt; expire a 100 (integer) 1 127.0.0.1:6379&gt; ttl a (integer) 98 127.0.0.1:6379&gt; persist a (integer) 1 127.0.0.1:6379&gt; persist a (integer) 0 备注: PERSIST 返回值如下： 1 表示 key 的 timeout 被删除, 0 表示 key 不存在或者 key 没设 timeout 值。 MSET: 设置多个 keys123456127.0.0.1:6379&gt; mset d 5 e 6 OK 127.0.0.1:6379&gt; get d \"5\" 127.0.0.1:6379&gt; get e \"6\" exists: 判断 key 是否存在1234567127.0.0.1:6379&gt; set a 1 OK 127.0.0.1:6379&gt; exists a (integer) 1 127.0.0.1:6379&gt; exists aa (integer) 0备注: 返回 1 表示存在， 0 表示不存在。 参考 redis commands","categories":[{"name":"Redis","slug":"Redis","permalink":"https://postgres.fun/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://postgres.fun/tags/Redis/"}]},{"title":"MySQL：Mysqldump 初步使用","slug":"20141203175435","date":"2014-12-03T09:54:35.000Z","updated":"2018-09-04T01:34:14.485Z","comments":true,"path":"20141203175435.html","link":"","permalink":"https://postgres.fun/20141203175435.html","excerpt":"","text":"mysqldump 是 mysql 的逻辑备份工具，可以逻辑导出某个时间点数据库内容，导出的格式可以为文本, csv ,或 XML 等格式，内容很多，这里记录些例子备忘。 基本语法12345NAME mysqldump - a database backup program SYNOPSIS mysqldump [options] [db_name [tbl_name ...]] 环境信息1234567891011[mysql@db1 ~]$ mysql -u root francs root@localhost:francs&gt;show tables; +------------------+ | Tables_in_francs | +------------------+ | test_1 | | test_2 | | test_3 | +------------------+ 3 rows in set (0.00 sec) 导出数据库 francs (包括表结构和数据)1[mysql@db1 tf]$ mysqldump -h 127.0.0.1 -u root -v francs &gt; francs.sql 仅导出数据库 francs 的结构1[mysql@db1 tf]$ mysqldump -h 127.0.0.1 -u root -d -v francs &gt; francs.sql 备注： -d 表示不导出数据。 导出单张表1[mysql@db1 tf]$ mysqldump -h 127.0.0.1 -u root -v francs test_1 &gt; test_1.sql 导出多张表1[mysql@db1 tf]$ mysqldump -h 127.0.0.1 -u root -v francs test_1 test_2 &gt; data.sql 去掉脚本中的一些特性1[mysql@db1 tf]$ mysqldump -h 127.0.0.1 -u root -v --skip-opt francs test_1 &gt; test_1.sql 备注: –opt 包括以下属性： –add-drop-table –add-locks –create-options –disable-keys –extended-insert –lock-tables –quick –set-charset。 带 where 条件的导出1mysqldump -h 127.0.0.1 -u root -w \" id &lt; 5\" -v francs test_1 &gt; test_1.sql 备注: -w 表示 where 带条件。也可以用 –where=’where_condition’ 表示，如果 where 条件中有单引号，可以如下表示 –where=”user=’jimf’” 参考 mysqldump ― A Database Backup Program","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：创建只读帐号","slug":"20141203151820","date":"2014-12-03T07:18:20.000Z","updated":"2018-09-04T01:34:14.422Z","comments":true,"path":"20141203151820.html","link":"","permalink":"https://postgres.fun/20141203151820.html","excerpt":"","text":"今天看了帐号相关的内容，生产库中经常会有申请查询帐号的情况，这里演示开通查询帐号，非常简单。 环境信息1234567891011121314151617root@localhost:francs&gt;select database(); +------------+ | database() | +------------+ | francs | +------------+ 1 row in set (0.00 sec) root@localhost:francs&gt;show tables; +------------------+ | Tables_in_francs | +------------------+ | test_1 | | test_2 | | test_3 | +------------------+ 3 rows in set (0.00 sec) 备注： francs 库中有三张表，现需要开通一查询帐号，仅具有对 francs 库表 test_1 的查询权限。 创建帐号12root@localhost:(none)&gt;grant select on francs.test_1 to 'francs_select'@'192.168.2.%' identified by '123456'; Query OK, 0 rows affected (0.11 sec) 备注：创建帐号和赋权可以一条命令完成， 在 MySQL 中定义用户和其它数据库中不同，格式为 ‘user_name‘@’host_name’， user_name 为用户名， host_name 为客户端 IP，也可以先创建帐号，再赋权。 客户端验证客户端 IP 192.168.2.1123456789101112131415161718192021222324252627francs@francs:~$ mysql -ufrancs_select -D francs -p Enter password: Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Welcome to the MySQL monitor. Commands end with ; or g. Your MySQL connection id is 18 Server version: 5.6.20-log Source distribution Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or 'h' for help. Type 'c' to clear the current input statement. mysql&gt; select * from test_1 limit 1; +----+------+ | id | name | +----+------+ | 0 | 0_a | +----+------+ 1 row in set (0.00 sec) mysql&gt; select * from test_2 limit 1; ERROR 1142 (42000): SELECT command denied to user 'francs_select'@'192.168.2.1' for table 'test_2' 备注：francs_select 用户具有对表 test_1 的查询权限，但没有权限查询表 test_2. 查看权限1234567root@localhost:mysql&gt;select * From tables_priv where User='francs_select'; +-------------+--------+---------------+------------+----------------+---------------------+------------+-------------+ | Host | Db | User | Table_name | Grantor | Timestamp | Table_priv | Column_priv | +-------------+--------+---------------+------------+----------------+---------------------+------------+-------------+ | 192.168.2.% | francs | francs_select | test_1 | root@localhost | 0000-00-00 00:00:00 | Select | | +-------------+--------+---------------+------------+----------------+---------------------+------------+-------------+ 1 row in set (0.00 sec) 参考 Privileges Provided by MySQL Adding a read-only MySQL user CREATE USER Syntax","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：开启 Binary Logging","slug":"20141202171148","date":"2014-12-02T09:11:48.000Z","updated":"2018-09-04T01:34:14.360Z","comments":true,"path":"20141202171148.html","link":"","permalink":"https://postgres.fun/20141202171148.html","excerpt":"","text":"MySQL 的 Binary Logging 日志文件类似 PostgreSQL 的 WAL 和 Oracle 的 redo ，记录数据库的变化信息，开启 binary logging 比较简单，但仍费了些功夫，记录下。 一 开启 Binary Logging 功能设置 my.cnf123log_bin = on log_bin_basename = \"/database/mysql/data/binlog/bin-log\" binlog_format = \"MIXED\" 重启 mysql 失败123[root@db1 ~]# /etc/init.d/mysql restart Shutting down MySQL.... [OK ] Starting MySQL.....The server quit without updating PID fil[FAILED]base/mysql/data/db1.pid). db1.err 日志12014-08-19 11:09:57 24489 [ERROR] /opt/mysql/bin/mysqld: unknown variable 'log_bin_basename=/database/mysql/data/binlog/bin-log' 备注：日志中说 log_bin_basename 设置不对，文档中也没说这个参数不让设置。http://dev.mysql.com/doc/refman/5.6/en/replication-options-binary-log.html#sysvar_log_bin_basename ，后来查了下资料，说是更改 log_bin 参数成日志绝对路径即可，但文档中说 log_bin 参数值为 on 或 off,感觉很奇怪，先试试。 重新设置 my.cnf123log_bin = \"/database/mysql/data/binlog/bin-log\" binlog_format = \"STATEMENT\" binlog_format = \"MIXED\" 备注：这里没设置 log_bin_basename。而将 log_bin 设置成日志绝对路径。 重启 mysql123[root@db1 ~]# /etc/init.d/mysql restart MySQL server PID file could not be found! [FAILED] Starting MySQL..... [OK ] 备注：这下可以正常启动了。 查看变量12345678910root@localhost:(none)&gt;show variables like 'log_bin%'; +---------------------------------+-------------------------------------------+ | Variable_name | Value | +---------------------------------+-------------------------------------------+ | log_bin | ON | | log_bin_basename | /database/mysql/data/binlog/bin-log | | log_bin_index | /database/mysql/data/binlog/bin-log.index | | log_bin_trust_function_creators | OFF | | log_bin_use_v1_row_events | OFF | +---------------------------------+-------------------------------------------+ 备注：奇怪，这里 log_bin 显示为 on 了，而 log_bin_basename 变量的值变成之前设置的 log_bin 变量值了。 二 清理 Binary Log数据库中查看 binary log1234567891011121314151617181920212223242526272829303132333435363738root@localhost:francs&gt;show binary logs; +----------------+-----------+ | Log_name | File_size | +----------------+-----------+ | bin-log.000005 | 143 | | bin-log.000006 | 143 | | bin-log.000007 | 165 | | bin-log.000008 | 165 | | bin-log.000009 | 723 | | bin-log.000010 | 165 | | bin-log.000011 | 285 | | bin-log.000012 | 459 | | bin-log.000013 | 143 | | bin-log.000014 | 143 | | bin-log.000015 | 143 | | bin-log.000016 | 374 | | bin-log.000017 | 143 | | bin-log.000018 | 418 | | bin-log.000019 | 143 | | bin-log.000020 | 143 | | bin-log.000021 | 143 | | bin-log.000022 | 143 | | bin-log.000023 | 1587 | | bin-log.000024 | 165 | | bin-log.000025 | 165 | | bin-log.000026 | 385 | | bin-log.000027 | 143 | | bin-log.000028 | 143 | | bin-log.000029 | 120 | | bin-log.000030 | 120 | | bin-log.000031 | 120 | | bin-log.000032 | 120 | | bin-log.000033 | 120 | | bin-log.000034 | 120 | | bin-log.000035 | 143 | | bin-log.000036 | 10758 | +----------------+-----------+ 32 rows in set (0.01 sec) purge 语法1234Description: Syntax: PURGE &#123; BINARY | MASTER &#125; LOGS &#123; TO 'log_name' | BEFORE datetime_expr &#125; 清除 binary log123456789101112131415161718192021222324252627282930313233343536root@localhost:francs&gt;purge binary logs to 'bin-log.000010'; Query OK, 0 rows affected (0.11 sec) root@localhost:francs&gt;show binary logs; +----------------+-----------+ | Log_name | File_size | +----------------+-----------+ | bin-log.000010 | 165 | | bin-log.000011 | 285 | | bin-log.000012 | 459 | | bin-log.000013 | 143 | | bin-log.000014 | 143 | | bin-log.000015 | 143 | | bin-log.000016 | 374 | | bin-log.000017 | 143 | | bin-log.000018 | 418 | | bin-log.000019 | 143 | | bin-log.000020 | 143 | | bin-log.000021 | 143 | | bin-log.000022 | 143 | | bin-log.000023 | 1587 | | bin-log.000024 | 165 | | bin-log.000025 | 165 | | bin-log.000026 | 385 | | bin-log.000027 | 143 | | bin-log.000028 | 143 | | bin-log.000029 | 120 | | bin-log.000030 | 120 | | bin-log.000031 | 120 | | bin-log.000032 | 120 | | bin-log.000033 | 120 | | bin-log.000034 | 120 | | bin-log.000035 | 143 | | bin-log.000036 | 10758 | +----------------+-----------+ 27 rows in set (0.00 sec) 备注: 使用 purge binary logs to ‘log_name’ 命令将会清理除 log_name 以外之前的 binary logs, 命令执行后 binlog 目录里的 bin-log.index 文件会被更新。 三 参考 The Binary Log log_bin_basename Mysql Binary Log Tutorial PURGE BINARY LOGS Syntax","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：常用 Show 命令","slug":"20141202150550","date":"2014-12-02T07:05:50.000Z","updated":"2018-09-04T01:34:14.297Z","comments":true,"path":"20141202150550.html","link":"","permalink":"https://postgres.fun/20141202150550.html","excerpt":"","text":"常用 show 命令，持续更新。 参考http://dev.mysql.com/doc/refman/5.6/en/show.html 显示数据库列表1234567891011root@localhost:(none)&gt;show databases; +--------------------+ | Database | +--------------------+ | information_schema | | francs | | mysql | | performance_schema | | test | +--------------------+ 5 rows in set (0.01 sec) 显示当前数据库中表12345678root@localhost:francs&gt;show tables; +------------------+ | Tables_in_francs | +------------------+ | test_1 | | test_2 | +------------------+ 2 rows in set (0.00 sec) 查看表结构12345678root@localhost:francs&gt;desc test_1; +-------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+-------+ | id | int(11) | NO | PRI | NULL | | | name | varchar(32) | YES | MUL | NULL | | +-------+-------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) 查看表上的索引情况123456789101112131415161718192021222324252627282930313233root@localhost:francs&gt;show index from francs.test_1G; * 1. row * Table: test_1 Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: id Collation: A Cardinality: 10231 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment: Index_comment: * 2. row * Table: test_1 Non_unique: 1 Key_name: idx_test_1_name Seq_in_index: 1 Column_name: name Collation: A Cardinality: 10231 Sub_part: NULL Packed: NULL Null: YES Index_type: BTREE Comment: Index_comment: 2 rows in set (0.00 sec) ERROR: No query specified 查看建表 DDL12345678910root@localhost:francs&gt;show create table test_1G * 1. row * Table: test_1 Create Table: CREATE TABLE `test_1` ( `id` int(11) NOT NULL, `name` varchar(32) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_test_1_name` (`name`) USING BTREE ) ENGINE=InnoDB DEFAULT CHARSET=utf8 1 row in set (0.00 sec) 查看表详细信息1234567891011121314151617181920root@localhost:francs&gt;show table statusG * 1. row * Name: test_1 Engine: InnoDB Version: 10 Row_format: Compact Rows: 10231 Avg_row_length: 35 Data_length: 360448 Max_data_length: 0 Index_length: 0 Data_free: 0 Auto_increment: NULL Create_time: 2014-08-15 17:16:30 Update_time: NULL Check_time: NULL Collation: utf8_general_ci Checksum: NULL Create_options: Comment: 查看指定表信息123456789101112131415161718192021222324francs@localhost:francs&gt;show table status from francs like 'tbl_access_log'G * 1. row * Name: tbl_access_log Engine: InnoDB Version: 10 Row_format: Compact Rows: 6 Avg_row_length: 10922 Data_length: 65536 Max_data_length: 0 Index_length: 0 Data_free: 0 Auto_increment: NULL Create_time: NULL Update_time: NULL Check_time: NULL Collation: utf8_general_ci Checksum: NULL Create_options: partitioned Comment: 1 row in set (0.00 sec) ERROR: No query specified 查看存储过程信息1234567891011121314151617181920212223242526root@localhost:francs&gt;show procedure statusG * 1. row * Db: francs Name: proc1 Type: PROCEDURE Definer: francs@localhost Modified: 2014-08-15 14:30:54 Created: 2014-08-15 14:30:54 Security_type: DEFINER Comment: character_set_client: utf8 collation_connection: utf8_general_ci Database Collation: utf8_general_ci * 2. row * Db: francs Name: pro_insert Type: PROCEDURE Definer: francs@localhost Modified: 2014-08-15 14:41:22 Created: 2014-08-15 14:41:22 Security_type: DEFINER Comment: character_set_client: utf8 collation_connection: utf8_general_ci Database Collation: utf8_general_ci 2 rows in set (0.00 sec) 查看存储过程 code123456789101112131415161718root@localhost:francs&gt;show create procedure pro_insertG * 1. row * Procedure: pro_insert sql_mode: STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION Create Procedure: CREATE DEFINER=`francs`@`localhost` PROCEDURE `pro_insert`() BEGIN DECLARE i INT; SET i = 0; WHILE i &lt;= 10000 DO insert into test_1(id, name ) values(i,CONCAT(ID,'_a')); SET i = i + 1; END WHILE; END character_set_client: utf8 collation_connection: utf8_general_ci Database Collation: utf8_general_ci 1 row in set (0.00 sec) 查看系统变量12345678910111213141516171819root@localhost:francs&gt;show variables like 'log_%'; +----------------------------------------+------------------------------+ | Variable_name | Value | +----------------------------------------+------------------------------+ | log_bin | OFF | | log_bin_basename | | | log_bin_index | | | log_bin_trust_function_creators | OFF | | log_bin_use_v1_row_events | OFF | | log_error | /database/mysql/data/db1.err | | log_output | FILE | | log_queries_not_using_indexes | OFF | | log_slave_updates | OFF | | log_slow_admin_statements | OFF | | log_slow_slave_statements | OFF | | log_throttle_queries_not_using_indexes | 0 | | log_warnings | 1 | +----------------------------------------+------------------------------+ 13 rows in set (0.00 sec)","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：批量插入数据","slug":"20141201170747","date":"2014-12-01T09:07:47.000Z","updated":"2018-09-04T01:34:14.235Z","comments":true,"path":"20141201170747.html","link":"","permalink":"https://postgres.fun/20141201170747.html","excerpt":"","text":"不知道 MySQL 有没有类似 PostgreSQL 的 generate_series () 函数，在生成测试数据时非常方便，不需要编写存储过程。这里记录下通过存储过程批量插入数据。 创建测试表1234567891011francs@localhost:francs&gt;create table test_1(id int4 primary key, name ichar(32)); Query OK, 0 rows affected (0.16 sec) francs@localhost:francs&gt;desc test_1; +-------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+-------+ | id | int(11) | NO | PRI | NULL | | | name | ichar(32) | YES | | NULL | | +-------+-------------+------+-----+---------+-------+ 2 rows in set (0.00 sec) 批量插入数据存储过程12345678910111213DELIMITER // CREATE PROCEDURE pro_insert() BEGIN DECLARE i INT; SET i = 0; WHILE i &lt;= 10000 DO insert into test_1(id, name ) values(i,CONCAT(ID,'_a')); SET i = i + 1; END WHILE; END // DELIMITER ; 创建存储过程12345678910111213141516francs@localhost:francs&gt;DELIMITER // francs@localhost:francs&gt;CREATE PROCEDURE pro_insert() -&gt; BEGIN -&gt; DECLARE i INT; -&gt;SET i = 0; -&gt; -&gt;WHILE i &lt;= 10000 DO -&gt;insert into test_1(id, name ) values(i,CONCAT(ID,'_a')); -&gt;SET i = i + 1; -&gt; END WHILE; -&gt; END -&gt; // DELIMITER ; Query OK, 0 rows affected (0.07 sec) francs@localhost:francs&gt; DELIMITER ; 备注： 这里使用了分隔符 delimiter，开始的时候将分隔符由分号改成 //，存储过程执行后，再将分隔符修改回来，稍微有点烦琐，但可以理解。 执行存储过程12francs@localhost:francs&gt;call pro_insert(); Query OK, 1 row affected (16.11 sec) 备注：插入一万条数据需要 16.11 秒，有点慢，与表上有 pk 有关系，同时 与MySQL 的参数没有优化有关。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：Prompt 设置","slug":"20141130143946","date":"2014-11-30T06:39:46.000Z","updated":"2018-09-04T01:34:14.172Z","comments":true,"path":"20141130143946.html","link":"","permalink":"https://postgres.fun/20141130143946.html","excerpt":"","text":"默认情况连接到 mysql 库后不会显示当前连接的数据库名，数据库用户信息，在维护过程中不太方便，好在可以使用 prompt 属性进行设置。 /etc/my.cnf 添加以下12[mysql] prompt=\"u@h:d&gt;\" 备注：设置后重启 MySQL 服务。 u 表示用户名, h 表示主机名， d 表示当前数据库。 重启 MySQL 服务123[root@db1 bin]# /etc/init.d/mysql restart Shutting down MySQL. [OK ] Starting MySQL.. [OK ] 连接测试123456789101112131415161718192021[mysql@db1 tf]$ mysql Welcome to the MySQL monitor. Commands end with ; or g. Your MySQL connection id is 2 Server version: 5.6.20 Source distribution Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or 'h' for help. Type 'c' to clear the current input statement. root@localhost:(none)&gt; root@localhost:(none)&gt;use mysql; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed root@localhost:mysql&gt; 备注：注意上面的这行 , 正是我们在 /etc/my.cnf 设置的格式。 Prompt 更多设置1234567891011121314151617181920212223242526272829303132OptionDescription Option Description\\c A counter that increments for each statement you issue\\D The full current date\\d The default database\\h The server host\\l The current delimiter\\m Minutes of the current time\\n A newline character\\O The current month in three-letter format (Jan, Feb, …)\\o The current month in numeric format\\P am/pm\\p The current TCP/IP port or socket file\\R The current time, in 24-hour military time (0–23)\\r The current time, standard 12-hour time (1–12)\\S Semicolon\\s Seconds of the current time\\t A tab character\\U Your full user_name@host_name account name\\u Your user name\\v The server version\\w The current day of the week in three-letter format (Mon, Tue, …)\\Y The current year, four digits\\y The current year, two digits\\_ A space\\ A space (a space follows the backslash)\\' Single quote\\\" Double quote\\\\ A literal “\\” backslash character\\x x, for any “x” not listed above","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：格式化输出","slug":"20141130121509","date":"2014-11-30T04:15:09.000Z","updated":"2018-09-04T01:34:14.110Z","comments":true,"path":"20141130121509.html","link":"","permalink":"https://postgres.fun/20141130121509.html","excerpt":"","text":"MySQL 客户端工具同样非常人性化 ， 例如当查询结果集很多时，可以设置以列模式显示，还可以调用操作系统的命令显示。 先看一个查询1234567mysql&gt; select * from information_schema.tables where table_name='test_1'; +---------------+--------------+------------+------------+--------+---------+------------+------------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+-------------+------------+-----------------+----------+----------------+---------------+ | TABLE_CATALOG | TABLE_SCHEMA | TABLE_NAME | TABLE_TYPE | ENGINE | VERSION | ROW_FORMAT | TABLE_ROWS | AVG_ROW_LENGTH | DATA_LENGTH | MAX_DATA_LENGTH | INDEX_LENGTH | DATA_FREE | AUTO_INCREMENT | CREATE_TIME | UPDATE_TIME | CHECK_TIME | TABLE_COLLATION | CHECKSUM | CREATE_OPTIONS | TABLE_COMMENT | +---------------+--------------+------------+------------+--------+---------+------------+------------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+-------------+------------+-----------------+----------+----------------+---------------+ | def | francs | test_1 | BASE TABLE | InnoDB | 10 | Compact | 3 | 5461 | 16384 | 0 | 0 | 0 |NULL | 2014-08-14 09:30:23 | NULL | NULL | utf8_general_ci |NULL | | | +---------------+--------------+------------+------------+--------+---------+------------+------------+----------------+-------------+-----------------+--------------+-----------+----------------+---------------------+-------------+------------+-----------------+----------+----------------+---------------+ 1 row in set (0.00 sec) 备注：这个查询很长，不便于观看。 \\G 设置列模式显示123456789101112131415161718192021222324mysql&gt; select * from information_schema.tables where table_name='test_1'\\G * 1. row * TABLE_CATALOG: def TABLE_SCHEMA: francs TABLE_NAME: test_1 TABLE_TYPE: BASE TABLE ENGINE: InnoDB VERSION: 10 ROW_FORMAT: Compact TABLE_ROWS: 3 AVG_ROW_LENGTH: 5461 DATA_LENGTH: 16384 MAX_DATA_LENGTH: 0 INDEX_LENGTH: 0 DATA_FREE: 0 AUTO_INCREMENT: NULL CREATE_TIME: 2014-08-14 09:30:23 UPDATE_TIME: NULL CHECK_TIME: NULL TABLE_COLLATION: utf8_general_ci CHECKSUM: NULL CREATE_OPTIONS: TABLE_COMMENT: 1 row in set (0.00 sec) 备注： 在查询结尾使用G 代替分号便是以列模式显示，太棒了！ 文档上的解释12ego, \\G Send the current statement to the server to be executed and display the result using vertical format. 备注：如果查询结果很多， mysql 也能设置调用 OS 的系统命令显示查询结果，例如调用 linux 的 more, less 命令。 使用 Pager 设置显示方式12mysql&gt; pager less PAGER set to 'less' 取消 Pager 的设置12mysql&gt; nopager ; PAGER set to stdout 备注： pager 可以调用操作系统命令显示结果例 ，例如 less, more 等， 确实很方便，同样 mysql 客户端的帮助命令也很人性化。 查看 Create Index 帮助命令12345678910111213141516171819202122232425262728293031323334mysql&gt; \\h create index Name: 'CREATE INDEX' Description: Syntax: CREATE [UNIQUE|FULLTEXT|SPATIAL] INDEX index_name [index_type] ON tbl_name (index_col_name,...) [index_option] [algorithm_option | lock_option] ... index_col_name: col_name [(length)] [ASC | DESC] index_type: USING &#123;BTREE | HASH&#125; index_option: KEY_BLOCK_SIZE [=] value | index_type | WITH PARSER parser_name | COMMENT 'string' algorithm_option: ALGORITHM [=] &#123;DEFAULT|INPLACE|COPY&#125; lock_option: LOCK [=] &#123;DEFAULT|NONE|SHARED|EXCLUSIVE&#125; CREATE INDEX is mapped to an ALTER TABLE statement to create indexes. See [HELP ALTER TABLE]. CREATE INDEX cannot be used to create a PRIMARY KEY; use ALTER TABLE instead. For more information about indexes, see http://dev.mysql.com/doc/refman/5.6/en/mysql-indexes.html. URL: http://dev.mysql.com/doc/refman/5.6/en/create-index.html 参考 MySQL Commands","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：如何设置远程登陆不需要输入密码？","slug":"20141129192316","date":"2014-11-29T11:23:16.000Z","updated":"2018-09-04T01:34:14.047Z","comments":true,"path":"20141129192316.html","link":"","permalink":"https://postgres.fun/20141129192316.html","excerpt":"","text":"有时希望远程连接 MySQL 数据库执行定时维护任务，为了维护方便，希望不输入密码, 当然可以如下操作： 不安全的方式12francs@francs:~$ mysql -h 192.168.2.37 -P 3306 -D mysql -u root -pxxxx Warning: Using a password on the command line interface can be insecure. 备注： 把密码写到命令中很不安全，一个 history 命令密码就出来了，较安全的方式是设置 ~/.my.cnf 文件。 创建 .my.cnf 文件12345touch ~/.my.cnf 输入以下: [mysql] host=192.168.2.37 user=root password=root 更改权限1francs@francs:~$ chmod 600 .my.cnf 客户端测试1234567891011121314151617francs@francs:~$ mysql -h 192.168.2.37 -P 3306 -D mysql -u root Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Welcome to the MySQL monitor. Commands end with ; or g. Your MySQL connection id is 46 Server version: 5.6.20 Source distribution Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or 'h' for help. Type 'c' to clear the current input statement. mysql&gt; 备注：这时果然不需要输入密码，上面是在 ubuntu 上远程连接测试，这很像 PostgreSQL 的 .pgpass 文件，太棒了！ 参考http://xmodulo.com/2013/02/how-to-log-in-to-mysql-server-without-password.html","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"MySQL：MySQL5.6 编译安装","slug":"20141129175327","date":"2014-11-29T09:53:27.000Z","updated":"2018-09-04T01:34:13.985Z","comments":true,"path":"20141129175327.html","link":"","permalink":"https://postgres.fun/20141129175327.html","excerpt":"","text":"本文简单介绍 MySQL5.6 编译安装过程。 一 环境准备下载http://dev.mysql.com/get/Downloads/MySQL-5.6/mysql-5.6.20.tar.gz 安装 cmake1[root@db1 soft_bak]# yum install cmake 备注： cmake 用来编译 mysql. 增加 mysql 帐号123[root@db1 ~]# groupadd mysql [root@db1 ~]# useradd -g mysql mysql [root@db1 ~]# passwd mysql 创建目录12[root@db1 opt]# mkdir -p /opt/mysql [root@db1 opt]# mkdir -p /database/mysql/data 二 安装 MySQL解压1[root@db1 soft_bak]# tar zxvf mysql-5.6.20.tar.gz 编译123cmake . -DCMAKE_INSTALL_PREFIX=/opt/mysql -DMYSQL_DATADIR=/database/mysql/data -DDEFAULT_CHARSET=utf8 -DDEFAULT_COLLATION=utf8_general_ci -DWITH_DEBUG=1 make make install 备注：参数说明： -DCMAKE_INSTALL_PREFIX=/usr/local/mysql //安装目录 -DINSTALL_DATADIR=/usr/local/mysql/data //数据库存放目录 -DDEFAULT_CHARSET=utf8 //使用utf8字符 -DDEFAULT_COLLATION=utf8_general_ci //校验字符 -DWITH_DEBUG=1 开启 debug 支持 cmake 报错12CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage CMake Error: Internal CMake error, TryCompile configure of cmake failed 备注： cmke 遇到以上错误，网上查了下，是因为缺少 gcc-c++ 包，安装即可。 解决方法1yum install gcc-c++ 如果重新编译，需执行以下123make clean rm CMakeCache.txt rm /etc/my.cnf 修改权限1chown -R mysql:mysql /database/mysql/data 创建系统数据库的表1[root@db1 mysql]# scripts/mysql_install_db --user=mysql --basedir=/opt/mysql --datadir=/database/mysql/data &amp; 设置 mysql 用户环境变量1234567export LANG=en_US.utf8 export MYSQL_HOME=/opt/mysql export MYSQL_DATA=/database/mysql/data export PATH=$MYSQL_HOME/bin:$PATH:. alias rm='rm -i' alias ll='ls -lh' 备注： source .bash_profile 生效。 修改配置123456cp my.cnf /etc/my.cnf vim /etc/my.cnf basedir = /opt/mysql datadir = /database/mysql/data port = 3306 备注：目前 仅修改以上配置。 手工启动 mysql12[mysql@db1 mysql]$ cd /opt/mysql [mysql@db1 mysql]$ ./bin/mysqld_safe --user=mysql &amp; 查看 mysql 进程123[root@db1 mysql]# ps -ef | grep mysql root 14199 23253 0 15:25 pts/2 00:00:00 /bin/sh ./bin/mysqld_safe --user=mysql mysql 14376 14199 17 15:25 pts/2 00:00:02 /opt/mysql/bin/mysqld --basedir=/opt/mysql --datadir=/database/mysql/data --plugin-dir=/opt/mysql/lib/plugin --user=mysql --log-error=/database/mysql/data/db1.err --pid-file=/database/mysql/data/db1.pid --port=3306 关闭 mysql12[mysql@db1 data]$ mysqladmin -u root -p shutdown Enter password: 三 开启 Root 远程访问并修改密码12345678910[mysql@db1 mysql]$ mysql mysql&gt; use mysql; mysql&gt; GRANT ALL PRIVILEGES on *.* to 'root'@'192.168.%.%' ; mysql&gt; update user set Password=password('root') where User='root'; mysql&gt; flush privileges; Query OK, 0 rows affected (0.00 sec) mysql&gt; exit Bye 备注：并开启防火墙。 ubuntu 客户端测试12345678910111213141516francs@francs:~$ mysql -h 192.168.2.37 -P 3306 -D mysql -u root -p Enter password: Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Welcome to the MySQL monitor. Commands end with ; or g. Your MySQL connection id is 43 Server version: 5.6.20 Source distribution Copyright (c) 2000, 2014, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or 'h' for help. Type 'c' to clear the current input statement. 另一种 mysql 启停方法123[root@db1 mysql]# service mysql start [root@db1 mysql]# service mysql stop备注：如果提示服务不存在，执行以下。 将mysql的启动服务添加到系统服务中1cp support-files/mysql.server /etc/init.d/mysql 四 参考 Installing MySQL Using a Standard Source Distribution Linux MySQL 源码安装","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"Redis：启动和关闭","slug":"20141128151229","date":"2014-11-28T07:12:29.000Z","updated":"2018-09-04T01:34:13.922Z","comments":true,"path":"20141128151229.html","link":"","permalink":"https://postgres.fun/20141128151229.html","excerpt":"","text":"redis 的启动和关闭非常简单，目前发现两种 redis 启动方式，如下: 方式一：直接启动编译安装完 redis 后会在 src 目录下产生 redis-server 等可执行文件， 配置好 redis_6379.conf 文件后直接启动即可。 配置文件 redis_6379.conf123456daemonize yes pidfile /usr/local/redis/redis_6379.pid port 6379 logfile \"/usr/local/redis/redis.log\" dbfilename dump.rdb dir /usr/local/redis 备注： 仅修改以上配置参数。 启动 redis1[redis@db1 redis]$ redis-server redis_6379.conf 关闭 redis1[redis@db1 redis]$ redis-cli shutdown 方式二：通过服务启动 redis 启动脚本位于 redis-2.8.17/utils/ 目录下， 名为 redis_init_script ，通过以下步骤配置 copy 文件1# cp redis_init_script /etc/init.d/redisd 修改 redisd 文件12345678910#!/bin/sh # Simple Redis init.d script conceived to work on Linux systems # as it does use of the /proc filesystem. REDISPORT=6379 EXEC=/usr/local/redis/bin/redis-server CLIEXEC=/usr/local/redis/bin/redis-cli PIDFILE=/usr/local/redis/redis_$&#123;REDISPORT&#125;.pid CONF=\"/usr/local/redis/redis_$&#123;REDISPORT&#125;.conf\" 备注：主要根据实际情况修改 redisd 文件头部配置项。 启动 redis12[root@db1 init.d]# /etc/init.d/redisd start Starting Redis server... 关闭 redis123[root@db1 init.d]# /etc/init.d/redisd stop Stopping ... Redis stopped 加入自启动12[root@db1 init.d]# chkconfig redisd on service redisd does not support chkconfig 备注： 加入 redis 服务自启动报错，需要修改 /etc/init.d/redisd 文件，增加以下两行 12# chkconfig: 2345 90 10 # description: Redis is a persistent key-value database 再次执行 chkconfig1[root@db1 init.d]# chkconfig redisd on 附: 本实验配置的 redisd 文件1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@db1 init.d]# cat /etc/init.d/redisd #!/bin/sh # chkconfig: 2345 90 10 # description: Redis is a persistent key-value database # Simple Redis init.d script conceived to work on Linux systems # as it does use of the /proc filesystem. REDISPORT=6379 EXEC=/usr/local/redis/bin/redis-server CLIEXEC=/usr/local/redis/bin/redis-cli PIDFILE=/usr/local/redis/redis_$&#123;REDISPORT&#125;.pid CONF=\"/usr/local/redis/redis_$&#123;REDISPORT&#125;.conf\" case \"$1\" in start) if [ -f $PIDFILE ] then echo \"$PIDFILE exists, process is already running or crashed\" else echo \"Starting Redis server...\" $EXEC $CONF fi ;; stop) if [ ! -f $PIDFILE ] then echo \"$PIDFILE does not exist, process is not running\" else PID=$(cat $PIDFILE) echo \"Stopping ...\" $CLIEXEC -p $REDISPORT shutdown while [ -x /proc/$&#123;PID&#125; ] do echo \"Waiting for Redis to shutdown ...\" sleep 1 done echo \"Redis stopped\" fi ;; *) echo \"Please use start or stop as first argument\" ;; esac 参考 Redis的三种启动方式","categories":[{"name":"Redis","slug":"Redis","permalink":"https://postgres.fun/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://postgres.fun/tags/Redis/"}]},{"title":"Redis: 安装简介","slug":"20141128144533","date":"2014-11-28T06:45:33.000Z","updated":"2018-09-04T01:34:13.860Z","comments":true,"path":"20141128144533.html","link":"","permalink":"https://postgres.fun/20141128144533.html","excerpt":"","text":"Redis 是一个 key-value 存储系统，和 Memcached 类似, 是当前非常热门的 NoSQL 之一， 这段时间打算了解下。 下载 1[redis@db1 ~]$ wget http://download.redis.io/releases/redis-2.8.17.tar.gz 创建目录12[root@db1 local]# mkdir -p /usr/local/redis/bin [root@db1 redis]# chown -R redis:redis /usr/local/redis 安装123[redis@db1 ~]$ tar zxvf redis-2.8.17.tar.gz [redis@db1 ~]$ cd redis-2.8.17 [redis@db1 redis-2.8.17]$ make copy 文件123[redis@db1 redis-2.8.17]$ cd src [redis@db1 src]$ cp redis-benchmark redis-cli redis-check-aof redis-check-dump redis-sentinel redis-server /usr/local/redis/bin [redis@db1 redis-2.8.17]$ cp redis.conf /usr/local/redis/ 备注：这里将 make 生成后的可执行文件复制到目录/usr/local/redis/bin ，便于管理。 手册上关于可执行文件的解释 redis-server is the Redis Server itself. redis-sentinel is the Redis Sentinel executable (monitoring and failover). redis-cli is the command line interface utility to talk with Redis. redis-benchmark is used to check Redis performances. redis-check-aof and redis-check-dump are useful in the rare event of corrupted data files. 修改 redis.conf 参数12345daemonize yes pidfile /usr/local/redis/redis.pid logfile \"/usr/local/redis/redis.log\" dbfilename dump.rdb dir /usr/local/redis 备注：目前仅设置以上参数，其余参数默认。 启动1[redis@db1 redis]$ redis-server /usr/local/redis/redis.conf 测试12345[redis@db1 ~]$ redis-cli 127.0.0.1:6379&gt; set a 1 OK 127.0.0.1:6379&gt; get a \"1\" 备注： 安装完成。 参考 http://redis.io/download http://redis.io/topics/quickstart http://www.oschina.net/question/12_18065","categories":[{"name":"Redis","slug":"Redis","permalink":"https://postgres.fun/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://postgres.fun/tags/Redis/"}]},{"title":"2014 PostgreSQL 全国用户大会","slug":"20141024102406","date":"2014-10-24T02:24:06.000Z","updated":"2018-09-04T01:34:13.797Z","comments":true,"path":"20141024102406.html","link":"","permalink":"https://postgres.fun/20141024102406.html","excerpt":"","text":"本次会议由国内PostgreSQL的志愿者发起，旨在促进PostgreSQL在中国发展的非营利性的会议。PostgreSQL具有15年历史， 免费、具有强大的企业级功能。PostgreSQL的知名用户包括Skype、NTT、Saleforce的Heroku云数据库平台、Etsy等大型企业，在日本有超过60%以上的市场占有率，大量的制造业、游戏行业、企业ERP系统都采用了。这两年来，PostgreSQL在国内迅速发展，越来越多的企业重视，去哪儿网、斯凯网络、腾讯等企业也开始在逐步进行使用，在阿里云里面也提供了Postgres数据库服务。 大会时间: 2014年12月12日 - 12月13日 大会地点: 深圳 主 办：Postgres中国用户会 QQ群：3336901大会主题：Postgres的最新特性、路标、应用案例、运维；大数据处理费 用：免费(11月20日以前)联 系 人： 李元佳 13710389225 galylee (at) gmail.com 大会日程12月12日 Day 1 基础培训及新数据处理技术专场1: Postgres培训 9:00 - 12:00 Postgres DBA管理入门 萧少聪 (神州立诚 CTO) 13:30 - 16:30 Postgre DBA 高级管理 萧少聪 (神州立诚 CTO)专场2: 新处理技术 1. 分布式数据库 金雪峰 （华为资深架构师） 2. MongoDB的特点及应用 唐建法 (MongoDB大中华区首席技术顾问) 3. Postgres的Pythonic体验(待定) 周琦(CPyUG创始人之一) 4. HAWQ, GPSQL for Hadoop 王伟询 (EMC Greenplum高级经理) 嘉宾晚宴（仅限嘉宾及受邀人员）18:00 - 20:00 12月13日 Day 2 Postgres大会8:15 - 8:45 大会报到8:45 - 9:00 大会开始及简介9:00 - 9:45 主题演讲9:45 - 10:30 浙江移动的去IOE思考 郭岳(浙江移动)10:30 - 11:15 去哪儿的应用（题目待定） 王冬（去哪儿）11:15 - 12:00 Postgres在移动网络云计算中的应用 邵宏志分会场1：应用案例及运维 13:30 - 16:00 PG在腾讯中的应用(题目待定) 赵伟(腾讯) Postgre-XC集群的实践 张文升 (真旅网) Postgres在电信设备管理场景中的应用 田文罡 （华为资深数据库架构师） 高性能Postgres的实践 朱贤文 利用Docker部署Postgres 周正中 (斯凯网络DBA主管) 分会场2：应用程序开发 13:30 - 16:00 Oracle向Postgres的迁移 唐诚 （阿里巴巴） PG中如何使用FDW对外部数据源进行访问 王涛(巨杉软件CTO) Postgres的表分区实践 许中清 （华为资深数据库架构师） Jsonb + hstore 萧少聪(神州立诚的CTO) go语言中Postgres的动态建模 刘鑫 利用Hook机制修改Postgres的操作 王硕(山东瀚高)大会活动及结束辞 16:15 - 17:00晚宴(开放式，AA制)18:00 -20:00 报名http://postgres2014.eventdove.com/","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PITR: 使用 ZFS 的 Snapshots 制定 PITR 方案 ","slug":"20141013112739","date":"2014-10-13T03:27:39.000Z","updated":"2018-09-04T01:34:13.734Z","comments":true,"path":"20141013112739.html","link":"","permalink":"https://postgres.fun/20141013112739.html","excerpt":"","text":"一 环境信息主节点IP 192.168.2.200系统 RHEL6.2 备节点IP 192.168.2.40系统 FreeBSD 10.0 二 FreeBSD 安装 pg 备节点环境变量 ~/.prpfile123456789101112export PGPORT=1921 export PGUSER=postgres export PGDATA=/database/pg93/pg_root export LANG=en_US.utf8 export PGHOME=/opt/pgsql9.3.5 export PGPOOL_HOME=/opt/pgpool export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib # 这里的设置可能要改下 export DATE=`date +\"%Y%m%d%H%M\"` export PATH=/opt/pgbouncer/bin:$PGHOME/bin:$PGPOOL_HOME/bin:$PATH:. export MANPATH=$PGHOME/share/man:$MANPATH alias rm='rm -i' alias ll='ls -lh' 编译并安装123root@francs:/ # ./configure --prefix=/opt/pgsql9.3.5 --with-pgport=1921 --with-wal-blocksize=16 root@francs:/ # gmake root@francs:/ # gmake install 创建数据目录并赋权123456789root@francs:/ # zfs create -o mountpoint=/database/ zp_db/database root@francs:~ # zfs list -r zp_db NAME USED AVAIL REFER MOUNTPOINT zp_db 174M 29.1G 31K /zp_db zp_db/database 31K 29.1G 31K /database/ zp_db/opt 174M 29.1G 174M /opt root@francs:/ # chown -R francs:francs /database 编辑密码文件12[francs@francs ~]$ cat ~/.pgpass 192.168.2.200:1921:replication:repuser:rep123us345er 备注：同时修改主节点的 pg_hba.conf。 pg_basebackup 创建流复制备节点123456[francs@francs ~]$ pg_basebackup -D /database/pg93/pg_root -Fp -Xs -v -P -h 192.168.2.200 -p 1921 -U repuser recovery.conf 配置 recovery_target_timeline = 'latest' standby_mode = on primary_conninfo = 'host=192.168.2.200 port=1921 user=repuser' pg_ctl 启动备节点12345678[francs@francs /database/pg93/pg_root]$ pg_ctl start server starting [francs@francs /database/pg93/pg_root]$ FATAL: XX000: could not create semaphores: No space left on device DETAIL: Failed system call was semget(1921021, 17, 03600). HINT: This error does *not* mean that you have run out of disk space. It occurs when either the system limit for the maximum number of semaphore sets (SEMMNI), or the system wide maximum number of semaphores (SEMMNS), would be exceeded. You need to raise the respective kernel parameter. Alternatively, reduce PostgreSQL s consumption of semaphores by reducing its max_connections parameter. The PostgreSQL documentation contains more information about configuring your system for PostgreSQL. LOCATION: InternalIpcSemaphoreCreate, pg_sema.c:125 备注：启动报错，内核设置 kernel 的共享内存不够，设置以下即可。 配置/boot/loader.conf123kern.ipc.semmni=512 kern.ipc.semmns=1024 kern.ipc.semmnu=256 配置/etc/sysctl.conf123sysctl kern.ipc.shmall=32768 sysctl kern.ipc.shmmax=134217728 sysctl kern.ipc.semmap=256 备注：这些设置满足实验目的，reboot 生效。 三 生成快照create_snap.sh 脚本1234567891011121314151617root@francs:/usr/local/bin # cat /usr/local/bin/create_snap.sh #!/usr/local/bin/bash # create snapshot every 1 minutes while true do current_time=`date \"+%Y%m%d_%H%M%S\"` zfs snapshot zp_db/database@\"$current_time\" if [ $? -ne 0 ]; then echo `date \"+%Y-%m-%d %H:%M:%S\"` \"zp_db/database snapshot generates failed!\" break fi echo `date \"+%Y-%m-%d %H:%M:%S\"` \"zp_db/database snapshot generates successfuly!\" &gt;&gt; /tmp/snapshot.log sleep 60 done 备注： 创建一个生成快照的脚本，每分钟创建一个快照。 开启脚本1nohup /usr/local/bin/create_snap.sh 2&gt;&amp;1 &amp; 打开 .zfs 隐藏目录1234567891011121314151617root@francs:/usr/local/bin # ls -a /database . .. pg93 root@francs:/usr/local/bin # zfs set snapdir=visible zp_db/database root@francs:/usr/local/bin # ls -a /database . .. .zfs pg93 root@francs:~ # ll /database/.zfs/snapshot/ total 140 drwxr-xr-x 3 francs francs 3 10 9 21:11 20141010_114052/ drwxr-xr-x 3 francs francs 3 10 9 21:11 20141010_134732/ drwxr-xr-x 3 francs francs 3 10 9 21:11 20141010_135359/ drwxr-xr-x 3 francs francs 3 10 9 21:11 20141010_140045/ drwxr-xr-x 3 francs francs 3 10 9 21:11 20141010_140413/ drwxr-xr-x 3 francs francs 3 10 9 21:11 20141010_140513/.... 备注：查看快照有两种方式，一种是通过” zfs list -t snapshot” 命令， 另一种是查看 .zfs 隐藏目录下，可以打开数据集的 snapdir 属性使 .zfs 目录可见。 四 恢复测试主节点1234567[pg93@db1 script]$ cat pitr_insert.sh #!/bin/bash for((i=1;i&gt;0;i=1)) do psql -d postgres -U postgres -c \"insert into test_pitr (name) values('test_pitr');\" sleep 10; done 备注：主节点跑一个数据插入脚本，每 10 秒向主节点插入一条数据。通过这个程序，测试 ZFS 是否能做基于时间点的数据恢复。 表结构如下123456Table \"public.test_pitr\" Column | Type | Modifiers -------------+--------------------------------+-------------------------------------------------------- id | integer | not null default nextval('test_pitr_id_seq'::regclass) name | character varying(64) | create_time | timestamp(0) without time zone | default clock_timestamp() zfs clone12345678root@francs:/usr/local/bin # zfs clone zp_db/database@20141010_141412 zp_db/pitr141412 root@francs:/usr/local/bin # zfs list -r zp_db NAME USED AVAIL REFER MOUNTPOINT zp_db 1.14G 28.1G 32K /zp_db zp_db/database 990M 28.1G 947M /database/ zp_db/opt 174M 28.1G 174M /opt zp_db/pitr141412 1K 28.1G 947M /zp_db/pitr141412 备注： 我们将数据恢复到 2014-10-10 14-14-12 时刻，新的数据目录为 /zp_db/pitr141412，可以看到这个目录空间使用为 1K ，几乎没占用额外空间。 修改 recovery.conf, postgresql.conf由于此台上已经跑了一个 pg 备库，为了不和之前的备节点信息重复，需要修改 postgresql.conf 里的 port 参数，并且最好是移走 recovery.conf 参数，将恢复的数据库以主库模式启动。 启动恢复目录的 pg1[francs@francs ~]$ pg_ctl start -D /zp_db/pitr141412/pg93/pg_root 五 数据验证1234567891011121314151617[francs@francs /zp_db/pitr141412/pg93/pg_root/pg_log]$ psql -p 1922 psql (9.3.5) Type \"help\" for help. postgres=# select * from test_pitr order by id desc limit 3; id |name | create_time ------+-----------+--------------------- 7909 | test_pitr | 2014-10-10 14:14:08 7908 | test_pitr | 2014-10-10 14:13:58 7907 | test_pitr | 2014-10-10 14:13:47 (3 rows) postgres=# select count(*) from test_pitr; count ------- 935 (1 row) 备注：数据成功恢复到 2014-10-10 14-14-12 时刻。 六 总结 利用 ZFS 的 snapshot 和 clone 功能能够轻松满足 PostgreSQL 数据库的 PITR 需求，而且具有实施简单，低成本，恢复时间短的优点。 七 参考 Managing Kernel Resources ZFS: Snapshots and Clones","categories":[{"name":"PG备份与恢复","slug":"PG备份与恢复","permalink":"https://postgres.fun/categories/PG备份与恢复/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"},{"name":"ZFS","slug":"ZFS","permalink":"https://postgres.fun/tags/ZFS/"}]},{"title":"ZFS: Snapshot And Clone","slug":"20141013112005","date":"2014-10-13T03:20:05.000Z","updated":"2018-09-04T01:34:13.672Z","comments":true,"path":"20141013112005.html","link":"","permalink":"https://postgres.fun/20141013112005.html","excerpt":"","text":"之前看过一段时间 ZFS, 有个 snapshot 和 clone 功能一直没看，今天补上。 创建 snapshot 操作瞬间可以完成，代价是很低的， 并且不需要额外的空间，当有新数据写入时才使用额外空间，下面简单测试这个功能。 当前目录情况12345678910111213141516171819root@francs:~ # df -Th Filesystem Type Size Used Avail Capacity Mounted on zroot/ROOT/defaultzfs 16G 3.4G 12G 22% / devfs devfs 1.0K 1.0K 0B 100% /dev zroot/tmp zfs 12G 19M 12G 0% /tmp zroot/usr/home zfs 12G 200K 12G 0% /usr/home zroot/usr/ports zfs 13G 874M 12G 7% /usr/ports zroot/usr/src zfs 13G 545M 12G 4% /usr/src zroot/varzfs 13G 636M 12G 5% /var zroot/var/crash zfs 12G 148K 12G 0% /var/crash zroot/var/log zfs 12G 396K 12G 0% /var/log zroot/var/mail zfs 12G 160K 12G 0% /var/mail zroot/var/tmp zfs 12G 152K 12G 0% /var/tmp zp1 zfs 1.9G 32K 1.9G 0% /zp1 zp1/data1 zfs 1.9G 3.6M 1.9G 0% /data1 zp1/data2 zfs 1.9G 16M 1.9G 1% /data2 zp1/data3 zfs 1.9G 12M 1.9G 1% /data3 zp1/data4 zfs 1.9G 31K 1.9G 0% /data4 zp_db zfs 29G 31K 29G 0% /zp_db 创建 snapshot1root@francs:~ # zfs snapshot zp1/data1@20141009 备注： 创建快照方式有两种： pool snapshots 或者 dataset snapshots， 我这里做的是 dataset snapshots。 查看快照123root@francs:~ # zfs list -t snapshot NAME USED AVAIL REFER MOUNTPOINT zp1/data1@20141009 0 - 3.62M - 创建测试文件1234567891011root@francs:~ # cd /data1 root@francs:/data1 # ll total 3672 -rw-rw-r-- 1 root wheel 16777216 7 4 16:44 0000000600005ca200000032 -rw-r--r-- 1 root wheel 6291456 7 10 14:51 test_1.img root@francs:/data1 # echo \"test\" &gt; a root@francs:/data1 # cat a test 备注：在目录 /data1 下新建一个文件 a， 之所以这么操作，是想测试 zfs 的 clone 功能。 zfs clone1root@francs:~ # zfs clone zp1/data1@20141009 zp1/pitr1 备注：基于刚才创建的快照做下 zfs clone，并且将快照 mount 到目录 /zp1/pitr1。 查看 zp1 池123456789root@francs:~ # zfs list -r zp1 NAME USED AVAIL REFER MOUNTPOINT zp1 32.1M 1.92G 33K /zp1 zp1/data1 3.64M 1.92G 3.62M /data1 zp1/data2 16.0M 1.92G 16.0M /data2 zp1/data3 12.0M 1.92G 12.0M /data3 zp1/data4 31K 1.92G 31K /data4 zp1/pitr1 1K 1.92G 3.62M /zp1/pitr1备注： 这里可以看到 /zp1/pitr1 目录了。 查看 /zp1/pitr1 文件12root@francs:~ # ls /zp1/pitr1 0000000600005ca200000032 test_1.img 备注： 这时新创建的文件 a 并没有显示，说明文件系统恢复到了创建快照的那个时间点。 接下来看下删除 clone 和 snapshot 操作，非常简单。 Destroying Clones1root@francs:~ # zfs destroy zp1/pitr1 Destroying Snapshots1root@francs:~ # zfs destroy zp1/data1@20141009 备注： 如果基于 snapshot 创建了 clone，得先删除 clone 才能删除 snapshot. 参考 Snapshots and Clones","categories":[{"name":"FreeBSD_ZFS","slug":"FreeBSD-ZFS","permalink":"https://postgres.fun/categories/FreeBSD-ZFS/"}],"tags":[{"name":"ZFS","slug":"ZFS","permalink":"https://postgres.fun/tags/ZFS/"}]},{"title":"Pgpool-II 流复制模式压力测试","slug":"20141004113758","date":"2014-10-04T03:37:58.000Z","updated":"2018-09-04T01:34:13.609Z","comments":true,"path":"20141004113758.html","link":"","permalink":"https://postgres.fun/20141004113758.html","excerpt":"","text":"上篇 blog: PostgreSQL 流复制 + Pgpool-II 实现高可用 HA 介绍了使用 pgpool 结合 PostgreSQL 自身的流复制功能搭建 HA 环境，之前听说 pgpool 会影响性能，那么今天利用上篇 blog 刚搭好的 HA 环境，简单做下测试，环境信息请参考上篇 blog。 测试数据准备创建测试用户、测试表，并插入测试数据，如下：123456789101112131415161718192021222324252627建用户 REATE ROLE pgpool_db1 LOGIN ENCRYPTED PASSWORD 'pgpool_db1' nosuperuser noinherit nocreatedb nocreaterole ; 创建表空间 ( 两节点操作 ) mkdir -p /database/pg93/pg_tbs/tbs_pgpool_db1 create tablespace tbs_pgpool_db1 owner postgres LOCATION '/database/pg93/pg_tbs/tbs_pgpool_db1'; 创建数据库 CREATE DATABASE pgpool_db1 WITH OWNER = pgpool_db1 TEMPLATE = template1 ENCODING = 'UTF8'TABLESPACE = tbs_pgpool_db1; 创建模式 postgres=# c pgpool_db1 pgpool_db1 You are now connected to database \"pgpool_db1\" as user \"pgpool_db1\". pgpool_db1=&gt; create schema pgpool_db1; CREATE SCHEMA 创建测试表，并插入 500 万测试数据。 pgpool_db1=&gt; create table test_1(id int8,name text,creat_time timestamp(0) without time zone default clock_timestamp()); CREATE TABLE pgpool_db1=&gt; insert into test_1(id,name) select n,n||'_test' from generate_series(1,5000000) n; INSERT 0 5000000 pgpool_db1=&gt; alter table test_1 add primary key(id); ALTER TABLE Pgpool 配置查询 pgpool_db1 md5 密码12345postgres=# select rolname,rolpassword from pg_authid where rolname='pgpool_db1'; rolname | rolpassword ------------+------------------------------------- pgpool_db1 | md594f532461c9e3e3a591e77a373da0493(1 row) pool_passwd 中加入密码信息1234[pg93@db2 load_test]$ echo \"pgpool_db1:md594f532461c9e3e3a591e77a373da0493\" &gt;&gt; /opt/pgpool/etc/pool_passwd reload pgpool [pg93@db2 load_test]$ pgpool reload 性能测试测试 SQL1234[pg93@db2 load_test]$ cat get_name.sql setrandom v_id 1 5000000 select name from test_1 where id=:v_id; 直连 PostgreSQL 测试测试不同连接数时此SQL的TPS。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061pgbench -c 2 -T 20 -n -N -M prepared -d pgpool_db1 -U pgpool_db1 -f get_name.sql &gt; get_name.out 2&gt;&amp;1 &amp; 连接数 2 pghost: pgport: 1921 nclients: 2 duration: 20 dbName: pgpool_db1 transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 2 number of threads: 1 duration: 20 s number of transactions actually processed: 61021 tps = 3050.684070 (including connections establishing) tps = 3051.633320 (excluding connections establishing) 连接数 4 pghost: pgport: 1921 nclients: 4 duration: 20 dbName: pgpool_db1 transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 4 number of threads: 1 duration: 20 s number of transactions actually processed: 64743 tps = 3236.352239 (including connections establishing) tps = 3239.085678 (excluding connections establishing) 连接数 8 pghost: pgport: 1921 nclients: 8 duration: 20 dbName: pgpool_db1 transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 8 number of threads: 1 duration: 20 s number of transactions actually processed: 47789 tps = 2387.004156 (including connections establishing) tps = 2390.072375 (excluding connections establishing) 连接数 16 pghost: pgport: 1921 nclients: 16 duration: 20 dbName: pgpool_db1 transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 16 number of threads: 1 duration: 20 s number of transactions actually processed: 44099 tps = 2201.601035 (including connections establishing) tps = 2206.286789 (excluding connections establishing) 连接数 32 pghost: pgport: 1921 nclients: 32 duration: 20 dbName: pgpool_db1 transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 32 number of threads: 1 duration: 20 s number of transactions actually processed: 42541 tps = 2125.029203 (including connections establishing) tps = 2135.919727 (excluding connections establishing) 连接 Pgpool 测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061pgbench -h 127.0.0.1 -p 9999 -c 2 -T 20 -n -N -M prepared -d pgpool_db1 -U pgpool_db1 -f get_name.sql &gt; get_name.out 2&gt;&amp;1 &amp; 连接数 2 pghost: 127.0.0.1 pgport: 9999 nclients: 2 duration: 20 dbName: pgpool_db1 transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 2 number of threads: 1 duration: 20 s number of transactions actually processed: 13570 tps = 678.426052 (including connections establishing) tps = 678.522323 (excluding connections establishing) 连接数 4 pghost: 127.0.0.1 pgport: 9999 nclients: 4 duration: 20 dbName: pgpool_db1 transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 4 number of threads: 1 duration: 20 s number of transactions actually processed: 20307 tps = 1015.210206 (including connections establishing) tps = 1016.593395 (excluding connections establishing) 连接数 8 pghost: 127.0.0.1 pgport: 9999 nclients: 8 duration: 20 dbName: pgpool_db1 transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 8 number of threads: 1 duration: 20 s number of transactions actually processed: 35957 tps = 1796.691134 (including connections establishing) tps = 1799.634067 (excluding connections establishing) 连接数 16 pghost: 127.0.0.1 pgport: 9999 nclients: 16 duration: 20 dbName: pgpool_db1 transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 16 number of threads: 1 duration: 20 s number of transactions actually processed: 33079 tps = 1653.351817 (including connections establishing) tps = 1661.947374 (excluding connections establishing) 连接数 32 pghost: 127.0.0.1 pgport: 9999 nclients: 32 duration: 20 dbName: pgpool_db1 transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 32 number of threads: 1 duration: 20 s number of transactions actually processed: 32259 tps = 1611.528149 (including connections establishing) tps = 1615.327601 (excluding connections establishing) 压测数据统计拉张图更直观些备注：从图中看到使用 pgpool 性能比直连方式降低很多，特别是在并发连接数少的情况差距越明显，达到 70% 左右，随着并发连接数数增加，性能差距缩小到 20%- 30% 左右。 参考 pgpool-II performance lossy pgpool -II page pgbench PostgreSQL 流复制 + Pgpool-II 实现高可用 HA","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"},{"name":"压力测试","slug":"压力测试","permalink":"https://postgres.fun/tags/压力测试/"}]},{"title":"PostgreSQL 流复制 + Pgpool-II 实现高可用 HA","slug":"20141003185223","date":"2014-10-03T10:52:23.000Z","updated":"2018-09-04T01:34:13.547Z","comments":true,"path":"20141003185223.html","link":"","permalink":"https://postgres.fun/20141003185223.html","excerpt":"","text":"最近准备研究下 Pgpool-II(后续简称pgpool) ，之前很早就了解 pgpool 具有连接池，复制，负载均衡，并行查询，以及 HA 功能，今天打算测试下 pgpool 的 HA 功能，暂不考虑其它。 环境信息基本信息硬件：笔记本两台虚拟机db1: 192.168.2.37db2: 192.168.2.38pgpool: 192.168.2.200 ( pgpool 使用 VIP, VIP 可以在 db1 , db2 上漂移 )系统: RHEL 6.2PostgreSQL 版本： 9.3.5备注：PostgreSQL 流复制环境搭建略，也可参考 PostgreSQL：使用 pg_basebackup 搭建流复制环境 。 Pgpool 实验环境架构图 pgpool 部署架构规划图 备注: watchdog可以避免 pgpool 的单点故障。 编译安装 Pgpool ( 两节点安装)下载pgpool软件1http://www.pgpool.net/download.php?f=pgpool-II-3.3.4.tar.gz 安装 pgpool1234567891011[root@db1 soft_bak]# tar xvf pgpool-II-3.3.4.tar.gz [root@db1 pgpool-II-3.3.4]# mkdir -p /opt/pgpool [root@db1 pgpool-II-3.3.4]# source /home/pg93/.bash_profile [root@db1 pgpool-II-3.3.4]#./configure --prefix=/opt/pgpool -with-pgsql=path -with-pgsql=/opt/pgsql_9.3.3 --with-openssl [root@db1 pgpool-II-3.3.4]# make [root@db1 pgpool-II-3.3.4]# make install configure 错误 checking for PQexecPrepared in -lpq... no configure: error: libpq is not installed or libpq is old 备注：如果出现这个错误，说明找不到 PostgreSQL lib 库， configure 加上 -with-pgsql 选项即可。 安装 Pgpool 相关函数pgpool 函数虽然不是必需安装，但建议安装 pgpool_regclass, pgpool_recovery 函数。 安装 pgpool_regclass, pgpool_recovery (recommended)1234[root@db1 sql]# cd /opt/soft_bak/pgpool-II-3.3.4/sql [root@db1 sql]# source /home/pg93/.bash_profile [root@db1 sql]# make [root@db1 sql]# make install 备注：之后在 $PGHOME/share/extension 目录下生成以下文件。 pgpool 相关文件1234567[pg93@db1 extension]$ ll /opt/pgsql_9.3.3/share/extension/pgpool* -rw-r--r-- 1 root root 791 Sep 30 15:56 /opt/pgsql_9.3.3/share/extension/pgpool_recovery--1.0.sql -rw-r--r-- 1 root root 160 Sep 30 15:56 /opt/pgsql_9.3.3/share/extension/pgpool_recovery.control -rw-r--r-- 1 root root 551 Sep 30 15:56 /opt/pgsql_9.3.3/share/extension/pgpool-recovery.sql -rw-r--r-- 1 root root 283 Sep 30 15:56 /opt/pgsql_9.3.3/share/extension/pgpool_regclass--1.0.sql -rw-r--r-- 1 root root 152 Sep 30 15:56 /opt/pgsql_9.3.3/share/extension/pgpool_regclass.control -rw-r--r-- 1 root root 142 Sep 30 15:56 /opt/pgsql_9.3.3/share/extension/pgpool-regclass.sql 在模板库 template1 创建 pgpool_regclass123456789[pg93@db1 extension]$ psql template1 psql (9.3.3) Type \"help\" for help. template1=# create extension pgpool_regclass; CREATE EXTENSION template1=# CREATE EXTENSION pgpool_recovery; CREATE EXTENSION 查看新增加的recovery 管理函数123456789template1=# \\df List of functions Schema | Name | Result data type | Argument data types | Type --------+---------------------+------------------+----------------------------------------------------------------+-------- public | pgpool_pgctl | boolean | action text, stop_mode text | normal public | pgpool_recovery | boolean | script_name text, remote_host text, remote_data_directory text | normal public | pgpool_remote_start | boolean | remote_host text, remote_data_directory text | normal public | pgpool_switch_xlog | text | arcive_dir text | normal (4 rows) 备注： 每个库都需要安装 pgpool_regclass，为了方便在 template1 上安装 pgpool_regclass，以后新建的库都以 template1 为模板库。 数据库配置recovery.conf 配置123recovery_target_timeline = 'latest' standby_mode = on primary_conninfo = 'host=192.168.2.38 port=1921 user=repuser' 备注: primary_conninfo 的 host 分别配置对端 host IP. .pgpass文件配置123[pg93@db1 pg_root]$ cat ~/.pgpass 192.168.2.37:1921:replication:repuser:rep123us345er 192.168.2.38:1921:replication:repuser:rep123us345er postgresql.conf 配置略 配置 Pgpool-II ( 两节点操作)配置 pcp.conf123456[root@db1 etc]# cd /opt/pgpool/etc [root@db1 etc]# cp pcp.conf.sample pcp.conf [pg93@db1 etc]$ pg_md5 -u pgpool -p password: ba777e4c2f15c11ea8ac3be7e0440aa0 备注: pgpool 提供 pcp 接口，可以查看，管理 pgpool 的状态，并且可以远程操作 pgpool ， pcp.conf 用来对 pcp 相关命令认证的文件，格式为 USERID:MD5PASSWD。 编写 pcp.conf 文件，写入以下12# USERID:MD5PASSWD pgpool:ba777e4c2f15c11ea8ac3be7e0440aa0 配置 /etc/hosts123456[root@db1 ~]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.2.37 db1 192.168.2.38 db2 配置 ifconfig, arping 执行权限123[root@db1 pgpool]# chmod u+s /sbin/ifconfig [root@db1 pgpool]# chmod u+s /usr/sbin/备注: 以理普通用户能够执行以上命令， failover_command 命令要用到。 配置两节点信任关系 , 例如 db1 上的 pg93 配置信任关系，免密码登录 db2.1234pg93@db1 ~]$ ssh-keygen [pg93@db1 ~]$ ssh-copy-id pg93@db2 [pg93@db1 ~]$ ssh pg93@db2 [pg93@db2 ~]$ exit 备注: 此操作在 db2 上也操作一次, failover_command 命令要用到。 配置 pgpool.conf12[root@db1 etc]# cd /opt/pgpool/etc [root@db1 etc]# cp pgpool.conf.sample pgpool.conf 开启日志在日志 /etc/rsyslog.conf 加入以下行12# pgpool local0.* /var/log/pgpool.log 重启 rsyslog 服务123[root@db1 ~]# /etc/init.d/rsyslog restart Shutting down system logger: [OK ] Starting system logger: [OK ] pgpool.conf 配置以下行1log_destination = 'syslog' 备注： 这里使用 syslog。 主节点的 pgpool.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125[pg93@db1 etc]$ grep ^[a-z] pgpool.conf listen_addresses = '*' port = 9999 socket_dir = '/tmp' pcp_port = 9898 pcp_socket_dir = '/tmp' backend_hostname0 = 'db1' ##配置数据节点 db1 backend_port0 = 1921 backend_weight0 = 1 backend_flag0 = 'ALLOW_TO_FAILOVER' backend_hostname1 = 'db2' ##配置数据节点 db2 backend_port1 = 1921 backend_weight1 = 1 backend_flag1 = 'ALLOW_TO_FAILOVER' enable_pool_hba = on pool_passwd = 'pool_passwd' authentication_timeout = 60 ssl = off num_init_children = 32 max_pool = 4 child_life_time = 300 child_max_connections = 0 connection_life_time = 0 client_idle_limit = 0 log_destination = 'syslog' print_timestamp = on log_connections = on log_hostname = on log_statement = on log_per_node_statement = off log_standby_delay = 'none' syslog_facility = 'LOCAL0' syslog_ident = 'pgpool' debug_level = 0 pid_file_name = '/opt/pgpool/pgpool.pid' logdir = '/tmp' connection_cache = on reset_query_list = 'ABORT; DISCARD ALL' replication_mode = off replicate_select = off insert_lock = on lobj_lock_table = '' replication_stop_on_mismatch = off failover_if_affected_tuples_mismatch = off load_balance_mode = on ignore_leading_white_space = on white_function_list = '' black_function_list = 'nextval,setval' master_slave_mode = on # 设置流复制模式 master_slave_sub_mode = 'stream' # 设置流复制模式 sr_check_period = 5 sr_check_user = 'repuser' sr_check_password = 'rep123us345er' delay_threshold = 16000 follow_master_command = '' parallel_mode = off pgpool2_hostname = '' system_db_hostname = 'localhost' system_db_port = 5432 system_db_dbname = 'pgpool' system_db_schema = 'pgpool_catalog' system_db_user = 'pgpool' system_db_password = '' health_check_period = 5 health_check_timeout = 20 health_check_user = 'repuser' health_check_password = 'rep123us345er' health_check_max_retries = 3 health_check_retry_delay = 1 failover_command = '/opt/pgpool/failover_stream.sh %H ' ## 配置 failover 脚本，脚本内容下面会贴出。 failback_command = '' fail_over_on_backend_error = on search_primary_node_timeout = 10 recovery_user = 'nobody' recovery_password = '' recovery_1st_stage_command = '' recovery_2nd_stage_command = '' recovery_timeout = 90 client_idle_limit_in_recovery = 0 use_watchdog = on trusted_servers = '' ping_path = '/bin' wd_hostname = 'db1' wd_port = 9000 wd_authkey = '' delegate_IP = '192.168.2.200' ## 配置 pgpool 的 VIP，避免 pgpool 的单点故障 ifconfig_path = '/sbin' ## 以下几个网卡命令不需要修改。 if_up_cmd = 'ifconfig eth0:0 inet $_IP_$ netmask 255.255.255.0' if_down_cmd = 'ifconfig eth0:0 down' arping_path = '/usr/sbin' # arping command path arping_cmd = 'arping -U $_IP_$ -w 1' clear_memqcache_on_escalation = on wd_escalation_command = '' wd_lifecheck_method = 'heartbeat' wd_interval = 10 wd_heartbeat_port = 9694 wd_heartbeat_keepalive = 2 wd_heartbeat_deadtime = 30 heartbeat_destination0 = 'db2' # 配置对端的 hostname heartbeat_destination_port0 = 9694 heartbeat_device0 = 'eth0' wd_life_point = 3 wd_lifecheck_query = 'SELECT 1' wd_lifecheck_dbname = 'template1' wd_lifecheck_user = 'nobody' wd_lifecheck_password = '' other_pgpool_hostname0 = 'db2' ## 配置对端的 pgpool other_pgpool_port0 = 9999 other_wd_port0 = 9000 relcache_expire = 0 relcache_size = 256 check_temp_table = on memory_cache_enabled = off memqcache_method = 'shmem' memqcache_memcached_host = 'localhost' memqcache_memcached_port = 11211 memqcache_total_size = 67108864 memqcache_max_num_cache = 1000000 memqcache_expire = 0 memqcache_auto_cache_invalidation = on memqcache_maxcache = 409600 memqcache_cache_block_size = 1048576 memqcache_oiddir = '/var/log/pgpool/oiddir' white_memqcache_table_list = '' black_memqcache_table_list = '' 备节点的 pgpool.conf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125[pg93@db2 etc]$ grep ^[a-z] pgpool.conf listen_addresses = '*' port = 9999 socket_dir = '/tmp' pcp_port = 9898 pcp_socket_dir = '/tmp' backend_hostname0 = 'db1' backend_port0 = 1921 backend_weight0 = 1 backend_flag0 = 'ALLOW_TO_FAILOVER' backend_hostname1 = 'db2' backend_port1 = 1921 backend_weight1 = 1 backend_flag1 = 'ALLOW_TO_FAILOVER' enable_pool_hba = on pool_passwd = 'pool_passwd' authentication_timeout = 60 ssl = off num_init_children = 32 max_pool = 4 child_life_time = 300 child_max_connections = 0 connection_life_time = 0 client_idle_limit = 0 log_destination = 'syslog' print_timestamp = on log_connections = on log_hostname = on log_statement = on log_per_node_statement = off log_standby_delay = 'none' syslog_facility = 'LOCAL0' syslog_ident = 'pgpool' debug_level = 0 pid_file_name = '/opt/pgpool/pgpool.pid' logdir = '/tmp' connection_cache = on reset_query_list = 'ABORT; DISCARD ALL' replication_mode = off replicate_select = off insert_lock = on lobj_lock_table = '' replication_stop_on_mismatch = off failover_if_affected_tuples_mismatch = off load_balance_mode = on ignore_leading_white_space = on white_function_list = '' black_function_list = 'nextval,setval' master_slave_mode = on master_slave_sub_mode = 'stream' sr_check_period = 0 sr_check_user = 'repuser' sr_check_password = 'rep123us345er' delay_threshold = 16000 follow_master_command = '' parallel_mode = off pgpool2_hostname = '' system_db_hostname = 'localhost' system_db_port = 5432 system_db_dbname = 'pgpool' system_db_schema = 'pgpool_catalog' system_db_user = 'pgpool' system_db_password = '' health_check_period = 0 health_check_timeout = 20 health_check_user = 'nobody' health_check_password = '' health_check_max_retries = 0 health_check_retry_delay = 1 failover_command = '/opt/pgpool/failover_stream.sh %H ' failback_command = '' fail_over_on_backend_error = on search_primary_node_timeout = 10 recovery_user = 'nobody' recovery_password = '' recovery_1st_stage_command = '' recovery_2nd_stage_command = '' recovery_timeout = 90 client_idle_limit_in_recovery = 0 use_watchdog = on trusted_servers = '' ping_path = '/bin' wd_hostname = 'db2' wd_port = 9000 wd_authkey = '' delegate_IP = '192.168.2.200' ifconfig_path = '/sbin' if_up_cmd = 'ifconfig eth0:0 inet $_IP_$ netmask 255.255.255.0' if_down_cmd = 'ifconfig eth0:0 down' arping_path = '/usr/sbin' # arping command path arping_cmd = 'arping -U $_IP_$ -w 1' clear_memqcache_on_escalation = on wd_escalation_command = '' wd_lifecheck_method = 'heartbeat' wd_interval = 10 wd_heartbeat_port = 9694 wd_heartbeat_keepalive = 2 wd_heartbeat_deadtime = 30 heartbeat_destination0 = 'db1' heartbeat_destination_port0 = 9694 heartbeat_device0 = 'eth0' wd_life_point = 3 wd_lifecheck_query = 'SELECT 1' wd_lifecheck_dbname = 'template1' wd_lifecheck_user = 'nobody' wd_lifecheck_password = '' other_pgpool_hostname0 = 'db1' other_pgpool_port0 = 9999 other_wd_port0 = 9000 relcache_expire = 0 relcache_size = 256 check_temp_table = on memory_cache_enabled = off memqcache_method = 'shmem' memqcache_memcached_host = 'localhost' memqcache_memcached_port = 11211 memqcache_total_size = 67108864 memqcache_max_num_cache = 1000000 memqcache_expire = 0 memqcache_auto_cache_invalidation = on memqcache_maxcache = 409600 memqcache_cache_block_size = 1048576 memqcache_oiddir = '/var/log/pgpool/oiddir' white_memqcache_table_list = '' black_memqcache_table_list = '' /opt/pgpool/failover_stream.sh 脚本内容123456789101112[pg93@db1 etc]$ cat /opt/pgpool/failover_stream.sh #! /bin/sh # Failover command for streaming replication. # Arguments: $1: new master hostname. new_master=$1 trigger_command=\"$PGHOME/bin/pg_ctl promote -D $PGDATA\" # Prompte standby database. /usr/bin/ssh -T $new_master $trigger_command exit 0; 备注： 我这里定义的 failover 脚本和 pgpool 手册上的脚本不同，这里使用了 pg_ctl promote 的切换方式，一方面以文件触发的形式个人觉得不是很好。另一方面：当以 trigger file 形式实现 HA 时会遇到不能来回切换的问题。 如果仔细看，可以看到这个切换脚本并不严谨，每当有节点离线时，它都会触发一次，也就是说如果当前掉线的是备节点，它也会到对端主库执行一次 failover_command 命令，不过没关系，并不影响。 启动 pgpool1[pg93@db2 etc]$ pgpool 备注： 此时可以查看 /var/log/pgpool.log 日志了，注意两节点都启动。 pgpool 关闭命令1[pg93@db2 etc]$ pgpool -m fast stop pgpool reload 命令1[pg93@db1 etc]$ pgpool reload 关于连接异常问题备注： 这个表格很好地描述了各种连接异常问题，对照表格很容易找到原因。 在本机 ubuntu 查看 pgpool 状态1234567891011121314151617francs@francs:~$ lsb_release -a No LSB modules are available. Distributor ID:Ubuntu Description:Ubuntu 14.04.1 LTS Release:14.04 Codename: trusty francs@francs:~$ psql -h 192.168.2.200 -p 9999 postgres postgres Password for user postgres: psql (9.3.5, server 9.3.3) Type \"help\" for help. postgres=# show pool_nodes; node_id | hostname | port | status | lb_weight | role ---------+----------+------+--------+-----------+--------- 0 | db1 | 1921 | 2 | 0.500000 | standby 1 | db2 | 1921 | 2 | 0.500000 | primary 备注: db2 为 primary, db1 为 standby。 status 为 2 ，表示正常连接; 关于 status 状态，如下 0 - This state is only used during the initialization. PCP will never display it. 1 - Node is up. No connections yet. 2 - Node is up. Connections are pooled. 3 - Node is down. HA 验证 根据上面 show_nodes 输出， 此时 db2 节点为主库， db1 节点为备库，并且 pgpool 目前跑在 db1 上，下面分别测试以下情形: 关 db2 上的数据库db2 关闭数据库123[pg93@db2 etc]$ pg_ctl stop -m fast waiting for server to shut down....... done server stopped db1 上测试123456[pg93@db1 etc]$ pg_controldata | grep cluster Database cluster state: in production [pg93@db1 etc]$ [pg93@db1 etc]$ ll /database/pg93/pg_root/recovery.done -rw-r--r-- 1 pg93 pg93 4.8K Oct 2 09:05 /database/pg93/pg_root/recovery.done 备注：可以看到db1 节点已完成从 standby 角色切换到 primary ，并且 $PGDATA/recovery.conf 文件变成 recovery.done 。 查看 pgpool 状态1234567891011francs@francs:~$ psql -h 192.168.2.200 -p 9999 postgres postgres Password for user postgres: psql (9.3.5, server 9.3.3) Type \"help\" for help. postgres=# show pool_nodes; node_id | hostname | port | status | lb_weight | role ---------+----------+------+--------+-----------+--------- 0 | db1 | 1921 | 2 | 0.500000 | primary 1 | db2 | 1921 | 3 | 0.500000 | standby (2 rows) 备注:db1 已转换成primary 角色。 db2 状态为 3 ，表示 down 状态。 接下来以 standby 身份启动 db212pg93@db2 pg_root]$ mv recovery.done recovery.conf [pg93@db2 pg_root]$ pg_ctl start db1 上操作，添加 db2 节点信息123456[pg93@db1 etc]$ pcp_attach_node -d 5 db2 9898 pgpool pgpool 1 DEBUG: send: tos=\"R\", len=44 DEBUG: recv: tos=\"r\", len=21, data=AuthenticationOK DEBUG: send: tos=\"D\", len=6 DEBUG: recv: tos=\"c\", len=20, data=CommandComplete DEBUG: send: tos=\"X\", len=4 db2 状态恢复1234567891011[pg93@db1 etc]$ psql -p 9999 -U postgres Password for user postgres: psql (9.3.3) Type \"help\" for help. postgres=# show pool_nodes; node_id | hostname | port | status | lb_weight | role ---------+----------+------+--------+-----------+--------- 0 | db1 | 1921 | 2 | 0.500000 | primary 1 | db2 | 1921 | 2 | 0.500000 | standby (2 rows) 备注：切换成功 关 pgpool masterpgpool VIP :192.168.2.200 从 db1 飘到 db2，注意此时仅 pgpool 的 VIP 飘移， db1,db2 上的数据库角色不变, pgpool 的切换非常容易，关闭 pgpool 的脚本如下：1[pg93@db2 etc]$ pgpool -m fast stop db1 掉电 power off这里通过 vmwaer 执行 power off 模拟断电的情况， 在 db1(现在的 Primary 节点) 执行 power off ，发现 pgpool VIP 能切换到 db1 ，同时 db1 上的数据库切换成 primary ，切换成功。 第二种，第三种情况就不贴详细日志了。 参考 pgpool -II page FAQ 使用pgpool II的watchdog消除单点故障 postgresql 9.1的failover配置及其管理 Pgpool 流复制模式压力测试","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"Pgpool-II","slug":"Pgpool-II","permalink":"https://postgres.fun/tags/Pgpool-II/"}]},{"title":"PL/Proxy之三: 增加数据节点","slug":"20140922093513","date":"2014-09-22T01:35:13.000Z","updated":"2018-09-04T01:34:13.484Z","comments":true,"path":"20140922093513.html","link":"","permalink":"https://postgres.fun/20140922093513.html","excerpt":"","text":"关于扩分区扩分区意味着数据需要重新分布，这个操作比较费时。查阅了 PL/proxy 关于扩分区的内容，手册中给出的这条建议还是不错的: 预先分配分区： 假如在项目初期如果认为两台服务器可以承担负载，那么可以将数据表分成 8 个分区(当然也能多分些分区)，每台服务器上操持四个，当数据量上升时，那么仅需将分区迁移到其它服务器，而不需要涉及到数据重分布。 我这里的实验是在 PL/Proxy 之一: 使用外部表 SQL/MED搭建 PL/Proxy 搭建的基础之上，扩两个数据节点，手工处理数据重分布。 环境信息原来的架构123proxy db0 db1 备注：之前的PL/Proxy 架构： 一个 proxy 节点，两个数据节点。 目标增加两个数据库12db2 db3 增加数据节点创建表空间目录12mkdir -p /database/pg93/pg_tbs/tbs_db2 mkdir -p /database/pg93/pg_tbs/tbs_db3 创建表空间12create tablespace tbs_db2 owner proxy LOCATION '/database/pg93/pg_tbs/tbs_db2'; create tablespace tbs_db3 owner proxy LOCATION '/database/pg93/pg_tbs/tbs_db3'; 创建数据库12CREATE DATABASE db2 WITH OWNER = proxy TEMPLATE = template0 ENCODING = 'UTF8'TABLESPACE = tbs_db2; CREATE DATABASE db3 WITH OWNER = proxy TEMPLATE = template0 ENCODING = 'UTF8'TABLESPACE = tbs_db3; 在 db2, db3 创建以下表1234CREATE TABLE users ( username text, email text ); 创建 insert_user 函数，在 db2, db3 库上执行12345CREATE OR REPLACE FUNCTION insert_user(i_username text, i_emailaddress text) RETURNS integer AS $$ INSERT INTO users (username, email) VALUES ($1,$2); SELECT 1; $$ LANGUAGE SQL; 创建DDL 函数, 在 db2, db3 库上执行 ( 此步可选，为了维护方便)1234567CREATE OR REPLACE FUNCTION func_ddl(sql text) RETURNS integer AS $$ begin execute sql; return 1; end; $$ LANGUAGE plpgsql; 给 server 添加数据节点配置信息123456789101112131415proxy=# Alter server proxy_srv options(add p2 'dbname=db2 host=127.0.0.1' ,p3 'dbname=db3 host=127.0.0.1'); ALTER SERVER proxy=# \\des+ List of foreign servers -[ RECORD 1 ]--------+------------------------------------------------------------------------------------------------------------------------------------------------------------- Name | proxy_srv Owner | postgres Foreign-data wrapper | plproxy Access privileges | postgres=U/postgres | proxy=U/postgres Type | Version | FDW Options | (connection_lifetime '1800', p0 'dbname=db0 host=127.0.0.1', p1 'dbname=db1 host=127.0.0.1', p2 'dbname=db2 host=127.0.0.1', p3 'dbname=db3 host=127.0.0.1') Description 测试清理数据: proxy 节点执行123456789101112proxy=&gt; select func_ddl('truncate table users;'); NOTICE: PL/Proxy: dropping stale conn NOTICE: PL/Proxy: dropping stale conn NOTICE: PL/Proxy: dropping stale conn NOTICE: PL/Proxy: dropping stale conn func_ddl ---------- 1 1 1 1 (4 rows) 插入10000 条测试数据, proxy 节点执行1234567proxy=&gt; select insert_user('user_'||n, 'user_'||n || '@163.com') from generate_series(1,10000) n; insert_user ------------- 1 1 1 .... 备注：省略输出内容。 数据验证, 各数据节点执行1234567891011121314151617181920212223242526272829db0=&gt; select count(*) from users ; count ------- 2530 (1 row) db0=&gt; \\c db1 You are now connected to database \"db1\" as user \"proxy\". db1=&gt; select count(*) from users ; count ------- 2504 (1 row) db1=&gt; \\c db2 You are now connected to database \"db2\" as user \"proxy\". db2=&gt; select count(*) from users ; count ------- 2497 (1 row) db2=&gt; \\c db3 You are now connected to database \"db3\" as user \"proxy\". db3=&gt; select count(*) from users ; count ------- 2469 (1 row) 备注：可见数据均匀分布到了四个节点。 参考 PL/Proxy 之一: 使用外部表 SQL/MED搭建 PL/Proxy PL/Proxy 之二: Dynamic Records","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"PL/Proxy","slug":"PL-Proxy","permalink":"https://postgres.fun/tags/PL-Proxy/"}]},{"title":"PL/Proxy之二: Dynamic Records","slug":"20140918192517","date":"2014-09-18T11:25:17.000Z","updated":"2018-09-04T01:34:13.422Z","comments":true,"path":"20140918192517.html","link":"","permalink":"https://postgres.fun/20140918192517.html","excerpt":"","text":"关于 PL/Proxy 搭建参考 PL/Proxy 之一: 使用外部表 SQL/MED搭建 PL/Proxy ， 本文紧接着上篇 blog 介绍 PL/Proxy 使用方面， 由于 PL/Proxy 使用通过函数封装，如果每张表的插入，查询操作都单独写函数太烦琐，这里介绍传递 SQL 的用法。 回顾下环境信息,数据库节点信息123proxy db0 db1 备注: proxy 库为 proxy 节点, db0, db1 为数据节点，均在同一台虚拟机的一个 PostgreSQL 实例上。 创建三类函数123func_ddl(sql text) : 传递 ddl 函数 func_dml(sql text) 传递 dml 函数 func_query(sql text) 传递 查询sql 函数 备注：当然仅这三个函数不足以包含所有数据库操作。 一 Proxy 节点执行DDL 函数12345CREATE OR REPLACE FUNCTION func_ddl(sql text) RETURNS SETOF text AS $$ CLUSTER 'proxy_srv'; RUN ON ALL; $$ LANGUAGE plproxy; DML 函数12345CREATE OR REPLACE FUNCTION func_dml(sql text) RETURNS SETOF text AS $$ CLUSTER 'proxy_srv'; RUN ON ANY; $$ LANGUAGE plproxy; query 函数12345CREATE OR REPLACE FUNCTION func_query(sql text) RETURNS SETOF RECORD AS $$ CLUSTER 'proxy_srv'; RUN ON ALL; $$ LANGUAGE plproxy; 二 db0,db1 数据节点执行DDL 函数1234567CREATE OR REPLACE FUNCTION func_ddl(sql text) RETURNS integer AS $$ begin execute sql; return 1; end; $$ LANGUAGE plpgsql; DML 函数1234567CREATE OR REPLACE FUNCTION func_dml(sql text) RETURNS integer AS $$ begin execute sql; return 1; end; $$ LANGUAGE plpgsql; query 函数123456789101112CREATE OR REPLACE FUNCTION func_query(sql text) RETURNS SETOF RECORD AS $$ DECLARE rec RECORD; BEGIN FOR rec IN EXECUTE sql LOOP RETURN NEXT rec; END LOOP; RETURN; END; $$ LANGUAGE plpgsql; 三 测试DDL 函数测试 ，proxy 结点执行12345678proxy=&gt; select func_ddl('create table tbl_proxy1(id int4,name text,create_time timestamp(0) without time zone default clock_timestamp())'); NOTICE: PL/Proxy: dropping stale conn NOTICE: PL/Proxy: dropping stale conn func_ddl ---------- 1 1 (2 rows db0,db1 验证12345678910111213141516db0=&gt; \\dt tbl_proxy1 List of relations Schema | Name | Type | Owner --------+------------+-------+------- public | tbl_proxy1 | table | proxy (1 row) db0=&gt; \\c db1 You are now connected to database \"db1\" as user \"proxy\". db1=&gt; \\dt tbl_proxy1 List of relations Schema | Name | Type | Owner --------+------------+-------+------- public | tbl_proxy1 | table | proxy (1 row) DML 函数测试proxy 节点1234567891011121314151617proxy=&gt; select func_dml('insert into tbl_proxy1 (id,name) values(1,''a'')'); func_dml ---------- 1 (1 row) proxy=&gt; select func_dml('insert into tbl_proxy1 (id,name) values(2,''b'')'); func_dml ---------- 1 (1 row) proxy=&gt; select func_dml('insert into tbl_proxy1 (id,name) values(3,''c'')'); func_dml ---------- 1 (1 row) 数据节点验证123456789101112131415db1=&gt; select * from tbl_proxy1 ; id | name | create_time ----+------+--------------------- 1 | a | 2014-09-18 18:45:08 2 | b | 2014-09-18 18:45:55 (2 rows) db1=&gt; \\c db0 You are now connected to database \"db0\" as user \"proxy\". db0=&gt; select * from tbl_proxy1 ; id | name | create_time ----+------+--------------------- 3 | c | 2014-09-18 18:45:58 (1 row) query 测试1234567proxy=&gt; SELECT * FROM func_query('SELECT * FROM tbl_proxy1;') AS (id integer, name text,create_time timestamp(0) without time zone ); id | name | create_time ----+------+--------------------- 2 | b | 2014-09-18 18:55:13 3 | c | 2014-09-18 18:55:17 1 | a | 2014-09-18 18:55:07 (3 rows) 备注：这里需要定义返回列类型. 四 性能分析我们想插入 10 万数据简单测试这种模式性能，由于 generate_series()函数调用后数据只落在一个数据节点，这里通过写个 shell 脚本插入 10 万数据。 生成 insert 语句的脚本123456[pg93@db1 script]$ cat insert.sh #!/bin/bash for ((i=1;i&lt;=100000;i++)); do echo \"select func_dml('insert into tbl_proxy1(id,name) values($&#123;i&#125;,''$&#123;i&#125;_test'')');\"; done 备注：插入 10 万数据。 清理 db0,db1 数据123456proxy=&gt; select func_ddl('truncate table tbl_proxy1;'); func_ddl ---------- 1 1 (2 rows) 导入数据1[pg93@db1 script]$ ./insert.sh | psql proxy proxy &gt; insert.out 数据验证12345678910111213db1=&gt; select count(*) from tbl_proxy1; count ------- 50020 (1 row) db1=&gt; \\c db0 You are now connected to database \"db0\" as user \"proxy\". db0=&gt; select count(*) from tbl_proxy1; count ------- 49980 (1 row) 备注：数据分布较均匀。 创建索引1234567891011121314151617181920212223242526272829proxy=&gt; select func_ddl('create unique index uk_proxy_id on tbl_proxy1 using btree (id);'); func_ddl ---------- 1 1 (2 rows) db0=&gt; \\d tbl_proxy1 Table \"public.tbl_proxy1\" Column | Type | Modifiers -------------+--------------------------------+--------------------------- id | integer | name | text | create_time | timestamp(0) without time zone | default clock_timestamp() Indexes: \"uk_proxy_id\" UNIQUE, btree (id) db0=&gt; \\c db1 You are now connected to database \"db1\" as user \"proxy\". db1=&gt; \\d tbl_proxy1 Table \"public.tbl_proxy1\" Column | Type | Modifiers -------------+--------------------------------+--------------------------- id | integer | name | text | create_time | timestamp(0) without time zone | default clock_timestamp() Indexes: \"uk_proxy_id\" UNIQUE, btree (id) 备注： 索引已在数据节点创建。 proxy 节点上查询123456proxy=&gt; explain analyze SELECT * FROM func_query('SELECT id,name FROM tbl_proxy1 where id=1;') AS (id integer, name text); QUERY PLAN -------------------------------------------------------------------------------------------------------------- Function Scan on func_query (cost=0.25..10.25 rows=1000 width=36) (actual time=1.803..1.805 rows=1 loops=1) Total runtime: 1.847 ms (2 rows) 数据节点查询1234567db1=&gt; explain analyze SELECT id,name FROM tbl_proxy1 where id=1; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------- Index Scan using uk_proxy_id on tbl_proxy1 (cost=0.29..4.31 rows=1 width=14) (actual time=0.032..0.034 rows=1 loops=1) Index Cond: (id = 1) Total runtime: 0.086 ms (3 rows) 备注: 同样的查询， proxy 节点查询需要 1.847 ms, 数据节点查询仅需 0.086 ms，性能差别较大，所以从这个测试来看，传递 SQL 的这三个函数在性能方面会有下降，不建议使用这种传递 SQL 的方式，建议函数调用方式。 五 参考 PL/Proxy 之一: 使用外部表 SQL/MED搭建 PL/Proxy Ubuntu下PostgreSQL数据库集群(PL/Proxy)配置方法 PL/Proxy Language Syntax Scaling PostgreSQL the Skype way Getting Startted with PL/Proxy PostSQL Using PostgreSQL as a better NoSQL","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"PL/Proxy","slug":"PL-Proxy","permalink":"https://postgres.fun/tags/PL-Proxy/"}]},{"title":"PL/Proxy之一: 使用外部表 SQL/MED搭建 PL/Proxy","slug":"20140918150315","date":"2014-09-18T07:03:15.000Z","updated":"2018-09-04T01:34:13.359Z","comments":true,"path":"20140918150315.html","link":"","permalink":"https://postgres.fun/20140918150315.html","excerpt":"","text":"PL/Proxy 是 PostgreSQL 的 partitioning 解决方案，也能称为集群，据说 skype 公司在用，是成熟，稳定的生产系统解决方案。 PL/Proxy 特性 PL/Proxy functions detect remote functions to be called from their own signature. Function can be run on one, some or all members of the cluster. If query is executed on several partitions, it will happen in parallel. Queries are run in auto-commit mode on the remote server. Query parameters are sent separately from query body, thus avoiding quoting/unquoting overhead on both sides. Does not contain code connection pooling, works with external pooler if needed, preferably PgBouncer.备注：引自 wiki, 不翻译了，接下来准备在笔记本一台虚拟机中搭建 PL/Proxy. PL/Proxy 架构图 安装环境硬件环境：Vmware 虚拟机:数据库 : proxy, db0, db1，且都创建在一个实例上，其中 proxy 为代理结点， db0,db1 为数据结点。备注：环境简单，proxy 结点和数据节点在同一台机器上，纯属实验目的。 安装 PL/Proxy下载 plproxyhttp://pgfoundry.org/frs/download.php/3392/plproxy-2.5.tar.gz 安装1234567891011[root@db1 soft_bak]# tar zxvf plproxy-2.5.tar.gz [root@db1 plproxy-2.5]# source /home/pg93/.bash_profile [root@db1 plproxy-2.5]# make [root@db1 plproxy-2.5]# make install /bin/mkdir -p '/opt/pgsql_9.3.3/lib' /bin/mkdir -p '/opt/pgsql_9.3.3/share/extension' /bin/mkdir -p '/opt/pgsql_9.3.3/share/extension' /usr/bin/install -c -m 755 plproxy.so '/opt/pgsql_9.3.3/lib/plproxy.so' /usr/bin/install -c -m 644 ./plproxy.control '/opt/pgsql_9.3.3/share/extension/' /usr/bin/install -c -m 644 sql/plproxy--2.5.0.sql sql/plproxy--2.3.0--2.5.0.sql sql/plproxy--2.4.0--2.5.0.sql sql/plproxy--unpackaged--2.5.0.sql '/opt/pgsql_9.3.3/share/extension/' 备注：plproxy 安装完成 ，在目录 /opt/pgsql_9.3.3/share/extension/ 下会生成一些 plproxy 脚本，如下 plproxy 相关脚本123456[pg93@db1 extension]$ ll /opt/pgsql_9.3.3/share/extension/plproxy* -rw-r--r-- 1 root root 0 Sep 18 10:51 /opt/pgsql_9.3.3/share/extension/plproxy--2.3.0--2.5.0.sql -rw-r--r-- 1 root root 0 Sep 18 10:51 /opt/pgsql_9.3.3/share/extension/plproxy--2.4.0--2.5.0.sql -rw-r--r-- 1 root root 389 Sep 18 10:51 /opt/pgsql_9.3.3/share/extension/plproxy--2.5.0.sql -rw-r--r-- 1 root root 210 Sep 18 10:51 /opt/pgsql_9.3.3/share/extension/plproxy.control -rw-r--r-- 1 root root 239 Sep 18 10:51 /opt/pgsql_9.3.3/share/extension/plproxy--unpackaged--2.5.0.sql make 过程遇到的错误12345678910[pg93@db1 plproxy-2.5]$ make flex -osrc/scanner.c src/scanner.l make: flex: Command not found make: * [src/scanner.c] Error 127 [root@db1 plproxy-2.5]# make flex -osrc/scanner.c src/scanner.l bison -b src/parser -d src/parser.y make: bison: Command not found make: * [src/parser.tab.c] Error 127 解决方法1yum install flex bison 使用 SQL/MED 方式配置 PL/Proxy计划创建以下三个数据库123proxy db0 db1 创建用户1CREATE ROLE proxy LOGIN ENCRYPTED PASSWORD 'proxy_user' nosuperuser noinherit nocreatedb nocreaterole ; 创建表空间目录123mkdir -p /database/pg93/pg_tbs/tbs_proxy mkdir -p /database/pg93/pg_tbs/tbs_db0 mkdir -p /database/pg93/pg_tbs/tbs_db1 创建表空间123create tablespace tbs_proxy owner proxy LOCATION '/database/pg93/pg_tbs/tbs_proxy'; create tablespace tbs_db0 owner proxy LOCATION '/database/pg93/pg_tbs/tbs_db0'; create tablespace tbs_db1 owner proxy LOCATION '/database/pg93/pg_tbs/tbs_db1'; 创建数据库123CREATE DATABASE proxy WITH OWNER = proxy TEMPLATE = template0 ENCODING = 'UTF8'TABLESPACE = tbs_proxy; CREATE DATABASE db0 WITH OWNER = proxy TEMPLATE = template0 ENCODING = 'UTF8'TABLESPACE = tbs_db0; CREATE DATABASE db1 WITH OWNER = proxy TEMPLATE = template0 ENCODING = 'UTF8'TABLESPACE = tbs_db1; 在 db0 db1 创建以下表1234CREATE TABLE users ( username text, email text ); 创建 proxy 模式，并导入 proxy 模块123456789101112[pg93@db1 extension]$ psql proxy proxy psql (9.3.3) Type \"help\" for help. proxy=&gt; create schema proxy; CREATE SCHEMA [pg93@db1 extension]$ psql -d proxy -U postgres -f /opt/pgsql_9.3.3/share/extension/plproxy--2.5.0.sql CREATE FUNCTION CREATE LANGUAGE CREATE FUNCTION CREATE FOREIGN DATA WRAPPER 创建 server12345proxy=# CREATE SERVER proxy_srv FOREIGN DATA WRAPPER plproxy OPTIONS (connection_lifetime '1800', p0 'dbname=db0 host=127.0.0.1', p1 'dbname=db1 host=127.0.0.1' ); 备注： connection_lifetime 表示连接生存周期，秒为单位，更多参数解释参考手册 PL/Proxy Cluster Configuration 。 赋权1proxy=# grant usage on foreign server proxy_srv to proxy; create mapping user1proxy=&gt;CREATE USER MAPPING FOR proxy SERVER proxy_srv OPTIONS (user 'proxy', password 'proxy_user'); 创建 Partitioned、Remote 函数获取用户 email 函数, 在 proxy 库上执行123456CREATE OR REPLACE FUNCTION get_user_email(i_username text) RETURNS SETOF text AS $$ CLUSTER 'proxy_srv'; RUN ON hashtext(i_username) ; SELECT email FROM users WHERE username = i_username; $$ LANGUAGE plproxy; 插入函数，在 db0, db1 库上执行12345CREATE OR REPLACE FUNCTION insert_user(i_username text, i_emailaddress text) RETURNS integer AS $$ INSERT INTO users (username, email) VALUES ($1,$2); SELECT 1; $$ LANGUAGE SQL; 分发 insert sql 函数, 在 proxy 库上执行12345CREATE OR REPLACE FUNCTION insert_user(i_username text, i_emailaddress text) RETURNS integer AS $$ CLUSTER 'proxy_srv'; RUN ON hashtext(i_username); $$ LANGUAGE plproxy; 测试插入数据: proxy 库上执行12345SELECT insert_user('Sven','sven@somewhere.com'); SELECT insert_user('Marko', 'marko@somewhere.com'); SELECT insert_user('Steve','steve@somewhere.cm'); SELECT insert_user('francs','francs@somewhere.cm'); SELECT insert_user('test','test@somewhere.cm'); 数据查询: proxy 库上执行12345678910111213141516proxy=&gt; SELECT get_user_email('Sven'); get_user_email -------------------- sven@somewhere.com (1 row) proxy=&gt; SELECT get_user_email('francs'); get_user_email --------------------- francs@somewhere.cm (1 row) proxy=&gt; SELECT get_user_email('test'); get_user_email ------------------- test@somewhere.cm 数据分布1234567891011121314151617181920212223[pg93@db1 extension]$ psql psql (9.3.3) Type \"help\" for help. postgres=# \\c db0 You are now connected to database \"db0\" as user \"postgres\". db0=# select * from users ; username | email ----------+--------------------- francs | francs@somewhere.cm (1 row) db0=# \\c db1 You are now connected to database \"db1\" as user \"postgres\". db1=# select * from users ; username | email ----------+--------------------- Sven | sven@somewhere.com Marko | marko@somewhere.com Steve | steve@somewhere.cm test | test@somewhere.cm (4 rows) 备注： 以上是最简单的 pl/proxy 搭建方式，手册上提到当查询请求通过 proxy 库分发时，proxy 到数据节点在一定程度上会浪费连接，所以建议使用连接池，如 pgBouncer. 关于性能目前暂未测试，待续。 参考 Skype Plans for PostgreSQL to Scale to 1 Billion Users PostgreSQL cluster: partitioning with plproxy (part I) Digoal.Zhou_PostgreSQL_PLProxy原理与实践.pdf PL/Proxy tutorial wiki/PL/Proxy PL/Proxy Language Syntax Ubuntu下PostgreSQL数据库集群(PL/Proxy)配置方法","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"PL/Proxy","slug":"PL-Proxy","permalink":"https://postgres.fun/tags/PL-Proxy/"}]},{"title":"PostgreSQL: 如何插入包含单引号的字符串 ? ","slug":"20140916111754","date":"2014-09-16T03:17:54.000Z","updated":"2018-09-04T01:34:13.297Z","comments":true,"path":"20140916111754.html","link":"","permalink":"https://postgres.fun/20140916111754.html","excerpt":"","text":"今天有开发人员咨询如何插入包含单引号的字符串，例如 kate’s horse，网上查了下资料，最后还是文档中给出了解释，如下: 单引号的转义 To include a single-quote character within a string constant, write two adjacent single quotes, e.g., ‘Dianne’’s horse’. Note that this is not the same as a double-quote character (“). 备注：文档中说的很清楚，写两个单引号即可，注意不能写成双引号。 测试12345678910francs=&gt; create table test_3(id int4,name text); CREATE TABLE francs=&gt; insert into test_3 (id,name) values (1,'kate''s horse'); INSERT 0 1 francs=&gt; select * from test_3; id | name ----+-------------- 1 | kate's horse 参考 http://www.postgresql.org/docs/9.3/static/sql-syntax-lexical.html#SQL-SYNTAX-CONSTANTS","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Zabbix: 源码编译安装","slug":"20140902105751","date":"2014-09-02T02:57:51.000Z","updated":"2018-09-04T01:34:13.234Z","comments":true,"path":"20140902105751.html","link":"","permalink":"https://postgres.fun/20140902105751.html","excerpt":"","text":"zabbix 是一个企业级开源分布式监控软件， 打算学习下，这篇 blog 介绍 zabbix 源码安装。 环境信息zabbix server: 192.168.2.37zabbix agent: 192.168.2.38 Zabbix 服务端安装下载http://sourceforge.net/projects/zabbix/files/ZABBIX%20Latest%20Stable/2.2.6/zabbix-2.2.6.tar.gz/download 解压1[root@db1 soft_bak]# tar zxvf zabbix-2.2.6.tar.gz 创建 zabbix 用户123[root@db1 soft_bak]# groupadd zabbix [root@db1 soft_bak]# useradd -g zabbix zabbix [root@db1 soft_bak]# passwd zabbix 创建 zabbix 数据库123456789101112131415161718192021222324252627[pg94@db1 ~]$ mkdir -p /database/pg94/pg_tbs/tbs_zabbix [pg94@db1 ~]$ psql -h 127.0.0.1 psql (9.4beta1) Type \"help\" for help. postgres=# CREATE ROLE zabbix LOGIN ENCRYPTED PASSWORD 'zabbix' nosuperuser noinherit nocreatedb nocreaterole ; CREATE ROLE [pg94@db1 ~]$ psql -h 127.0.0.1 psql (9.4beta1) Type \"help\" for help. postgres=# create tablespace tbs_zabbix owner postgres LOCATION '/database/pg94/pg_tbs/tbs_zabbix'; CREATE TABLESPACE postgres=# CREATE DATABASE zabbix postgres-# WITH OWNER = postgres postgres-# TEMPLATE = template0 postgres-# ENCODING = 'UTF8' postgres-# TABLESPACE = tbs_zabbix; CREATE DATABASE postgres=# grant all on database zabbix to zabbix with grant option; GRANT postgres=# grant all on tablespace tbs_zabbix to zabbix; GRANT 导入 zabbix 数据1234[pg93@db1 ~]$ cd /opt/soft_bak/zabbix-2.2.6/database/postgresql/ [pg94@db1 postgresql]$ psql -h 127.0.0.1 -d zabbix -U zabbix -a -f schema.sql [pg94@db1 postgresql]$ psql -h 127.0.0.1 -d zabbix -U zabbix -a -f images.sql [pg94@db1 postgresql]$ psql -h 127.0.0.1 -d zabbix -U zabbix -a -f data.sql 备注： zabbix 数据库字符集为 UTF-8。 编译12345678910111213141516171819[root@db1 zabbix-2.2.6]# mkdir /usr/local/zabbix [root@db1 zabbix-2.2.6]# ./configure --prefix=/usr/local/zabbix --enable-server --enable-agent --with-postgresql --enable-ipv6 --with-net-snmp --with-libcurl --with-libxml2 编译报错 .... checking for file /proc/stat... yes checking for file /proc/cpuinfo... yes checking for file /proc/0/psinfo... no checking for file /proc/loadavg... yes checking for file /proc/net/dev... yes checking for long long format... no checking for -rdynamic linking option... yes checking for libperfstat 5.2.0.40 fileset... no checking for libperfstat 5.3.0.60 fileset... no checking for architecture... linux (linux-gnu) checking for the linux kernel version... 2.6 family (2.6.32-220.el6.i686) checking size of void *... 4 checking for Oracle support... no checking for pg_config... no configure: error: PostgreSQL library not found 备注：找不到 postgresql 相关信息，环境变量没加上，加上环境变量编译通过，如果还没其它包没装， yum 安装即可。 yum 补充安装的包123yum install libxml2-devel yum install net-snmp-devel.i686 yum install libcurl-devel 安装1[root@db1 zabbix-2.2.6]# make install 配置 php 前端12[root@db1 php]# mkdir -p /var/www/html/zabbix [root@db1 php]# cp -r /opt/soft_bak/zabbix-2.2.6/frontends/php/* /var/www/html/zabbix/ 备注：将php 程序复制到指定目录即可。 设置 zabbix 环境变量1234export ZABBIX_HOME=/usr/local/zabbix export PGHOME=/opt/pgsql_9.4beta1 export LD_LIBRARY_PATH=$PGHOME/lib:$ZABBIX_HOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib export PATH=$ZABBIX_HOME/bin:$ZABBIX_HOME/sbin:$PATH. 打开浏览器http://192.168.2.37/zabbix/setup.php 点 Next , 进入 “Check of pre-requisites” 界面，一开始，我这步有很多选项没通过，没通过的参数只需要更改 /etc/php.ini 里的相应配置，并重启 httpd 即可，其它缺少的包需要安装。 接下来配置数据库连接，我这边用的是 PostgreSQL 数据库，遇到这个问题: 报错代码 “the frontend does not match zabbix database” ， 经分析是由于在 zabbix 数据库中创建了 zabbix 模式，而默认的情况 zabbix 读取的是 public 模式，所以不通过了 全部配置通过后，看到如下界面： 默认用户名 Admin ，密码 zabbix 。 如需查看 php 配置1234[root@db1 ~]# cat /var/www/html/index.php &lt;?php phpinfo(); ?&gt; 备注：浏览器输入http://192.168.2.37/index.php 可查看 php 配置。 配置 /usr/local/etc/zabbix_server.conf123456[root@db1 etc]# grep \"^[A-Z]\" zabbix_server.conf LogFile=/tmp/zabbix_server.log DBHost=127.0.0.1 DBName=zabbix DBUser=zabbix DBPort=1921 开启 zabbix 服务1[zabbix@db1 ~]$ zabbix_server 可能遇到的错误12[zabbix@db1 ~]$ zabbix_server --help zabbix_server: error while loading shared libraries: libpq.so.5: cannot open shared object file: No such file or directory 备注：是由于没配好 LD_LIBRARY_PATH 的原因。 Zabbix 客户端安装下载http://www.zabbix.com/downloads/2.2.5/zabbix_agents_2.2.5.linux2_6.i386.tar.gz 解压12[root@db2 zabbix]# cd /usr/local/zabbix [root@db2 zabbix]# tar zxvf zabbix_agents_2.2.5.linux2_6.i386.tar.gz 配置 /usr/local/zabbix/conf/zabbix_agentd.conf12345LogFile=/tmp/zabbix_agentd.log SourceIP=192.168.2.38 ## zabbix 客户端 IP Server=192.168.2.37 ## zabbix 服务端 IP ListenPort=10050 Hostname=db2 开启 agentd1[zabbix@db2 sbin]$ zabbix_agentd -c /usr/local/zabbix/conf/zabbix_agentd.conf 查看 agentd 进程1234567[root@db2 ~]# ps -ef | grep zabbix_agent zabbix 3292 1 0 09:47 ? 00:00:00 zabbix_agentd -c /usr/local/zabbix/conf/zabbix_agentd.conf zabbix 3293 3292 0 09:47 ? 00:00:02 zabbix_agentd: collector [idle 1 sec] zabbix 3294 3292 0 09:47 ? 00:00:00 zabbix_agentd: listener #1 [waiting for connection] zabbix 3295 3292 0 09:47 ? 00:00:00 zabbix_agentd: listener #2 [waiting for connection] zabbix 3296 3292 0 09:47 ? 00:00:00 zabbix_agentd: listener #3 [waiting for connection] root 4163 3648 0 10:31 pts/2 00:00:00 grep zabbix_agent 可能遇到的问题 浏览器输入”http://192.168.2.37/zabbix/&quot; 进入主界面，看到了如下告警1Zabbix agent on db2 is unreachable for 5 minutes 备注：经过一段时间检查，发现 agentd 的端口号配置错了，配置页面 Configuraton -&gt; Hosts -&gt; DB2 ，如下图 参考 Zabbix Installation from sources Use zabbix to Monitor your servers","categories":[{"name":"监控","slug":"监控","permalink":"https://postgres.fun/categories/监控/"}],"tags":[{"name":"Zabbix","slug":"Zabbix","permalink":"https://postgres.fun/tags/Zabbix/"}]},{"title":"Ubuntu: 安装 Oracle 客户端 ","slug":"20140826113021","date":"2014-08-26T03:30:21.000Z","updated":"2018-09-04T01:34:13.172Z","comments":true,"path":"20140826113021.html","link":"","permalink":"https://postgres.fun/20140826113021.html","excerpt":"","text":"在 ubuntu 桌面上有很多图形化工具连接 oracle 数据库, 比如 DBeaver 等， 但有时还是需要通过 sqlplus 连接远程数据库，这时需要安装 oracle 客户端了。本文介绍 ubuntu 下安装 oracle 12.2.0.5 客户端的步骤。 下载1http://www.oracle.com/technetwork/topics/linuxx86-64soft-092277.html 备注：根据需要下载相应版本的客户端。 下载以下 rpm 包1234francs@francs:~/Downloads$ ll *.rpm -rw-rw-r-- 1 francs francs 34M 8月 26 09:40 oracle-instantclient-basic-10.2.0.5-1.x86_64.rpm -rw-rw-r-- 1 francs francs 284K 8月 26 09:58 oracle-instantclient-devel-10.2.0.5-1.x86_64.rpm -rw-rw-r-- 1 francs francs 776K 8月 26 09:57 oracle-instantclient-sqlplus-10.2.0.5-1.x86_64.rpm 安装 Alien123456789101112131415161718192021francs@francs:~/Downloads$ sudo alien -i *.rpm dpkg --no-force-overwrite -i oracle-instantclient-basic_10.2.0.5-2_amd64.deb Selecting previously unselected package oracle-instantclient-basic. (正在读取数据库 ... 系统当前共安装有 309521 个文件和目录。) Preparing to unpack oracle-instantclient-basic_10.2.0.5-2_amd64.deb ... Unpacking oracle-instantclient-basic (10.2.0.5-2) ... 正在设置 oracle-instantclient-basic (10.2.0.5-2) ... Processing triggers for libc-bin (2.19-0ubuntu6.1) ... dpkg --no-force-overwrite -i oracle-instantclient-devel_10.2.0.5-2_amd64.deb Selecting previously unselected package oracle-instantclient-devel. (正在读取数据库 ... 系统当前共安装有 309536 个文件和目录。) Preparing to unpack oracle-instantclient-devel_10.2.0.5-2_amd64.deb ... Unpacking oracle-instantclient-devel (10.2.0.5-2) ... 正在设置 oracle-instantclient-devel (10.2.0.5-2) ... dpkg --no-force-overwrite -i oracle-instantclient-sqlplus_10.2.0.5-2_amd64.deb Selecting previously unselected package oracle-instantclient-sqlplus. (正在读取数据库 ... 系统当前共安装有 309579 个文件和目录。) Preparing to unpack oracle-instantclient-sqlplus_10.2.0.5-2_amd64.deb ... Unpacking oracle-instantclient-sqlplus (10.2.0.5-2) ... 正在设置 oracle-instantclient-sqlplus (10.2.0.5-2) ...备注：安装完成。 运行 Sqlplus12francs@francs:/usr/lib/oracle/10.2.0.5/client64/bin$ ./sqlplus ./sqlplus: error while loading shared libraries: libsqlplus.so: cannot open shared object file: No such file or directory 备注：是因为 lib 环境变量没加。 环境变量添加以下12345francs@francs:~$ vim ~/.bashrc # env export ORACLE_HOME=/usr/lib/oracle/10.2.0.5/client64 export PATH=$ORACLE_HOME/bin:$PATH export LD_LIBRARY_PATH=$ORACLE_HOME/lib:$LD_LIBRARY_PATH 再次测试123456francs@francs:~$ source .bashrc francs@francs:~$ which sqlplus /usr/lib/oracle/10.2.0.5/client64/bin/sqlplus sqlplus username/password@//dbhost:1521/SID 备注：这时测试 sqlplus 连接远程 oracle 数据库成功，文档中提到如果此步出现缺少 libaio.so.1 文件的错误，则需要运行 “sudo apt-get install libaio1” 命令。 参考 Oracle Instant Client","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"PostgreSQL：通过 .psqlrc 定制监控脚本- 续","slug":"20140725110317","date":"2014-07-25T03:03:17.000Z","updated":"2018-09-04T01:34:13.109Z","comments":true,"path":"20140725110317.html","link":"","permalink":"https://postgres.fun/20140725110317.html","excerpt":"","text":"之前blog PostgreSQL：通过 .psqlrc 定制监控脚本 介绍了通过 .psqlrc 制定监控脚本的例子, 今天接着补充下。 先来看几个不带参数的监控脚本， 以下实验均在 PostgreSQL 9.2.4 下测试。 不带参数的监控脚本修改 ~/.psqlrc 文件，加入以下内容1set active_session 'select pid, datname,usename,query_start,client_addr,query ,waiting from pg_stat_activity where state='active' and pid &lt;&gt; pg_backend_pid() order by query;' 备注： 此查询用来查询当前活动会话。 执行后的结果如下： 再来看一个查询，在 .psqlrc 文件加入以下行： 查询连接数1set connections 'select datname,usename,client_addr,count(*) from pg_stat_activity where pid &lt;&gt; pg_backend_pid() group by 1,2,3 order by 1,2,4 desc;' 执行后的结果备注： 这里是根据数据库名，用户名，应用端 IP 进行统计 。 不带参数的监控脚本比较容易定制，接着演示带参数的监控脚本。 带参数的监控脚本: 参数类型为整型在 .psqlrc 文件加入以下行根据 oid 查询对像信息1set get_oid 'select oid,relname,relkind,reltuples,relfilenode from pg_class where oid=:v_oid;' 备注： get_oid 脚本根据数据库对像 oid 查询对像信息，这里使用了参数 v_oid 为传入参数。 调用 get_oid 脚本1234567891011[pg92@db1 ~]$ psql francs francs psql (9.2.4) Type \"help\" for help. francs=&gt; set v_oid 39285 francs=&gt; select oid,relname,relkind,reltuples,relfilenode from pg_class where oid=39285; oid |relname | relkind | reltuples | relfilenode -------+-------------------------------------+---------+-----------+------------- 39285 | tbl_wo_account_permission_user_pkey | i | 257 | 39285 (1 row) 备注： 参数类型为整型容易处理，那么参数类型为字符型应该如何处理呢？ 这里研究了好久。 带参数的监控脚本: 参数类型为字符型在 .psqlrc 文件加入以下行查询指定表空间表占用大小 top101set top10_ts_table 'select relname, relkind, relpages,pg_size_pretty(pg_relation_size(a.oid)),reltablespace,relowner from pg_class a, pg_tablespace tb where a.relkind in ('r', 'i') and a.reltablespace=tb.oid and tb.spcname=':v_spcname' order by a.relpages desc limit 10;' 备注：变量 v_spcname 用来传入表空间的名称。 测试123456789101112131415161718192021222324[pg92@db1 ~]$ psql francs francs psql (9.2.4) Type \"help\" for help. francs=&gt; \\db List of tablespaces Name | Owner | Location -------------------+----------+----------------------------------------- pg_default | postgres | pg_global | postgres | tbs_db_francs | postgres | /database/pg92/pg_tbs/tbs_db_francs tbs_francs | postgres | /database/pg92/pg_tbs/tbs_francs tbs_francs_idx | postgres | /database/pg92/pg_tbs/tbs_francs_idx (5 rows) francs=&gt; set v_spcname tbs_francs_idx francs=&gt; echo :v_spcname tbs_francs_idx francs=&gt; :top10_ts_table relname | relkind | relpages | pg_size_pretty | reltablespace | relowner ---------+---------+----------+----------------+---------------+---------- (0 rows) 备注：调用为空，测试前我已经手工将多张表移到了 tbs_francs_idx 表空间上，为什么查询出来为空呢？ 看下数据库日志： 数据库日志12014-07-25 02:28:53.831 GMT,\"francs\",\"francs\",6722,\"[local]\",53d1c0db.1a42,4,\"idle\",2014-07-25 02:28:43 GMT,2/24,0,LOG,00000,\"statement: select relname, relkind, relpages,pg_size_pretty(pg_relation_size(a.oid)),reltablespace,relowner from pg_class a, pg_tablespace tb where a.relkind in ('r', 'i') and a.reltablespace=tb.oid and tb.spcname=':v_spcname' order by a.relpages desc limit 10;\",,,,,,,,,\"psql\" 备注：从日志看出， 变量 v_spcname 的值根本没有传进来，接着换一种测试方式。 修改 .psqlrc 文件1set top10_ts_table 'select relname, relkind, relpages,pg_size_pretty(pg_relation_size(a.oid)),reltablespace,relowner from pg_class a, pg_tablespace tb where a.relkind in ('r', 'i') and a.reltablespace=tb.oid and tb.spcname=:v_spcname order by a.relpages desc limit 10;' 备注： 修改了 v_spcname 变量调用部分，去掉了两边的单引号。 调用测试123456789101112[pg92@db1 ~]$ psql francs francs psql (9.2.4) Type \"help\" for help. francs=&gt; set v_spcname 'tbs_francs_idx' francs=&gt; :top10_ts_table ERROR:column \"tbs_francs_idx\" does not exist LINE 1: ...', 'i') and a.reltablespace=tb.oid and tb.spcname=tbs_skypcs... ^ francs=&gt; echo :v_spcname tbs_francs_idx 备注：调用报错，报错原因很明显, 表空间名为字符类型，单引号没传进去，接着测试。 再次测试12345678910111213141516171819202122[pg92@db1 ~]$ psql francs francs psql (9.2.4) Type \"help\" for help. francs=&gt; set v_spcname ''tbs_francs_idx'' francs=&gt; echo :v_spcname 'tbs_francs_idx' --上面说明已将两个单引号传进来了 francs=&gt; :top10_ :top10_index :top10_table :top10_ts_table francs=&gt; :top10_ts_table relname | relkind | relpages | pg_size_pretty | reltablespace | relowner -----------------------+---------+----------+----------------+---------------+---------- tbl_wo_channel | r | 8520 | 67 MB | 49152 | 16388 tbl_wo_channel_status | r | 2655 | 21 MB | 49152 | 16388 tbl_wo_channel_change | r | 9 | 72 kB | 49152 | 16388 pg_toast_16427_index | i | 1 | 8192 bytes | 49152 | 16388 pg_toast_16528_index | i | 1 | 8192 bytes | 49152 | 16388 pg_toast_16512_index | i | 1 | 8192 bytes | 49152 | 16388 (6 rows) 备注：从上看出，需要通过 转义将两个单引号传入，同时 .psqlrc 写好的脚本支持 Tab 补全，非常好用。这时得到了我们想要的结果。 总结通过 .psqlrc 定制自己日常工作中经常使用的监控脚本可以提高工作效率，遇到问题时不需要手工敲打大量脚本。这里只是抛砖引玉，其它监控脚本需要大家在日常工作中挖掘。 之所以研究这些，是因为之前在维护 Oracle 数据库习惯了将大量维护脚本写在本地，连上数据库后通过@脚本名的方式调用 ，感觉很帅。 参考 PostgreSQL：通过 .psqlrc 定制监控脚本 psql","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"psql","slug":"psql","permalink":"https://postgres.fun/tags/psql/"}]},{"title":"PostgreSQL: 流复制备库 PITR 恢复一例 ","slug":"20140716163713","date":"2014-07-16T08:37:13.000Z","updated":"2018-09-04T01:34:13.062Z","comments":true,"path":"20140716163713.html","link":"","permalink":"https://postgres.fun/20140716163713.html","excerpt":"","text":"今天发现一套流复制环境备库异常, 已经不能和主库同步了, 这个库是日志库, 经常出现主备数据同步延迟的告警, 这类告警经常被俺疏忽, 环境信息如下: 流复制环境PG 版本: 9.2.8数据库大小 : &gt; 3 TB业务类型: 日志库高可用: 流复制WAL SIZE: 16 MB每天归档量: 30000 个左右 WAL 备库数据库日志1232014-07-16 10:14:33.515 CST,,,1202,,53c5e009.4b2,2,,2014-07-16 10:14:33 CST,,0,FATAL,XX000,\"could not receive data from WAL stream: FATAL: requested WAL segment 00000006000060EB000000A2 has already been removed\",,,,,,,,\"libpqrcv_receive, libpqwalreceiver.c:389\",\"\" 2014-07-16 10:14:38.520 CST,,,1210,,53c5e00e.4ba,1,,2014-07-16 10:14:38 CST,,0,LOG,00000,\"streaming replication successfully connected to primary\",,,,,,,,\"libpqrcv_connect, libpqwalreceiver.c:171\",\"\" 2014-07-16 10:14:38.520 CST,,,1210,,53c5e00e.4ba,2,,2014-07-16 10:14:38 CST,,0,FATAL,XX000,\"could not receive data from WAL stream: FATAL: requested WAL segment 00000006000060EB000000A2 has already been removed\" 备注:大量上述日志, 关于这个问题, 之前写了 blog ,如下:PostgreSQL：“ FATAL: requested WAL segment 0000000800002A0000000000 has already been removed” 之前的处理方法是重做备库, 这次不需要, 因为前段时间已开启了此库的归档。 查看主库的归档123456789postgres@db--&gt; psql psql (9.2.8) Type \"help\" for help. postgres=# show archive_command ; archive_command ------------------------------------------------------------------------------------------------ DIR=/pg_arch/arch/`date +%F`; test ! -d $DIR &amp;&amp; mkdir $DIR; test ! -f $DIR/%f &amp;&amp; cp %p $DIR/%f (1 row) 备注: 每天一个归档目录, 既然有了归档, 计划将归档目录 nfs 共享给备库做恢复 归档目录123postgres@db--&gt; ls /pg_arch/arch/ 2014-06-30 2014-07-02 2014-07-04 2014-07-06 2014-07-08 2014-07-10 2014-07-12 2014-07-14 2014-07-16 2014-07-01 2014-07-03 2014-07-05 2014-07-07 2014-07-09 2014-07-11 2014-07-13 2014-07-15 配置 NFS修改主库 /etc/exports1/pg_arch 192.168.xxx.xxx/32(rw,no_root_squash,sync) 重启 nfs 服务1234567891011[root@db- arch]# service nfs restart Shutting down NFS daemon: [OK ] Shutting down NFS mountd: [OK ] Shutting down NFS quotas: [OK ] Shutting down NFS services: [OK ] Shutting down RPC idmapd: [OK ] Starting NFS services: [OK ] Starting NFS quotas: [OK ] Starting NFS mountd: [OK ] Starting NFS daemon: [OK ] [[AStarting RPC idmapd: [OK ] 备库 mount 归档目录12[root@db2 ~]# mkdir /pg_restore [root@db2 ~]# mount -t nfs 192.168.xxx.xxx:/pg_arch /pg_restore 备注: 我们将归档目录 mount 到 /pg_restore 目录, nfs 其它配置这里不记录了.请查看 nfs 相关文档,接下来可以进行还原操作了. 修改备库配置修改备库 recovery.conf 文件的以下参数1restore_command = 'cp /pg_restore/arch/2014-07-[1-3][0-9]/%f %p;' 备注: 因为要取多天归档, 所以用了正则. 修改主库配置操作前, 调整主库参数1wal_keep_segments = 20000 备注： 之前设置得太小了，调整后执行 reload 操作。 重启备库1pg_ctl restart -m fast -D $PGDATA 备注： 接下来进行备库恢复操作， 重启备库。 查看备库日志1234567891011121314152014-07-16 10:14:54.583 CST,,,1305,,53c5e01e.519,1,,2014-07-16 10:14:54 CST,,0,LOG,00000,\"database system was shut down in recovery at 2014-07-16 10:14:42 CST\",,,,,,,,\"StartupXLOG, xlog.c:6273\",\"\" 2014-07-16 10:15:09.231 CST,,,1619,\"\",53c5e02d.653,1,\"\",2014-07-16 10:15:09 CST,,0,LOG,00000,\"connection received: host=192.168.100.3 port=50518\",,,,,,,,\"BackendInitialize, postmaster.c:3471\",\"\" 2014-07-16 10:15:09.234 CST,,,1619,\"192.168.100.3:50518\",53c5e02d.653,2,\"\",2014-07-16 10:15:09 CST,,0,LOG,08P01,\"incomplete startup packet\",,,,,,,,\"ProcessStartupPacket, postmaster.c:1505\",\"\" 2014-07-16 10:15:50.825 CST,,,2241,\"\",53c5e056.8c1,1,\"\",2014-07-16 10:15:50 CST,,0,LOG,00000,\"connection received: host=127.0.0.1 port=11732\",,,,,,,,\"BackendInitialize, postmaster.c:3471\",\"\" 2014-07-16 10:15:50.825 CST,\"postgres\",\"postgres\",2241,\"127.0.0.1:11732\",53c5e056.8c1,2,\"\",2014-07-16 10:15:50 CST,,0,FATAL,57P03,\"the database system is starting up\",,,,,,,,\"ProcessStartupPacket, postmaster.c:1759\",\"\" 2014-07-16 10:15:52.906 CST,,,2258,\"\",53c5e058.8d2,1,\"\",2014-07-16 10:15:52 CST,,0,LOG,00000,\"connection received: host=127.0.0.1 port=11734\",,,,,,,,\"BackendInitialize, postmaster.c:3471\",\"\" .... ... 2014-07-16 10:23:15.393 CST,,,1305,,53c5e01e.519,11,,2014-07-16 10:14:54 CST,1/0,0,LOG,00000,\"restored log file \"\"00000006000060EB0000000C\"\" from archive\",,,,,,,,\"RestoreArchivedFile, xlog.c:3273\",\"\" 2014-07-16 10:23:18.318 CST,,,1305,,53c5e01e.519,12,,2014-07-16 10:14:54 CST,1/0,0,LOG,00000,\"restored log file \"\"00000006000060EB0000000D\"\" from archive\",,,,,,,,\"RestoreArchivedFile, xlog.c:3273\",\"\" 2014-07-16 10:24:17.714 CST,,,1305,,53c5e01e.519,13,,2014-07-16 10:14:54 CST,1/0,0,LOG,00000,\"restored log file \"\"00000006000060EB0000000E\"\" from archive\",,,,,,,,\"RestoreArchivedFile, xlog.c:3273\",\"\" 2014-07-16 10:24:20.552 CST,,,1305,,53c5e01e.519,14,,2014-07-16 10:14:54 CST,1/0,0,LOG,00000,\"restored log file \"\"00000006000060EB0000000F\"\" from archive\",,,,,,,,\"RestoreArchivedFile, xlog.c:3273\",\"\" .... 省略部分日志 备注: 可以看到, 备库在拼命地读归档日志并 apply, 库比较大, WAL 也很多, 共有两天的 WAL 日志需要应用 ， 估计没个一天半载跑不完。( 后来花了一天半才恢复！) 后续措施 调高 wal_keep_segments 参数 加大监控力度 参考 正则表达式 restore_command (string) PostgreSQL：“ FATAL: requested WAL segment 0000000800002A0000000000 has already been removed”","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"},{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"Ubuntu: 通过 SecureCRT 连接本地 Ubuntu 系统 ","slug":"20140716114651","date":"2014-07-16T03:46:51.000Z","updated":"2018-09-04T01:34:13.000Z","comments":true,"path":"20140716114651.html","link":"","permalink":"https://postgres.fun/20140716114651.html","excerpt":"","text":"ubuntu 自带的终端有时不方便操作, 所以希望通过 SecureCRT 连接本机的 ubuntu 系统, 但通过 ssh 方式不合适 , 一方面因为需要多开启一个 sshd 服务, 另一方面需要在公司和家办公, IP 也不同; 幸运的是 SecureCRT 支持本地连接方式, 很方便就能连接本地 ubuntu 系统, 如下操作: 设置 SecureCRT打开SecureCRT，开启新连接, 注意 Protocol 类型选择 “ Local Shell “ 连接测试之后连接测试，如下：备注: 以前不知道可以这么做, 太爽了.","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"CentOS 6.5: KVM 安装 ","slug":"20140712165727","date":"2014-07-12T08:57:27.000Z","updated":"2018-09-04T01:34:12.922Z","comments":true,"path":"20140712165727.html","link":"","permalink":"https://postgres.fun/20140712165727.html","excerpt":"","text":"KVM 是 kernel-based Virtual Machine的简称，是一个开源的系统虚拟化模块，用于服务器虚拟化，KVM的虚拟化需要硬件支持, 下面记录下 KVM 的安装。 环境信息设备信息和操作系统版本，如下：机器：ProLiant DL360 G5系统: CentOS 6.5(x86_64) 查看 cpu 是否支持虚拟化。12345[root@db soft_bak]# grep -E -o \"vmx|svm\" /proc/cpuinfo vmx vmx vmx vmx 安装相关包1yum install kvm kmod-kvm qemu kvm-qemu-img virt-viewer virt-manager bridge-utils tunctl libvirt 检查模块是否加载123[root@db ~]# lsmod | grep kvm kvm_intel 54317 0 kvm 333542 1 kvm_intel 验证时报错123[root@db ~]# virsh -c qemu:///system list error: Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such file or directory error: failed to connect to the hypervisor 报错信息 解决方法1[root@db ~]# service libvirtd start 备注：启动 libvirtd 服务即可。 再次测试显示正常123[root@db soft_bak]# virsh -c qemu:///system list Id Name State ---------------------------------------------------- 启动 KVM运行 virt-manager 打开 kvm 管理界面，如下：备注：到这里说明 kvm 已安装成功了，接下来可以安装系统了，刚开始玩 KVM, 不熟悉的地方还很多。 参考 http://www.howtoforge.com/virtualization-with-kvm-on-a-centos-6.3-server http://blog.yunvi.com/html/803.html http://libvirt.org/","categories":[{"name":"虚拟化","slug":"虚拟化","permalink":"https://postgres.fun/categories/虚拟化/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"https://postgres.fun/tags/KVM/"}]},{"title":"ZFS: 关于压缩(Compression) ","slug":"20140710114452","date":"2014-07-10T03:44:52.000Z","updated":"2018-09-04T01:34:12.859Z","comments":true,"path":"20140710114452.html","link":"","permalink":"https://postgres.fun/20140710114452.html","excerpt":"","text":"支持压缩是 zfs 文件系统的一个重要特性，zfs 的压缩功能对用户是透明的，也就是说压缩和解压操作由 zfs 文件系统自动完成。支持的压缩算法有 LZJB, ZLE, gzip[1-9]，lz4。 zfs 的压缩算法对 CPU 的消耗很低，并且某些压缩算法速度很快，因此压缩的代价是很低的，建议开启 zfs 文件系统的压缩功能。 另外, 可以对 zfs 池的一部分 dataset 开启压缩功能，也可以对池的一部分 dataset 不开启压缩; 开启压缩功能后，仅对新写入的数据生效，dataset 上之前的数据不会被压缩。 1 zfs 的 compression 选项1compression=on | off | lzjb | gzip | gzip-N | zle | lz4 备注：zfs 默认关闭压缩功能， 设置 compression=on 将默认使用 LZJB 压缩方式。 2 gzip 压缩举例 12345678910111213141516root@francs:/data2 # zfs set compression=gzip zp1/data1 root@francs:/data2 # zfs get all zp1/data1 | grep compress zp1/data1 compressratio 1.00x - zp1/data1 compression gzip local zp1/data1 refcompressratio 1.00x - root@francs:/data1 # cd /data1 root@francs:/data1 # lrz lrz waiting to receive. Starting zmodem transfer. Press Ctrl+C to cancel. Transferring 0000000600005ca200000032... 100% 16384 KB 5461 KB/sec 00:00:03 0 Errors root@francs:/data1 # du -sm 0000000600005ca200000032 4 0000000600005ca200000032 备注： gzip 默认压缩级别为6, 上传了一个 17 M 的文件到压缩目录 /data1，压缩后为 4M。 1234567891011121314root@francs:/data1 # zfs get all zp1/data2 | grep compress zp1/data2 compressratio 1.00x - zp1/data2 compression off default zp1/data2 refcompressratio 1.00x - root@francs:/data1 # cd /data2 root@francs:/data2 # lrz lrz waiting to receive. Starting zmodem transfer. Press Ctrl+C to cancel. Transferring 0000000600005ca200000032... 100% 16384 KB 5461 KB/sec 00:00:03 0 Errors root@francs:/data2 # du -sm * 17 0000000600005ca200000032 备注: /data2 目录未开启压缩功能，文件未被压缩，还是 17 M。 3查看压缩比 123root@francs:/data1 # zfs get compressratio zp1/data1 NAME PROPERTY VALUE SOURCE zp1/data1 compressratio 4.45x 关于各种压缩算法的效率，德哥这篇 blog 有测试，可以参考，这里不作演示了; 值得一提的是 lz4 压缩算法作为 lzjb 的替代，具有较高的压缩和解压性能，同时具有折中的压缩比，推荐使用。ZFS compression algorithm lzjb, gzip, gzip-[0-9], zle, lz4 compare 4 参考 ZFS compression algorithm lzjb, gzip, gzip-[0-9], zle, lz4 compare ZFS Compression and Deduplication","categories":[{"name":"FreeBSD_ZFS","slug":"FreeBSD-ZFS","permalink":"https://postgres.fun/categories/FreeBSD-ZFS/"}],"tags":[{"name":"ZFS","slug":"ZFS","permalink":"https://postgres.fun/tags/ZFS/"}]},{"title":"ZFS: 文件系统空间使用率的疑惑 ","slug":"20140709170043","date":"2014-07-09T09:00:43.000Z","updated":"2018-09-04T01:34:12.812Z","comments":true,"path":"20140709170043.html","link":"","permalink":"https://postgres.fun/20140709170043.html","excerpt":"","text":"刚开始使用 ZFS 时,对 ZFS 文件系统的使用率比较疑惑, 发现初始时每个已 mount 的 zfs 文件系统的 Size( df -Th 输出 ) 都一样. zfs 对文件系统的处理有所不同, 每个数据集(dataset)都能使用整个池子的空间. 每个 dataset 的可使用最大值为所在池子剩余空间大小, 这和 LVM 的理念不同, 下面通过实验测试下: 1 创建文件12dd if=/dev/zero of=/tmp/file1 bs=1G count=1 dd if=/dev/zero of=/tmp/file2 bs=1G count=1 备注: 预先分配大小的文件也可用来创建 ZFS, 但不推荐使用, 这里仅用来做测试. 2 创建 zpool123456789101112root@francs:~ # zpool create zp1 /tmp/file1 /tmp/file2 root@francs:/data1 # zpool status zp1 pool: zp1 state: ONLINE scan: none requested config: NAME STATE READ WRITE CKSUM zp1 ONLINE 0 0 0 /tmp/file1 ONLINE 0 0 0 /tmp/file2 ONLINE 0 0 0 备注:创建一个大小为 2.0 GB 的池子. 3 创建四个 dataset1234root@francs:~ # zfs create -o mountpoint=/data1 zp1/data1 root@francs:~ # zfs create -o mountpoint=/data2 zp1/data2 root@francs:~ # zfs create -o mountpoint=/data3 zp1/data3 root@francs:~ # zfs create -o mountpoint=/data4 zp1/data4 4 查看文件系统123456789101112131415161718root@francs:~ # df -Th Filesystem Type Size Used Avail Capacity Mounted on zroot/ROOT/defaultzfs 16G 3.4G 12G 22% / devfs devfs 1.0K 1.0K 0B 100% /dev zroot/tmp zfs 12G 704K 12G 0% /tmp zroot/usr/home zfs 12G 156K 12G 0% /usr/home zroot/usr/ports zfs 13G 874M 12G 7% /usr/ports zroot/usr/src zfs 13G 545M 12G 4% /usr/src zroot/varzfs 13G 636M 12G 5% /var zroot/var/crash zfs 12G 148K 12G 0% /var/crash zroot/var/log zfs 12G 384K 12G 0% /var/log zroot/var/mail zfs 12G 152K 12G 0% /var/mail zroot/var/tmp zfs 12G 152K 12G 0% /var/tmp zp1 zfs 2.0G 31K 2.0G 0% /zp1 zp1/data1 zfs 2.0G 31K 2.0G 0% /data1 zp1/data2 zfs 2.0G 31K 2.0G 0% /data2 zp1/data3 zfs 2.0G 31K 2.0G 0% /data3 zp1/data4 zfs 2.0G 31K 2.0G 0% /data4 备注: 可以看到 /data1 ,/data2, /data3,/data4 四个已 mount 的目录. 每个目录的 Size 都为 2.0 GB. 5 向 /data1 目录写入数据1234root@francs:/data1 # dd if=/dev/zero of=/data1/file_test.img bs=100M count=3 3+0 records in 3+0 records out 314572800 bytes transferred in 37.706116 secs (8342753 bytes/sec) 备注: 在 /data1 目录里写入一个 300 MB 的文件. 6 再次查看文件系统使用率123456789101112131415161718root@francs:/data1 # df -Th Filesystem Type Size Used Avail Capacity Mounted on zroot/ROOT/defaultzfs 16G 3.4G 12G 22% / devfs devfs 1.0K 1.0K 0B 100% /dev zroot/tmp zfs 12G 2.1M 12G 0% /tmp zroot/usr/home zfs 12G 156K 12G 0% /usr/home zroot/usr/ports zfs 13G 874M 12G 7% /usr/ports zroot/usr/src zfs 13G 545M 12G 4% /usr/src zroot/varzfs 13G 636M 12G 5% /var zroot/var/crash zfs 12G 148K 12G 0% /var/crash zroot/var/log zfs 12G 384K 12G 0% /var/log zroot/var/mail zfs 12G 152K 12G 0% /var/mail zroot/var/tmp zfs 12G 152K 12G 0% /var/tmp zp1 zfs 1.7G 31K 1.7G 0% /zp1 zp1/data1 zfs 2.0G 300M 1.7G 15% /data1 zp1/data2 zfs 1.7G 31K 1.7G 0% /data2 zp1/data3 zfs 1.7G 31K 1.7G 0% /data3 zp1/data4 zfs 1.7G 31K 1.7G 0% /data4 备注:发现 /data2,/data3,/data4 目录的 SIze 都降为 1.7GB , 说明 zfs 文件目录的最大使用 Size 是会变化的, 每一个 zfs 目录都可以使用整个池子的容量. 7 接着往 /data2 目录写入一个 200M 的文件1234root@francs:/data1 # dd if=/dev/zero of=/data2/file_test.img bs=100M count=2 2+0 records in 2+0 records out 209715200 bytes transferred in 19.487790 secs (10761364 bytes/sec) 8 再次查看文件系统使用率123456789101112131415161718root@francs:/data1 # df -Th Filesystem Type Size Used Avail Capacity Mounted on zroot/ROOT/defaultzfs 16G 3.4G 12G 22% / devfs devfs 1.0K 1.0K 0B 100% /dev zroot/tmp zfs 12G 2.8M 12G 0% /tmp zroot/usr/home zfs 12G 156K 12G 0% /usr/home zroot/usr/ports zfs 13G 874M 12G 7% /usr/ports zroot/usr/src zfs 13G 545M 12G 4% /usr/src zroot/varzfs 13G 636M 12G 5% /var zroot/var/crash zfs 12G 148K 12G 0% /var/crash zroot/var/log zfs 12G 384K 12G 0% /var/log zroot/var/mail zfs 12G 152K 12G 0% /var/mail zroot/var/tmp zfs 12G 152K 12G 0% /var/tmp zp1 zfs 1.5G 31K 1.5G 0% /zp1 zp1/data1 zfs 1.8G 300M 1.5G 17% /data1 zp1/data2 zfs 1.7G 200M 1.5G 12% /data2 zp1/data3 zfs 1.5G 31K 1.5G 0% /data3 zp1/data4 zfs 1.5G 31K 1.5G 0% /data4 备注: 这时就比较清楚了. 池子好比是公共资源, 同一个池子的每一个目录都能使用这些资源, 先用先得。 9 查看 ZFS 空间使用12345678root@francs:~ # zpool iostat -v zp1 capacity operations bandwidth pool alloc free read write read write ------------ ----- ----- ----- ----- ----- ----- zp1 501M 1.50G 0 2 21 94.8K /tmp/file1 251M 765M 0 1 10 47.5K /tmp/file2 250M 766M 0 1 11 47.3K ------------ ----- ----- ----- ----- ----- ----- 备注: 数据均匀地写入到了文件 /tmp/file1 和 /tmp/file2。","categories":[{"name":"FreeBSD_ZFS","slug":"FreeBSD-ZFS","permalink":"https://postgres.fun/categories/FreeBSD-ZFS/"}],"tags":[{"name":"ZFS","slug":"ZFS","permalink":"https://postgres.fun/tags/ZFS/"}]},{"title":"2014: 杭州 PostgreSQL 线下交流 ","slug":"20140708155322","date":"2014-07-08T07:53:22.000Z","updated":"2018-09-04T01:34:12.249Z","comments":true,"path":"20140708155322.html","link":"","permalink":"https://postgres.fun/20140708155322.html","excerpt":"","text":"线下交流的议题 9.4新特性, PG基于块的增量备份方案, Bi-Directional Replication, PG performance tuning by flashcache, PG内核虚拟团队 时间杭州：2014年6月28日周六 下午 13:30 - 17:00。 地点杭州场：福云咖啡 , 杭州市西湖区天目山路335号福地创业园5楼 PPT 可在德哥的百度云盘下载:http://pan.baidu.com/s/1gdEGizDhttp://pan.baidu.com/s/1bn7wGJP 我分享的话题PostgreSQL9.4 新特性分享.pptx在线观看 http://pan.baidu.com/s/1o6M99Uy","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"ZFS Intent Log","slug":"20140707164706","date":"2014-07-07T08:47:06.000Z","updated":"2018-09-04T01:34:12.171Z","comments":true,"path":"20140707164706.html","link":"","permalink":"https://postgres.fun/20140707164706.html","excerpt":"","text":"ZFS 文件系统也有类似数据库的重做日志，被称为 ZFS Intent Log ，简称 ZIL, 还有个术语，被称为 Separate Intent Log, 简称 SLOG，是指存储 ZIL日志的独立的设备， 手册上提到使用 ZIL 可以提高文件系统性能，还能够用来恢复文件系统，接下来在虚拟机实验下增加 ZIL。 在虚拟机上新增三块盘 da1,da2,da3, 其中 da1,da2 各为 1GB, da3 为10 GB，这块盘这里没用到，备用。 1 /var/log/messages 日志123456789101112131415Jul 6 22:33:15 francs kernel: da1 at mpt0 bus 0 scbus2 target 1 lun 0 Jul 6 22:33:15 francs kernel: da1: &lt;VMware, VMware Virtual S 1.0&gt; Fixed Direct Access SCSI-2 device Jul 6 22:33:15 francs kernel: da1: 320.000MB/s transfers (160.000MHz DT, offset 127, 16bit) Jul 6 22:33:15 francs kernel: da1: Command Queueing enabled Jul 6 22:33:15 francs kernel: da1: 1024MB (2097152 512 byte sectors: 64H 32S/T 1024C) Jul 6 22:33:15 francs kernel: da2 at mpt0 bus 0 scbus2 target 2 lun 0 Jul 6 22:33:15 francs kernel: da2: &lt;VMware, VMware Virtual S 1.0&gt; Fixed Direct Access SCSI-2 device Jul 6 22:33:15 francs kernel: da2: 320.000MB/s transfers (160.000MHz DT, offset 127, 16bit) Jul 6 22:33:15 francs kernel: da2: Command Queueing enabled Jul 6 22:33:15 francs kernel: da2: 1024MB (2097152 512 byte sectors: 64H 32S/T 1024C) Jul 6 22:33:15 francs kernel: da3 at mpt0 bus 0 scbus2 target 3 lun 0 Jul 6 22:33:15 francs kernel: da3: &lt;VMware, VMware Virtual S 1.0&gt; Fixed Direct Access SCSI-2 device Jul 6 22:33:15 francs kernel: da3: 320.000MB/s transfers (160.000MHz DT, offset 127, 16bit) Jul 6 22:33:15 francs kernel: da3: Command Queueing enabled Jul 6 22:33:15 francs kernel: da3: 10240MB (20971520 512 byte sectors: 255H 63S/T 1305C) 备注： 有时候不知道设备名称，日志里可以看到详细信息。 2 查看硬盘文件12root@francs:~ # ls /dev/da* /dev/da0 /dev/da1 /dev/da2 /dev/da3 3 查看池状态1234567891011root@francs:~ # zpool status pool: zp1 state: ONLINE scan: none requested config: NAME STATE READ WRITE CKSUM zp1 ONLINE 0 0 0 da0 ONLINE 0 0 0 errors: No known data errors 备注：这里仅有 da0 盘。 4 给 zp1 池增加 SLOG 设备1root@francs:~ # zpool add zp1 log mirror da1 da2 备注： 给 zp1 池增加 SLOG 设备，并且 da1,da2 做成镜像。 5 再次查看 zp1 状态123456789101112131415root@francs:~ # zpool status zp1 pool: zp1 state: ONLINE scan: none requested config: NAME STATE READ WRITE CKSUM zp1 ONLINE 0 0 0 da0 ONLINE 0 0 0 logs mirror-1ONLINE 0 0 0 da1 ONLINE 0 0 0 da2 ONLINE 0 0 0 errors: No known data errors 备注：看到 da1,da2 已经在 logs 组里了。 6 查看池的使用情况1234567891011root@francs:~ # zpool iostat -v zp1 capacity operations bandwidth pool alloc free read write read write ---------- ----- ----- ----- ----- ----- ----- zp1 5.72M 29.7G 0 0 54 11.3K da0 5.72M 29.7G 0 0 54 11.3K logs - - - - - - mirror 0 1016M 0 0 0 0 da1 - - 0 0 191 18.4K da2 - - 0 0 191 18.4K ---------- ----- ----- ----- ----- ----- ----- 备注：这里可以清楚的看到池的空间使用情况，logs 为 mirror, 大小为 1GB。 7 参考 The ZFS Intent Log ZIL (ZFS intent log) zil.c","categories":[{"name":"FreeBSD_ZFS","slug":"FreeBSD-ZFS","permalink":"https://postgres.fun/categories/FreeBSD-ZFS/"}],"tags":[{"name":"ZFS","slug":"ZFS","permalink":"https://postgres.fun/tags/ZFS/"}]},{"title":"ZFS: 创建池","slug":"20140706174735","date":"2014-07-06T09:47:35.000Z","updated":"2018-09-04T01:34:12.109Z","comments":true,"path":"20140706174735.html","link":"","permalink":"https://postgres.fun/20140706174735.html","excerpt":"","text":"前两天刚把 FreeBSD 安装好，接下来准备学习 ZFS 的使用， zfs 文件系统特性网上内容很多，这里不介绍了，这里仅记录 zfs 的简单使用，做个笔记。 实验环境为 FreeBSD 10.0 虚拟机，先创建一块 30 G 的虚拟盘，步骤略。 1 创建 zpool123456root@francs:~ # zpool create zp1 /dev/da0 root@francs:~ # zpool list NAME SIZE ALLOC FREE CAP DEDUP HEALTH ALTROOT zp1 29.8G 134K 29.7G 0% 1.00xONLINE - zroot 17.9G 5.17G 12.7G 28% 1.00xONLINE - 2 查看池状态123456789root@francs:/zp1/database # zpool status zp1 pool: zp1 state: ONLINE scan: none requested config: NAME STATE READ WRITE CKSUM zp1 ONLINE 0 0 0 da0 ONLINE 0 0 0 3 查看池属性123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051root@francs:/zp1/database # zfs get all zp1 NAME PROPERTY VALUE SOURCE zp1 type filesystem - zp1 creation 日 7 6 12:57 2014 - zp1 used 151M - zp1 available 29.1G - zp1 referenced 32.5K - zp1 compressratio 1.01x - zp1 mounted yes - zp1 quota none default zp1 reservation none default zp1 recordsize 128K default zp1 mountpoint /zp1 default zp1 sharenfs off default zp1 checksum on default zp1 compression off default zp1 atime on default zp1 devices on default zp1 execon default zp1 setuid on default zp1 readonlyoff default zp1 jailed off default zp1 snapdir hidden default zp1 aclmode discard default zp1 aclinherit restricted default zp1 canmount on default zp1 xattr off temporary zp1 copies 1 default zp1 version 5 - zp1 utf8only off - zp1 normalization none - zp1 casesensitivity sensitive - zp1 vscan off default zp1 nbmand off default zp1 sharesmb off default zp1 refquota none default zp1 refreservation none default zp1 primarycache all default zp1 secondarycache all default zp1 usedbysnapshots 0 - zp1 usedbydataset 32.5K - zp1 usedbychildren 151M - zp1 usedbyrefreservation 0 - zp1 logbias latency default zp1 dedup off default zp1 mlslabel - zp1 sync standard default zp1 refcompressratio 1.00x - zp1 written 32.5K - zp1 logicalused 152M - zp1 logicalreferenced 16.5K - 4 创建具有压缩功能的 zfs 文件系统12root@francs:~ # zfs create zp1/database root@francs:/zp1/database # zfs set compression=gzip zp1/database 5 查看属性1234root@francs:~ # zfs get -r compression zp1 NAME PROPERTY VALUE SOURCE zp1 compression off default zp1/database compression lz4 local 6 传一个文件，验证是否压缩12345678root@francs:/zp1/database # du -k /root/db_francs库恢复日志.sql 1545 /root/db_francs库恢复日志.sql root@francs:/zp1/database # cp /root/db_francs库恢复日志.sql . root@francs:/zp1/database # du -k db_francs库恢复日志.sql 84 db_francs库恢复日志.sql备注：这里设置了 gzip 压缩方法，还支持 LZJB, ZLE等压缩方法，这里不测试了。 7 写入 /etc/fstab123# Device Mountpoint FStype Options Dump Pass# /dev/gpt/swap0 none swap sw 0 0 zp1/database /zp1/database zfs rw,noatime 0 0 备注：这步可选，之前 reboot 后，发现新建的 /zp1 目录会自动 mount。 8 参考 Z 文件系统 (ZFS) 在 Linux 上运行 ZFS ZFS Compression","categories":[{"name":"FreeBSD_ZFS","slug":"FreeBSD-ZFS","permalink":"https://postgres.fun/categories/FreeBSD-ZFS/"}],"tags":[{"name":"ZFS","slug":"ZFS","permalink":"https://postgres.fun/tags/ZFS/"}]},{"title":"FreeBSD: 中文显示乱码","slug":"20140705210311","date":"2014-07-05T13:03:11.000Z","updated":"2018-09-04T01:34:12.046Z","comments":true,"path":"20140705210311.html","link":"","permalink":"https://postgres.fun/20140705210311.html","excerpt":"","text":"发现 FreeBSD 中文无法正常显示， 如下。 1 中文无法显示1234root@francs:/usr/home # ll total 9 -rw-rw-r-- 1 root wheel 942 Jul 5 13:54 gnome ??????.sql -rw-rw-r-- 1 root wheel 1898 Jul 4 15:26 install.sql 2 查看 locale 设置123456789root@francs:/usr/home # locale LANG= LC_CTYPE=\"C\" LC_COLLATE=\"C\" LC_TIME=\"C\" LC_NUMERIC=\"C\" LC_MONETARY=\"C\" LC_MESSAGES=\"C\" LC_ALL= 备注：网上查了下，只需在 ~/.cshrc 文件添加以下几行即可。 3 在~/.cshrc下面增加：123setenv LANG zh_CN.UTF-8 setenv LC_CTYPE zh_CN.UTF-8 setenv LC_ALL zh_CN.UTF-8 备注：添加完后退出当前会话，重新进入。 4 中文显示正常1234root@francs:/usr/home # ll total 9 -rw-rw-r-- 1 root wheel 942 7 5 13:54 gnome 安装.sql -rw-rw-r-- 1 root wheel 1898 7 4 15:26 install.sql 5 再次查看 locale123456789root@francs:/usr/home # locale LANG=zh_CN.UTF-8 LC_CTYPE=\"zh_CN.UTF-8\" LC_COLLATE=\"zh_CN.UTF-8\" LC_TIME=\"zh_CN.UTF-8\" LC_NUMERIC=\"zh_CN.UTF-8\" LC_MONETARY=\"zh_CN.UTF-8\" LC_MESSAGES=\"zh_CN.UTF-8\" LC_ALL=zh_CN.UTF-8 6 参考 FreeBSD 8.1 Relase 版本中文乱码问题","categories":[{"name":"FreeBSD_ZFS","slug":"FreeBSD-ZFS","permalink":"https://postgres.fun/categories/FreeBSD-ZFS/"}],"tags":[{"name":"FreeBSD","slug":"FreeBSD","permalink":"https://postgres.fun/tags/FreeBSD/"}]},{"title":"FreeBsd: 安装 rz , sz 工具","slug":"20140705203602","date":"2014-07-05T12:36:02.000Z","updated":"2018-09-04T01:34:11.984Z","comments":true,"path":"20140705203602.html","link":"","permalink":"https://postgres.fun/20140705203602.html","excerpt":"","text":"习惯了在 Linux 平台下使用 rz,sz 命令简单的上传，下载小文件， 在 FreeBSD 下也能安装，这里介绍两种安装方式。 方法一: pkg 安装方式1.1 pkg 安装 lrzsz123456789101112131415161718192021root@francs:~ # pkg install lrzsz Updating repository catalogue The following 2 packages will be installed: Upgrading gettext: 0.18.3.1 -&gt; 0.18.3.1_1 Installing lrzsz: 0.12.20_4 The installation will require 423 KB more space 66 KB to be downloaded Proceed with installing packages [y/N]: y lrzsz-0.12.20_4.txz 100% 66KB 65.6KB/s 65.6KB/s 00:01 Checking integrity... done [1/2] Upgrading gettext from 0.18.3.1 to 0.18.3.1_1... done [2/2] Installing lrzsz-0.12.20_4... done root@francs:/usr/ports/comms/lrzsz # lrz Makefile distinfo files/ pkg-descr pkg-plist root@francs:/usr/ports/comms/lrzsz # lrz lrz waiting to receive. Starting zmodem transfer. Press Ctrl+C to cancel. 备注: 这里需要联网下载安装包，如果包较大，费时较长。 1.2 删除 lrzsz123456789root@francs:~ # pkg delete lrzsz Deinstallation has been requested for the following 1 packages: lrzsz-0.12.20_4 The deinstallation will free 423 KB Proceed with deinstalling packages [y/N]: y [1/1] Deleting lrzsz-0.12.20_4... done 方法二: 通过 Ports 系统编译安装目前对 Ports 系统还不是很了解，仅知道这是 FreeBSD 上软件管理平台，使用 Ports 可以对软件进行编译安装，关于 Ports 系统的详细内容以后再学习。 2.1 编译安装 lrzsz12root@francs:/usr/ports/comms/lrzsz # cd /usr/ports/comms/lrzsz root@francs:/usr/ports/comms/lrzsz # make install clean 备注：这种方式为编译安装，安装时同样需要联网下载。 2.2 卸载 lrzsz12345678910root@francs:/usr/local/bin # cd /usr/ports/comms/lrzsz/ root@francs:/usr/ports/comms/lrzsz # make deinstall clean ===&gt; Deinstalling for comms/lrzsz ===&gt; Deinstalling Deinstallation has been requested for the following 1 packages: lrzsz-0.12.20_4 The deinstallation will free 423 KB [1/1] Deleting lrzsz-0.12.20_4... done 参考 使用Ports Collection FreeBSD 的 Ports 系统","categories":[{"name":"FreeBSD_ZFS","slug":"FreeBSD-ZFS","permalink":"https://postgres.fun/categories/FreeBSD-ZFS/"}],"tags":[{"name":"FreeBSD","slug":"FreeBSD","permalink":"https://postgres.fun/tags/FreeBSD/"}]},{"title":"FreeBSD: 图形化桌面 Gnome 安装","slug":"20140705135155","date":"2014-07-05T05:51:55.000Z","updated":"2018-09-04T01:34:11.921Z","comments":true,"path":"20140705135155.html","link":"","permalink":"https://postgres.fun/20140705135155.html","excerpt":"","text":"FreeBSD 安装完后默认是不带图形化界面的，爱折腾，计划装个 gnome 桌面玩玩，gnome 安装方式有两种。 方法一： 通过 pkg 网络安装1root@francs:~ # pkg install gnome2 备注：这种方法安装需要联网下载 300 多 MB 的包， 并且下载速度慢，不建议使用。 方法二： 通过 bsdconfig 文本安装程序安装 bsdinstall/ bsdconfig 是 FreeBSD 提供的基于文本的安装程序， FreeBSD 9.0-RELEASE 版本之前叫做 sysinstall。接下来介绍通过 bsdconfig 安装 gnome 桌面。 运行 bsdconfig 程序: 选择 “3 Packages” ,按回车 选择 “1 CD/DVD” ，按回车。 选择”FREEBSD_INSTALL”: 选择 X11 选择 gdm, gnome 相关包, 按空格键表示选择。 同时选择 xorg 包 选择好后，执行 back 按纽回到上一个对话框 ，之后回到第一个名为”Review” 的选项 ，按回车 选择 “Proceed” 按回车，就可进行安装了。接下来就进入安装过程，需要持续一段时间。 安装完后，重启 FreeBSD 也看不到图形化界面，需要将以下两行写入 /etc/rc.conf 文件。 /etc/rc.conf 文件尾部增加以下两行12gdm_enable=”YES” （启动GDM） gnome_enable=”YES （启动所有gnome服务） 备注，增加完后，重启 FreeBSD, 就可看到图形化界面了；到这里 gnome 桌面安装完成，bsdconfig 还是很方便的。 参考 介绍 bsdinstall 安装 GNOME FreeBSD 10安装KDE桌面环境简介","categories":[{"name":"FreeBSD_ZFS","slug":"FreeBSD-ZFS","permalink":"https://postgres.fun/categories/FreeBSD-ZFS/"}],"tags":[{"name":"FreeBSD","slug":"FreeBSD","permalink":"https://postgres.fun/tags/FreeBSD/"}]},{"title":"FreeBSD 10.0 安装","slug":"20140704153546","date":"2014-07-04T07:35:46.000Z","updated":"2018-09-04T01:34:11.859Z","comments":true,"path":"20140704153546.html","link":"","permalink":"https://postgres.fun/20140704153546.html","excerpt":"","text":"最近也想玩玩 FreeBSD， 德哥写了篇 FreeBSD 10.0 的安装博客， 很详细， 今天参考这篇 blog 在虚拟机成功安装 FreeBSD 10.0。 一 FreeBSD 安装http://blog.163.com/digoal@126/blog/static/16387704020145114251585/ 介质下载1ftp://ftp.freebsd.org/pub/FreeBSD/releases/i386/i386/ISO-IMAGES/10.0/FreeBSD-10.0-RELEASE-i386-dvd1.iso 备注：安装步骤这里不介绍了，有一点需要指出，安装完后无法通过 root 远程 ssh 连接 FreeBSD，需要修改以下参数。 修改 /etc/ssh/sshd_config 以下参数12345# Authentication: PermitRootLogin yes # Change to yes to enable built-in password authentication. PasswordAuthentication yes 备注：修改后，重启 sshd 服务。 重启 sshd 服务123456root@francs:~ # service sshd restart Performing sanity check on sshd configuration. Stopping sshd. Waiting for PIDS: 704. Performing sanity check on sshd configuration. Starting sshd. 二 VIM 安装 还是习惯了使用 vim 编辑器，并习惯了它的语法高亮颜色属性，接着安装 vim。 2.1 搜索 vim 相关包123456root@francs:~ # pkg search vim ja-jvim-3.0.j2.1b_1 vim-7.4.334 vim-lite-7.4.334 vimpager-1.8.9 xpi-vimperator-3.5 2.2 安装 vim-lite 包1234567891011121314root@francs:~ # pkg install vim-lite-7.4.334 Updating repository catalogue The following 1 packages will be installed: Installing vim-lite: 7.4.334 The installation will require 20 MB more space 5 MB to be downloaded Proceed with installing packages [y/N]: y vim-lite-7.4.334.txz 100% 4647KB 94.8KB/s 57.3KB/s 00:49 Checking integrity... done [1/1] Installing vim-lite-7.4.334... done 2.2 安装完 vim-lite 后配, 配置以下增加语法检查及颜色功能echo “syntax on”&gt;&gt;/root/.vimrc 三 参考 FreeBSD Install FreeBSD里简单安装配置vim编辑器","categories":[{"name":"FreeBSD_ZFS","slug":"FreeBSD-ZFS","permalink":"https://postgres.fun/categories/FreeBSD-ZFS/"}],"tags":[{"name":"FreeBSD","slug":"FreeBSD","permalink":"https://postgres.fun/tags/FreeBSD/"}]},{"title":"Ubuntu: 断点续传下载文件","slug":"20140704143756","date":"2014-07-04T06:37:56.000Z","updated":"2018-09-04T01:34:11.796Z","comments":true,"path":"20140704143756.html","link":"","permalink":"https://postgres.fun/20140704143756.html","excerpt":"","text":"今天通过浏览器下载了个大文件，速度又慢， 停了后，重新下载失败，咨询同事后，wget 支持断点续传，挂个脚本后台跑，爽得很。 下载脚本1wget -c ftp://ftp.freebsd.org/pub/FreeBSD/releases/i386/i386/ISO-IMAGES/10.0/FreeBSD-10.0-RELEASE-i386-dvd1.iso &gt; wget.log 2&gt;&amp;1 &amp; 备注： 刚测试了，网络重启后，依然可以断点续传。 参数解释123-c --continue Continue getting a partially-downloaded file. This is useful when you want to finish up a download started by a previous instance of Wget, or by another program. For instance:","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"PostgreSQL: 索引膨胀评估 SQL","slug":"20140625114815","date":"2014-06-25T03:48:15.000Z","updated":"2018-09-04T01:34:11.734Z","comments":true,"path":"20140625114815.html","link":"","permalink":"https://postgres.fun/20140625114815.html","excerpt":"","text":"老外 blog 上分享一个索引膨胀评估的 SQL ， 支持 PostgreSQL 7.4 到最新版本(9.4), 很牛，源文 blog: http://blog.ioguix.net/postgresql/2014/06/24/More-work-on-index-bloat-estimation-query.html 索引膨胀评估索引膨胀评估，SQL 如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667SELECT current_database(), nspname AS schemaname, c.relname AS tablename, indexname, bs*(sub.relpages)::bigint AS real_size, bs*otta::bigint as estimated_size, bs*(sub.relpages-otta)::bigint AS bloat_size, bs*(sub.relpages-otta)::bigint * 100 / (bs*(sub.relpages)::bigint) AS bloat_ratio -- , index_tuple_hdr_bm, maxalign, pagehdr, nulldatawidth, nulldatahdrwidth, datawidth, sub.reltuples, sub.relpages -- (DEBUG INFO)FROM ( SELECT bs, nspname, table_oid, indexname, relpages, coalesce( ceil((reltuples*(4+nulldatahdrwidth))/(bs-pagehdr::float)) + 1, 0 -- ItemIdData size + computed avg size of a tuple (nulldatahdrwidth) ) AS otta -- , index_tuple_hdr_bm, maxalign, pagehdr, nulldatawidth, nulldatahdrwidth, datawidth, reltuples -- (DEBUG INFO)FROM ( SELECT maxalign, bs, nspname, relname AS indexname, reltuples, relpages, relam, table_oid, ( index_tuple_hdr_bm +maxalign - CASE -- Add padding to the index tuple header to align on MAXALIGN WHEN index_tuple_hdr_bm%maxalign = 0 THEN maxalign ELSE index_tuple_hdr_bm%maxalign END + nulldatawidth + maxalign - CASE -- Add padding to the data to align on MAXALIGNWHEN nulldatawidth = 0 THEN 0 WHEN nulldatawidth::integer%maxalign = 0 THEN maxalign ELSE nulldatawidth::integer%maxalign END )::numeric AS nulldatahdrwidth, pagehdr -- , index_tuple_hdr_bm, nulldatawidth, datawidth -- (DEBUG INFO)FROM ( SELECT i.nspname, i.relname, i.reltuples, i.relpages, i.relam, s.starelid, a.attrelid AS table_oid, CASE cluster_version.v &gt; 7WHEN true THEN current_setting('block_size')::numericELSE 8192::numeric END AS bs, CASE -- MAXALIGN: 4 on 32bits, 8 on 64bits (and mingw32 ?) WHEN version() ~ 'mingw32' OR version() ~ '64-bit|x86_64|ppc64|ia64|amd64' THEN 8ELSE 4 END AS maxalign, /* per page header, fixed size: 20 for 7.X, 24 for others */ CASE WHEN cluster_version.v &gt; 7THEN 24ELSE 20 END AS pagehdr, /* per tuple header: add IndexAttributeBitMapData if some cols are null-able */ CASE WHEN max(coalesce(s.stanullfrac,0)) = 0THEN 2 -- IndexTupleData size ELSE 2 + (( 32 + 8 - 1 ) / 8) -- IndexTupleData size + IndexAttributeBitMapData size ( max num filed per index + 8 - 1 /8) END AS index_tuple_hdr_bm, /* data len: we remove null values save space using it fractionnal part from stats */ sum( (1-coalesce(s.stanullfrac, 0)) * coalesce(s.stawidth, 1024) ) AS nulldatawidth -- , sum( s.stawidth ) AS datawidth -- (DEBUG INFO) FROM pg_attribute AS a JOIN pg_statistic AS s ON s.starelid=a.attrelid AND s.staattnum = a.attnumJOIN ( SELECT nspname, relname, reltuples, relpages, indrelid, relam, string_to_array(pg_catalog.textin(pg_catalog.int2vectorout(indkey)), ' ')::smallint[] AS attnum FROM pg_index JOIN pg_class ON pg_class.oid=pg_index.indexrelid JOIN pg_namespace ON pg_namespace.oid = pg_class.relnamespace ) AS i ON i.indrelid = a.attrelid AND a.attnum = ANY (i.attnum), ( SELECT substring(current_setting('server_version') FROM '#\"[0-9]+#\"%' FOR '#')::integer ) AS cluster_version(v) WHERE a.attnum &gt; 0GROUP BY 1, 2, 3, 4, 5, 6, 7, 8, 9, cluster_version.v ) AS s1 ) AS s2 JOIN pg_am am ON s2.relam = am.oid WHERE am.amname = 'btree') as subJOIN pg_class c ON c.oid=sub.table_oidWHERE sub.relpages &gt; 2ORDER BY 2,3,4; 备注：此 SQL 需要超级用户权限， 测试了PostgreSQL 9.1, 9.2, 9.3, 9.4beat1 版本 都能跑出结果， 跑出的结果类似以下: SQL 运行结果current_database | schemaname | tablename | indexname| real_size | estimated_size | bloat_size | bloat_ratio ------------------+------------+-----------+-------------------------+-----------+----------------+------------+--------------------- francs | francs | test_1| idx_test_1| 18006016 | 12050432 | 5955584 | 33.0755232029117379 francs | pg_catalog | pg_amop | pg_amop_fam_strat_index | 32768 | 24576 | 8192 | 25.0000000000000000 francs | pg_catalog | pg_amop | pg_amop_opr_fam_index | 32768 | 24576 | 8192 | 25.0000000000000000 (3 rows) 根据查询结果验证几个索引大小，膨胀率 bloat_ratio 有时很准，有时有一定偏差，供参考。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.4 and JSON","slug":"20140623175307","date":"2014-06-23T09:53:07.000Z","updated":"2018-09-04T01:34:11.671Z","comments":true,"path":"20140623175307.html","link":"","permalink":"https://postgres.fun/20140623175307.html","excerpt":"","text":"一篇来自国外 PostgreSQL 大会的介绍 PostgreSQL JSON 的 ppt ，刚看完， 忍不住要上来分享下， 对于想了解 PostgreSQL JSON 的同学有福了。 部分内容 PPT 下载http://francs3.blog.163.com/blog/static/40576727201452355137632/","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"JSON/JSONB","slug":"JSON-JSONB","permalink":"https://postgres.fun/tags/JSON-JSONB/"}]},{"title":"PostgreSQL9.4: Jsonb 性能测试","slug":"20140622094700","date":"2014-06-22T01:47:00.000Z","updated":"2018-09-04T01:34:11.593Z","comments":true,"path":"20140622094700.html","link":"","permalink":"https://postgres.fun/20140622094700.html","excerpt":"","text":"json 特性的提升是9.4 的关键特性之一, 本人对于 json 的关注较少, 一方面由于之前版本的 json 并不十分成熟, 使用时需要配合使用外部模块如 PLV8, PLPerl 来弥补 JSON 功能的不足,一方面由于太懒没花精力研究; 但 9.4 版本的 JSON 功能完善很多, jsonb 的出现带来了更多的函数, 更多的索引创建方式, 更多的操作符和更高的性能. 接下来通过一些例子来讲解, 希望更多的朋友能够了解并测试 PostgreSQL 的 json 功能. 之前写的关于 json 的博客 PostgreSQL 9.3Beta1: Json 功能增强 PostgreSQL 9.4: 新增 Jsonb 数据类型 环境准备1.2 测试环境硬件: 笔记本虚拟机系统: RHEL 6.2PG 版本: 9.4Beta1 生成测试数据2.1 测试表user_ini: 基础数据表, 200 万数据.tbl_user_json: 含有 json 数据类型表, 200 万数据tbl_user_jsonb: 含有 jsonb 数据类型表, 200 万数据 2.2 创建基础数据测试表12345francs=&gt; create table user_ini(id int4 ,user_id int8, user_name character varying(64),create_time timestamp(6) with time zone default clock_timestamp());CREATE TABLE francs=&gt; insert into user_ini(id,user_id,user_name) select r,round(random()*2000000), r || '_francs' from generate_series(1,2000000) as r;INSERT 0 2000000 备注: 生成 200 万测试数据. 2.3 生成 json 测试数据123456francs=&gt; create table tbl_user_json(id serial, user_info json);CREATE TABLE francs=&gt; insert into tbl_user_json(user_info) select row_to_json(user_ini) from user_ini;INSERT 0 2000000Time: 63469.336 ms 2.4 生成 jsonb 测试数据123456francs=&gt; create table tbl_user_jsonb(id serial, user_info jsonb);CREATE TABLE francs=&gt; insert into tbl_user_jsonb(user_info) select row_to_json(user_ini)::jsonb from user_ini; INSERT 0 2000000Time: 78300.553 ms 备注: 从时间来看, jsonb 插入速度比 json 插入速度稍慢, 再来看下两个表的大小如何? 2.5 比较表大小12345678910111213francs=&gt; \\dt+ tbl_user_jsonList of relationsSchema | Name | Type | Owner | Size | Description--------+---------------+-------+--------+--------+-------------francs | tbl_user_json | table | francs | 269 MB |(1 row) francs=&gt; \\dt+ tbl_user_jsonb List of relationsSchema | Name | Type | Owner | Size | Description--------+----------------+-------+--------+--------+-------------francs | tbl_user_jsonb | table | francs | 329 MB |(1 row) 2.6 查看几条测试数据1234567francs=&gt; select * from tbl_user_jsonb limit 3;id | user_info----+------------------------------------------------------------------------------------------------------- 1 | &#123;\"id\": 1, \"user_id\": 116179, \"user_name\": \"1_francs\", \"create_time\": \"2014-06-21 11:54:38.371774+00\"&#125; 2 | &#123;\"id\": 2, \"user_id\": 956659, \"user_name\": \"2_francs\", \"create_time\": \"2014-06-21 11:54:38.373425+00\"&#125; 3 | &#123;\"id\": 3, \"user_id\": 1017031, \"user_name\": \"3_francs\", \"create_time\": \"2014-06-21 11:54:38.37344+00\"&#125;(3 rows) 备注: 以上是生成的测试数据, 列几条出来,方便查阅, 接下来看一个查询. 基于 Jsonb 字段 KEY 值的检索效率3.1 根据 user_info 字段的 user_name key 检索12345francs=&gt; select * from tbl_user_jsonb where user_info-&gt;&gt;'user_name'= '1_francs'; id | user_info----+------------------------------------------------------------------------------------------------------- 1 | &#123;\"id\": 1, \"user_id\": 116179, \"user_name\": \"1_francs\", \"create_time\": \"2014-06-21 11:54:38.371774+00\"&#125;(1 row) 3.2 执行计划和执行时间123456789francs=&gt; explain analyze select * from tbl_user_jsonb where user_info-&gt;&gt;'user_name'= '1_francs'; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------Seq Scan on tbl_user_jsonb(cost=0.00..72097.82 rows=10000 width=140) (actual time=0.033..2965.837 rows=1 loops=1)Filter: ((user_info -&gt;&gt; 'user_name'::text) = '1_francs'::text)Rows Removed by Filter: 1999999Planning time: 1.657 msExecution time: 2966.380 ms(5 rows) 备注: 此时还没建索引,走的全表扫, 花了将近 3 秒. 3.3 创建索引12francs=&gt; create index idx_gin_user_infob_user_name on tbl_user_jsonb using btree ((user_info -&gt;&gt; 'user_name'));CREATE INDEX 3.4 再次查看 plan1234567891011francs=&gt; explain analyze select * from tbl_user_jsonb where user_info-&gt;&gt;'user_name'= '1_francs'; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------------Bitmap Heap Scan on tbl_user_jsonb(cost=233.93..23782.62 rows=10000 width=140) (actual time=0.046..0.047 rows=1 loops=1)Recheck Cond: ((user_info -&gt;&gt; 'user_name'::text) = '1_francs'::text)Heap Blocks: exact=1-&gt; Bitmap Index Scan on idx_gin_user_infob_user_name(cost=0.00..231.43 rows=10000 width=0) (actual time=0.035..0.035 rows=1 loops=1)Index Cond: ((user_info -&gt;&gt; 'user_name'::text) = '1_francs'::text)Planning time: 0.144 msExecution time: 0.101 ms(7 rows) 备注: 创建索引后, 上述查询走了索引, 仅花 0.101 ms 完成检索, 挺给力! 3.5 根据 user_info 字段的 user_id 检索12345678910francs=&gt; explain analyze select * from tbl_user_jsonb where user_info-&gt;&gt;'user_id'= '1'; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------Seq Scan on tbl_user_jsonb(cost=0.00..72098.00 rows=10000 width=140) (actual time=2483.198..4289.888 rows=1 loops=1)Filter: ((user_info -&gt;&gt; 'user_id'::text) = '1'::text)Rows Removed by Filter: 1999999Planning time: 3.304 msExecution time: 4292.158 ms(5 rows)Time: 4321.349 ms 备注: 没走索引,花了 4 秒多,因为没建这个 key 上的索引. 使用 GIN 索引 可以给 jsonb 字段创建 GIN 索引, GIN 索引有两种模式, 默认模式支持 @&gt;, ?, ?&amp; 和 ?| 的索引查询, 我们这里使用默认模式.4.1 删除之前索引,新建 gin 索引12345678910francs=&gt; create index idx_tbl_user_jsonb_user_Info on tbl_user_jsonb using gin (user_Info);CREATE INDEXTime: 214253.873 ms francs=&gt; di+ idx_tbl_user_jsonb_user_InfoList of relationsSchema | Name | Type | Owner | Table | Size | Description--------+------------------------------+-------+--------+----------------+--------+-------------francs | idx_tbl_user_jsonb_user_info | index | francs | tbl_user_jsonb | 428 MB |(1 row) 备注: 索引很大,创建很慢,一般不会这么建索引. 4.2 基于 key/value 检索可以使用索引123456789101112francs=&gt; explain analyze select * from tbl_user_jsonb where user_info @&gt; '&#123;\"user_id\": 1017031&#125;'; QUERY PLAN -----------------------------------------------------------------------------------------------------------------------------------------Bitmap Heap Scan on tbl_user_jsonb(cost=59.50..6637.58 rows=2000 width=140) (actual time=0.340..0.345 rows=1 loops=1)Recheck Cond: (user_info @&gt; '&#123;\"user_id\": 1017031&#125;'::jsonb)Rows Removed by Index Recheck: 1Heap Blocks: exact=2-&gt; Bitmap Index Scan on idx_tbl_user_jsonb_user_info(cost=0.00..59.00 rows=2000 width=0) (actual time=0.319..0.319 rows=2 loops=1)Index Cond: (user_info @&gt; '&#123;\"user_id\": 1017031&#125;'::jsonb)Planning time: 0.118 msExecution time: 0.391 ms(8 rows) 4.3 以下查询不走索引12345678910111213141516171819francs=&gt; explain analyze select * from tbl_user_jsonb where user_info-&gt;&gt;'user_name' ='4_francs'; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------Seq Scan on tbl_user_jsonb(cost=0.00..72098.00 rows=10000 width=140) (actual time=0.036..4640.794 rows=1 loops=1)Filter: ((user_info -&gt;&gt; 'user_name'::text) = '4_francs'::text)Rows Removed by Filter: 1999999Planning time: 1.101 msExecution time: 4640.851 ms(5 rows) francs=&gt; explain analyze select * from tbl_user_jsonb where user_info-&gt;'user_name' ?'4_francs'; QUERY PLAN --------------------------------------------------------------------------------------------------------------------Seq Scan on tbl_user_jsonb(cost=0.00..72098.00 rows=2000 width=140) (actual time=0.187..5387.658 rows=1 loops=1)Filter: ((user_info -&gt; 'user_name'::text) ? '4_francs'::text)Rows Removed by Filter: 1999999Planning time: 0.382 msExecution time: 5387.762 ms(5 rows) 备注: 以上的 ? 操作没走索引, 但 ? 操作支持索引检索,创建以下索引. 4.4 删除之前索引并新建以下索引1234567891011121314francs=&gt; create index idx_gin_user_info_user_name on tbl_user_jsonb using gin((user_info -&gt; 'user_name'));CREATE INDEX francs=&gt; explain analyze select * from tbl_user_jsonb where user_info-&gt;'user_name' ?'4_francs'; QUERY PLAN -----------------------------------------------------------------------------------------------------------------------------------Bitmap Heap Scan on tbl_user_jsonb(cost=35.50..6618.58 rows=2000 width=140) (actual time=0.067..0.069 rows=1 loops=1)Recheck Cond: ((user_info -&gt; 'user_name'::text) ? '4_francs'::text)Heap Blocks: exact=1-&gt; Bitmap Index Scan on idx_gin_user_info_user_name(cost=0.00..35.00 rows=2000 width=0) (actual time=0.037..0.037 rows=1 loops=1)Index Cond: ((user_info -&gt; 'user_name'::text) ? '4_francs'::text)Planning time: 0.151 msExecution time: 0.129 ms(7 rows) 备注: 速度很快. 对比 Json 和 Jsonb 的检索性能 文档上提到了 jsonb 的检索效率要高于 json 的检索效率, 下面通过例子测试.5.1 删除之前创建的所有索引并创建函数索引12345francs=&gt; create index idx_gin_user_info_id on tbl_user_json using btree (((user_info -&gt;&gt; 'id')::integer)); CREATE INDEX francs=&gt; create index idx_gin_user_infob_id on tbl_user_jsonb using btree (((user_info -&gt;&gt; 'id')::integer)); CREATE INDEX 备注: 为什么使用函数索引? 由于 –&gt; 操作返回的是 text 类型, 接下来的查询会用到 id 字段比较, 需要转换成整型. 5.2 json 表范围扫描1234567891011francs=&gt; explain analyze select id,user_info-&gt;'id',user_info-&gt;'user_name' from tbl_user_json where (user_info-&gt;&gt;'id')::int4 &gt; '1' and (user_info-&gt;&gt;'id')::int4 &lt; '10000'; QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------------Bitmap Heap Scan on tbl_user_json(cost=190.94..22275.60 rows=10000 width=36) (actual time=2.417..60.585 rows=9998 loops=1)Recheck Cond: ((((user_info -&gt;&gt; 'id'::text))::integer &gt; 1) AND (((user_info -&gt;&gt; 'id'::text))::integer &lt; 10000))Heap Blocks: exact=167-&gt; Bitmap Index Scan on idx_gin_user_info_id(cost=0.00..188.44 rows=10000 width=0) (actual time=2.329..2.329 rows=9998 loops=1)Index Cond: ((((user_info -&gt;&gt; 'id'::text))::integer &gt; 1) AND (((user_info -&gt;&gt; 'id'::text))::integer &lt; 10000))Planning time: 0.183 msExecution time: 64.116 ms(7 rows) 5.3 jsonb 表范围扫描1234567891011francs=&gt; explain analyze select id,user_info-&gt;'id',user_info-&gt;'user_name' from tbl_user_jsonb where (user_info-&gt;&gt;'id')::int4 &gt; '1' and (user_info-&gt;&gt;'id')::int4 &lt; '10000'; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------------Bitmap Heap Scan on tbl_user_jsonb(cost=190.94..23939.63 rows=10000 width=140) (actual time=2.593..24.308 rows=9998 loops=1)Recheck Cond: ((((user_info -&gt;&gt; 'id'::text))::integer &gt; 1) AND (((user_info -&gt;&gt; 'id'::text))::integer &lt; 10000))Heap Blocks: exact=197-&gt; Bitmap Index Scan on idx_gin_user_infob_id(cost=0.00..188.44 rows=10000 width=0) (actual time=2.494..2.494 rows=9998 loops=1)Index Cond: ((((user_info -&gt;&gt; 'id'::text))::integer &gt; 1) AND (((user_info -&gt;&gt; 'id'::text))::integer &lt; 10000))Planning time: 0.142 msExecution time: 27.851 ms(7 rows) 备注: 这里实验发现, jsonb 检索确实比 json 要快很多, 而本文开头插入数据时 jsonb 比 json 稍慢, 这也正好验证了 “jsonb 写入比 json 慢,但检索较 json 快的说法.”, 我在之前的博客 PostgreSQL 9.4: 新增 JSONB 数据类型 有提到过. 参考 JSON Types JSON Functions and Operators PostgreSQL 9.3Beta1：Json 功能增强 PostgreSQL 9.4: 新增 Jsonb 数据类型","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"JSON/JSONB","slug":"JSON-JSONB","permalink":"https://postgres.fun/tags/JSON-JSONB/"}]},{"title":"PostgreSQL中文社区网站上线 www.postgres.cn","slug":"20140619141334","date":"2014-06-19T06:13:34.000Z","updated":"2018-09-04T01:34:11.530Z","comments":true,"path":"20140619141334.html","link":"","permalink":"https://postgres.fun/20140619141334.html","excerpt":"","text":"刚在 bbs 中文论坛看到消息, 在社区周到京, 少聪, 阿弟, Aida 等社区兄弟努力下，PostgreSQL中文社区网站自2014/06/18正式使用www.postgres.cn域名 . 终于有了 PostgreSQL 中文网站了, 感谢社区兄弟们的努力.","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL: 根据日志文件列出历史超时 SQL ","slug":"20140618142830","date":"2014-06-18T06:28:30.000Z","updated":"2018-09-04T01:34:11.468Z","comments":true,"path":"20140618142830.html","link":"","permalink":"https://postgres.fun/20140618142830.html","excerpt":"","text":"发现一生产库有较多超时 SQL , 大家知道设置超时参数 log_min_duration_statement 可以将超过设置值的 SQL 打出, 例如 postgresql.conf 参数设置1log_min_duration_statement = 1s 备注: 这里设置的是 1 s ,那么执行时间超过 1 s 的 SQL 都将被记录. 数据库 CSVLOG 日志122014-06-17 21:58:53.935 CST,\"francs\",\"francs\",16017,\"192.168.xxx.xx:55985\",53a0499b.3e91,4,\"UPDATE\",2014-06-17 21:58:51 CST,192/0,0,LOG,00000,\"duration: 2113.625 ms\",,,,,,,,\"exec_simple_query, postgres.c:1118\",\"\" ...省略 备注: 注意 “duration: 2113.625 ms” 信息, 这个 sql 执行时间大于 2 秒, 那么如何从大量的 csvlog 日志中列出超过指定值的日志呢? 咨询同事后可以用 awk 实现, 如下: 列出执行时间超过 2 秒的 SQL1cat postgresql-2014-06-17_215* |grep \"duration: [0-9].* ms\"| awk -F 'statement:| ms|duration: ' '&#123;b=2000;if($2-b&gt;0) print $0&#125;' 备注: 当然也可以将 csvlog 日志导入表中, 之后就可以在数据库里通过 sql 分析了, 参考PostgreSQL日志分析：将 CSV日志导入到表中 参考 PostgreSQL日志分析：将 CSV日志导入到表中","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.4：新特性汇总","slug":"20140611151820","date":"2014-06-11T07:18:20.000Z","updated":"2018-09-04T01:34:11.405Z","comments":true,"path":"20140611151820.html","link":"","permalink":"https://postgres.fun/20140611151820.html","excerpt":"","text":"以下是对 PostgreSQL9.4 新特性总结，内容参考 PostgreSQL 9.4 Beta 1 Released，这些文章大部分仅对新特性做些简单测试并总结出来，希望对有需要的朋友有帮助。 个人感觉 9.4 版本的 nosql 特性比较突出，这方面我没做太多的测试，希望对 nosql 熟悉的朋友做些 9.4 版 jsonb 方面的测试并分享。 主要特性 PostgreSQL 9.4：新增 Jsonb 数据类型 PostgreSQL 9.4：Jsonb 性能测试 PostgreSQL 9.4：Replication Slots PostgreSQL 9.4：初识逻辑解析 ( Logical Decoding ) PostgreSQL 9.4：REFRESH MATERIALIZED VIEW 新增 CONCURRENTLY 参数 PostgreSQL 9.4：新增 ALTER SYSTEM 命令 其它特性 PostgreSQL 9.4：pg_basebackup 增加 –Max-rate 限速选项 PostgreSQL 9.4：聚合函数新增 FILTER 属性 PostgreSQL 9.4：增加时间构造函数 PostgreSQL 9.4：增加 pg_sleep_for(interval) , pg_sleep_until(timestamp) 延迟执行函数 PostgreSQL 9.4：Explain 输出的变化 PostgreSQL 9.4：支持备库延迟复制(Delayed Standbys) PostgreSQL 9.4：新增 Pg_stat_archiver 系统视图 PostgreSQL 9.4：支持 Background workers 后台进程动态注册，启动，停止。 PostgreSQL 9.4：返回结果集函数增加 WITH ORDINALITY 属性 PostgreSQL 9.4： Pg_prewarm 数据缓存预加载模块","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.4: 初识逻辑解析 ( Logical Decoding ) ","slug":"20140609174223","date":"2014-06-09T09:42:23.000Z","updated":"2018-09-04T01:34:11.343Z","comments":true,"path":"20140609174223.html","link":"","permalink":"https://postgres.fun/20140609174223.html","excerpt":"","text":"逻辑复制( Logical decoding) 是 PostgreSQL9.4 的关键特性之一，Logical decoding 允许读取 WAL 日志，并将数据变化解析成目标格式， 这块内容很多，下面仅做一些简单的分享。 在开启逻辑复制之前，需要设置 wal_level 和 max_replication_slots 参数. 创建逻辑 Slot设置 postgresql.conf 参数12wal_level = logicalmax_replication_slots = 2 备注： max_replication_slots 值最少需设置成 1，设置后重启数据库生效。 创建逻辑 slot123456789101112131415[pg94@db1 ~]$ psqlpsql (9.4beta1)Type \"help\" for help. postgres=# SELECT * FROM pg_create_logical_replication_slot('log_slot1', 'test_decoding');slotname | xlog_position-----------+---------------log_slot1 | 0/540001E8(1 row) postgres=# select * from pg_replication_slots where slot_name='log_slot1';slot_name | plugin | slot_type | datoid | database | active | xmin | catalog_xmin | restart_lsn-----------+---------------+-----------+--------+----------+--------+------+--------------+-------------log_slot1 | test_decoding | logical | 12993 | postgres | f | | 1902 | 0/540001B4(1 row) DDL 操作不被记录123456789101112131415161718postgres=# SELECT * FROM pg_logical_slot_get_changes('log_slot1', NULL, NULL);location | xid | data----------+-----+------(0 rows)postgres=# create table test_logical(id int4);CREATE TABLE postgres=# SELECT * FROM pg_logical_slot_get_changes('log_slot1', NULL, NULL); location | xid | data ------------+------+-------------0/540003F8 | 1902 | BEGIN 19020/54013CE4 | 1902 | COMMIT 1902(2 rows) postgres=# SELECT * FROM pg_logical_slot_get_changes('log_slot1', NULL, NULL);location | xid | data----------+-----+------(0 rows) 备注：记录只能查询一次，之后查询为空。 DML 操作被解析1234567891011121314151617181920212223postgres=# insert into test_logical (id) values (1);INSERT 0 1 postgres=# SELECT * FROM pg_logical_slot_get_changes('log_slot1', NULL, NULL); location | xid | data ------------+------+--------------------------------------------------0/54013DE0 | 1903 | BEGIN 19030/54013DE0 | 1903 | table public.test_logical: INSERT: id[integer]:10/54013E58 | 1903 | COMMIT 1903(3 rows) postgres=# insert into test_logical (id) values (2),(3),(4); INSERT 0 3 postgres=# SELECT * FROM pg_logical_slot_get_changes('log_slot1', NULL, NULL); location | xid | data ------------+------+--------------------------------------------------0/54013F38 | 1905 | BEGIN 19050/54013F38 | 1905 | table public.test_logical: INSERT: id[integer]:20/54013F74 | 1905 | table public.test_logical: INSERT: id[integer]:30/54013FB0 | 1905 | table public.test_logical: INSERT: id[integer]:40/5401403C | 1905 | COMMIT 1905(5 rows) 查询解析日志重复查询解析日志123456789101112131415161718postgres=# insert into test_logical (id) values (7); INSERT 0 1 postgres=# select * from pg_logical_slot_peek_changes('log_slot1',NULL,NULL); location | xid | data ------------+------+--------------------------------------------------0/54014380 | 1908 | BEGIN 19080/54014380 | 1908 | table public.test_logical: INSERT: id[integer]:70/540143F8 | 1908 | COMMIT 1908(3 rows) postgres=# select * from pg_logical_slot_peek_changes('log_slot1',NULL,NULL); location | xid | data ------------+------+--------------------------------------------------0/54014380 | 1908 | BEGIN 19080/54014380 | 1908 | table public.test_logical: INSERT: id[integer]:70/540143F8 | 1908 | COMMIT 1908(3 rows) 备注:如果想重复查询到日志，需使用 pg_logical_slot_peek_changes() 函数。接下来使用工具接收数据变化，这里使用 pg_recvlogical。 使用 pg_recvlogical 接收数据变化1pg_recvlogical -h 127.0.0.1 -d postgres --slot log_slot1 --start -f - 会话1: 创建表并插入数据12345678postgres=# create table test_1(id int4);CREATE TABLEpostgres=# insert into test_1 values (1);INSERT 0 1postgres=# insert into test_1 values (2);INSERT 0 1postgres=# delete from test_1 where id=1;DELETE 1 日志被解析12345678910111213[pg94@db1 ~]$ pg_recvlogical -h 127.0.0.1 -d postgres --slot log_slot1 --start -f - BEGIN 1909COMMIT 1909BEGIN 1910table public.test_1: INSERT: id[integer]:1COMMIT 1910BEGIN 1911table public.test_1: INSERT: id[integer]:2COMMIT 1911BEGIN 1912table public.test_1: DELETE: (no-tuple-data)COMMIT 1912 备注： 数据日志被解析出来，以上仅是对逻辑复制简单实验，更多内容还需探索。 参考 Postgres 9.4 feature highlight: Basics about logical decoding Logical Decoding Example pg_recvlogical PostgreSQL 9.4: Replication slots","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.4: pg_basebackup 增加 --max-rate 限速选项 ","slug":"20140606150543","date":"2014-06-06T07:05:43.000Z","updated":"2018-09-04T01:34:11.280Z","comments":true,"path":"20140606150543.html","link":"","permalink":"https://postgres.fun/20140606150543.html","excerpt":"","text":"使用pg_basebackup 创建流复制备库时，需要复制并传送数据，带来大量 I/O，9.4 版本 pg_basebackup 增加流量控制选项，搭建流复制备库时降低对主库的性能影响。 关于 –max-rate 选项指数据的最大传输速率，默认是 kb/s 为单位，也可以是 Mb/s, 允许的值范围是 32K 到 1024 Mb，超出这个范围为报以下错误。 超出速率范围12345[pg94@db2 pg94]$ pg_basebackup -D /database/pg94/pg_root -Fp -Xs -v -P -h 192.168.2.37 -p 1924 -U repuser --max-rate=2pg_basebackup: transfer rate \"2\" is out of range [pg94@db2 pg94]$ pg_basebackup -D /database/pg94/pg_root -Fp -Xs -v -P -h 192.168.2.37 -p 1924 -U repuser --max-rate=1025Mpg_basebackup: transfer rate \"1025M\" is out of range 设置最大传输速率 64 KB/s1234[pg94@db2 pg94]$ pg_basebackup -D /database/pg94/pg_root -Fp -Xs -v -P -h 192.168.2.37 -p 1924 -U repuser --max-rate=64transaction log start point: 0/50000058 on timeline 1pg_basebackup: starting background WAL receiver 10145/1449790 kB (0%), 0/2 tablespaces (...ncs/PG_9.4_201405111/16386/16587) 查看网卡流量123456789101112131415161718192021222324[root@db2 ~]# sar -n DEV 1 100002:39:22 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s02:39:23 PM lo 0.00 0.00 0.00 0.00 0.00 0.00 0.0002:39:23 PM eth0 50.00 41.00 66.65 2.61 0.00 0.00 0.00 02:39:23 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s02:39:24 PM lo0.00 0.00 0.00 0.00 0.00 0.00 0.0002:39:24 PM eth0 56.57 48.48 75.72 3.01 0.00 0.00 0.00 02:39:24 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s02:39:25 PM lo0.00 0.00 0.00 0.00 0.00 0.00 0.0002:39:25 PM eth0 50.51 47.47 67.32 2.95 0.00 0.00 0.00 02:39:25 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s02:39:26 PM lo0.00 0.00 0.00 0.00 0.00 0.00 0.0002:39:26 PM eth0 51.52 35.35 67.41 2.31 0.00 0.00 0.00 02:39:26 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s02:39:27 PM lo0.00 0.00 0.00 0.00 0.00 0.00 0.0002:39:27 PM eth0 56.00 31.00 74.96 2.08 0.00 0.00 0.00 02:39:27 PM IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s02:39:28 PM lo0.00 0.00 0.00 0.00 0.00 0.00 0.0002:39:28 PM eth0 56.57 48.48 75.72 3.01 0.00 0.00 0.00 参考 PostgreSQL pending patch : pg_basebackup throttling, limit network transfer rate PostgreSQL：使用 pg_basebackup 搭建流复制环境 pg_basebackup","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"PostgreSQL9.4: 聚合函数新增 FILTER 属性 ","slug":"20140605113133","date":"2014-06-05T03:31:33.000Z","updated":"2018-09-04T01:34:11.218Z","comments":true,"path":"20140605113133.html","link":"","permalink":"https://postgres.fun/20140605113133.html","excerpt":"","text":"Release Note Add control over which values are passed into aggregate functions using the FILTER clause (David Fetter) 手册中提到聚合函数增加 FILTER 属性， 也就是说在聚合函数 avg(), min(), max(), sum() 等可以使用 FILTER 属性过滤数据， 之前可能用到到以下方式实现:1select sum( case when ... then 1 else 0 end ) from .. 例如： 12345678francs=&gt; select count(*) as unfiltered, sum(case when a &gt;6 then 1 else 0 end) as filteredFROM generate_series(1,10) as a;unfiltered | filtered------------+----------10 | 4(1 row) 使用 FILTER 属性12345678francs=&gt; SELECT count(*) AS unfiltered, count(*) FILTER (WHERE a &gt; 6) AS filteredFROM generate_series(1,10) AS a;unfiltered | filtered------------+----------10 | 4(1 row) 备注：这样方便很多，方便平常 SQL 写得多的同学们。 使用 Sum 函数12345678francs=&gt; SELECTsum (a)AS unfiltered,sum (a)FILTER (WHERE a &gt; 6) AS filteredFROM generate_series(1,10) AS a;unfiltered | filtered------------+----------55 | 34(1 row) 1 到 20 按条件分组123456789francs=&gt; SELECT array_agg(i) FILTER (WHERE i % 2 = 0) AS twos, array_agg(i) FILTER (WHERE i % 3 = 0) AS threes, array_agg(i) FILTER (WHERE i % 5 = 0) AS fives, array_agg(i) FILTER (WHERE i % 7 = 0) AS sevens FROM generate_series(1, 20) AS g(i);twos | threes| fives | sevens-----------------------------+------------------+--------------+--------&#123;2,4,6,8,10,12,14,16,18,20&#125; | &#123;3,6,9,12,15,18&#125; | &#123;5,10,15,20&#125; | &#123;7,14&#125;(1 row) 参考 Aggregate Expressions WAITING FOR 9.4 – IMPLEMENT THE FILTER CLAUSE FOR AGGREGATE FUNCTION CALLS","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.4：增加时间构造函数","slug":"20140527174045","date":"2014-05-27T09:40:45.000Z","updated":"2018-09-04T01:34:11.155Z","comments":true,"path":"20140527174045.html","link":"","permalink":"https://postgres.fun/20140527174045.html","excerpt":"","text":"9.4 增加时间构造函数，release note 说明如下： Add functions to construct times, dates, timestamps, timestamptzs, and intervals from individual values, rather than strings (Pavel Stehule)These functions are prefixed with make_, e.g. make_date(). 有了这些函数，操作更简便了，不需要通过 character varying 类型转换。 9.3 版本123456789101112131415[pg93@db1 ~]$ psql francs francspsql (9.3.3)Type \"help\" for help. francs=&gt; select '2014-05-27'::date; date ------------2014-05-27(1 row) francs=&gt; select cast('2014-05-21' as date); date ------------ 2014-05-21 (1 row) 备注：之前版本要构造时间变量, 一般先转换成字符类型 ，再转换成 date 类型，9.4 版本则可简便。 9.4 版本构造日期(date)12345francs=&gt; select make_date(2014,05,24); make_date ------------2014-05-24(1 row) 构造时间(time)12345francs=&gt; select make_time(19,05,05);make_time-----------19:05:05(1 row) 构造时间(timestamp)12345francs=&gt; select make_timestamp(2014,05,24,20,0,0); make_timestamp ---------------------2014-05-24 20:00:00(1 row) 构造 interval1234567891011francs=&gt; select make_interval(days:=1);make_interval---------------1 day(1 row) francs=&gt; select make_interval(years:=2);make_interval---------------2 years(1 row) 参考 浅谈 PostgreSQL 类型转换 Date/Time Functions","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.4: 增加 pg_sleep_for(interval) , pg_sleep_until(timestamp) 延迟执行函数 ","slug":"20140527141304","date":"2014-05-27T06:13:04.000Z","updated":"2018-09-04T01:34:11.093Z","comments":true,"path":"20140527141304.html","link":"","permalink":"https://postgres.fun/20140527141304.html","excerpt":"","text":"之前版本仅支持 pg_sleep(seconds) 延迟执行函数， PostgreSQL 9.4 版本增加两个延迟执行函数，支持指定复杂的延迟。 pg_sleep_for(interval) pg_sleep_until(timestamp with time zone) pg_sleep(seconds): 当前会话延迟执行指定秒，以秒为单位指定 pg_sleep_for(interval): 支持更大的时间延迟，以 interval 形式指定。 pg_sleep_until(timestamp with time zone): 指定时间被唤醒。 测试 Pg_sleep12345678910111213[pg94@db1 ~]$ psql francs francspsql (9.4beta1)Type \"help\" for help. francs=&gt; \\timingTiming is on. francs=&gt; select pg_sleep(10); pg_sleep ----------(1 row) Time: 10020.590 ms 备注: 延迟执行 10 秒。 测试 Pg_sleep_for1234567francs=&gt; select pg_sleep_for('2 min'); pg_sleep_for -------------- (1 row) Time: 120046.302 ms 备注：延迟执行 2 分钟钟。 测试 Pg_sleep_until123456789101112francs=&gt; select now(); now -------------------------------2014-05-27 14:03:14.013931-07(1 row) francs=&gt; select pg_sleep_until('2014-05-27 14:05:00'); pg_sleep_until ----------------(1 row) Time: 102027.964 ms 备注：延迟执行到指定时间，用法非常简单。 查看 Pg_stat_activity123456postgres=# select pid, datname,usename,query_start,query,state,waiting from pg_stat_activity where state='active' and pid &lt;&gt; pg_backend_pid() order by query; pid | datname | usename | query_start | query | state | waiting ------+---------+---------+-------------------------------+-----------------------------------------------+--------+--------- 4464 | francs | francs | 2014-05-27 14:01:26.987006-07 | select pg_sleep_for('2 min'); | active | f 4693 | francs | francs | 2014-05-27 14:01:22.992145-07 | select pg_sleep_until('2014-05-27 14:03:00'); | active | f(2 rows) 备注: 在之前函数还未执行完成的时候查询 pg_stat_activity，说明函数处于运行状态。 参考 Delaying Execution","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.4: Explain 输出的变化","slug":"20140526161943","date":"2014-05-26T08:19:43.000Z","updated":"2018-09-04T01:34:11.030Z","comments":true,"path":"20140526161943.html","link":"","permalink":"https://postgres.fun/20140526161943.html","excerpt":"","text":"9.4 版本的 EXPLAIN 输出变化不大，release note 提到以下三点: Have EXPLAIN ANALYZE output planning time (Andreas Karlsson) Have EXPLAIN print the grouping columns in Agg and Group nodes (Tom Lane) Have EXPLAIN ANALYZE show bitmap heap scan exact/lossy block information (Etsuro Fujita) 简单测试如下： Explain Analyze 输出包含 Planning Time9.3 版本123456francs=&gt; explain analyze select 1; QUERY PLAN ------------------------------------------------------------------------------------Result (cost=0.00..0.01 rows=1 width=0) (actual time=0.004..0.006 rows=1 loops=1)Total runtime: 0.185 ms(2 rows) 9.4 版本1234567francs=&gt; explain analyze select 1; QUERY PLAN ------------------------------------------------------------------------------------Result (cost=0.00..0.01 rows=1 width=0) (actual time=0.003..0.004 rows=1 loops=1)Planning time: 0.116 msExecution time: 0.128 ms(3 rows) 备注：可以看到 9.4 版本的 explain analyze 命令输出包含了 Planning time 信息。 Explain 输出包含 Group 字段名称9.3 版本123456francs=&gt; explain select id,count(*) from test_1 group by id; QUERY PLAN --------------------------------------------------------------------------------------------GroupAggregate (cost=0.29..3934.29 rows=100000 width=4)-&gt; Index Only Scan using idx_test_1 on test_1(cost=0.29..2434.29 rows=100000 width=4)(2 rows) 9.4 版本12345678francs=&gt; explain select id,count(*) from test_1 group by id; QUERY PLAN ----------------------------------------------------------------------------------------------GroupAggregate (cost=0.42..44678.43 rows=1000000 width=4)Group Key: id-&gt; Index Only Scan using idx_test_1 on test_1(cost=0.42..29678.42 rows=1000000 width=4)Planning time: 0.301 ms(4 rows) 备注：9.4 版本 explain 命令输出包含了 Group Key 信息。 Bitmap Heap Scan 中包含 Block 信息9.3 版本1234567891011francs=&gt; explain analyze select id,name from test_1 where id&lt;100 or id&gt;999900; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------Bitmap Heap Scan on test_1(cost=5.39..149.72 rows=100 width=10) (actual time=0.157..0.226 rows=99 loops=1)Recheck Cond: ((id &lt; 100) OR (id &gt; 999900))-&gt; BitmapOr (cost=5.39..5.39 rows=100 width=0) (actual time=0.038..0.038 rows=0 loops=1)-&gt; Bitmap Index Scan on idx_test_1(cost=0.00..3.04 rows=100 width=0) (actual time=0.025..0.025 rows=99 loops=1)Index Cond: (id &lt; 100)-&gt; Bitmap Index Scan on idx_test_1(cost=0.00..2.30 rows=1 width=0) (actual time=0.008..0.008 rows=0 loops=1)Index Cond: (id &gt; 999900)Total runtime: 0.336 ms 9.4 版本1234567891011121314francs=&gt; explain analyze select id,name from test_1 where id&lt;100 or id&gt;999900; QUERY PLAN -----------------------------------------------------------------------------------------------------------------------------Bitmap Heap Scan on test_1(cost=10.47..702.88 rows=203 width=11) (actual time=7.860..7.933 rows=199 loops=1)Recheck Cond: ((id &lt; 100) OR (id &gt; 999900))Heap Blocks: exact=3-&gt; BitmapOr (cost=10.47..10.47 rows=203 width=0) (actual time=7.841..7.841 rows=0 loops=1)-&gt; Bitmap Index Scan on idx_test_1(cost=0.00..5.19 rows=102 width=0) (actual time=0.016..0.016 rows=99 loops=1)Index Cond: (id &lt; 100)-&gt; Bitmap Index Scan on idx_test_1(cost=0.00..5.17 rows=100 width=0) (actual time=7.822..7.822 rows=100 loops=1)Index Cond: (id &gt; 999900)Planning time: 59.523 msExecution time: 8.024 ms(10 rows) 备注: 9.4 版本增加 Heap Blocks: exact=3 信息。 参考 EXPLAIN","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.4：支持延迟复制(Delayed Standbys) ","slug":"20140526140838","date":"2014-05-26T06:08:38.000Z","updated":"2018-09-04T01:34:10.968Z","comments":true,"path":"20140526140838.html","link":"","permalink":"https://postgres.fun/20140526140838.html","excerpt":"","text":"9.4 版本之前，在流复制环境中，默认情况下备节点会实时地和主节点保留同步， 9.4 版本 在 recovery.conf 文件中新增 recovery_min_apply_delay 参数，支持备库延迟复制。 延迟复制的意义延迟复制是有意义的，比如在某些情况下由于某种原因 误删( drop ) 一张表，在 9.4 版本之前的流复制环境，那么备节点几乎实时地会和主节点同步，如果没有备份的话，这张表就找不回了，有了延迟复制，这张表在备节点上还能保留一段时间，从而给出了较大的恢复时间。 下面通过实验验证，流复制环境搭建略。 配置延迟复制2.1 设置备节点 recovery.conf 以下参数1recovery_min_apply_delay = 1min 备注：此参数默认单位为毫秒，这里设置成 1分钟，便于实验，支持的的时间单位如下： ms (obviously) s (seconds) min (minutes) h (hours) d (days) 2.2 重启备节点1234[pg94@db2 pg_root]$ pg_ctl -m fast restart -D $PGDATAwaiting for server to shut down.... doneserver stoppedserver starting 备注：recovery_min_apply_delay 参数调整后，需重启数据库才生效。 2.3 主库插入数据1234567891011postgres=# create table test_ha(id int4 primary key,create_time timestamp(6) without time zone default clock_timestamp());CREATE TABLE postgres=# insert into test_ha(id) values (1);INSERT 0 1 postgres=# select * from test_ha;id | create_time ----+---------------------------- 1 | 2014-05-26 11:34:54.294264(1 row) 备注：主库创建一张表，并插入一条数据。 2.4 备库验证12345678910postgres=# select *,now() from test_ha order by create_time desc limit 1;id | create_time | now----+-------------+-----(0 rows) postgres=# select *,now() from test_ha order by create_time desc limit 1;id | create_time | now ----+----------------------------+------------------------------- 1 | 2014-05-26 11:34:54.294264 | 2014-05-26 11:35:58.386896-07(1 row) 备注：之后在备库上简单的重复执行以上查询，根据以上结果，大概在 1 分钟后，备节点才有了这条数据。 延迟复制注意事项 Synchronous replication 模式的复制不受 recovery_min_apply_delay 参数的影响。 hot_standby_feedback 参数会受 recovery_min_apply_delay 参数的影响，并给主库带来膨胀。 设置此参数会带来 pg_xlog 目录的增长量，因为 WAL 要保留更长的时间。 recovery_min_apply_delay 参数的设置各有利弊，需根据实际情况进行选择。 由于 recovery_min_apply_delay 参数值为 32-bit integer, 最大值为 2147483647 ( 24 天 20 小时左右 )。 参考 recovery_min_apply_delay (integer) Postgres 9.4 feature highlight: delayed standbys WAITING FOR 9.4 – ALLOW TIME DELAYED STANDBYS AND RECOVERY","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"PostgreSQL9.4 新增 pg_stat_archiver 系统视图","slug":"20140523141111","date":"2014-05-23T06:11:11.000Z","updated":"2018-09-04T01:34:10.921Z","comments":true,"path":"20140523141111.html","link":"","permalink":"https://postgres.fun/20140523141111.html","excerpt":"","text":"PostgreSQL9.4 新增 pg_stat_archiver 系统视图用来记录 WAL 归档信息。 关于 pg_stat_archiver 备注：字段含义依次为：已归档的 wal文件总数，最近成功归档的wal文件名, 最近成功归档时间，归档失败 wal 数量，最近归档失败的 wal 文件名, 最近归档失败时间, 最近统计信息重置时间。 查询 pg_stat_archiver123456789francs=# select * from pg_stat_archiver;-[ RECORD 1 ]------+------------------------------archived_count | 132466 last_archived_wal | 00000001000000000000003F last_archived_time | 2014-05-23 10:20:29.668785-07failed_count | 0 last_failed_wal | last_failed_time | stats_reset | 2014-05-21 01:57:03.518831-07 切换 WAL123456789101112131415francs=# select pg_switch_xlog();pg_switch_xlog----------------0/400199C0(1 row) francs=# select * from pg_stat_archiver ;-[ RECORD 1 ]------+------------------------------archived_count | 132467 last_archived_wal | 000000010000000000000040 last_archived_time | 2014-05-23 13:40:37.291166-07failed_count | 0 last_failed_wal | last_failed_time | stats_reset | 2014-05-21 01:57:03.518831-07 备注：切换xlog 后，last_archived_wal 和 last_archived_time 信息已改变。 查看 pg_stat_archiver 数据来源12345678910111213141516171819202122232425francs=# \\d+ pg_stat_archiver View \"pg_catalog.pg_stat_archiver\" Column | Type | Modifiers | Storage | Description--------------------+--------------------------+-----------+----------+-------------archived_count | bigint | | plain |last_archived_wal | text | | extended |last_archived_time | timestamp with time zone | | plain |failed_count | bigint | | plain |last_failed_wal | text | | extended |last_failed_time | timestamp with time zone | | plain |stats_reset | timestamp with time zone | | plain |View definition:SELECT s.archived_count, s.last_archived_wal, s.last_archived_time, s.failed_count, s.last_failed_wal, s.last_failed_time, s.stats_reset FROM pg_stat_get_archiver() s(archived_count, last_archived_wal, last_archived_time, failed_count, last_failed_wal, last_failed_time, stats_reset); francs=# select pg_stat_get_archiver(); pg_stat_get_archiver -------------------------------------------------------------------------------------------------------(132467,000000010000000000000040,\"2014-05-23 13:40:37.291166-07\",0,,,\"2014-05-21 01:57:03.518831-07\") 备注：pg_stat_archiver 数据来源于一个名为 pg_stat_get_archiver() 函数。 参考 pg_stat_archiver View","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.4: 新增 Jsonb 数据类型 ","slug":"20140522184738","date":"2014-05-22T10:47:38.000Z","updated":"2018-09-04T01:34:10.843Z","comments":true,"path":"20140522184738.html","link":"","permalink":"https://postgres.fun/20140522184738.html","excerpt":"","text":"PostgreSQL9.4 新增 JSONB 数据类型， JSONB 同时属于 JSON (JavaScript Object Notation) 数据类型，jsonb 和 json 的输入数据几乎完全通用，最大的差别体现在效率上，json 存储的数据几乎和输入数据一样，存储的是未解析的数据，调用函数时使用效率较低; 而 jsonb 存储的是分解的 binary 格式数据，使用时不需要再解析了，因此使用上效率较高; 另一方面 json 在写入时较快，而 jsonb 写入时由于需要转换导致写入较慢。下面通过些简单的例子了解两者的差异。 两个示例这个例子两者没啥差异 1234567891011francs=&gt; SELECT '[1, 2, \"foo\", null]'::json; json ---------------------[1, 2, \"foo\", null](1 row) francs=&gt; SELECT '[1, 2, \"foo\", null]'::jsonb; jsonb ---------------------[1, 2, \"foo\", null](1 row) 备注: json 类型输出的内容和写入的内容一样，不会对输出的结果改变，而 jsonb不一样，看下面的例子。 Jsonb: 输出内容顺序不一样1234567891011francs=&gt; SELECT '&#123;\"bar\": \"baz\", \"balance\": 7.77, \"active\":false&#125;'::json; json -------------------------------------------------&#123;\"bar\": \"baz\", \"balance\": 7.77, \"active\":false&#125;(1 row) francs=&gt; SELECT '&#123;\"bar\": \"baz\", \"balance\": 7.77, \"active\":false&#125;'::jsonb; jsonb --------------------------------------------------&#123;\"bar\": \"baz\", \"active\": false, \"balance\": 7.77&#125;(1 row) Jsonb: 整数类型输出不一样12345francs=&gt; SELECT '&#123;\"reading\": 1.230e-5&#125;'::json, '&#123;\"reading\": 1.230e-5&#125;'::jsonb; json | jsonb-----------------------+-------------------------&#123;\"reading\": 1.230e-5&#125; | &#123;\"reading\": 0.00001230&#125;(1 row) Jsonb: 去掉了空格1234567891011121314151617181920francs=&gt; select ' &#123;\"id\":1,\"name\":\"francs\",\"remark\":\"a good guy!\"&#125;'::json; json ------------------------ &#123;\"id\":1, +\"name\":\"francs\", +\"remark\":\"a good guy!\"+&#125;(1 row) francs=&gt; select ' &#123;\"id\":1,\"name\":\"francs\",\"remark\":\"a good guy!\"&#125;'::jsonb; jsonb ------------------------------------------------------&#123;\"id\": 1, \"name\": \"francs\", \"remark\": \"a good guy!\"&#125;(1 row) Jsonb: 重复元素值保留最后一个123456789francs=&gt; select ' &#123;\"id\":1,\"name\":\"francs\",\"remark\":\"a good guy!\",\"name\":\"test\"&#125;'::jsonb; jsonb ----------------------------------------------------&#123;\"id\": 1, \"name\": \"test\", \"remark\": \"a good guy!\"&#125;(1 row) 备注： json 类型的输出和输入一样，会保留所有重复的元素，而 jsonb 对于重复的元素仅保留最后出现的重复元素。 Jsonb/Json 索引GIN 索引支持 jsonb 类型，支持大的 jsonb 表中基于 keys 或者 key/values 模式的检索。默认的 GIN 索引模式支持带有 @&gt;, ?, ?&amp; 和 ?| 操作的查询，关于这些操作符的含义参考本文的附录。 假如有一个文档：123456789101112131415&#123; \"guid\": \"9c36adc1-7fb5-4d5b-83b4-90356a46061a\", \"name\": \"Angela Barton\", \"is_active\": true, \"company\": \"Magnafone\", \"address\": \"178 Howard Place, Gulf, Washington, 702\", \"registered\": \"2009-11-07T08:53:22 +08:00\", \"latitude\": 19.793713, \"longitude\": 86.513373, \"tags\": [ \"enim\", \"aliquip\", \"qui\" ]&#125; 我们将表名定义为 api, jsonb 字段为 jdoc，创建如下索引1CREATE INDEX idx_gin_api_jdoc ON api USING gin (jdoc); 那么如下的查询可以使用索引12-- Find documents in which the key \"company\" has value \"Magnafone\"SELECT jdoc-&gt;'guid', jdoc-&gt;'name' FROM api WHERE jdoc @&gt; '&#123;\"company\": \"Magnafone\"&#125;'; 备注：上面这个例子来自手册。 参考 WAITING FOR 9.4 – INTRODUCE JSONB, A STRUCTURED FORMAT FOR STORING JSON PostgreSQL 9.4 new data type jsonb - do not need to reparser when used PostgreSQL 9.4 Beta 1发布，支持JSONB JSON Functions and Operators","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"JSON/JSONB","slug":"JSON-JSONB","permalink":"https://postgres.fun/tags/JSON-JSONB/"}]},{"title":"PostgreSQL 9.4：支持 Background Workers 后台进程动态注册，启动，停止","slug":"20140522135622","date":"2014-05-22T05:56:22.000Z","updated":"2018-09-04T01:34:10.796Z","comments":true,"path":"20140522135622.html","link":"","permalink":"https://postgres.fun/20140522135622.html","excerpt":"","text":"PostgreSQL 9.4 支持 background workers 后台进程动态注册，启动，停止， 这块工作是并行查询的基础， 从而实现大查询可以分布到多 cpu 上; 但是目前并不支持并行查询，只是支持动态注册，启动，停止后台进程，动态的意思是可以在数据库启动以后动态加载，不需要重启，下面简单演示。 手册中的说明 Allow background workers to be dynamically registered, started and terminated 创建 Worker_Spi 模块123456789101112[pg94@db1 ~]$ psql francspsql (9.4beta1)Type \"help\" for help. francs=# create extension worker_spi ;CREATE EXTENSION francs=# \\df worker_spi_launch List of functionsSchema | Name | Result data type | Argument data types | Type --------+-------------------+------------------+---------------------+--------public | worker_spi_launch | integer | integer | normal 备注： worker_spi 模块现在可以以 extension 方式创建了, 创建后会生成 worker_spi_launch 函数，此函数用来动态注册后台进程。 注册后台进程1234567891011francs=# select worker_spi_launch(1);worker_spi_launch------------------- 31047(1 row) francs=# select worker_spi_launch(2);worker_spi_launch------------------- 31055(1 row) 备注：函数返回的是后台进程号，可以从下面的输出验证。 验证后台进程查看后台进程：1234[pg94@db1 pg_root]$ ps -ef | grep bgworkerpg94 31047 29721 0 10:27 ? 00:00:00 postgres: bgworker: worker 1 pg94 31055 29721 0 10:27 ? 00:00:00 postgres: bgworker: worker 2 pg94 31058 30597 0 10:27 pts/1 00:00:00 grep bgworker 备注： 现在可以看到两个 bgworker 进程，想试下是否支持并行查询，编写一个代码断，让它不停地运行。 测试代码断12345678910do $$declarei int4;begin i:=0; while i&lt; 1 loop perform * from test_1 order by id desc limit 100; end loop;end;$$ language 'plpgsql'; 备注：开启一个会话执行上面的代码断. 观察数据库 cpu 使用情况，top 输出如下：备注：结果还是在用一个 cpu 核， 而没有将负载分担到其它 cpu 核上，这里纯属 yy。 调高主库的 max_worker_processes 设置后，从库的日志如下12342014-05-22 12:58:33.176 PDT,,,6547,,537e569c.1993,7,,2014-05-22 12:57:16 PDT,1/0,0,FATAL,22023,\"hot standby is not possible because max_worker_processes = 10 is a lower setting than on the master server (its value was 12)\",,,,,\"xlog redo parameter change: max_connections=500 max_worker_processes=12 max_prepared_xacts=0 max_locks_per_xact=64 wal_level=hot_standby\",,,,\"\"2014-05-22 12:58:33.177 PDT,,,6545,,537e569c.1991,3,,2014-05-22 12:57:16 PDT,,0,LOG,00000,\"startup process (PID 6547) exited with exit code 1\",,,,,,,,,\"\"2014-05-22 12:58:33.177 PDT,,,6545,,537e569c.1991,4,,2014-05-22 12:57:16 PDT,,0,LOG,00000,\"terminating any other active server processes\",,,,,,,,,\"\"2014-05-22 12:58:33.177 PDT,\"postgres\",\"postgres\",6563,\"[local]\",537e56b0.19a3,1,\"idle\",2014-05-22 12:57:36 PDT,2/0,0,WARNING,57P02,\"terminating connection because of crash of another server process\",\"The postmaster has commanded this server process to roll back the current transaction and exit, because another server process exited abnormally and possibly corrupted shared memory.\",\"In a moment you should be able to reconnect to the database and repeat your command.\",,,,,,,\"psql\" 备注：将主库的 max_worker_processes 调整成 12 并重启主库，之后从库宕机并报以上错误，解决方法： 这时可能需要重做从库了。 调低从库的 max_worker_processes 设置后，从库重启的日志如下122014-05-22 12:51:22.027 PDT,,,6443,,537e553a.192b,3,,2014-05-22 12:51:22 PDT,,0,FATAL,22023,\"hot standby is not possible because max_worker_processes = 9 is a lower setting than on the master server (its value was 10)\",,,,,,,,,\"\" 备注： 将从库的 max_worker_processes调整成 9 并重启从库，这时从库日志报以上错误; 解决方法： 只需要将从库的这个参数值调成比主库大或者相等，之后重启从库即可。 注意事项 后台进程的最大数由参数 max_worker_processes 限制，默认值为 8，修改后需重启生效。 流复制环境从库的 max_worker_processes 设置值必须大于或等于主库设置的值，否则从库不可查询，下面分别针对两种情形做了测试: 初始状态主库和从库的 max_worker_processes 参数值设置成 10.。 关于 max_worker_processes (integer) 参数 Sets the maximum number of background processes that the system can support. This parameter can only be set at server start. When running a standby server, you must set this parameter to the same or higher value than on the master server. Otherwise, queries will not be allowed in the standby server. 参考 Postgres 9.4 feature highlight: dynamic background workers Background Worker Processes","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.4 : 返回结果集函数增加 WITH ORDINALITY 属性 ","slug":"20140521142239","date":"2014-05-21T06:22:39.000Z","updated":"2018-09-04T01:34:10.718Z","comments":true,"path":"20140521142239.html","link":"","permalink":"https://postgres.fun/20140521142239.html","excerpt":"","text":"当 from 语句后面的函数加上 WITH ORDINALITY 属性后，那么返回的结果集将增加一个整数列，这个整数列从 1 开始，并且按 1 递增，例如： 例子123456789101112131415francs=# select * from generate_series(4,6) with ordinality; generate_series | ordinality-----------------+------------ 4 | 1 5 | 2 6 | 3(3 rows) francs=# select * from pg_ls_dir('pg_log') with ordinality limit 3; pg_ls_dir | ordinality----------------------------------+------------postgresql-2014-05-21_004931.csv | 1postgresql-2014-05-21_005605.csv | 2postgresql-2014-05-21_031005.csv | 3(3 rows) 备注：新增的列名默认为 ordinality，也可以手工更改列名，如下： 指定列名1234567francs=# select * from pg_ls_dir('pg_log') with ordinality as t(csvfile_name,num) limit 3; csvfile_name | num----------------------------------+-----postgresql-2014-05-21_004931.csv | 1postgresql-2014-05-21_005605.csv | 2postgresql-2014-05-21_031005.csv | 3(3 rows) 备注： 文档上说这个属性对于返回结果集的函数特别有用，例如 unnest()。 Unnest()函数拆数组1234567891011121314francs=# select array[4,5,6,7]; array -----------&#123;4,5,6,7&#125;(1 row) francs=# select * from unnest(array[4,5,6,7]) with ordinality;unnest | ordinality--------+------------ 4 | 1 5 | 2 6 | 3 7 | 4(4 rows) 参考 Postgres 9.4 feature highlight: WITH ORDINALITY unnest(anyarray) Set Returning Functions","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.4: REFRESH MATERIALIZED VIEW 新增 CONCURRENTLY 参数 ","slug":"20140521104402","date":"2014-05-21T02:44:02.000Z","updated":"2018-09-04T01:34:10.671Z","comments":true,"path":"20140521104402.html","link":"","permalink":"https://postgres.fun/20140521104402.html","excerpt":"","text":"PostgreSQL 9.3 版本开始支持物化视图(MATERIALIZED VIEW), 但物化视图的使用局限较大; 9.4 版本在物化视图方面做了改进, 例如 9.4 版本在刷新视图时新增 CONCURRENTLY 参数, 刷新过程中不会影响物化视图的查询操作. 下面通过实验演示。 刷新物化视图语法 REFRESH MATERIALIZED VIEW [ CONCURRENTLY ] name [ WITH [ NO ] DATA ] 创建物化视图1234567891011121314151617181920[pg94@db1 ~]$ psql francs francspsql (9.4beta1)Type \"help\" for help. francs=&gt; create table test_1 (id int4, name text, create_time timestamp(6) without time zone default clock_timestamp());CREATE TABLE francs=&gt; insert into test_1(id,name) select generate_series(1,3),generate_series(1,3) || 'a';INSERT 0 3 francs=&gt; CREATE MATERIALIZED VIEW mv_test_1 as select * from test_1;SELECT 3 francs=&gt; select * From mv_test_1;id | name | create_time----+------+---------------------------- 1 | 1a | 2014-05-21 00:05:51.751824 2 | 2a | 2014-05-21 00:05:51.752527 3 | 3a | 2014-05-21 00:05:51.752545(3 rows) 刷新物化视图sesstion A 刷新物化视图123456789101112francs=&gt; begin;BEGINfrancs=&gt; select pg_backend_pid();pg_backend_pid---------------- 15936(1 row) francs=&gt; refresh materialized view mv_test_1;REFRESH MATERIALIZED VIEWfrancs=&gt; --注意: session A 未提交 session B 查看 pg_locks12345678francs=&gt; select pid,mode,relation,granted from pg_locks where relation='mv_test_1'::regclass; pid | mode | relation | granted-------+---------------------+----------+---------15936 | AccessShareLock | 16408 | t15936 | ShareLock | 16408 | t15936 | ExclusiveLock | 16408 | t15936 | AccessExclusiveLock | 16408 | t(4 rows) 备注 : session B 用来查看物化视图 mv_test_1 上的锁情况, 这里刷新物化视图时获取的是 “AccessExclusiveLock”锁。 session C 查询 mv_test_112345678francs=&gt; select pg_backend_pid();pg_backend_pid---------------- 16125(1 row) francs=&gt; select * from mv_test_1 ;.. -- 此时 session C 处于等待状态. session B 再次查看 pg_locks123456789francs=&gt; select pid,mode,relation,granted from pg_locks where relation='mv_test_1'::regclass; pid | mode | relation | granted-------+---------------------+----------+---------16125 | AccessShareLock | 16408 | f15936 | AccessShareLock | 16408 | t15936 | ShareLock | 16408 | t15936 | ExclusiveLock | 16408 | t15936 | AccessExclusiveLock | 16408 | t(5 rows) 备注: 说明 session C 已经被 session A 阻塞了,因为 session A 获取的是 AccessExclusiveLock 锁。 在线刷新物化视图 接着测试 CONCURRENTLY 参数的在线刷新方式。 session A在线刷新物化视图123francs=&gt; refresh materialized view CONCURRENTLY mv_test_1 ;ERROR: cannot refresh materialized view \"francs.mv_test_1\" concurrentlyHINT: Create a UNIQUE index with no WHERE clause on one or more columns of the materialized view. 备注: 说是要创建不带 where 条件的 unique 索引. 创建索引12francs=&gt; create unique index idx_mv_test_1 on mv_test_1 using btree (id);CREATE INDEX 开启事务12345678910111213francs=&gt; begin;BEGINfrancs=&gt; select pg_backend_pid();pg_backend_pid---------------- 15936(1 row) francs=&gt; refresh materialized view CONCURRENTLY mv_test_1 ;REFRESH MATERIALIZED VIEWfrancs=&gt; --注意: session A 未提交 session B 查看 pg_locks1234567francs=&gt; select pid,mode,relation,granted from pg_locks where relation='mv_test_1'::regclass; pid | mode | relation | granted-------+------------------+----------+---------15936 | AccessShareLock | 16408 | t15936 | RowExclusiveLock | 16408 | t15936 | ExclusiveLock | 16408 | t(3 rows) 备注:可见在线刷新方式获取的是行锁 RowExclusiveLock 。 session C 查询 mv_test_11234567francs=&gt; select * from mv_test_1 ;id | name | create_time----+------+---------------------------- 1 | 1a | 2014-05-21 00:05:51.751824 2 | 2a | 2014-05-21 00:05:51.752527 3 | 3a | 2014-05-21 00:05:51.752545(3 rows) 备注: session C 查询正在刷新的物化视图正常。 性能比较插入 100 万数据1234567891011francs=&gt; truncate test_1;TRUNCATE TABLE francs=&gt; insert into test_1(id,name) select generate_series(1,1000000),generate_series(1,1000000) || 'a';INSERT 0 1000000 francs=&gt; \\dt+ test_1List of relationsSchema | Name | Type | Owner | Size | Description--------+--------+-------+--------+-------+-------------francs | test_1 | table | francs | 46 MB | 普通刷新1234567francs=&gt; refresh materialized view mv_test_1;REFRESH MATERIALIZED VIEWTime: 6813.920 ms francs=&gt; refresh materialized view mv_test_1;REFRESH MATERIALIZED VIEWTime: 5137.114 ms 备注: 大概花了 5 秒左右。 在线刷新1234567francs=&gt; refresh materialized view CONCURRENTLY mv_test_1;REFRESH MATERIALIZED VIEWTime: 35975.159 ms francs=&gt; refresh materialized view CONCURRENTLY mv_test_1;REFRESH MATERIALIZED VIEWTime: 37304.300 ms 备注:在线刷新花了 35 秒左右, 耗时为普通刷新方式的 7 倍左右。 再次理解 NO DATA 选项12345678910111213141516francs=&gt; \\dm+ mv_test_1List of relationsSchema | Name | Type | Owner | Size | Description--------+-----------+-------------------+--------+-------+-------------francs | mv_test_1 | materialized view | francs | 46 MB |(1 row) francs=&gt; refresh materialized view mv_test_1 with no data;REFRESH MATERIALIZED VIEW francs=&gt; \\dm+ mv_test_1List of relationsSchema | Name | Type | Owner | Size | Description--------+-----------+-------------------+--------+------------+-------------francs | mv_test_1 | materialized view | francs | 8192 bytes |(1 row) 备注: no data 选项其实是清空物化视图, 之后物化视图无法查看。 验证:再次查看123francs=&gt; select * from mv_test_1 limit 1;ERROR: materialized view \"mv_test_1\" has not been populatedHINT: Use the REFRESH MATERIALIZED VIEW command. 总结 在线刷新方式(CONCURRENTLY)在刷新物化视图过程中不会阻塞查询操作; 在线刷方式性能比普通方式慢很多, 从这里的测试标题来看, 慢了7 倍左右; 在线刷新方式要求物化视图上至少有一个 unique 索引, 并且这个索引不能是表达式索引或带 where 条件的部分索引。 参考 Waiting for PostgreSQL9.3：增加物化视图 (MATERIALIZED VIEW) PostgreSQL 锁浅析 MongoDB：锁机制 (Concurrency REFRESH MATERIALIZED VIEW","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.4: 新增 ALTER SYSTEM 命令","slug":"20140519164902","date":"2014-05-19T08:49:02.000Z","updated":"2018-09-04T01:34:10.592Z","comments":true,"path":"20140519164902.html","link":"","permalink":"https://postgres.fun/20140519164902.html","excerpt":"","text":"9.4 版本以前如果要修改数据库配置参数是通过修改 postgresql.conf 文件实现的, 9.4 版本之后, 新增 “ALTER SYSTEM” 命令可以在 psql 客户端通过命令更改数据库参数了. 语法1ALTER SYSTEM SET configuration_parameter &#123; TO | = &#125; &#123; value | 'value' | DEFAULT &#125; 备注: 命令很简单, 命令设置的参数值会写入到一个名为 postgresql.auto.conf 的文件, 这个文件和配置文件 postgresql.conf 不同, 它是二进制文件,不能手工编辑; 如果将参数值设置成 DEFAULT , 将在动态文件 postgresql.auto.conf 删除参数设置. 和之前一样,参数设置后需要 reload 或者 重启数据库生效,这和之前版本是一样的. ALTER SYSTEM 测试log_min_duration_statement 参数的默认值是 -1 ,表示不启用此参数, 现通过 ALTER SYSTEM 将它设置成 100 ms.1.1 设置123456789101112131415161718[pg94@db1 ~]$ psql psql (9.4beta1)Type \"help\" for help. postgres=# show log_min_duration_statement;log_min_duration_statement-----------------------------1(1 row) postgres=# ALTER SYSTEM SET log_min_duration_statement=100;ALTER SYSTEM postgres=# show log_min_duration_statement; log_min_duration_statement-----------------------------1(1 row) 备注: 此时没生效. 1.2 reload 后生效123456789101112[pg94@db1 ~]$ pg_ctl reload -D $PGDATAserver signaled [pg94@db1 ~]$ psql psql (9.4beta1)Type \"help\" for help. postgres=# show log_min_duration_statement;log_min_duration_statement----------------------------100ms(1 row) 1.3 查看 postgresql.conf 对应参数值12[pg94@db1 ~]$ grep 'log_min_duration_statement' $PGDATA/postgresql.conf#log_min_duration_statement = -1 # -1 is disabled, 0 logs all statements 备注: 发现 ALTER SYSTEM 命令并不会修改 postgresql.conf 参数值,而只是影响 postgresql.auto.conf 参数, 这很像 oracle 的 spfile 文件. 1.4 取消 postgresql.auto.conf 的参数设置12postgres=# ALTER SYSTEM SET log_min_duration_statement=default;ALTER SYSTEM 备注:取消后, reload 的后生效,这里不演示了. 优先性测试postgresql.conf 参数可以理解成初始文件, 而 postgresql.auto.conf 文件为动态二进制文件,这很像 oracle 的 pfile 和 spfile ,那么如果同一个参数两边都设置,以哪边的为准呢? 其实猜下就知道了,还是简单测试下: 2.1 ALTER SYSTEM 设置12postgres=# ALTER SYSTEM SET log_min_duration_statement=100; ALTER SYSTEM 2.2 postgresql.conf 设置12[pg94@db1 ~]$ grep 'log_min_duration_statement' $PGDATA/postgresql.conflog_min_duration_statement = 1000 # -1 is disabled, 0 logs all statements 2.3 reload12[pg94@db1 ~]$ pg_ctl reload -D $PGDATAserver signaled 2.4 验证123456789[pg94@db1 ~]$ psql -h 127.0.0.1psql (9.4beta1)Type \"help\" for help. postgres=# show log_min_duration_statement;log_min_duration_statement----------------------------100ms(1 row) 备注: 根据测试结果, 如果 postgresql.conf 和 postgresql.auto.conf 设置同一个参数值, reload 后以后者设置的值为准. 参考 ALTER SYSTEM","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.4 : Pg_prewarm 数据缓存预加载模块","slug":"20140519114519","date":"2014-05-19T03:45:19.000Z","updated":"2018-09-04T01:34:10.530Z","comments":true,"path":"20140519114519.html","link":"","permalink":"https://postgres.fun/20140519114519.html","excerpt":"","text":"重启数据库后, 数据库缓存将被清空, 应用程序需要从硬盘读数据, 在开始的一段时间内, 会影响一定的性能; 当然, 之前有 pgfincore 工具,可以将一些访问频繁的表预先加载到 OS 缓存, 我在之前的 blog 提到过,具体参考pgfincore: cache data to OS cache , 9.4 之后, PostgreSQL 提供 pg_prewarm 模块, 可以将数据预先加载到数据库中. pg_prewarm 概述PostgreSQL 9.4 开始支持 pg_prewarm 模块, pg_prewarm 模块可以将数据预先加载到数据库缓存,也可以预先加载到操作系统缓存. pg_prewarm 函数 pg_prewarm(regclass, mode text default ‘buffer’, fork text default ‘main’,first_block int8 default null,last_block int8 default null) RETURNS int8 备注: regclass 参数为数据库对像,通常情况为表名; modex 参数指加载模式,可选项有 ‘prefetch’, ‘read’,’buffer’, 默认为 ‘buffer’ 具体稍后介绍; fork 表示对像模式,可选项有 ‘main’, ‘fsm’, ‘vm’, 默认为 ‘main’, first_block 表示开始 prewarm 的数据块,last_block 表示最后 prewarm 的数据块. pg_prewarm 加载模式mode 参数指加载模式,可选项有 ‘prefetch’, ‘read’,’buffer’, 默认为 ‘buffer’. prefetch: 异步地将数据预加载到操作系统缓存; read: 最终结果和 prefetch 一样,但它是同步方式,支持所有平台. buffer: 将数据预加载到数据库缓存 pg_prewarm 演示2.1 安装 pg_prewarm123456[pg94@db1 ~]$ psql francspsql (9.4beta1)Type \"help\" for help. francs=# create extension pg_prewarm;CREATE EXTENSION 2.2 创建测试表1234567891011francs=&gt; create table test_pre (id int4,name character varying(64),creat_time timestamp(6) without time zone);CREATE TABLE francs=&gt; insert into test_pre select generate_series(1,100000),generate_series(1,100000)|| '_pre',clock_timestamp();INSERT 0 100000 francs=&gt; select pg_size_pretty(pg_relation_size('test_pre')); pg_size_pretty----------------5096 kB(1 row) 2.3 加载数据到数据库缓存1234567891011121314151617francs=&gt; select pg_prewarm('test_pre','buffer');pg_prewarm------------ 637(1 row) francs=&gt; select current_setting('block_size');current_setting-----------------8192(1 row) francs=&gt; select 637*8;?column?----------5096(1 row) 备注: pg_prewarm 函数返回的是加载后的数据块数,这里返回的是 637 个块, 我设置的数据库块大小为 8 k. 如果表比较大, 也可以指定表的 block 范围. 总结 pg_prewarm 本文没做太多的测试,仅介绍用法, pg_prewarm 适合数据库需要重启的情况, 个人觉得特别适合加载访问频繁的小表. 参考 pg_prewarm pgfincore: cache data to OS cache PostgreSQL: 使用 pgfincore 预加载数据优化一列","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"pg_prewarm","slug":"pg-prewarm","permalink":"https://postgres.fun/tags/pg-prewarm/"}]},{"title":"PostgreSQL 9.4: Replication Slots","slug":"20140517101713","date":"2014-05-17T02:17:13.000Z","updated":"2018-09-04T01:34:10.452Z","comments":true,"path":"20140517101713.html","link":"","permalink":"https://postgres.fun/20140517101713.html","excerpt":"","text":"“Replication slots” 是 PostgreSQL 9.4 的主要特性之一, 看了这方面的相关资料,可能理解会比较模糊,先从概念入手. Replication Slots 概述replication slots 是 9.4 的主要特性之一, standby 使用 replication slots 后会自动将 WAL 的应用情况发给 Primary, 尽管当 standby 由于某种原因掉线或者连接断开时, 这些信息依然对 Primary 可见; 流复制并不会默认开启 replication slots, 如果需要使用 replication slots 特性, 需手工配置. Replication Slots 的意义大家知道在流复制环境中, WAL 日志起着至关重要的作用, 特别是对于比较繁忙的库, WAL 的日志量是非常大的,如果备库由于某种原因掉线, 那么再次启动备库后, 很有可能由于所需的 WAL 被主库覆盖了,而出现以下错误1ERROR: requested WAL segment 00000001000000010000002D has already been removed 关于这个错误,我在之前的blog 有介绍: 详见: https://postgres.fun/20130702145542.html 为了防止这种情况, 通常的做法是设置较大的 wal_keep_segments , 从而使主库保留较大的 wal 日志文件, 当然,也可以开启归档, 这都是可以操作的, 前提是需要将监控做好, 不要因为 pg_xlog 撑暴硬盘了. 设置 Replication slots 后, 那么主库随时知道从库应用 wal 的情况 , 哪怕从库掉线,主库依然保留 WAL, 当然这种机制也有缺点, 如果从库掉线很久, 那么主库的 pg_xlog 会一直保留以至于撑暴硬盘, 这时监控需要做到位. 另一方面Replication slots 的设置也是用来支持logical replication, 本文不做介绍. 下面简单演示 Replication slots 的使用, 流复制搭建略. Replication slots 的使用3.1 主库配置12max_replication_slots = 1wal_level = hot_standby 备注:其它参数未列出,设置后重启生效. 3.2 重启主库1234[pg94@db1 pg_root]$ pg_ctl restart -D $PGDATA -m fastwaiting for server to shut down..... doneserver stoppedserver starting 3.3 在主库上创建 slot12345postgres=# select * from pg_create_physical_replication_slot('slot1');slotname | xlog_position----------+---------------slot1 |(1 row) 3.4 从库配置 recovery.conf1primary_slotname = 'slot1' 备注: 从库的 recovery.conf 文件设置 primary_slotname 参数, 指向主库创建的 ‘slot1’. 3.5 主库上验证1234567891011postgres=# select * from pg_replication_slots ;-[ RECORD 1 ]+----------slot_name | slot1 plugin | slot_type | physical datoid | database | active | t xmin | catalog_xmin | restart_lsn | 0/E0009E8 备注: active 表示是否使用, 关于这个视图的含义,参考本文后面的附录, 如果想关闭 replication_slot ,那么建议删掉刚创建的 slots. 3.6 删除 slot12postgres=# select pg_drop_replication_slot('slot1');ERROR: replication slot \"slot1\" is already active 备注: 活跃状态的 slot 不可以删除,这时需要取消从库的 primary_slotname = ‘slot1’ 设置, 之后重启从库. 3.7 再次删除主库 slot12345678910postgres=# select * from pg_replication_slots ;slot_name | plugin | slot_type | datoid | database | active | xmin | catalog_xmin | restart_lsn-----------+--------+-----------+--------+----------+--------+------+--------------+-------------slot1 | | physical | | | f | | | 0/E000AB0(1 row) postgres=# select pg_drop_replication_slot('slot1');pg_drop_replication_slot--------------------------(1 row) 备注: 之后发现主库的 slot1 处于不活动状态,再次删除成功. 参考 Postgres 9.4 feature highlight: Physical slots for replication pg_replication_slots","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"PostgreSQL 9.4 Beta 1 Released","slug":"20140516102850","date":"2014-05-16T02:28:50.000Z","updated":"2018-09-04T01:34:09.889Z","comments":true,"path":"20140516102850.html","link":"","permalink":"https://postgres.fun/20140516102850.html","excerpt":"","text":"详见: http://www.postgresql.org/about/news/1522/接下来要好好了解下这些特性. The PostgreSQL Global Development Group announced that the first beta release of PostgreSQL 9.4, the latest version of the world’s leading open source database, is available today. This beta contains previews of all of the features which will be available in version 9.4, and is ready for testing by the worldwide PostgreSQL community. Please download, test, and report what you find. Major FeaturesThe new major features available for testing in this beta include: JSONB: 9.4 includes the new JSONB “binary JSON” type. This new storage format for document data is higher-performance, and comes with indexing, functions and operators for manipulating JSON data. Replication: The new Data Change Streaming API allows decoding and transformation of the replication stream. This lays the foundation for new replication tools that support high-speed and more flexible replication and scale-out solutions. Materialized Views with “Refresh Concurrently”, which permit fast-response background summary reports for complex data. ALTER SYSTEM SET, which enables modifications to postgresql.conf from the SQL command line and from remote clients, easing administration tasks. These features expand the capabilities of PostgreSQL, and introduce new syntax, APIs, and management interfaces. Additional FeaturesThere are many other features in the 9.4 beta, all of which need testing by you: Dynamic Background Workers Replication Slots Write Scalability improvements Aggregate performance improvements Reductions in WAL volume GIN indexes up to 50% smaller and faster Updatable security barrier views New array manipulation and table functions Time-delayed standbys MVCC system catalog updates Decrease lock level for some ALTER TABLE commands Backup throttling WITHIN GROUP There have also been many internal changes in the inner workings of the Write Ahead Log (WAL), GIN indexes, replication, aggregation, and management of the system catalogs. The means we need you to help us find any new bugs that we may have introduced in these areas_before_the full release of 9.4. For a full listing of the features in version 9.4 Beta, please see therelease notes. Additional descriptions and notes on the new features are available on the9.4 Features Wiki Page. Test 9.4 Beta 1 NowWe depend on our community to help test the next version in order to guarantee that it is high-performance and bug-free. Please download PostgreSQL 9.4 Beta 1 and try it with your workloads and applications as soon as you can, and give feedback to the PostgreSQL developers. Features and APIs in Beta 1 will not change substantially before final release, so it is now safe to start building applications against the new features.More information on how to test and report issues Get the PostgreSQL 9.4 Beta 1, including binaries and installers for Windows, Linux and Mac fromour download page. Full documentation of the new versionis available online, and also installs with PostgreSQL.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：四舍五入函数和取整函数","slug":"20140508101444","date":"2014-05-08T02:14:44.000Z","updated":"2018-09-04T01:34:09.342Z","comments":true,"path":"20140508101444.html","link":"","permalink":"https://postgres.fun/20140508101444.html","excerpt":"","text":"今天 bbs 论坛有朋友问到 PostgreSQL 四舍五入函数问题，今天整理下，顺序把取整函数给列出，方便查阅。 四舍五入函数 Function Return Type Description round(dp or numeric) (same as input) round to nearest integer round(v numeric, s int) numeric round to s decimal places 1234567891011121314151617181920212223242526272829francs=&gt; select round(10.2); round ------- 10(1 row) francs=&gt; select round(10.9); round ------- 11(1 row) francs=&gt; select 10.1/3; ?column? --------------------3.3666666666666667(1 row) francs=&gt; select round(10.1/3); round -------3(1 row) francs=&gt; select round(10.1/3,2); round ------- 3.37(1 row) 取整函数 Function Return Type Description ceil(dp or numeric) (same as input) smallest integer not less than argument floor(dp or numeric) (same as input) largest integer not greater than argument 返回大于或等于给出数字的最小整数1234567891011francs=&gt; select ceil(3.36); ceil ------ 4(1 row) francs=&gt; select ceil(-3.36); ceil -------3(1 row) 返回小于或等于给出数字的最大整数1234567891011francs=&gt; select floor(3.36); floor -------3(1 row) francs=&gt; select floor(-3.36); floor ------- -4(1 row) 备注：如果没记错的话，这几个函数在 oracle 当中是通用的。 参考 Mathematical Functions","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostGIS-2.0.4 Bug 一例?  postgis-2.0.so: undefined symbol: GETSTRUCT","slug":"20140423173449","date":"2014-04-23T09:34:49.000Z","updated":"2018-09-04T01:34:09.295Z","comments":true,"path":"20140423173449.html","link":"","permalink":"https://postgres.fun/20140423173449.html","excerpt":"","text":"今天搭建一测试环境，需要安装 postgis ， postgis 是我最不愿意安装的 PG 组件，没有之一，特别是 postgis 2.0 以后，安装 postgis 的同时需要安装很多其它组件，例如 GDAL, LibXML2, JSON-C 等。 今天安装的是 postgis-2.0.4, 关于安装请参考 PostgreSQL：PostGIS 2.0 安装。 环境信息系统： CentOS release 5.4数据库: PostgreSQL 9.3.4PostGIS: 2.0.4 系统比较老，在编译安装过程中遇到了不少问题，有个问题如下： 创建 PostGIS 模块报错123456pg93@host_db-&gt; psql -h 127.0.0.1 db_lbspsql (9.3.4)Type \"help\" for help. db_lbs=# create extension postgis;ERROR: could not load library \"/opt/pgsql9.3.4/lib/postgis-2.0.so\": /opt/pgsql9.3.4/lib/postgis-2.0.so: undefined symbol: GETSTRUCT 备注: postgis 安装成功，但在通过 “create extension postgis” 加载组件时报以上错误，网上查了下说是 postgis 的一个 bug，解决方法如下： 解决过程修改以下文件vim /opt/soft_bak/postgis-2.0.4/postgis/geometry_estimate.c在文件开始部分增加以下:12345678910#include \"utils/rel.h\" #include \"../postgis_config.h\" //add#if POSTGIS_PGSQL_VERSION &gt;= 93 #include \"access/htup_details.h\"#endif #include \"liblwgeom.h\" 备注：之后重新编译，以下是我的编译命令。 编译安装123# ./configure --with-pgconfig=/opt/pgsql9.3.4/bin/pg_config --with-projdir=/usr/local/pg_tool/proj --with-geosconfig=/usr/local/pg_tool/geos/bin/geos-config --with-gdalconfig=/usr/local/pg_tool/gdal/bin/gdal-config --with-jsondir=/usr/local/pg_tool/json# make# make install 验证之后再次加载 postgis 组件正常123456789101112pg93@host_db-&gt; psql -h 127.0.0.1 db_lbspsql (9.3.4)Type \"help\" for help. db_lbs=# create extension postgis;CREATE EXTENSIONdb_lbs=# create extension postgis_topology;CREATE EXTENSIONdb_lbs=# CREATE EXTENSION fuzzystrmatch;CREATE EXTENSION 参考 http://trac.osgeo.org/postgis/changeset/10321 http://postgis.net/source PostgreSQL：PostGIS 2.0 安装","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PostGIS","slug":"PostGIS","permalink":"https://postgres.fun/tags/PostGIS/"}]},{"title":"Ubuntu：刻录光盘","slug":"20140416144714","date":"2014-04-16T06:47:14.000Z","updated":"2018-09-04T01:34:09.217Z","comments":true,"path":"20140416144714.html","link":"","permalink":"https://postgres.fun/20140416144714.html","excerpt":"","text":"ubuntu 刻录光盘非常方便, Brasero 作为默认的 DVD 光盘刻录软件，界面如下： 我这次用的是第五个选项 “刻录镜像” ， 将 .iso 文件刻录成光盘。如果仅是数据文件的刻录更方便了，使用文件管理器刻录即可。插入光盘，弹出界面： 选择好目标文件，点右上脚的 “写入到盘片 “即可。","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"PostgreSQL 外部表汇总","slug":"20140416110600","date":"2014-04-16T03:06:00.000Z","updated":"2018-09-04T01:34:09.154Z","comments":true,"path":"20140416110600.html","link":"","permalink":"https://postgres.fun/20140416110600.html","excerpt":"","text":"以下是之前 blog 总结的 PostgreSQL 外部表相关文章，汇总如下 ，供查阅。 pgsql_fdw oracle_fdw mysql_fdw file_fdw mongo_fdw Writeable Foreign Tables","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：Mongo_fdw 外部表测试","slug":"20140416105802","date":"2014-04-16T02:58:02.000Z","updated":"2018-09-04T01:34:09.107Z","comments":true,"path":"20140416105802.html","link":"","permalink":"https://postgres.fun/20140416105802.html","excerpt":"","text":"今天bbs论坛有朋友提到 mongo_fdw 相关的问题，之前测试外部表没测试 mongo_fdw，今天补上。 环境信息系统: RHEL 6.2MongoDB: 2.2.1PostgreSQL 9.3.0备注: MongoDB, PostgreSQL 安装略。 MongoDB 环境准备12345678910[mongo@redhatB ~]$ mongo 127.0.0.1:27018MongoDB shell version: 2.2.1connecting to: 127.0.0.1:27018/test rs0:PRIMARY&gt; db.test_fdw.save(&#123;id:1,name:'francs'&#125; );rs0:PRIMARY&gt; db.test_fdw.save(&#123;id:2,name:'zhou'&#125; ); rs0:PRIMARY&gt; db.test_fdw.find();&#123; \"_id\" : ObjectId(\"534ded23453824de92745949\"), \"id\" : 1, \"name\" : \"francs\" &#125;&#123; \"_id\" : ObjectId(\"534ded2c453824de9274594a\"), \"id\" : 2, \"name\" : \"zhou\" &#125; 备注：在 MongoDB 中的 test 库创建一张测试表 test_fdw。 Mongo_fdw 安装下载https://github.com/citusdata/mongo_fdw备注：下载并解压。 编译，安装12345678910[root@redhatB mongo_fdw-master]# source /home/pg93/.bash_profile[root@redhatB mongo_fdw-master]# make[root@redhatB mongo_fdw-master]# make install/bin/mkdir -p '/opt/pgsql9.3.0/lib'/bin/mkdir -p '/opt/pgsql9.3.0/share/extension'/bin/mkdir -p '/opt/pgsql9.3.0/share/extension'/usr/bin/install -c -m 755 mongo_fdw.so '/opt/pgsql9.3.0/lib/mongo_fdw.so'/usr/bin/install -c -m 644 ./mongo_fdw.control '/opt/pgsql9.3.0/share/extension/'/usr/bin/install -c -m 644 ./mongo_fdw--1.0.sql '/opt/pgsql9.3.0/share/extension/' 备注： 由于编译会用到 pg_config 和 $PGHOME，执行前需要加载普通用户的环境变量。 加载 mongo_fdw 模块12345678[pg93@redhatB ~]$ psql -h 127.0.0.1psql (9.3.0)Type \"help\" for help. postgres=# \\c francsYou are now connected to database \"francs\" as user \"postgres\".francs=# create extension mongo_fdw ;CREATE EXTENSION 创建 FOREIGN SERVER12345francs=# CREATE SERVER mongo_server FOREIGN DATA WRAPPER mongo_fdw OPTIONS (address '127.0.0.1', port '27018');CREATE SERVER francs=# grant usage on FOREIGN server mongo_server to francs;GRANT 创建外部表12345678CREATE FOREIGN TABLE ft_test_fdw( _id NAME, id int4, name text)SERVER mongo_serverOPTIONS (database 'test', collection 'test_fdw'); Mongo_fdw 测试123456789francs=&gt; analyze ft_test_fdw;ANALYZE francs=&gt; select * from ft_test_fdw ; _id | id | name--------------------------+----+--------534ded23453824de92745949 | 1 | francs534ded2c453824de9274594a | 2 | zhou(2 rows) 备注：可以成功查询 MongoDB 数据测试成功。 参考 https://github.com/citusdata/mongo_fdw","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"mongo_fdw","slug":"mongo-fdw","permalink":"https://postgres.fun/tags/mongo-fdw/"}]},{"title":"PostgreSQL：老外用 SQL 画了个米老鼠","slug":"20140403140738","date":"2014-04-03T06:07:38.000Z","updated":"2018-09-04T01:34:09.045Z","comments":true,"path":"20140403140738.html","link":"","permalink":"https://postgres.fun/20140403140738.html","excerpt":"","text":"老外用 SQL 画了个米老鼠，真有才, http://manojadinesh.blogspot.com/2014/04/cartoon-in-pg.html 米老鼠 SQL12345678910111213141516171819202122232425SELECT * FROM (SELECT array_to_string(array_agg(CASE WHEN (POWER((xx.x - 25), 2) / 130 + POWER((yy.y - 25), 2) / 130) = 1 THEN '$' WHEN (SQRT(POWER(xx.x - 20, 2) + POWER(yy.y - 20, 2))) &lt; 2 THEN '#' WHEN (SQRT(POWER(xx.x - 20, 2) + POWER(yy.y - 30, 2))) &lt; 2 THEN '#' WHEN (SQRT(POWER(xx.x - 29, 2) + POWER(yy.y - 25, 2))) &lt; 4 THEN '#' WHEN (POWER((xx.x - 10), 2) / 40 + POWER((yy.y - 10), 2) / 40) = 1 THEN '$' WHEN (POWER((xx.x - 10), 2) / 40 + POWER((yy.y - 40), 2) / 40 = 1) THEN '$' ELSE ' ' END), ' ') AS cartoon FROM (SELECT generate_series(1, 40) AS x) AS xx, (SELECT generate_series(1, 50) AS y) AS yy GROUP BY xx.x ORDER BY xx.x) AS co_ord; 生成的图","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Ubuntu：百度网盘 Python 客户端的使用","slug":"20140402165530","date":"2014-04-02T08:55:30.000Z","updated":"2018-09-04T01:34:08.982Z","comments":true,"path":"20140402165530.html","link":"","permalink":"https://postgres.fun/20140402165530.html","excerpt":"","text":"百度云官方没有提供 Ubuntu 的客户端，之前一直使用百度云网页版进行操作，但网页版只能上传文件，不能上传文件目录，非常不方便，听同事推荐，有一款 python 写的 百度云客户端，可以上传文件，于是测试下： 下载https://github.com/houtianze/bypy 12345678francs@francs:~/Desktop/byby$ git clone https://github.com/houtianze/bypy byby/Cloning into 'byby'...remote: Reusing existing pack: 179, done.remote: Counting objects: 3, done.remote: Compressing objects: 100% (3/3), done.remote: Total 182 (delta 0), reused 0 (delta 0)Receiving objects: 100% (182/182), 142.77 KiB | 33 KiB/s, done.Resolving deltas: 100% (108/108), done. 安装 Python Requests 库12345678francs@francs:~/Desktop/byby/byby$ sudo pip install requestsDownloading/unpacking requests Downloading requests-2.2.1.tar.gz (421Kb): 421Kb downloaded Running setup.py egg_info for package requestsInstalling collected packages: requests Running setup.py install for requestsSuccessfully installed requestsCleaning up... 备注：开始执行这步时报错，翻墙后执行正常，执行完后还有个百度的网页进行授权过程，根据提示点击链接，之后复制授权码即可。 同步本地目录到百度云1francs@francs:/media/work/byby/byby$ python bypy.py syncup /media/work/娱乐//fc500game.exe 备注：这样就可以同步文件夹了。 列出远程根目录文件12345678910francs@francs:/media/work/byby/byby$ python bypy.py listToken file: '/home/francs/.bypy.json'Hash Cache file: '/home/francs/.bypy.pickle'App root path at Baidu Yun '/apps/bypy'sys.stdin.encoding = UTF-8sys.stdout.encoding = UTF-8---- /apps/bypy ($t $f $s $m $d):D fc500game 0 2014-04-02, 16:36:13 列出远程 /apps/bypy/fc500game 文件12345678910111213francs@francs:/media/work/byby/byby$ python bypy.py list /fc500gameToken file: '/home/francs/.bypy.json'Hash Cache file: '/home/francs/.bypy.pickle'App root path at Baidu Yun '/apps/bypy'sys.stdin.encoding = UTF-8sys.stdout.encoding = UTF-8---- /apps/bypy/fc500game ($t $f $s $m $d):D rom 0 2014-04-02, 16:36:18F aq7z.dll 96256 2014-04-02, 16:36:18 53014f3764238d08a48590e2e1f5f4b9F nnnesterJ.exe 770048 2014-04-02, 16:36:15 c350676fe0e7ae08bc9e3fafb0eb8284F PlayGame.exe 2490384 2014-04-02, 16:36:17 487c986d2a7bdac4e4c859ee3cef625c 比较远程目录和本地目录1234567891011121314151617python bypy.py compare /fc500game /media/work/娱乐/fc500game==== Same files ===....省略...==== Different files ===F - rom/洛克人X.zipF - rom/地道战(中文)/ddz.zipF - rom/血之战士.zip==== Local only ======== Remote only ==== Statistics:--------------------------------Same: 915Different: 3Local only: 0Remote only: 0 备注： compare 命令很有用 ，上面看到不同的文件有 3 个，重新 syncup 一次就一样了。 附: 常用命令显示使用帮助和所有命令（英文）:123456789101112131415161718192021222324252627bypy.py更详细的了解某一个命令： bypy.py help &lt;command&gt;显示在云盘（程序的）根目录下文件列表： bypy.py list把当前目录同步到云盘： bypy.py syncupor bypy.py upload把云盘内容同步到本地来： bypy.py syncdownor bypy.py downdir /比较本地当前目录和云盘（程序的）根目录（个人认为非常有用）： bypy.py compare还有一些其他命令 ... 哈希值的计算加入了缓存处理，使得第一次以后的计算速度有所提高。 运行时添加 -v 参数，程序会显示进度详情；添加 -d ，程序会显示一些调试信息。 参考 https://github.com/houtianze/bypy","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"PostgreSQL： 表回收站工具 Pgtrashcan 介绍 ","slug":"20140402134132","date":"2014-04-02T05:41:32.000Z","updated":"2018-09-04T01:34:08.904Z","comments":true,"path":"20140402134132.html","link":"","permalink":"https://postgres.fun/20140402134132.html","excerpt":"","text":"今天发现了一个名为 pgtrashcan 垃圾回收工具，当删除 PostgreSQL 表后，表并不立即物理删除，而是先保存到回收站中，在需要时可以恢复表，下载测试下。 Pgtrashcan 原理当 DROP TABLE 命令执行后，表会被移到一个名为 “Trash” 的模式下，如果想永久删除此表，可以删除 “Trash” 模式下的这张表或者删除整个 “Trash” 模式，这个模块仅对表有效，其它数据库对像被删除后不会转移到 “Trash” 模式。 Pgtrashcan 安装下载https://github.com/petere/pgtrashcan 安装文件1234567[root@db1 soft_bak]# cd pgtrashcan[root@db1 pgtrashcan]# lltotal 16-rw-rw-r--. 1 root root 168 Apr 1 19:43 Makefile-rw-rw-r--. 1 root root 4039 Apr 1 19:43 pgtrashcan.c-rw-rw-r--. 1 root root 1494 Apr 1 19:43 README.mddrwxrwxr-x. 4 root root 4096 Apr 1 19:43 test 编译并安装123456[root@db1 pgtrashcan]# make PG_CONFIG=/opt/pgsql_9.3.3/bin/pg_configgcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I. -I. -I/opt/pgsql_9.3.3/include/server -I/opt/pgsql_9.3.3/include/internal -D_GNU_SOURCE -c -o pgtrashcan.o pgtrashcan.cgcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv -fpic -shared -o pgtrashcan.so pgtrashcan.o -L/opt/pgsql_9.3.3/lib -Wl,--as-needed -Wl,-rpath,'/opt/pgsql_9.3.3/lib',--enable-new-dtags [root@db1 pgtrashcan]# make install PG_CONFIG=/opt/pgsql_9.3.3/bin/pg_config/bin/mkdir -p '/opt/pgsql_9.3.3/lib'/usr/bin/install -c -m 755 pgtrashcan.so '/opt/pgsql_9.3.3/lib/pgtrashcan.so' 修改 postgresql.conf, 设置以下参数1shared_preload_libraries = 'pgtrashcan' # (change requires restart) 备注：此参数设置后需重启数据库才生效。 重启数据库123456789[pg93@db1 bin]$ pg_ctl restart -m fast -D $PGDATAwaiting for server to shut down.... doneserver stoppedserver starting[pg93@db1 bin]$ LOG: 00000: loaded library \"pgtrashcan\"LOCATION: load_libraries, miscinit.c:1296LOG: 00000: redirecting log output to logging collector processHINT: Future log output will appear in directory \"pg_log\".LOCATION: SysLogger_Start, syslogger.c:649 备注: 根据上述信息， 看到 “pgtrashcan” 模块已被载入。 Pgtrashcan 测试创建表1234567891011121314151617181920212223242526francs=&gt; create table test_trash(id serial primary key, name character varying(64),create_time timestamp(6) without time zone);CREATE TABLEfrancs=&gt; insert into test_trash(name,create_time) select generate_series(1,100000)* random() || 'a',clock_timestamp();INSERT 0 100000francs=&gt; create index idx_test_trash_ctime on test_trash using btree (create_time);CREATE INDEXfrancs=&gt; \\d test_trash Table \"francs.test_trash\" Column | Type | Modifiers -------------+--------------------------------+--------------------------------------------------------- id | integer | not null default nextval('test_trash_id_seq'::regclass) name | character varying(64) | create_time | timestamp(6) without time zone | Indexes: \"test_trash_pkey\" PRIMARY KEY, btree (id) \"idx_test_trash_ctime\" btree (create_time)francs=&gt; select * from test_trash limit 3; id | name | create_time ----+--------------------+---------------------------- 1 | 0.939794037491083a | 2014-04-02 11:22:09.238519 2 | 1.51998516358435a | 2014-04-02 11:22:09.239463 3 | 1.02361420495436a | 2014-04-02 11:22:09.239518(3 rows) 以普通用户删除表1234francs=&gt; \\c francs francsYou are now connected to database \"francs\" as user \"francs\".francs=&gt; drop table test_trash ;ERROR: permission denied for schema Trash 备注：权限不够，删除表后 pgtrashcan 模块需要创建一个名为 “Trash” 的模式，但我已经给 “francs” 用户赋予创建 schema 的权限了，不知为何。 以超级用户删除表123456789101112131415161718192021222324francs=# drop table francs.test_trash ;DROP TABLEfrancs=# \\dn List of schemas Name | Owner --------+---------- Trash | postgres francs | francs public | postgres(3 rows)francs=# \\dt \"Trash\".* List of relations Schema | Name | Type | Owner --------+------------+-------+-------- Trash | test_trash | table | francs(1 row)francs=# \\ds \"Trash\".* List of relations Schema | Name | Type | Owner --------+-------------------+----------+-------- Trash | test_trash_id_seq | sequence | francs(1 row) 备注：以超级用户登陆后，才可以删除表，之后会在当前库创建 “Trash” 模式，并将删除的表和序列移到 “Trash” 模式下。 恢复删除的表123456789francs=# alter table \"Trash\".test_trash set schema francs;ALTER TABLEfrancs=# \\dt+ francs.test_trash List of relations Schema | Name | Type | Owner | Size | Description --------+------------+-------+--------+---------+------------- francs | test_trash | table | francs | 5912 kB | (1 row) 备注：恢复删除的表只需要更改表的 schema 即可。 函数删除测试创建函数123456francs=&gt; CREATE FUNCTION sales_tax(subtotal real) RETURNS real AS $$francs$&gt; BEGINfrancs$&gt; RETURN subtotal * 0.06;francs$&gt; END;francs$&gt; $$ LANGUAGE plpgsql;CREATE FUNCTION 删除函数12345678francs=&gt; drop function sales_tax(subtotal real) ;DROP FUNCTIONfrancs=&gt; \\df \"Trash\".* List of functions Schema | Name | Result data type | Argument data types | Type --------+------+------------------+---------------------+------(0 rows) 备注: 函数删除后，并不会移到 “Trash” 模式下。 永久删除表如果想永久删除此表，可以删除 “Trash” 模式下的这张表或者删除整个 “Trash” 模式。12345francs=&gt; \\c francs postgresYou are now connected to database \"francs\" as user \"postgres\".francs=# drop schema \"Trash\" cascade;NOTICE: drop cascades to table \"Trash\".test_trashDROP SCHEMA 备注：这里删除的是 “Trash” 整个模式。 总结模块 pgtrashcan 上生产需慎用，这里仅介绍使用，并没做太多的测试。 参考 pgtrashcan","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Ubuntu：安装任天堂红白机( 童年的回忆 ) ","slug":"20140401175429","date":"2014-04-01T09:54:29.000Z","updated":"2018-09-04T01:34:08.842Z","comments":true,"path":"20140401175429.html","link":"","permalink":"https://postgres.fun/20140401175429.html","excerpt":"","text":"小时候是个红白机迷，特别迷恋 “第二次超级机器人大战”，”重装机兵”, “吞食天地”, “足球小将”等游戏， 今天突然有个想法，是否可以在 ubuntu 安装红白机模拟器，这样就不需要切换到 windows 下体验红白机游戏了，网上查了下，非常简单，只需要安装 FCEUX 即可。 安装 Fceux1sudo apt-get install fceux 调整窗口大小开始运行 fceux 时窗口很小，需要调整窗口大小，在最上面菜单栏调整 “Options” –&gt; “Video Config” ，如下图: 安装后的界面备注： 多么熟悉的画面，曾经为了通关，好几天不睡觉的，还有儿时陪我一起战斗的小伙伴。 参考 http://www.fceux.com/web/home.html","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"[转载] PostgreSQL：体系结构( Pgsrcstructure )  ","slug":"20140326110628","date":"2014-03-26T03:06:28.000Z","updated":"2018-09-04T01:34:08.795Z","comments":true,"path":"20140326110628.html","link":"","permalink":"https://postgres.fun/20140326110628.html","excerpt":"","text":"找到一篇介绍 PostgreSQL 体系结构较好的文章，由李元佳老师翻译，转载过来。PostgreSQL及其代码的结构作者：石井達夫翻译: 李元佳 Galy原文：PostgreSQL の構造とソースツリー（１） 详见： http://wiki.postgresql.org/wiki/Pgsrcstructure PgsrcstructurePostgreSQL及其代码的结构作者：石井達夫翻译: 李元佳 Galy原文：PostgreSQL の構造とソースツリー（１）本文将对PostgreSQL全体的结构以及源码的结构概要进行说明。解析的版本是PostgreSQL9.1.x系列的版本，其他版本有可能会和本文说明的内容有些差别。 这篇文章将以笔者05年在技术评论社《WEB+DB PRESS》第26期发表的文章为基础，根据PostgreSQL9.1的实际情况加以修改而成。 Contents [hide]1 PostgreSQL的使用形态2 PostgreSQL的结构2.1 Potgres(常驻进程)2.2 Postgres(子进程)2.3 其他的进程2.3.1 Writer process2.3.2 WAL writer process2.3.3 Archive process2.3.4 stats collector process2.3.5 Logger process2.3.6 Autovacuum启动进程2.3.7 自动vacuum进程2.3.8 wal sender / wal receiver2.4 后端的处理流程3 PostgreSQL的源码3.1 第一级目录结构4 代码的阅读方法4.1 用调试器追踪代码4.2 实际使用gdb试试4.3 使用tag来跳转到相应的函数定义文件5 总结 PostgreSQL的使用形态PostgreSQL采用C/S(客户机/服务器)模式结构。应用层通过INET或者Unix Socket利用既定的协议与数据库服务器进行通信。 另外，还有一种‘Standalone Backend’使用的方式, 虽然通过这种方式也可以启动服务器，但是一般只在数据库的初始化(PostgreSQL的cluster的初始化，相当于其他数据库的instance的初始化)、紧急维护的时候使用，所以简单来说可以认为PostgreSQL是使用C/S的形式进行访问的。 PostgreSQL把客户端称为前端(Frontend),把服务器端成为后端(Backend), 后端有复数个进程构成，这个在后面会进行说明。 前端和后端通信的协议在PostgreSQL的官方文档中的《前端和后端的通信协议》一章中有详细的说明。简单来说，大体的工作模式是：前端向后端发送查询的SQL文，然后后端通过复数个报文把结果返回给前端。 由于需要进行连接的初始化、错误等各种各样处理，PostgreSQL的协议的处理也是相当复杂，如果要自己从头实现这些协议的处理的话，还是相当麻烦的，所以PostgreSQL本身提供了C语言写的libpq这样一个协议处理库，利用这个库可以比较轻松地和后端进行通信。PostgreSQL的话除了C以外，还支持Perl和PHP等其他语言，这些语言在内部也调用了libpq. 也有不使用libpq而直接与PostgreSQL通信的库。比较具有代表性的是Java, PostgreSQL的JDBC驱动是不依赖于libpq直接与PostgreSQL通信的. 另外后端的话，比较核心的是进行数据库处理的数据库引擎(Database Engine)。 数据库引擎可以对用户所编写的函数进行解析和处理，用户如果能够利用好这个功能的话，可以柔软地扩展PostgreSQL的功能。 比较经常使用的是存储过程(PostgreSQL中称为用户自定义函数)，PostgreSQL支持的用户定义函数的语言如下： 123456语言 对应的自定义函数C C函数SQL SQL 函数类似Oracle的PL/SQL的语言 PL/pgSQLPerl PL/PerlPython PL/Python PostgreSQL的话，用户可以自定义语言处理引擎。各种服务器脚本语言的解析引擎，以第三方的形式存在，主要的处理语言有Ruby、Java以及PHP等。 PostgreSQL的结构这里的话，再详细看看PostgreSQL的结构。 后端由几个进程构成。 Potgres(常驻进程)管理后端的常驻进程，也称为’postmaster’。其默认监听UNIX Domain Socket和TCP/IP（Windows等，一部分的平台只监听tcp/ip）的5432端口，等待来自前端的的连接处理。监听的端口号可以在PostgreSQL的设置文件postgresql.conf里面可以改。 一旦有前端连接过来，postgres会通过fork(2)生成子进程。没有Fork(2)的windows平台的话，则利用createProcess()生成新的进程。这种情形的话，和fork(2)不同的是，父进程的数据不会被继承过来，所以需要利用共享内存把父进程的数据继承过来。 Postgres(子进程)子进程根据pg_hba.conf定义的安全策略来判断是否允许进行连接，根据策略，会拒绝某些特定的IP及网络，或者也可以只允许某些特定的用户或者对某些数据库进行连接。Postgres会接受前端过来的查询，然后对数据库进行检索，最好把结果返回，有时也会对数据库进行更新。更新的数据同时还会记录在事务日志里面（PostgreSQL称为WAL日志），这个主要是当停电的时候，服务器当机，重新启动的时候进行恢复处理的时候使用的。另外，把日志归档保存起来，可在需要进行恢复的时候使用。在PostgreSQL 9.0以后，通过把WAL日志传送其他的postgreSQL，可以实时得进行数据库复制，这就是所谓的‘数据库复制’功能。 其他的进程Postgres之外还有一些辅助的进程。这些进程都是由常驻postgres启动的进程。 Writer processWriter process在适当的时间点把共享内存上的缓存写往磁盘。通过这个进程，可以防止在检查点的时候(checkpoint),大量的往磁盘写而导致性能恶化，使得服务器可以保持比较稳定的性能。Background writer起来以后就一直常驻内存，但是并非一直在工作，它会在工作一段时间后进行休眠，休眠的时间间隔通过postgresql.conf里面的参数bgwriter_delay设置，默认是200微秒。这个进程的另外一个重要的功能是定期执行检查点(checkpoint)。检查点的时候，会把共享内存上的缓存内容往数据库文件写，使得内存和文件的状态一致。通过这样，可以在系统崩溃的时候可以缩短从WAL恢复的时间，另外也可以防止WAL无限的增长。 可以通过postgresql.conf的checkpoint_segments、checkpoint_timeout指定执行检查点的时间间隔。 WAL writer processWAL writer process把共享内存上的WAL缓存在适当的时间点往磁盘写，通过这样，可以减轻后端进程在写自己的WAL缓存时的压力，提高性能。另外，非同步提交设为true的时候，可以保证在一定的时间间隔内，把WAL缓存上的内容写入WAL日志文件。 Archive processArchive process把WAL日志转移到归档日志里。如果保存了基础备份以及归档日志，即使实在磁盘完全损坏的时候，也可以回复数据库到最新的状态。 stats collector process统计信息的收集进程。收集好统计表的访问次数，磁盘的访问次数等信息。收集到的信息除了能被autovaccum利用，还可以给其他数据库管理员作为数据库管理的参考信息。 Logger process把postgresql的活动状态写到日志信息文件（并非事务日志），在指定的时间间隔里面，对日志文件进行rotate. Autovacuum启动进程autovacuum launcher process是依赖于postmaster间接启动vacuum进程。而其自身是不直接启动自动vacuum进程的。通过这样可以提高系统的可靠性。 自动vacuum进程autovacuum worker process进程实际执行vacuum的任务。有时候会同时启动多个vacuum进程。 wal sender / wal receiverwal sender 进程和wal receiver进程是实现postgresql复制(streaming replication)的进程。Wal sender进程通过网络传送WAL日志，而其他PostgreSQL实例的wal receiver进程则接收相应的日志。Wal receiver进程的宿主PostgreSQL（也称为Standby）接受到WAL日志后，在自身的数据库上还原，生成一个和发送端的PostgreSQL(也称为Master)完全一样的数据库。 后端的处理流程下面看看数据库引擎postgres子进程的处理概要。为了简单起见下面的说明中，把backend process简称为backend。Backend的main函数是PostgresMain (tcop/postgres.c)。 接收前端发送过来的查询(SQL文) SQL文是单纯的文字，电脑是认识不了的，所以要转换成比较容易处理的内部形式构文树parser tree,这个处理的称为构文解析。构文解析的模块称为parser.这个阶段只能够使用文字字面上得来的信息，所以只要没语法错误之类的错误，即使是select不存在的表也不会报错。这个阶段的构文树被称为raw parse tree. 构文处理的入口在raw_parser (parser/parser.c)。 构文树解析完以后，会转换为查询树(Query tree)。这个时候，会访问数据库，检查表是否存在，如果存在的话，则把表名转换为OID。这个处理称为分析处理(Analyze), 进行分析处理的模块是analyzer。 另外，PostgreSQL的代码里面提到构文树parser tree的时候，更多的时候是指查询树Query tree。分析处理的模块的入口在parse_analyze (parser/analyze.c) PostgreSQL还通过查询语句的重写实现视图(view)和规则(rule), 所以需要的时候，在这个阶段会对查询语句进行重写。这个处理称为重写(rewrite)，重写的入口在QueryRewrite (rewrite/rewriteHandler.c)。 通过解析查询树，可以实际生成计划树。生成查询树的处理称为‘执行计划处理’，最关键是要生成估计能在最短的时间内完成的计划树(plan tree)。这个步骤称为’查询优化’(不叫query optimize, 而是optimize), 而完成这个处理的模块称为查询优化器(不叫query optimizer,而是optimizer, 或者称为planner)。执行计划处理的入口在standard_planner (optimizer/plan/planner.c)。 按照执行计划里面的步骤可以完成查询要达到的目的。运行执行计划树里面步骤的处理称为执行处理‘execute’, 完成这个处理的模块称为执行器‘Executor’, 执行器的入口地址为，ExecutorRun (executor/execMain.c) 执行结果返回给前端。 返回到步骤一重复执行。 PostgreSQL的源码现在基本上理解了PostgreSQL的大体的结构，我们再来看看PostgreSQL代码的结构。 PostgreSQL初期的时候，大概只有20万行左右的代码，现在已经发展到100万行了。这个量来说，没有指导读起来是极为难理解的，这里把大概的代码结构说明一下，让大家对源码的结构有个理解。 第一级目录结构进入PostgreSQL的源码目录后，第一级的结构如下表所示。在这一级里，通过执行如下命令configure;make;make install可以立即进行简单的安装，实际上从PostgreSQL源码安装是极为简单的。123456789101112131415文件目录 说明COPYRIGHT 版权信息GUNMakefile 第一级目录的 MakefileGUNMakefile.in Makefile 的雏形HISTORY 修改历史INSTALL 安装方法简要说明Makefile Makefile模版README 简单说明aclocal.m4 config 用的文件的一部分config/ config 用的文件的目录configure configure 文件configure.in configure 文件的雏形contrib/ contribution 程序doc/ 文档目录src/ 源代码目录 PostgreSQL 的src下面有。12345678910111213141516171819202122文件目录 说明DEVELOPERS 面向开发人员的注视Makefile Makefile Makefile.global make 的设定值（从configure生成的）Makefile.global.in Configure使用的Makefile.global的雏形Makefile.port 平台相关的make的设定值，实际是一个到makefile/Makefile的连接. （从configure生成的）Makefile.shlib 共享库用的Makefilebackend/ 后端的源码目录bcc32.mak Win32 ポート用の Makefile (Borland C++ 用)bin/ psql 等 UNIX命令的代码include/ 头文件interfaces/ 前端相关的库的代码makefiles/ 平台相关的make 的设置值nls-global.mk 信息目录用的Makefile文件的规则pl/ 存储过程语言的代码port/ 平台移植相关的代码template/ 平台相关的设置值test/ 各种测试脚本timezone/ 时区相关代码tools/ 各自开发工具和文档tutorial/ 教程win32.mak Win32 ポート用の Makefile (Visual C++ 用) 这里比较核心的是backend,bin,interface这几个目录。Backend是对应于后端，bin和interface对应于前端。 bin里面有pgsql,initdb,pg_dump等各种工具的代码。interface里面有PostgreSQL的C语言的库libpq,另外可以在C里嵌入SQL的ECPG命令的相关代码。 Backend目录的结构如下：12345678910111213141516171819202122232425262728293031目录文件 说明Makefile makefileaccess/ 各种存储访问方法(在各个子目录下) common(共同函数)、gin (Generalized Inverted Index通用逆向索引) gist (Generalized Search Tree通用索引)、 hash (哈希索引)、heap (heap的访问方法)、 index (通用索引函数)、 nbtree (Btree函数)、transam (事务处理)bootstrap/ 数据库的初始化处理(initdb的时候)catalog/ 系统目录commands/ SELECT/INSERT/UPDATE/DELETE以为的SQL文的处理executor/ 执行器(访问的执行)foreign/ FDW(Foreign Data Wrapper)处理lib/ 共同函数libpq/ 前端/后端通信处理main/ postgres的主函数nodes/ 构文树节点相关的处理函数optimizer/ 优化器parser/ SQL构文解析器port/ 平台相关的代码postmaster/ postmaster的主函数 (常驻postgres)replication/ streaming replicationregex/ 正则处理rewrite/ 规则及视图相关的重写处理snowball/ 全文检索相关（语干处理）storage/ 共享内存、磁盘上的存储、缓存等全部一次/二次记录管理(以下的目录)buffer/(缓存管理)、 file/(文件)、freespace/(Fee Space Map管理) ipc/(进程间通信)、large_object /(大对象的访问函数)、 lmgr/(锁管理)、page/(页面访问相关函数)、 smgr/(存储管理器)tcop/ postgres (数据库引擎的进程)的主要部分tsearch/ 全文检索utils/ 各种模块(以下目录) adt/(嵌入的数据类型)、cache/(缓存管理)、 error/(错误处理)、fmgr/(函数管理)、hash/(hash函数)、 init/(数据库初始化、postgres的初期处理)、 mb/(多字节文字处理)、misc/(其他)、mmgr/(内存的管理函数)、 resowner/(查询处理中的数据(buffer pin及表锁)的管理)、 sort/(排序处理)、time/(事务的 MVCC 管理) backend等的代码的头文件包含在include里面。其组织虽然与backend的目录结构类似，但是并非完全相同，基本上来说下一级的子目录不再设下一级目录。例如backend的目录下面有utils这个目录，而util下面还有adt这个子目录，但是include里面省略了这个目录，变成了扁平的结构。1234567891011121314151617181920212223242526272829303132333435363738394041424344access/bootstrap/c.hcatalog/commands/dynloader.hexecutor/fmgr.hforeign/funcapi.hgetaddrinfo.hgetopt_long.hlib/libpq/mb/miscadmin.hnodes/optimizer/parser/pg_config.hpg_config.h.inpg_config.h.win32pg_config_manual.hpg_config_os.hpg_trace.hpgstat.hpgtime.hport/port.hportability/postgres.hpostgres_ext.hpostgres_fe.hpostmaster/regex/rewrite/rusagestub.hsnowball/stamp-hstorage/tcop/tsearch/utils/windowapi.h 代码的阅读方法用调试器追踪代码PostgreSQL那样的庞大系统，用眼睛来追踪源码并不容易。这里推荐用gdb这样的实际调试器来追踪代码的执行流程。可能有些人畏惧调试器，但是如果只是简单追踪代码的执行流的话，还是很简单的。 但是多少还是要做一些准备的，PostgreSQL在编译的时候一定要把调试开关打开。通常在编译的时候configure的时候加上–enable-debug的选项，然后可能的话可以编辑src/Makefile.global这个文件123CFLAGS = -O2 -Wall -Wmissing-prototypes -Wpointer-arith \\-Wdeclaration-after-statement -Wendif-labels -Wformat-security \\-fno-strict-aliasing -fwrapv 上面的行的”-O2”选项删除，然后加上”-g”123CFLAGS = -g -Wall -Wmissing-prototypes -Wpointer-arith \\-Wdeclaration-after-statement -Wendif-labels -Wformat-security \\-fno-strict-aliasing -fwrapv “-O2”是编译器的优化选项，如果打开了，代码的执行顺序会改变，使得追踪起代码来比较困难，所以要去除。当然这样的话，编译后的可执行文件会比较大，而且会比较慢，生产环境不太合适。大家需要理解这个操作仅仅是在学习的时候而设置的。 实际使用gdb试试下面实际使用gdb来看看比较简单点的select文。1select 1; select文执行后，至executor的其中一个函数ExecSelect停止，然后我们调查一下实际调用了那些函数。 首先以PostgreSQL的超级用户登录。我的环境是使用t-ishii这个用户安装PostgreSQL的，通常一般使用postgres这个用户，大家在阅读的时候替换一下即可。 然后，用psql和数据库进行连接，连接的状态可以通过ps命令调查。12$ ps x3714 ? Ss 0:00 postgres: t-ishii test [local] idle 可以看到上面的进程。这个就是后端的进程。这个是后端的进程，还有其他大量用户的PostgreSQL的连接也显示出来，比较难看清楚，所以还是准备好测试的环境来进行测试比较好。 启动gdb后，附加到ps里显示的进程号码。1234567891011121314151617181920212223242526272829$ gdb postgres 3714GNU gdb (GDB) 7.2Copyright (C) 2010 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type \"show copying\"and \"show warranty\" for details.This GDB was configured as \"x86_64-vine-linux\".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /usr/local/pgsql/bin/postgres...done.Attaching to program: /usr/local/pgsql/bin/postgres, process 3714Reading symbols from /lib64/libdl.so.2...done.Loaded symbols for /lib64/libdl.so.2Reading symbols from /lib64/libm.so.6...done.Loaded symbols for /lib64/libm.so.6Reading symbols from /lib64/libc.so.6...done.Loaded symbols for /lib64/libc.so.6Reading symbols from /lib64/ld-linux-x86-64.so.2...done.Loaded symbols for /lib64/ld-linux-x86-64.so.2Reading symbols from /lib64/libnss_files.so.2...done.Loaded symbols for /lib64/libnss_files.so.20x00007fad266f82e2 in __libc_recv (fd=&lt;value optimized out&gt;, buf=0xbe9900,n=8192, flags=&lt;value optimized out&gt;) at ../sysdeps/unix/sysv/linux/x86_64/recv.c:3030 ../sysdeps/unix/sysv/linux/x86_64/recv.c: in ../sysdeps/unix/sysv/linux/x86_64/recv.c(gdb) (gdb) 是gdb的命令行。在这个状态下，可以接受gdb的命令，如果输入b命令的话，在ExecResult可以设置断点。123(gdb) b ExecResultBreakpoint 1, ExecResult (node=0xd13eb0) at nodeResult.c:75(gdb) psql启动以后从终端执行select 1,输入以后，后端就会执行该命令。这个时候，postgres进程已经暂停，所以psql会动不了。要继续执行的话，可在gdb里执行”c”命令。执行了以后，就会在ExecResult 处停止。1234Continuing.Breakpoint 1, ExecResult (node=0xd13eb0) at nodeResult.c:7575 econtext = node-&gt;ps.ps_ExprContext;(gdb) 到ExecSelect为止的函数的调用路径可以用bt的命令显示出来。 1234567891011121314151617181920212223242526(gdb) bt#0 ExecResult (node=0xd13eb0) at nodeResult.c:75#1 0x00000000005b92a4 in ExecProcNode (node=0xd13eb0) at execProcnode.c:367#2 0x00000000005b71bb in ExecutePlan (estate=0xd13da0, planstate=0xd13eb0, operation=CMD_SELECT, sendTuples=1 '\\001', numberTuples=0, direction=ForwardScanDirection, dest=0xcf9938) at execMain.c:1439#3 0x00000000005b5835 in standard_ExecutorRun (queryDesc=0xc62820, direction=ForwardScanDirection, count=0) at execMain.c:313#4 0x00000000005b5729 in ExecutorRun (queryDesc=0xc62820, direction=ForwardScanDirection, count=0) at execMain.c:261#5 0x00000000006d2f79 in PortalRunSelect (portal=0xc60810, forward=1 '\\001', count=0, dest=0xcf9938) at pquery.c:943#6 0x00000000006d2c4e in PortalRun (portal=0xc60810, count=9223372036854775807, isTopLevel=1 '\\001', dest=0xcf9938, altdest=0xcf9938, completionTag=0x7fffa4b0eeb0 \"\") at pquery.c:787#7 0x00000000006cd135 in exec_simple_query (query_string=0xcf8420 \"select 1;\") at postgres.c:1018#8 0x00000000006d1144 in PostgresMain (argc=2, argv=0xc42da0, username=0xc42c40 \"t-ishii\") at postgres.c:3926#9 0x0000000000683ced in BackendRun (port=0xc65600) at postmaster.c:3600#10 0x00000000006833dc in BackendStartup (port=0xc65600) at postmaster.c:3285#11 0x0000000000680759 in ServerLoop () at postmaster.c:1454#12 0x000000000067ff4d in PostmasterMain (argc=3, argv=0xc40e00) at postmaster.c:1115#13 0x00000000005f7a39 in main (argc=3, argv=0xc40e00) at main.c:199(gdb) 说明一下看的方法，发起调用的函数在下面，被调用的函数在上面。也就是ExecProcNode调用了ExecResult，ExecutePlan调用了ExecProcNode，ExecutorRun调用了ExecProcNode，这样的形式来写。特别是中间的第7行。12#7 0x00000000006cd135 in exec_simple_query (query_string=0xcf8420 \"select 1;\") at postgres.c:1018 这样可以清楚看到在处理SELECT文。:)仔细看gdb的输出，可以发现这些细节。 gdb是源码调试器，所以可以看到和源代码的对应关系。例如list命令可以看到现在执行的行附近的代码。1234567891011(gdb) list70 TupleTableSlot *resultSlot;71 PlanState *outerPlan;72 ExprContext *econtext;73 ExprDoneCond isDone;74 75 econtext = node-&gt;ps.ps_ExprContext;76 77 /*78 * check constant qualifications like (2 &gt; 1), if not already done79 */ 利用up命令可以往上面的函数移动。下面用list命令，可以确认实际调用ExecSelect 的地方。1234567891011121314(gdb) up#1 0x00000000005b92a4 in ExecProcNode (node=0xd13eb0) at execProcnode.c:367367 result = ExecResult((ResultState *) node);(gdb) list362 &#123;363 /*364 * control nodes365 */366 case T_ResultState:367 result = ExecResult((ResultState *) node);368 break;369 370 case T_ModifyTableState:371 result = ExecModifyTable((ModifyTableState *) node); 利用down可以往下面的函数移动。利用up和down的组合，可以调查函数的调用关系。 要退出gdb的话可以用quit。1234(gdb) quitInferior 1 [process 3714] will be detached.Quit anyway? (y or n) yDetaching from program: /usr/local/pgsql/bin/postgres, process 3714 到了这里gdb就结束了，但是后端进程并不会终止。 使用tag来跳转到相应的函数定义文件我们已经使用了gdb来调查postgreSQL的运行，另外用gdb的list来追踪源码的话还是相当辛苦的，一般来说用emacs等编辑器一起调查和浏览代码，可以在边调试边查看代码。当然，在gdb模式下也可以使用。这个时候，例如如果想看看’exec_simple_query’的定义的话，使用emacs的tags命令可以立刻跳转到函数定义的地方。要使用tags的话，需要生产tags文件，PostgreSQL的话，带有生产tags文件的脚本。123$ cd /usr/local/src/postgresql-9.1.1/src$ tools/make_etags (使用emacs的场合)$ tools/make_tags (使用vi的场合) 这样就可以拉。 然后在emacs中，在exec_simple_query 处执行’ESC-.’(按了ESC键后输入逗号.)，即可打开光标所在文字所在的exec_simple_query函数的定义文件。 总结要完全理解PostgreSQL的话，通过调查源代码还是比较有效果的。要理解代码的话，可以按照目的自己追加必要的功能，改变一些功能的行为，大家可以最大限度的的享受开源带来的好处。这次为了让大家能够理解PostgreSQL的源代码，说明了PostgreSQL 9.1的全体结构，还有说明了代码树。然后还使用了调试器来追踪PostgreSQL的动作，接下来，看看PostgreSQL更详细的结构。下次将继续调查parser的结构。(2011年11月15日公開)","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：给日志库整库月分区表扩分区","slug":"20140325101317","date":"2014-03-25T02:13:17.000Z","updated":"2018-09-04T01:34:08.717Z","comments":true,"path":"20140325101317.html","link":"","permalink":"https://postgres.fun/20140325101317.html","excerpt":"","text":"日志库分区表快到期了，接下来需要做较费力的扩分区工作，日志表分区格式一般有两种，一种是按日分区，一种是按月分区，按日分区的函数比较好写，这里不再介绍; 这里主要介绍给日志库所有月分区表扩分区的方法: 月分区表信息1234567891011121314151617db_log=&gt; \\dt+ log_event*List of relations Schema | Name | Type | Owner | Size | Description---------------+--------------------------------+-------+---------------+------------+-------------db_log | log_event| table | db_log | 8192 bytes |db_log | log_event_201308 | table | db_log | 8192 bytes |db_log | log_event_201309 | table | db_log | 8192 bytes |db_log | log_event_201310 | table | db_log | 8192 bytes |db_log | log_event_201311 | table | db_log | 8192 bytes |db_log | log_event_201312 | table | db_log | 8192 bytes |db_log | log_event_201401 | table | db_log | 8192 bytes |db_log | log_event_201402 | table | db_log | 8192 bytes |db_log | log_event_201403 | table | db_log | 8192 bytes |db_log | log_event_201404 | table | db_log | 8192 bytes |db_log | log_event_201405 | table | db_log | 8192 bytes |db_log | log_event_201406 | table | db_log | 8192 bytes |db_log | log_event_201407 | table | db_log | 8192 bytes | 备注：这里的分区表通过子表继承父表实现，并且不通过 trigger 形式实现，再来看一个 SQL。 查询库中所有分区表查询库中所有分区表以及子表个数，如下： 1234567891011121314SELECTnspname ,relname , COUNT(*) AS partition_numFROMpg_class c ,pg_namespace n , pg_inherits iWHERE c.oid = i.inhparent AND c.relnamespace = n.oid AND c.relhassubclass AND c.relkind = 'r'GROUP BY 1,2 ORDER BY partition_num DESC; 在一个日志库上执行述查询，结果如下 其中 partition_num 表示分区表个数，红色框框起来的表示是日分区表，分区数在 30 左右的为月分区表。这里仅截了一部分表，这个库的月分区表在 90 张左右，现在的目标是批量给这些月分区表扩分区。 整库月表扩分区函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061create or replace function fun_create_child_table_month(begin_date date, add_partition_num int4 ) returns integer as $$declare v_des_date date; v_year char(4); --目标分区表年份v_month char(2); --目标分区表月份 i_cnt int4; table_rec record; --分区表父表结果集游标 date_rec record; --日期后辍结果集游标 v_partbl_name character varying(64); --父表名称 v_childtbl_name character varying(64); --子表名称 v_date_key character varying(32); --日期后辍begin--此函数功能：批量给整库月分区表扩分区，注意以下几点：--1 分区表子表格式: tablename_yyyymm--2 此函数仅支持月分区表，不支持列表分区，复合分区，取模分区等模式--3 此函数所涉及的分区表不支付触发器形式的分区表，此函数适合在日志库中使用--4 调用函数: select fun_create_child_table_month('2014-08-01',24); 表示从 2014-08-01 开始扩分区，给当前库所有月分区表扩 24 个分区--5 由于新增子表的权限模块处理较复杂， 因此此函数仅新增分区表，不涉及权限处理。--创建临时表并插入数据: 此表存储日期后辍标识create temporary table if not exists tbl_month_suffix( id serial,date_key character varying (32));FOR i_cnt IN 1 .. add_partition_num LOOPi_cnt := i_cnt-1;v_des_date := begin_date + i_cnt* interval '1 month';v_year := split_part(split_part(v_des_date::text,' ',1),'-',1);v_month :=split_part(split_part(v_des_date::text,' ',1),'-',2);--RAISE NOTICE 'date: %,%', v_year,v_month;execute 'insert into tbl_month_suffix (date_key) values (' || v_year || v_month || ')';END LOOP;--创建分区表--以下列出可能是月分区表，也有可能是日分区表，筛选条件 &lt; 100，比较粗暴，建议根据自己的环境调整此 SQLFOR table_rec IN (SELECTnspname ,relname , COUNT(*) AS partition_num FROMpg_class c ,pg_namespace n , pg_inherits i WHERE c.oid = i.inhparent AND c.relnamespace = n.oid AND c.relhassubclass AND c.relkind = 'r'GROUP BY 1,2 having count(*) &lt; 100 ORDER BY partition_num DESC) LOOPv_partbl_name := table_rec.relname;FOR date_rec in select date_key from tbl_month_suffix order by date_key LOOPv_date_key := date_rec.date_key;v_childtbl_name := v_partbl_name || '_' || v_date_key; execute 'create table if not exists ' || v_childtbl_name || ' ( like ' || v_partbl_name || ' including all ) inherits (' || v_partbl_name || ')' ; execute 'grant select on ' || v_childtbl_name || ' to dwetl'; END LOOP; END LOOP;RAISE NOTICE '分区表创建完成!';return 1;end;$$ language 'plpgsql'; 执行函数: 扩二年分区给数据库中所有月分区表扩两年分区，执行函数： 1db_log=&gt; select fun_create_child_table_month('2014-08-01',24); 备注：执行完后，查看下分区，看看是不是所有月分区表分区已从 201408 扩到 201607，扩完之后，接下来需要手工检查下新增表的权限，比如可能开通了查询帐号，这时权限需要手工处理，有兴趣的同学，也可以将权限模块加到函数中，做得更自动点。 总结 此扩分区脚本需要完善的地方很多，使用时建议根据实际情况修改。 参考 PostgreSQL：分区表的相关查询","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"RHEL6：Rsync 服务配置","slug":"20140322142510","date":"2014-03-22T06:25:14.000Z","updated":"2018-09-04T01:34:08.654Z","comments":true,"path":"20140322142510.html","link":"","permalink":"https://postgres.fun/20140322142510.html","excerpt":"","text":"今天简单的了解了 rsync 工具的使用，rsync 是一个Unix系统下的文件同步和传输工具，具有以下功能能 更新整个目录和树和文件系统； 有选择性的保持符号链链、硬链接、文件属于、权限、设备以及时间等； 对于安装来说，无任何特殊权限要求； 对于多个文件来说，内部流水线减少文件等待的延时； 能用rsh、ssh 或直接端口做为传输入端口； 支持匿名rsync 同步文件，是理想的镜像工具； 可以增量同步数据，文件传输效率高; 备注：今天仅做个简单的实验，将本地文件同步到远程，做个笔记备忘。 环境信息rsyncd 服务端 192.168.2.37客户端： 192.168.2.36 Rsyncd 服务端配置2.1 创建目录1[root@db1 etc]# mkdir -p /etc/rsyncd/ 2.2 创建配置文件12345678910111213141516[root@db1 etc]# touch /etc/rsyncd/rsyncd.conf--/etc/rsyncd/rsyncd.confport = 873uid = postgresgid = postgresuse chroot = yesmax connections = 100#syslog facility = local5read only = falsepid file = /var/run/rsyncd.pidlog file = /var/log/rsyncd.log[backup]path = /database/backupcomment = backup fileshosts allow = 192.168.2.36hosts deny = * 主要参数解释如下： - max connections 表示允许并行的最大客户端连接数 - read only = false 表示允许客户端上传文件到 rsyncd 服务端; - [backup] 表示模块别名，sync 传文件时需指定; - hosts allow 表示允许的客户端 IP 列表; - hosts deny 表示拒绝的客户端 IP 列表; 更多参数解释请参考手册 http://rsync.samba.org/ftp/rsync/rsyncd.conf.html 2.3 启动 rsyncd 服务1[root@db1 backup]# /usr/bin/rsync --daemon --config=/etc/rsyncd/rsyncd.conf 2.4 打开 rsyncd 服务器防火墙 如果打开了防火墙，需要开通 873 tcp 端口，此步略。 客户端测试测试将客户端 $PGDATA/pg_log 目录上传到 rsyncd 服务端。1[pg93@redhatB pg_root]$ rsync -acvz pg_log 192.168.2.37::backup 备注: 同步本地 pg_log 目录到 rsyncd 服务器, 后面接模块名 backup，对应的是 /database/backup 目录。 - -a, –archive 归档模式，表示以递归方式传输文件，并保持所有文件属性，等于-rlptgoD - -c, –checksum 打开校验开关，强制对文件传输进行校验 - -v, –verbose 详细模式输出 - -z, –compress 对备份的文件在传输时进行压缩处理 - -r, –recursive 对子目录以递归模式处理 附: Rsync 语法 NAME rsync — a fast, versatile, remote (and local) file-copying toolSYNOPSIS Local: rsync [OPTION…] SRC… [DEST] Access via remote shell: Pull: rsync [OPTION…] [USER@]HOST:SRC… [DEST] Push: rsync [OPTION…] SRC… [USER@]HOST:DEST Access via rsync daemon: Pull: rsync [OPTION…] [USER@]HOST::SRC… [DEST] rsync [OPTION…] rsync://[USER@]HOST[:PORT]/SRC… [DEST] Push: rsync [OPTION…] SRC… [USER@]HOST::DEST rsync [OPTION…] SRC… rsync://[USER@]HOST[:PORT]/DEST Usages with just one SRC arg and no DEST arg will list the source files instead of copying. 参考Linux下架设rsync服务器rsyncd.confrsync","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"PostgreSQL 数据库健康检查脚本","slug":"20140306151011","date":"2014-03-06T07:10:11.000Z","updated":"2018-09-04T01:34:08.107Z","comments":true,"path":"20140306151011.html","link":"","permalink":"https://postgres.fun/20140306151011.html","excerpt":"","text":"德哥最近设计了 &lt;数据库健康检查报告模板&gt; ，非常帅，根据德哥的模板，编写了相应的数据库健康检查脚本，欢迎下载试用，如发现 bug 或者遇到问题，联系下面邮箱: Email: francs3@163.com Introduction此脚本收集以下信息: 基本信息，如操作系统内核版本，数据库版本; 数据库控制文件信息; 数据库配置信息, 如 postgresql.conf , pg_hba.conf ,recovery.conf 的非默认配置参数; 数据库 csv 错误日志分析统计; 数据库基本信息, 包括数据库大小，字符编码，表空间信息，用户/角色信息; 数据库性能 top 10 SQL(按多维度统计); 数据库运行状态检查(连接数，数据库年龄，索引超过4的表，膨胀检查，垃圾数据) 分区表检查; 数据库回滚比例，长事务检查; Requirements 此程序需要用到pg_stat_statements组件 程序需要使用数据库环境变量如 $PGDATA, $PGPOT 等; Supported Platforms 操作系统: 目前仅支持 Linux 平台 数据库版本: 目前仅支持 9.0, 9.1, 9.2, 9.3, 暂不支持其它版本; README 此程序目前版本仅能在数据库主机上执行; 此程序需要数据库超级权限; 此程序执行过程中需要占用一定资源，建议在数据库空闲时段执行。 Usage./pg_healthcheck.sh备注：执行成功后生成报告文件 pg_healthcheck.report，格式为文本，附件可在下面下载。 Download 百度云： http://pan.baidu.com/s/1mggBGRQ github: https://github.com/francs/PostgreSQL-healthcheck-script","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"新书推荐：《数据库查询优化器的艺术》 (海翔老师著)  ","slug":"20140228172231","date":"2014-02-28T09:22:31.000Z","updated":"2018-09-04T01:34:08.044Z","comments":true,"path":"20140228172231.html","link":"","permalink":"https://postgres.fun/20140228172231.html","excerpt":"","text":"相比其它数据库类的书， PostgreSQL 的书真是少得可怜，在 《PostgreSQL 数据库内核分析》一书出版之后，国内新增一本数据库查询优化器方面的书籍 ，此书深层次介绍 MySQL 和 PostgreSQL 优化器，并解析两者的异同， 确是一本好书，值得一读，本人已购买，仍在学习中。 当当http://product.dangdang.com/23399773.html#catalog","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Book","slug":"PostgreSQL-Book","permalink":"https://postgres.fun/tags/PostgreSQL-Book/"}]},{"title":"PostgreSQL：\\copy 元子命令与 copy 命令的区别 ","slug":"20140221160000","date":"2014-02-21T08:00:00.000Z","updated":"2018-09-04T01:34:07.998Z","comments":true,"path":"20140221160000.html","link":"","permalink":"https://postgres.fun/20140221160000.html","excerpt":"","text":"我想用过 PostgreSQL 的朋友大多知道 copy 命令，使用 copy 命令导入，导出数据很高效，但用过 \\copy 元子命令的朋友可能不多，今天简单介绍下两者的区别和使用场景。 copy 命令属于 SQL 命令，执行时仅会在数据库服务端查找文件，如果需要将文件数据导入到表或导出表数据到文件时须以超级用户执行，而 \\copy 为元子命令，任何 psql 客户端登录数据库成功后都可以从客户端导入导出数据。 关于 copy 命令 NameCOPY – copy data between a file and a tableSynopsisCOPY table_name [ ( column_name [, …] ) ] FROM { ‘filename’ | PROGRAM ‘command’ | STDIN } [ [ WITH ] ( option [, …] ) ]COPY { table_name [ ( column_name [, …] ) ] | ( query ) } TO { ‘filename’ | PROGRAM ‘command’ | STDOUT } [ [ WITH ] ( option [, …] ) ]where option can be one of: FORMAT format_name OIDS [ boolean ] FREEZE [ boolean ] DELIMITER ‘delimiter_character’ NULL ‘null_string’ HEADER [ boolean ] QUOTE ‘quote_character’ ESCAPE ‘escape_character’ FORCE_QUOTE { ( column_name [, …] ) | * } FORCE_NOT_NULL ( column_name [, …] ) ENCODING ‘encoding_name’ 备注：上面是 copy 命令的用法，供查阅，这里不详细介绍，接下来测试下。 copy 和 \\copy 测试场景数据库: 192.168.2.36/1925 库名：francs客户端: ubuntu 笔记本桌面目标： 将ubuntu 本地文件导入数据库中的表 test_copy 2.1 服务端创建测试表12345[pg93@redhatB ~]$ psql francs francspsql (9.3.0)Type \"help\" for help.francs=&gt; create table test_copy(id int4,name character varying(32));CREATE TABLE 2.2 客户端创建数据文件1234francs@francs:~/script/tf$ cat test_copy.txt 1 a2 b3 c 2.3 在客户端将数据文件导入表 test_copy 中123456789101112131415161718francs@francs:~/script/tf$ psql -h 192.168.2.36 -p 1925 francs francs用户 francs 的口令：psql (9.3.2, 服务器 9.3.0)输入 \"help\" 来获取帮助信息.francs=&gt; \\! cat '/home/francs/script/tf/test_copy.txt';1 a2 b3 cfrancs=&gt; \\copy francs.test_copy from '/home/francs/script/tf/test_copy.txt';francs=&gt; select * from test_copy; id | name ----+------ 1 | a 2 | b 3 | c(3 行记录) 备注：导入成功，注意这里用的是 \\copy 元子命令，客户端使用 copy 命令会报文件找不到，如下： 2.4 报错12francs=# copy francs.test_copy from '/home/francs/script/tf/test_copy.txt';ERROR: could not open file \"/home/francs/script/tf/test_copy.txt\" for reading: No such file or directory 备注：由于 copy 命令会到服务端找文件，所以找不到。 2.5 测试导出文件1234567francs=&gt; copy francs.test_copy to '/home/francs/script/tf/test_copy2.txt';ERROR: must be superuser to COPY to or from a file提示: Anyone can COPY to stdout or from stdin. psql s \\copy command also works for anyone.francs=&gt; \\copy francs.test_copy to '/home/francs/script/tf/test_copy2.txt';francs-&gt; \\! lslib_bak test_copy2.txt test_copy.txt vpn1.sh 备注: 当需要导出表数据到文件时， copy 命令需要以超级用户执行。而 \\copy 元子命令不需要。 总结由于 copy 命令始终是到数据库服务端找文件，当以文件形式导入导出数据时需以超级用户执行，权限要求很高，适合数据库管理员操作;而 \\copy 命令可在客户端执行导入客户端的数据文件，权限要求没那么高，适合开发人员，测试人员使用，因为生产库的权限掌握在 DBA 手中。 参考 psql copy","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：通过 .psqlrc 定制监控脚本 ","slug":"20140221115413","date":"2014-02-21T03:54:13.000Z","updated":"2018-09-04T01:34:07.935Z","comments":true,"path":"20140221115413.html","link":"","permalink":"https://postgres.fun/20140221115413.html","excerpt":"","text":"在数据库维护过程中，定制监控脚本是很平常的工作，以提高工作效率，例如，下面这个 sql 是用来查询数据库当前的非空闲会话。 监控活动会话SQL12345678910111213141516SELECT pid , datname , usename , client_addr , query , query_start , state , waitingFROM pg_stat_activityWHERE query ! ~ '&lt;IDLE' AND pid &lt;&gt; pg_backend_pid() AND state = 'active'ORDER BY query; 备注：SQL 不算长，需要用时手工编写也没问题，当然也有更快捷的方法，可以将此 SQL 写入 .psqlrc 文件， 首先简单介绍下这个文件。 关于 .psqlrc 文件.psqlrc 文件用来定制 psql 客户端特性的配置文件，定制后对当前客户端全局生效，在 linux 环境下位于 ~/.psqlrc， windows 平台位于 %APPDATA%\\postgresql\\psqlrc.conf，下面是 .psqlrc 的一个简单例子。 .psqlrc 文件举例1234[pg93@redhatB ~]$ cat .psqlrc \\set active_session ' select pid, datname,usename,client_addr,query,query_start,state,waiting from pg_stat_activity where query !~''&lt;IDLE'' and pid &lt;&gt; pg_backend_pid() and state=''active'' order by query;'\\set wait_session ' select pid, datname,usename,client_addr,query,query_start,state,waiting from pg_stat_activity where waiting and query !~''&lt;IDLE'' and pid &lt;&gt; pg_backend_pid() order by query;' 备注：这里编写了两条SQL，第一条即为本文之前的查看当前会话的 SQL ，第二条 SQL 为查询处于等待状态的 SQL， 注意每条 SQL 为一行，接着测试。 测试一: 测试 active_session本次实验的 PostgreSQL 版本为 9.3.0。3.1 session112[pg93@redhatB tf]$ psql -c \"select pg_sleep(60);\" &amp;[1] 7578 备注：后台开启 sessoin1。 3.2 session212345678[pg93@redhatB tf]$ psql francs postgrespsql (9.3.0)Type \"help\" for help.francs=# :active_session pid | datname | usename | client_addr | query | query_start | state | waiting ------+----------+----------+-------------+----------------------+-------------------------------+--------+--------- 7579 | postgres | postgres | | select pg_sleep(60); | 2014-02-21 11:25:12.993091+08 | active | f(1 row) 备注：另开启 session2 用来监控，实验环境为虚拟机，非常空闲，所以之前开了个 pg_sleep(60)进程，调用方法很简单，执行 :active_session 即可，冒号用来解析变量，同时支持 TAB 补全功能，感觉非常帅。 测试二: 测试 wait_session4.1 session11234567891011[pg93@redhatB tf]$ psql francs francspsql (9.3.0)Type \"help\" for help.francs=&gt; create table test_wait(id int4);CREATE TABLEfrancs=&gt; begin;BEGINfrancs=&gt; drop table test_wait;DROP TABLE.. 注意事务未提交 备注： session1 创建一张测试表，并在事务中删除这张表，注意事务未结束。 4.2 session2另开一会话，准备往表 test_wait 中写一条数据，但 insert 操作处于等待状态。 12345[pg93@redhatB ~]$ psql francs francspsql (9.3.0)Type \"help\" for help.francs=&gt; insert into test_wait values (1);.. 这里处于等待状态。 4.3 session3再开一会话，用来监控。123456789[pg93@redhatB ~]$ psql francs postgrespsql (9.3.0)Type \"help\" for help.francs=# :wait_session pid | datname | usename | client_addr | query | query_start | state | waiting ------+---------+---------+-------------+-----------------------------------+-------------------------------+--------+--------- 7692 | francs | francs | | insert into test_wait values (1); | 2014-02-21 11:28:35.914326+08 | active | t(1 row) 备注： waiting = t 表示此会话处于等待状态，测试成功。 总结这里仅列出两个简单的监控 SQL ，有兴趣的朋友可以根据自己的工作环境定制 SQL ，提高工作效率。 参考 PostgreSQL: 关于 psql 的 Prompting 设置 PostgreSQL：如何传递参数到 SQL 脚本? PostgreSQL: psql 命令 ON_ERROR_STOP 选项讲解 psql pg_stat_activity","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"psql","slug":"psql","permalink":"https://postgres.fun/tags/psql/"}]},{"title":"Ubuntu：3D无法启用(左侧栏图标无法调小)","slug":"20140220105616","date":"2014-02-20T02:56:16.000Z","updated":"2018-09-04T01:34:07.857Z","comments":true,"path":"20140220105616.html","link":"","permalink":"https://postgres.fun/20140220105616.html","excerpt":"","text":"昨天开启家里的 Ubuntu 系统后，发现桌面左侧栏图标很大，而且 “更改桌面” –&gt; “外观” 里居然没有 “启动器图标大小”调整栏， 如下图所示; 网上找了不少资料也没弄好，后来咨询同事，删除 fglrx 驱动即可，命令1sudo apt-get remove fglrx 备注：删除完后，重启系统即可，具体参考同事blog: ubuntu unity 3D无法启用","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"PostgreSQL：深入理解 Template1 和 Template0  ","slug":"20140219140045","date":"2014-02-19T06:00:45.000Z","updated":"2018-09-04T01:34:07.794Z","comments":true,"path":"20140219140045.html","link":"","permalink":"https://postgres.fun/20140219140045.html","excerpt":"","text":"了解 PostgreSQL 的人肯定听过 template1 和 template0，这两个作为模板库,在建库的时候会用到，但这两者是有很大差别的，曾经一段时间对这两个模板库的使用比较模糊，今天再次查看了文档，决定通过实验记录下来。 默认数据库模板1.1 默认模板库为 template112postgres=# create database db1;CREATE DATABASE 备注：建库时如果不指定 TEMPLATE 属性，默认用的是 template1 模板库. 1.2 手工指定模板库12postgres=# create database db2 template template0;CREATE DATABASE 备注：也可以指定模板库为 template0 Template1 和 Template0 的区别？数据库初始化之后, 就有了 template0, template1 库，开始时这两个库的内容是一样的，但这两个库有啥异同呢？ 2.1 template1 可以连接并创建对象，template0 不可以连接12345678postgres=# \\c template1You are now connected to database \"template1\" as user \"postgres\".template1=# create table tmp_1( id int4);CREATE TABLEtemplate1=# \\c template0FATAL: database \"template0\" is not currently accepting connectionsPrevious connection kept 备注：当然可以通过其它方法连接 template0 库，有兴趣的同学自己研究下，这里不演示了。正因为 template1 可以创建对像，相比 template0 ，被称为非干净数据库，而 template0 被称为干净的数据库。 2.2 使用 template1 模板库建库时不可指定新的 encoding 和 locale，而 template0 可以123456template1=# create database db3 TEMPLATE template0 ENCODING 'SQL_ASCII' ;CREATE DATABASEtemplate1=# create database db4 TEMPLATE template1 ENCODING 'SQL_ASCII' ;ERROR: new encoding (SQL_ASCII) is incompatible with the encoding of the template database (UTF8)HINT: Use the same encoding as in the template database, or use template0 as template. Template0、Template1 库都不可删除12345postgres=# drop database template0;ERROR: cannot drop a template databasepostgres=# drop database template1;ERROR: cannot drop a template database 备注：当然有方法删除 template1 库，而且这个操作并不危险，需要修改系统表，这里不演示了。 克隆数据库之前简单介绍了 template0 和 template1 的异同，有必要介绍通过模板库复制库的操作，例如这里已经有个 francs 库了，现在想复制一个 francs1 库，内容和 francs 库一样。 4.1 复制库123456789101112131415161718postgres=# \\c francs You are now connected to database \"francs\" as user \"postgres\".francs=# select count(*) from pg_stat_user_tables ; count ------- 41(1 row)postgres=# create database francs1 TEMPLATE francs ;CREATE DATABASEpostgres=# \\c francs1 francsYou are now connected to database \"francs1\" as user \"francs\".francs1=&gt; select count(*) from pg_stat_user_tables ; count ------- 41(1 row) 备注：这种方法在复制数据库时提供了方便, 也可以定制自己的数据库模板， 但是这么操作有个前提， 复制时源库不可以连接， 复制过程中也不允许连接源库, 否则会报以下错误：4.2 错误代码12ERROR: source database \"francs\" is being accessed by other usersDETAIL: There is 1 other session using the database. 参考 Template Databases CREATE DATABASE","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Ubuntu：Thunderbird 设置邮件的默认编码","slug":"20140218151834","date":"2014-02-18T07:18:34.000Z","updated":"2018-09-04T01:34:07.732Z","comments":true,"path":"20140218151834.html","link":"","permalink":"https://postgres.fun/20140218151834.html","excerpt":"","text":"Thunderbird 可以手工更改当前邮件编码，但这样只能更改当前邮件编码，当然有地方设置“接收邮件” 和 “发送邮件”的字符编码，这里记录下，因为老是忘记。 查看并设置单封邮件编码打开 THunderbird ，在最上面，点击”查看” –&gt; “字符编码” ，这时显示的编码为邮件编码。 设置“接收邮件” 和 “发送邮件”的字符编码点”编辑” –&gt; “首选项” –&gt; “显示” –&gt; “字体”，如下图： 之后尝试设置“接收邮件” 和 “发送邮件”的字符编码为 GB2312，如下图： 测试为了验证上面设置是否生效，新建一封邮件并查看其编码，”新建消息” –&gt; “选项” –&gt; “字符编码”，如下图：这时显示的邮件编码为 GB2312， 设置生效。","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：死机时不需要 Rebooting 的方法","slug":"20140217152435","date":"2014-02-17T07:24:35.000Z","updated":"2018-09-04T01:34:07.669Z","comments":true,"path":"20140217152435.html","link":"","permalink":"https://postgres.fun/20140217152435.html","excerpt":"","text":"使用 Ubuntu 桌面以来已经第二次中招死机了，桌面卡死了啥都动不了，通过 Ctrl+Alt+F1 可以进入命令端，杀掉一些用户进程后，通过 Crtl+Alt+F7 再次回到图形化界面时，依然是死机，今天咨询了同事，找到命令端重启桌面(注销)的方法： 方法一：注销命令首先，Ctrl+Alt+F1 进入命令端，执行以下： sudo restart lightdm 方法二：设置快捷键“系统设置” –&gt; “键盘” –&gt; “布局设置” –&gt; “选项”，在“杀死 X 服务器的按键序列” 下打勾，如图： 参考 Restart X Server Ubuntu 12.04 Without Rebooting","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"PostgreSQL：分区表的相关查询","slug":"20140217112702","date":"2014-02-17T03:27:02.000Z","updated":"2018-09-04T01:34:07.622Z","comments":true,"path":"20140217112702.html","link":"","permalink":"https://postgres.fun/20140217112702.html","excerpt":"","text":"大家知道 PostgreSQL 的分区是通过继承来实现的，按分区方式，可以实现表的列表分区，范围分区，以及复合分区等, 这里不打算详细介绍分区表的使用，本篇 blog 仅介绍关于分区表的几个查询，方便维护和管理分区表。 查询指定分区表信息12345678910111213SELECT nmsp_parent.nspname AS parent_schema , parent.relname AS parent , nmsp_child.nspname AS child , child.relname AS child_schemaFROM pg_inherits JOIN pg_class parent ON pg_inherits.inhparent = parent.oid JOIN pg_class child ON pg_inherits.inhrelid = child.oid JOIN pg_namespace nmsp_parent ON nmsp_parent.oid = parent.relnamespace JOIN pg_namespace nmsp_child ON nmsp_child.oid = child.relnamespaceWHERE parent.relname = 'table_name'; 查询库中所有分区表子表个数1234567891011121314SELECT nspname , relname , COUNT(*) AS partition_numFROM pg_class c , pg_namespace n , pg_inherits iWHERE c.oid = i.inhparent AND c.relnamespace = n.oid AND c.relhassubclass AND c.relkind = 'r'GROUP BY 1,2 ORDER BY partition_num DESC; 备注：如果表是分区表，那么相应的 pg_class.relhassubclass 字段为 ‘t’，否则为 ‘f’，下面是我在生产库查询的例子。 备注：第一张表分区表子表个数为 940，第二张表分区表子表个数为 842，这两张表为按日分区，后面的表为按月分区，按月分区的表个数有差异，因为有些表快到期了，需要扩分区，有些分区表数据有清理策略，维护时删除了一些子表。 参考 设置 Constraint_exclusion 避免扫描 PostgreSQL 分区表所有分区 PostgreSQL: 分区表应用二(取模分区) pg_class pg_inherits","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"《PostgreSQL 9 Admin Cookbook》 中文电子版发布","slug":"20140114135421","date":"2014-01-14T05:54:21.000Z","updated":"2018-09-04T01:34:07.560Z","comments":true,"path":"20140114135421.html","link":"","permalink":"https://postgres.fun/20140114135421.html","excerpt":"","text":"作者：Simon.Riggs， Hannu Krosing译者：黄坚(网名：洞庭湖的泥鳅)， 谭峰(网名: francs) 历时两年， 终于和黄坚 (网名：洞庭湖的泥鳅) 将 PostgreSQL 9 Admin Cookbook 翻译完成。 2ndquadrant 购买地址http://www.2ndquadrant.com/en/books/postgresql-9-cookbook-chinese-edition/ 翻译很花时间，占用了大量的业余时间，在这里首先得感谢家人的支持，再次得感谢泥鳅，在翻译过程中，他给予了我很大帮助，才能最终坚持下来。 这本书适合对 PostgreSQL 有一定了解的人，如果您对 PostgreSQL 有一定了解，会上手很快，当然，如果英文 ok ，那建议还是读英文原版吧，毕竟原版的内容更直接，如果英文不是很好而且想快速学习此书的内容，那此书很适合上手 PostgreSQL，虽然 Admin Cookbook 是以 PostgreSQL 9.0 基础上编写，翻译完成后最新的 PostgreSQL 版本为 9.3 ， 但书的内容非常经典，依然不影响它的价值。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL Book","slug":"PostgreSQL-Book","permalink":"https://postgres.fun/tags/PostgreSQL-Book/"}]},{"title":"2012: 杭州 PostgreSQL 交流小聚","slug":"20140103112545","date":"2014-01-03T03:25:45.000Z","updated":"2018-09-04T01:34:07.497Z","comments":true,"path":"20140103112545.html","link":"","permalink":"https://postgres.fun/20140103112545.html","excerpt":"","text":"这里补上 2012年 11 月份，杭州 PostgreSQL 小聚的 PPT。 ppt 下载地址 [德哥] postgresql 负载均衡.pdf [德哥] postgresql ha.pdf [德哥] postgresql plproxy原理与实践.pdf [德哥] postgresql 备份.pdf [德哥] postgresql 管理和调优案例.pdf [德哥] postgresql 容灾.pdf [德哥] postgresql 9.3 or future ver upcoming features.pdf [陈立群] usage of window function in postgres.pdf [谭峰] converting mysql to postgresql.pdf 下载地址http://bbs.pgsqldb.com:8079/client/post_show.php?zt_auto_bh=56749 我分享的议题[谭峰] converting mysql to postgresql.pdfhttp://yun.baidu.com/s/1gdAC8rp","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL：关于二进制( Fc ) 格式备份文件","slug":"20131226162154","date":"2013-12-26T08:21:54.000Z","updated":"2018-09-04T01:34:07.435Z","comments":true,"path":"20131226162154.html","link":"","permalink":"https://postgres.fun/20131226162154.html","excerpt":"","text":"在某些时候需要恢复数据，对于压缩(pg_dump -Fc )的备份文件，如果不确定里面的内容可以通过以下方法确认。 查看备份文件信息和列表这里有个 francs.dmp 备份文件，如果查看这个备份文件的信息和内容，可以这么做。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192[pg93@redhatB tf]$ pg_restore -l francs.dmp;; Archive created at Thu Dec 26 15:48:43 2013; dbname: francs; TOC Entries: 178; Compression: -1; Dump Version: 1.12-0; Format: CUSTOM; Integer: 4 bytes; Offset: 8 bytes; Dumped from database version: 9.3.0; Dumped by pg_dump version: 9.3.0;;; Selected TOC Entries:;3074; 1262 16390 DATABASE - francs postgres6; 2615 16395 SCHEMA - fracns francs8; 2615 16391 SCHEMA - francs francs3075; 0 0 ACL - francs francs7; 2615 2200 SCHEMA - public postgres3076; 0 0 COMMENT - SCHEMA public postgres3077; 0 0 ACL - public postgres701; 2612 16399 PROCEDURAL LANGUAGE - plperlu postgres216; 3079 12616 EXTENSION - plpgsql3078; 0 0 COMMENT - EXTENSION plpgsql217; 3079 16400 EXTENSION - postgres_fdw3079; 0 0 COMMENT - EXTENSION postgres_fdw218; 1255 16705 FUNCTION francs dummy() francs233; 1255 16404 FUNCTION francs func_create_daily_table(character varying, integer) francs234; 1255 16405 FUNCTION francs multiply(integer, integer) francs235; 1255 16406 FUNCTION francs my_func() francs236; 1255 16407 FUNCTION public fun_log_drop_command() postgres1672; 1417 16408 SERVER - srv_source_db postgres3080; 0 0 ACL - srv_source_db postgres3081; 0 0 USER MAPPING - USER MAPPING francs SERVER srv_source_db postgres215; 1259 24860 TABLE francs New_Table francs3082; 0 0 ACL francs New_Table francs172; 1259 16410 TABLE francs foo francs3083; 0 0 ACL francs foo francs173; 1259 16413 FOREIGN TABLE francs ft_test_1 francs3084; 0 0 ACL francs ft_test_1 francs174; 1259 16416 TABLE francs test_1 francs3085; 0 0 ACL francs test_1 francs175; 1259 16419 TABLE francs test_1_20130725 francs3086; 0 0 ACL francs test_1_20130725 francs176; 1259 16422 TABLE francs test_1_20130726 francs3087; 0 0 ACL francs test_1_20130726 francs177; 1259 16425 TABLE francs test_2 francs3088; 0 0 ACL francs test_2 francs178; 1259 16428 TABLE francs test_3 francs3089; 0 0 ACL francs test_3 francs179; 1259 16431 TABLE francs test_4 francs3090; 0 0 ACL francs test_4 francs214; 1259 16706 TABLE francs test_5 francs3091; 0 0 ACL francs test_5 francs180; 1259 16437 TABLE francs test_6 francs3092; 0 0 ACL francs test_6 francs213; 1259 16668 TABLE francs test_archive francs3093; 0 0 ACL francs test_archive francs181; 1259 16441 TABLE francs test_array francs3094; 0 0 ACL francs test_array francs212; 1259 16665 TABLE francs test_big francs3095; 0 0 ACL francs test_big francs182; 1259 16447 TABLE francs test_cache francs3096; 0 0 ACL francs test_cache francs183; 1259 16453 TABLE francs test_con francs3097; 0 0 ACL francs test_con francs184; 1259 16457 TABLE francs test_full francs3098; 0 0 ACL francs test_full francs185; 1259 16460 TABLE francs test_full_20130625 francs3099; 0 0 ACL francs test_full_20130625 francs186; 1259 16463 TABLE francs test_full_20130626 francs3100; 0 0 ACL francs test_full_20130626 francs187; 1259 16466 TABLE francs test_full_20130627 francs3101; 0 0 ACL francs test_full_20130627 francs188; 1259 16469 TABLE francs test_full_20130628 francs3102; 0 0 ACL francs test_full_20130628 francs189; 1259 16472 TABLE francs test_full_20130629 francs3103; 0 0 ACL francs test_full_20130629 francs190; 1259 16475 TABLE francs test_full_20130630 francs3104; 0 0 ACL francs test_full_20130630 francs191; 1259 16478 TABLE francs test_full_20130701 francs3105; 0 0 ACL francs test_full_20130701 francs192; 1259 16481 TABLE francs test_full_20130702 francs3106; 0 0 ACL francs test_full_20130702 francs193; 1259 16484 TABLE francs test_full_20130703 francs3107; 0 0 ACL francs test_full_20130703 francs194; 1259 16487 TABLE francs test_full_20130704 francs3108; 0 0 ACL francs test_full_20130704 francs195; 1259 16490 TABLE francs test_full_20130705 francs3109; 0 0 ACL francs test_full_20130705 francs196; 1259 16496 TABLE francs test_json1 francs3110; 0 0 ACL francs test_json1 francs197; 1259 16503 SEQUENCE francs test_json1_id_seq francs3111; 0 0 SEQUENCE OWNED BY francs test_json1_id_seq francs198; 1259 16505 TABLE francs test_lock francs3112; 0 0 ACL francs test_lock francs199; 1259 16514 TABLE francs test_notice francs3113; 0 0 ACL francs test_notice francs200; 1259 16517 TABLE francs test_notice2 francs3114; 0 0 ACL francs test_notice2 francs201; 1259 16520 SEQUENCE francs test_notice2_id_seq francs3115; 0 0 SEQUENCE OWNED BY francs test_notice2_id_seq francs202; 1259 16522 SEQUENCE francs test_notice_id_seq francs3116; 0 0 SEQUENCE OWNED BY francs test_notice_id_seq francs211; 1259 16662 TABLE francs test_num francs3117; 0 0 ACL francs test_num francs203; 1259 16524 TABLE francs test_pri francs3118; 0 0 ACL francs test_pri francs204; 1259 16527 TABLE francs test_state francs3119; 0 0 ACL francs test_state francs205; 1259 16531 TABLE francs test_sub francs3120; 0 0 ACL francs test_sub francs206; 1259 16534 TABLE francs test_trigger_bk francs3121; 0 0 ACL francs test_trigger_bk francs207; 1259 16537 TABLE francs test_view1 francs3122; 0 0 ACL francs test_view1 francs208; 1259 16540 TABLE francs test_xlog francs3123; 0 0 ACL francs test_xlog francs209; 1259 16543 VIEW francs view1_test francs3124; 0 0 ACL francs view1_test francs210; 1259 16547 TABLE public tbl_ddl_drop_log postgres3125; 0 0 ACL public tbl_ddl_drop_log postgres2874; 2604 16553 DEFAULT francs id francs2875; 2604 16554 DEFAULT francs id francs2876; 2604 16555 DEFAULT francs id francs3069; 0 24860 TABLE DATA francs New_Table francs3028; 0 16410 TABLE DATA francs foo francs3029; 0 16416 TABLE DATA francs test_1 francs3030; 0 16419 TABLE DATA francs test_1_20130725 francs3031; 0 16422 TABLE DATA francs test_1_20130726 francs3032; 0 16425 TABLE DATA francs test_2 francs3033; 0 16428 TABLE DATA francs test_3 francs3034; 0 16431 TABLE DATA francs test_4 francs3068; 0 16706 TABLE DATA francs test_5 francs3035; 0 16437 TABLE DATA francs test_6 francs3067; 0 16668 TABLE DATA francs test_archive francs3036; 0 16441 TABLE DATA francs test_array francs3066; 0 16665 TABLE DATA francs test_big francs3037; 0 16447 TABLE DATA francs test_cache francs3038; 0 16453 TABLE DATA francs test_con francs3039; 0 16457 TABLE DATA francs test_full francs3040; 0 16460 TABLE DATA francs test_full_20130625 francs3041; 0 16463 TABLE DATA francs test_full_20130626 francs3042; 0 16466 TABLE DATA francs test_full_20130627 francs3043; 0 16469 TABLE DATA francs test_full_20130628 francs3044; 0 16472 TABLE DATA francs test_full_20130629 francs3045; 0 16475 TABLE DATA francs test_full_20130630 francs3046; 0 16478 TABLE DATA francs test_full_20130701 francs3047; 0 16481 TABLE DATA francs test_full_20130702 francs3048; 0 16484 TABLE DATA francs test_full_20130703 francs3049; 0 16487 TABLE DATA francs test_full_20130704 francs3050; 0 16490 TABLE DATA francs test_full_20130705 francs3051; 0 16496 TABLE DATA francs test_json1 francs3126; 0 0 SEQUENCE SET francs test_json1_id_seq francs3053; 0 16505 TABLE DATA francs test_lock francs3054; 0 16514 TABLE DATA francs test_notice francs3055; 0 16517 TABLE DATA francs test_notice2 francs3127; 0 0 SEQUENCE SET francs test_notice2_id_seq francs3128; 0 0 SEQUENCE SET francs test_notice_id_seq francs3065; 0 16662 TABLE DATA francs test_num francs3058; 0 16524 TABLE DATA francs test_pri francs3059; 0 16527 TABLE DATA francs test_state francs3060; 0 16531 TABLE DATA francs test_sub francs3061; 0 16534 TABLE DATA francs test_trigger_bk francs3062; 0 16537 TABLE DATA francs test_view1 francs3063; 0 16540 TABLE DATA francs test_xlog francs3064; 0 16547 TABLE DATA public tbl_ddl_drop_log postgres2879; 2606 16607 CONSTRAINT francs test_4_pkey francs2883; 2606 16609 CONSTRAINT francs test_full_20130625_pkey francs2885; 2606 16611 CONSTRAINT francs test_full_20130626_pkey francs2887; 2606 16613 CONSTRAINT francs test_full_20130627_pkey francs2889; 2606 16615 CONSTRAINT francs test_full_20130628_pkey francs2891; 2606 16617 CONSTRAINT francs test_full_20130629_pkey francs2893; 2606 16619 CONSTRAINT francs test_full_20130630_pkey francs2895; 2606 16621 CONSTRAINT francs test_full_20130701_pkey francs2897; 2606 16623 CONSTRAINT francs test_full_20130702_pkey francs2899; 2606 16625 CONSTRAINT francs test_full_20130703_pkey francs2901; 2606 16627 CONSTRAINT francs test_full_20130704_pkey francs2903; 2606 16629 CONSTRAINT francs test_full_20130705_pkey francs2881; 2606 16631 CONSTRAINT francs test_full_pkey francs2905; 2606 16633 CONSTRAINT francs test_json1_pkey francs2907; 2606 16635 CONSTRAINT francs test_lock_pkey francs2911; 2606 16641 CONSTRAINT francs test_notice2_pkey francs2909; 2606 16639 CONSTRAINT francs test_notice_pkey francs2913; 2606 16643 CONSTRAINT francs test_pri_pkey francs2916; 2606 16646 CONSTRAINT francs test_sub_pkey francs2918; 2606 16648 CONSTRAINT francs test_view1_pkey francs2914; 1259 16644 INDEX francs uni_test_pri_name francs3027; 2618 16603 RULE francs rule_foo francs2870; 3466 16602 EVENT TRIGGER - trg_log_drop_command postgres 备注：上面列出了数据库的函数，表，序列等列表，由于 francs.dmp 是压缩过的二进制文件，如果想转换成 sql 文件，应该如何做呢？ 转换二进制文件由于 .dmp 文件是通过 -Fc 压缩的二进制文件，且压缩比很大，如果想查看其详细SQL 内容，可通过以下命令查看，这里仅查看头部 50 行.123456789101112131415161718192021222324252627282930313233[pg93@redhatB tf]$ pg_restore francs.dmp | head -n 50---- PostgreSQL database dump--SET statement_timeout = 0;SET lock_timeout = 0;SET client_encoding = 'UTF8';SET standard_conforming_strings = on;SET check_function_bodies = false;SET client_min_messages = warning;---- Name: fracns; Type: SCHEMA; Schema: -; Owner: francs--CREATE SCHEMA fracns;ALTER SCHEMA fracns OWNER TO francs;---- Name: francs; Type: SCHEMA; Schema: -; Owner: francs--CREATE SCHEMA francs;ALTER SCHEMA francs OWNER TO francs;---- Name: plperlu; Type: PROCEDURAL LANGUAGE; Schema: -; Owner: postgres--CREATE OR REPLACE PROCEDURAL LANGUAGE plperlu;ALTER PROCEDURAL LANGUAGE plperlu OWNER TO postgres;---- Name: plpgsql; Type: EXTENSION; Schema: -; Owner:--CREATE EXTENSION IF NOT EXISTS plpgsql WITH SCHEMA pg_catalog;---- Name: EXTENSION plpgsql; Type: COMMENT; Schema: -; Owner:--COMMENT ON EXTENSION plpgsql IS 'PL/pgSQL procedural language'; 备注： pg_dump 备份时有四种格式 p(plain), c(custom), d(directory), t(tar)，其中 custom 格式最为灵活，建议使用，关于各参数详见以下文档。 参考 pg_dump pg_restore","categories":[{"name":"PG备份与恢复","slug":"PG备份与恢复","permalink":"https://postgres.fun/categories/PG备份与恢复/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"Ubuntu: 3G 网卡配置（中兴） ","slug":"20131223193431","date":"2013-12-23T11:34:31.000Z","updated":"2018-09-04T01:34:07.372Z","comments":true,"path":"20131223193431.html","link":"","permalink":"https://postgres.fun/20131223193431.html","excerpt":"","text":"ubuntu 3 G 网卡配置颇为费劲，根据步骤1配置完成后，有时候能连，有时候连接不成功，后来请教同事后才得以解决，这里仅分享解决方法，不保证对其它类型网卡有效。 步骤一：配置无线网络配置无线网络，如下图: 备注：根据网上的资料，配置完以上后，发现有时能上网，有时上不了，原因是每次无线网卡的 /dev/ttyUSB 口是不一样的, 而这里没有图形化界面配置，这里可以安装 wvdial 工具，在配置文件中指定 /dev/ttyUSB 口。 步骤二：安装配置 Wvdial2.1 安装 wvdial1sudo apt-get install wvdial 2.2 修改配置文件 修改配置文件，/etc/wvdial.conf，如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879[Dialer cdma3g]Init1 = ATZInit2 = ATQ0 V1 E1 S0=0 &amp;C1 &amp;D2 +FCLASS=0Init3 = ATE0V1Init5 = ATS0=0#Init6 = AT+CGDCONT=1,\"IP\",\"uninet\"Init7 = AT+CFUN=1Modem Type = Analog ModemISDN = 0New PPPD = yesPhone = #777Modem = /dev/ttyUSB0Password = vnet.mobiUsername = ctnet@mycdma.cnBaud = 4608000[Dialer cdma1x]Init1 = ATZInit2 = ATQ0 V1 E1 S0=0 &amp;C1 &amp;D2 +FCLASS=0Modem Type = Analog ModemISDN = 0New PPPD = yesPhone = #777Modem = /dev/ttyUSB0Username = CARDPassword = CARDBaud = 460800[Dialer shh]Init3 = ATM0[Dialer Defaults]Init1 = ATZStupid Mode = onDial Attempts = 1Password = vnet.mobiAsk Password = offCheck Def Route = onPhone = #777Idle Seconds = 0Abort on Busy = offAbort on No Dialtone = onModem Type = Analog ModemBaud = 9600Auto DNS = onDial Command = ATM1L3DTModem = /dev/ttyUSB0Init = ATX0ISDN = 0Username = ctnet@mycdma.cnInit2 = ATQ0 V1 E1 S0=0 &amp;C1 &amp;D2 +FCLASS=0Carrier Check = noAuto Reconnect = on--2.3 连接测试 francs@francs:~$ sudo wvdial--&gt; WvDial: Internet dialer version 1.61--&gt; Initializing modem.--&gt; Sending: ATX0OK--&gt; Sending: ATQ0 V1 E1 S0=0 &amp;C1 &amp;D2 +FCLASS=0OK--&gt; Modem initialized.--&gt; Sending: ATM1L3DT#777--&gt; Waiting for carrier.ATM1L3DT#777CONNECT--&gt; Carrier detected. Starting PPP immediately.--&gt; Starting pppd at Mon Dec 23 18:20:19 2013--&gt; Pid of pppd: 12177--&gt; Using interface ppp0--&gt; pppd: [17][7f]--&gt; pppd: [17][7f]--&gt; pppd: [17][7f]--&gt; pppd: [17][7f]--&gt; local IP address 10.164.207.205--&gt; pppd: [17][7f]--&gt; remote IP address 115.168.75.196--&gt; pppd: [17][7f]--&gt; primary DNS address 202.101.172.37--&gt; pppd: [17][7f]--&gt; secondary DNS address 202.101.173.157--&gt; pppd: [17][7f] 备注：如果这步不通，则可以尝试修改 /etc/wvdial.conf 中的 ttyUSB 其它口测试下，在高手苏茶林同学的帮助下，终于搞定了 3G 网卡，这里感谢下。 参考 Ubuntu下如何用电信3G上网卡","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：亮度调整后不能自动保存问题","slug":"20131216144930","date":"2013-12-16T06:49:30.000Z","updated":"2018-09-04T01:34:07.310Z","comments":true,"path":"20131216144930.html","link":"","permalink":"https://postgres.fun/20131216144930.html","excerpt":"","text":"最近觉察到使用 Fn+方向键调整屏幕亮度后，重启机器后没了，得重新调整，非常不爽，网上找了下，暂时没找到好的方法，现在暂时的做法是通过 /etc/rc.local 写死亮度值． 查看亮度文件12francs@francs:~$ cat /sys/class/backlight/acpi_video0/brightness12 备注：这个数值表示亮度，数值越大，越亮，经过测试．手工调整亮度，这个值会相应变化． 写入 /etc/rc.local在 /etc/rc.local 文件中写入这行．1echo 12 &gt; /sys/class/backlight/acpi_video0/brightness 备注：在 exit 0 这行前面增加，12 是我比较喜欢的亮度值． 测试重启机器后，发现屏幕开始较亮，后来一闪变暗，看来是脚本生效了．这只是种治标不治本的方法，手工调整亮度重启系统后依然不能保存，先暂时这么做．","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：调整桌面图标大小","slug":"20131214142958","date":"2013-12-14T06:29:58.000Z","updated":"2018-09-04T01:34:07.169Z","comments":true,"path":"20131214142958.html","link":"","permalink":"https://postgres.fun/20131214142958.html","excerpt":"","text":"ubuntu 桌面图标默认较大，如何调整桌面图标大小呢？1 任意打开一文件夹 2 选择最上面工具栏的”编辑”–&gt; “首选项”备注：之后弹出以下框． 3 调整”图标视图默认值”备注：调整图标大小百分比即可．","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：更改左侧启动图标大小","slug":"20131214141328","date":"2013-12-14T06:13:28.000Z","updated":"2018-09-04T01:34:07.107Z","comments":true,"path":"20131214141328.html","link":"","permalink":"https://postgres.fun/20131214141328.html","excerpt":"","text":"刚开始玩 ubuntu 时遇到很多问题，开始没时间记录，现在有空，觉得有必要记录下备忘．比如，刚装完 ubuntu 系统时，左侧启动图标是大图标，那么如何调小呢？ 桌面空白处单击右键弹出以下对话框，选择 “更改桌面背景”． 调整桌面图标为大图标备注：这是调整左侧启动图标最大，如下： 调整桌面图标为小图标备注：这是调整左侧启动图标最小，如下：","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：Crossover 模拟 Office","slug":"20131213191312","date":"2013-12-13T11:13:12.000Z","updated":"2018-09-04T01:34:07.060Z","comments":true,"path":"20131213191312.html","link":"","permalink":"https://postgres.fun/20131213191312.html","excerpt":"","text":"虽然将桌面转换成了 ubuntu，但免不了要用到 office，这时需要安装 crossover 模拟器，关于 crossover 的安装和 office 的安装本文略，重要的是在用 crossover 安装完 office 2007 后 excel 文件只能打同时打开一个，打开多个时无法显示， 同时打开 word，ppt 文件也有同样的问题，这里的解决方法是参考同事的 blog: 使用CrossOver模拟office . 查看总配置文件 进入目录 /home/francs/.local/share/applications，查看以 .sheet.desktop 结尾的配置文件1234567891011121314cat cxassoc-cxoffice-3270740f-972e-4b17-9d11-e0d6a002e143:application_vnd.openxmlformats-officedocument.spreadsheetml.sheet.desktopenxmlformats-officedocument.spreadsheetml.sheet.desktop[Desktop Entry]Encoding=UTF-8Type=ApplicationX-Created-By=cxoffice-3270740f-972e-4b17-9d11-e0d6a002e143NoDisplay=trueIcon=/home/francs/.cxoffice/Microsoft_Office_2007/windata/Associations/ed84aeba_xlicons.1.xpmName=Microsoft Office ExcelGenericName=Windows Association (CrossOver)Exec=/home/francs/.cxoffice/Microsoft_Office_2007/desktopdata/cxassoc/Scripts/cxoffice-3270740f-972e-4b17-9d11-e0d6a002e143:application_vnd.openxmlformats-officedocument.spreadsheetml.sheet %uTerminal=falseMimeType=application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;application/x-crossover-xlsx;InitialPreference=10 备注：注意红色字体的配置文件． 修改文件1进入目录 /home/francs/.cxoffice/Microsoft_Office_2007/desktopdata/cxassoc/Scripts，修改文件 cxoffice-3270740f-972e-4b17-9d11-e0d6a002e143:application_vnd.ms-excel123#!/bin/sh#exec \"/opt/cxoffice/bin/cxstart\" --bottle \"Microsoft_Office_2007\" --untrusted --wait-children --start-only \".csv:.slk:.sylk:.xla:.xlb:.xlc:.xld:.xlk:.xll:.xlm:.xls:.xlt:.xlw\" --start-default \"application/vnd.ms-excel\" \"$@\"exec \"/opt/cxoffice/bin/cxstart\" --bottle \"Microsoft_Office_2007\" --untrusted --wait-children --start-only \".xlsx:.csv:.slk:.sylk:.xla:.xlb:.xlc:.xld:.xlk:.xll:.xlm:.xls:.xlt:.xlw\" --start-mime \"application/vnd.ms-excel\" \"$@\" 备注：这个文件只有一行，将原来这行注释掉，新加入的行仅是将 –start-default 更改成 –start-mime，另外，在 –start-only 参数中加入需要支持更多的格式即可, 同理修改以下文件． 修改文件2进入目录 /home/francs/.cxoffice/Microsoft_Office_2007/desktopdata/cxassoc/Scripts,修改文件 cxoffice-3270740f-972e-4b17-9d11-e0d6a002e143:application_vnd.openxmlformats-officedocument.spreadsheetml.sheet12#!/bin/shexec \"/opt/cxoffice/bin/cxstart\" --bottle \"Microsoft_Office_2007\" --untrusted --wait-children --start-only \".xlsx:.csv:.slk:.sylk:.xla:.xlb:.xlc:.xld:.xlk:.xll:.xlm:.xls:.xlt:.xlw\" --start-mime\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\" \"$@\" 备注：修改完这两个文件后，就可以同时打开 office 文档了． 参考 使用CrossOver模拟office","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：安装 Pgadmin3","slug":"20131210171518","date":"2013-12-10T09:15:18.000Z","updated":"2018-09-04T01:34:06.997Z","comments":true,"path":"20131210171518.html","link":"","permalink":"https://postgres.fun/20131210171518.html","excerpt":"","text":"ubuntu 安装 pgadmin3 非常简单, 直接运行以下命令即可, 安装之前查看了包的版本为 1.18.1, 为当前最新版本. 安装 Pgadmin31francs@francs:~$ sudo apt-getinstall pgadmin3 测试 连接一个库测试下, 正常:备注: 安装完成, 虽然平常用得较少, 还是先装上. 参考 http://www.pgadmin.org/download/","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：文本编辑技巧","slug":"20131210144720","date":"2013-12-10T06:47:20.000Z","updated":"2018-09-04T01:34:06.919Z","comments":true,"path":"20131210144720.html","link":"","permalink":"https://postgres.fun/20131210144720.html","excerpt":"","text":"在 windows 下，习惯了使用 UE， UE 的列模式，文本替换很好用，并且可以实现行尾替换(例如，文本中所有行尾添加指定字符)，也可以将多行转换成一行，但在 ubuntu 的 MadEdit 中还没找到类似的功能，等价的方法是用 sed, awk 命令实现，记录下。 Tbl_blog 文件内容1234francs@francs:~/Desktop/冒泡助手$ cat tbl_blog.sql tbl_log_201301tbl_access_log_201301tbl_user_log_201301 在行尾增加双引号”12345francs@francs:~/Desktop/冒泡助手$ sed 's/$/&amp;\" /g' tbl_blog.sql tbl_log_201301\" tbl_access_log_201301\" tbl_user_log_201301\" francs@francs:~/Desktop/冒泡助手$ sed -i 's/$/&amp;\" /g' tbl_blog.sql 备注：满足要求。 符号 $ 表示行尾。 在行首添加 -t “1234567891011francs@francs:~/Desktop/冒泡助手$ sed 's/^/-t \"&amp;/g' tbl_blog.sql -t \"tbl_log_201301\" -t \"tbl_access_log_201301\" -t \"tbl_user_log_201301\" francs@francs:~/Desktop/冒泡助手$ sed -i 's/^/-t \"&amp;/g' tbl_blog.sql francs@francs:~/Desktop/冒泡助手$ cat tbl_blog.sql -t \"tbl_log_201301\" -t \"tbl_access_log_201301\" -t \"tbl_user_log_201301\" 备注：满足要求。 符号 ^ 表示行首。 将所有行转换成一行有时需要将一个文本中所有行转换成一行，在 windows 下 UE 可轻松实现，但在 ubuntu 的 MaEdit 不好弄，网上查了下，可以通过 awk 实现。123456789francs@francs:~/Desktop/冒泡助手$ cat tbl_blog.sql-t \"tbl_log_201301\" -t \"tbl_access_log_201301\" -t \"tbl_user_log_201301\" francs@francs:~/Desktop/冒泡助手$ cat tbl_blog.sql | awk '&#123;printf \"%s\",$0&#125;' &gt; tbl_blog1.sqlfrancs@francs:~/Desktop/冒泡助手$ cat tbl_blog1.sql-t \"tbl_log_201301\" -t \"tbl_access_log_201301\" -t \"tbl_user_log_201301\" 备注：满足要求。","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：右键打开终端","slug":"20131208113923","date":"2013-12-08T03:39:23.000Z","updated":"2018-09-04T01:34:06.856Z","comments":true,"path":"20131208113923.html","link":"","permalink":"https://postgres.fun/20131208113923.html","excerpt":"","text":"ubuntu 默认右键没有打开终端选项，安装nautilus-open-terminal 包即可： 查看包信息12francs@francs:~$ sudo apt-cache search nautilus-open-terminal nautilus-open-terminal - nautilus plugin for opening terminals in arbitrary paths 安装1francs@francs:~$ sudo apt-get install nautilus-open-terminal 备注：安装完后，注销或重启生效。 测试注销后，进入一文件夹，单击右键，便有“在终端中打开”选项","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：开启 Crontab 日志","slug":"20131208111659","date":"2013-12-08T03:16:59.000Z","updated":"2018-09-04T01:34:06.794Z","comments":true,"path":"20131208111659.html","link":"","permalink":"https://postgres.fun/20131208111659.html","excerpt":"","text":"今天准备编写个脚本放到任务计划 crontab 中，发现运行异常，于是想查看日志/var/log/cron.log，看看是啥原因，发现在 /var/log 目录中没有找到 cron 相关的日志文件，可以通过以下方法打开。 修改 Rsyslog 配置文件12vim /etc/rsyslog.d/50-default.confcron.* /var/log/cron.log 备注：找到上面这行，默认是注释掉的，把注释去掉保存。 重启 Rsyslog 服务123root@francs:~# service rsyslog restart rsyslog stop/waiting rsyslog start/running, process 6959 查看 Crontab 日志12root@francs:~# ls /var/log/cron.log /var/log/cron.log 备注：这里就有了 crontab 日志。","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：gpg: symbol lookup error: /usr/local/lib/libreadline.so.6: undefined symbol: UP ","slug":"20131207195239","date":"2013-12-07T11:52:39.000Z","updated":"2018-09-04T01:34:06.731Z","comments":true,"path":"20131207195239.html","link":"","permalink":"https://postgres.fun/20131207195239.html","excerpt":"","text":"今天在 ubuntu 系统上安装软件时，报了标题上的错误，经测试，通过 apt-get 安装或删除包时都会报这个错。 安装 Tora 时报错12345678910francs@francs:~$ sudo apt-get install tora .... ....省略部分 正在设置 pgdg-keyring (2013.2) ... gpg: symbol lookup error: /usr/local/lib/libreadline.so.6: undefined symbol: UP Importing apt.postgresql.org key: gpg: symbol lookup error: /usr/local/lib/libreadline.so.6: undefined symbol: UP dpkg：处理 pgdg-keyring (--configure)时出错： 子进程 已安装 post-installation 脚本 返回了错误号 127 .... ....省略部分 备注：这个错误不太明白，网上查了资料，找到以下解决方法。 解决方法12345678910111213141516171819root@francs:~# ll /usr/local/lib/总用量 2084drwxr-xr-x 3 root root 4096 12月 6 15:00 ./drwxr-xr-x 10 root root 4096 4月 26 2012 ../-rw-r--r-- 1 root root 170670 12月 6 15:00 libhistory.alrwxrwxrwx 1 root root 15 12月 6 15:00 libhistory.so -&gt; libhistory.so.6*lrwxrwxrwx 1 root root 17 12月 6 15:00 libhistory.so.6 -&gt; libhistory.so.6.2*-r-xr-xr-x 1 root root 110373 12月 6 15:00 libhistory.so.6.2*-rw-r--r-- 1 root root 1153060 12月 6 15:00 libreadline.alrwxrwxrwx 1 root root 16 12月 6 15:00 libreadline.so -&gt; libreadline.so.6*lrwxrwxrwx 1 root root 18 12月 6 15:00 libreadline.so.6 -&gt; libreadline.so.6.2*-r-xr-xr-x 1 root root 684007 12月 6 15:00 libreadline.so.6.2*drwxrwsr-x 4 root staff 4096 11月 27 20:11 python2.7/ su - rootmkdir tempmv /usr/local/lib/libreadline* templdconfigapt-get update 备注：这么操作后，再次通过 apt-get 安装或删除软件包时不再报错，最后将 temp 目录文件删掉。这种方法其实是通过删除/usr/local/lib/libreadline* 文件的方法，是否会对系统有影响暂不清楚，这里权且先记录下。 参考 http://ubuntuforums.org/showthread.php?t=1484848","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：文本编辑器 MadEdit 使用 ","slug":"20131207113709","date":"2013-12-07T03:37:09.000Z","updated":"2018-09-04T01:34:06.153Z","comments":true,"path":"20131207113709.html","link":"","permalink":"https://postgres.fun/20131207113709.html","excerpt":"","text":"切换到 ubuntu 桌面，MadEdit 可以作为文本编辑器 UltraEdit 的完美替代品，不仅支持各种代码高亮显示，同时支持列模式，编码转换，这里简单介绍下。 安装 MadEdit1sudo apt-get install madedit 备注：安装很简单，可以命令安装，也可以在 ubuntu 软件中心图形化工具安装。 语法高亮设置工作过程中，打开 ＳＱＬ 脚本的情况较多，可以设置语法高亮设置，达到如下效果。备注：设置步骤如下。 高亮设置选择“工具” –&gt; “语法高亮设置”; 设置 SQL的 Keywords , Type, Function 等颜色，如图： 列模式列模式功能是非常重要的功能，如下：备注：MadEdit 还有很多其它功能，暂时了解这些。","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：安装 PostgreSQL 9.3.2 版客户端","slug":"20131206160527","date":"2013-12-06T08:05:27.000Z","updated":"2018-09-04T01:34:06.091Z","comments":true,"path":"20131206160527.html","link":"","permalink":"https://postgres.fun/20131206160527.html","excerpt":"","text":"办公软件安装得差不多了，想在 ubuntu 桌面装个 PostgreSQL 客户端，仅想装客户端，能使用psql命令端工具连数据库就行，不需要建库，建服务，这样平常工作方便点。 1 安装1apt-get install 包名 备注：安装很简单，一条命令完成，但系统自带的版本比较老，只有 9.2，想装个最新版 9.3.2的客户端，过程如下。 2 创建文件：/etc/apt/sources.list.d/pgdg.list12vim/etc/apt/sources.list.d/pgdg.list 并添加以下：deb http://apt.postgresql.org/pub/repos/apt/ precise-pgdg main 3 导入signing key 并更新源123wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -sudo apt-get update 4 安装 PostgreSQL 9.3 客户端1root@francs:~# apt-get install postgresql-client-9.3 5 连接测试12345678910111213francs@francs:~$ psql -h 192.168.1.36 -p 1925 francs francs用户 francs 的口令：psql (9.3.2, 服务器 9.3.0)输入 \"help\" 来获取帮助信息. francs=&gt; \\ds 关联列表架构模式 | 名称 | 型别 | 拥有者----------+---------------------+--------+--------francs | test_json1_id_seq | 序列数 | francsfrancs | test_notice2_id_seq | 序列数 | francsfrancs | test_notice_id_seq | 序列数 | francs(3 行记录) 备注： 连接另一台 PostgreSQL 数据库，成功。 6 参考 http://www.postgresql.org/download/linux/ubuntu/","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：安装五笔拼音输入法","slug":"20131206110928","date":"2013-12-06T03:09:28.000Z","updated":"2018-09-04T01:34:06.028Z","comments":true,"path":"20131206110928.html","link":"","permalink":"https://postgres.fun/20131206110928.html","excerpt":"","text":"ubuntu 自带的 ibus 输入法不太好用，想换成 fcitx 小企鹅的五笔拼音输入法，记录下操作过程。 卸载先把 ibus 相关的软件删除掉。 添加源12sudo add-apt-repository ppa:fcitx-team/nightlysudo apt-get update 安装12sudo apt-get install fcitx fcitx-config-gtk fcitx-sunpinyinsudo apt-get install fcitx-table-all 注销注销系统，使输入法生效，如下所示：备注：平常用的输入法是五笔，遇到不会打的字才会用拼音，五笔拼音输入法既能用五笔，也能用拼音，满足我的需求。","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：安装 KeePass 乱码问题","slug":"20131206101322","date":"2013-12-06T02:13:22.000Z","updated":"2018-09-04T01:34:05.965Z","comments":true,"path":"20131206101322.html","link":"","permalink":"https://postgres.fun/20131206101322.html","excerpt":"","text":"切换到 ubuntu 桌面需要勇气，历程是艰辛的，在安装 KeePass 时遇到乱码问题，一时找不到原因，后来请教同事庆龙同学帮忙，才得以解决问题，在此记录下。 首先 KeePass 是款开源的密码管理软件，而且跨平台，存储密码信息非常实用。 下载 KeepPasshttp://keepass.info/download.html备注：安装很顺利，但运行软件后，界面出现乱码，如下。 乱码备注：界面出现很多方块形乱码。 解决方法根据网上资料尝试几种方法均未能解决，最后的解决方法是删除一个字体。 删除字体1sudo mv /etc/fonts/conf.d/49-sansserif.conf /etc/fonts/conf.d/49-sansserif.conf.bk 备注：需要删除 49-sansserif.conf 字体，这里通过重命名的方法，在需要时还能改回去。 再次打开 KeePass备注：删除字体后，乱码消失，正常显示中文。 参考KeePass 中文乱码","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"Ubuntu：安装 SecureCRT 遇到的问题","slug":"20131204144930","date":"2013-12-04T06:49:30.000Z","updated":"2018-09-04T01:34:05.903Z","comments":true,"path":"20131204144930.html","link":"","permalink":"https://postgres.fun/20131204144930.html","excerpt":"","text":"最近听同事忽悠说 Ubuntu 的多种优点，突然心血来潮，想把之前用了多年的 WIN 桌面换成 Ubuntu，开始了漫长的折腾之旅，好在有几个同事用 Ubuntu 很久了，有了同事的帮助，切换并不费劲，先来看看安装SecureCRT 遇到的问题，记录下。 问题描述1.1 下载 securecrthttp://vandyke.com/download/securecrt/ 备注：下载并安装都很顺利，接下来想把 WIN 下 SecureCrt 的会话配置文件复制过来。 也就是 Config 目录下的 Sessions 目录，配置文件复制到 Ubuntu 下目录 ~/.vandyke/SecureCRT/Config， 启动 SecureCRT连接时报以下错误。 1.2 报错信息和图片12THe filewall was not found in the database .This Session will attempt to connect withou a fire wall 备注：连接没问题但始终报这个错，这个问题困扰了好久，在网上找了些资料，说在 ~/.vandyke/SecureCRT/Config 目录创建个 FireWalls/ 目录，并在这个目录下创建个 .ini 文件，经测试后无效; 还有资料说是更改 /home/francs/.vandyke/SecureCRT/Config/Sessions/Default.ini 文件的 “Firewall Name” 参数，将其更改为 S:”Firewall Name”=None ,经测试，依然无效; 最后请教了了玩 Ubuntu 比较久的同事， 终于发现了问题，先查看其中一个会话文件的 “Firewall Name” 属性。 1.3 查看会话配置文件12cat xxx.xx.xx.xx ( 视频下载 ).ini | grep \"Fire\"S:\"Firewall Name\"=??one 备注：这里发现 S:”Firewall Name” 的值显示的是乱码，这是由于编码的原因，转换下编码即可。 1.4 查看编码12francs@francs:~/.vandyke/SecureCRT/Config/Sessions.bk/IDC/game$ file xxx.xxx.xxx.xx ( game_RHCS ).inixxx.xxx.xxx.xx ( game_RHCS ).ini: ISO-8859 text 1.5 转换编码成 utf-81francs@francs:~/.vandyke/SecureCRT/Config/Sessions.bk/IDC/game$ enconv xxx.xxx.xxx.xx ( game_RHCS ).ini 1.6 再次查看12francs@francs:~/.vandyke/SecureCRT/Config/Sessions.bk/IDC/game$ cat xxx.xxx.xxx.xx ( game_RHCS ).ini | grep \"Fire\"S:\"Firewall Name\"=无 备注：S:”Firewall Name”=无，将其更改成 S:”Firewall Name”=None 后，此错误消失。这个问题终于解决了，但新问题来了，Session目录下有上百个这样的文件，如何批量修改，思考了一会后，结合 find 和 sed 命令 可以搞定，解决方法如下。 批量更改配置文件参数在批量更改配置文件前，建议先备份下 ~/.vandyke/SecureCRT/Config/Sessions 目录。 2.1 进入 Sessions 目录1francs@francs:~$ cd ~/.vandyke/SecureCRT/Config/ 2.2 查看文件编码1find . -name '*.ini' -type f -exec file '&#123;&#125;' ; 备注：结果输出为 ISO-8859 text格式。 2.3 批量转换配置文件编码1find . -name '*.ini' -type f -exec enconv '&#123;&#125;' ; 2.4 批量更改配置文件 S:”Firewall Name”= 参数1find . -name '*.ini' -type f -exec sed -i 's/=无/=None/g' '&#123;&#125;' ; 备注：这条命令能批量更改 Sessions 目录及其子目录下的所有配置文件的 S:”Firewall Name” 属性。sed -i 表示直接替换目标文件。 2.5 更改前的 Firewall Name 参数1S:\"Firewall Name\"=无 2.6 更改后的 Firewall Name 参数1S:\"Firewall Name\"=None","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"PostgreSQL：测试 Readline 和 Zlib 选项","slug":"20131118171228","date":"2013-11-18T09:12:28.000Z","updated":"2018-09-04T01:34:05.840Z","comments":true,"path":"20131118171228.html","link":"","permalink":"https://postgres.fun/20131118171228.html","excerpt":"","text":"PostgreSQL 编译安装时默认支持 readline 和 zlib，其中 readline 选项支持 psql 历史命令查看等功能，zlib 选项支持 pg_dump 的压缩功能，下面测试这两个功能，测试方法：重新编译 PostgreSQL 并不带readline 和 zlib 选项： 重新编译安装创建用户123#groupadd pgtest#useradd -g pgtest pgtest#passwd pgtest 环境变量 .bash_profile123456789101112export PGPORT=1922export PGUSER=postgresexport PGDATA=/database/pgtest/pgdata/pg_rootexport LANG=en_US.utf8 export PGHOME=/opt/pgsql_testexport LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/libexport DATE=`date +\"%Y%m%d%H%M\"`export PATH=$PGHOME/bin:$PATH:.export MANPATH=$PGHOME/share/man:$MANPATHalias rm='rm -i'alias ll='ls -lh' 清理编译文件，重新编译12# gmake clean# gmake distclean 编译1./configure --prefix=/opt/pgsql_test --with-pgport=1922 --with-wal-blocksize=16 without-readline --without-zlib 备注：注意选项 –without-readline –without-zlib 安装12# gmake world# gmake install-world 初始化 initdb1$ initdb -D /database/pgtest/pgdata/pg_root -E UTF8 --locale=C -U postgres -W 备注：以数据库用户执行 initdb 操作即可，这里是以 pgtest 用户执行。 测试 Readlinereadline 测试，发现上下翻键无效，Backspace 键也无效1234567891011[pgtest@host172-16-3-215 ~]$ psqlpsql (9.1.9)Type \"help\" for help. postgres=# select now(); now -------------------------------2013-11-18 13:56:55.350743+08(1 row) postgres=# ^[[A ----上下翻键无效 备注：如果没安装Readline 库，既编译时加上 –without-readline 选项，那么 psql 端不能使用上下翻键和 Backspace 键，也不能查看历史 psql 命令，非常不方便。 测试 Zlib建库123456[pgtest@host172-16-3-215 ~]$ psqlpsql (9.1.9)Type \"help\" for help. postgres=# create database francs;CREATE DATABASE 建表12345francs=# create table test_1 (id int4);CREATE TABLE francs=# insert into test_1 select generate_series(1,1000);INSERT 0 1000 备份测试1234567891011121314151617181920212223242526272829303132333435363738[pgtest@host ~]$ pg_dump -h 127.0.0.1 -E UTF8 -Fc francs -v &gt; francs.dmppg_dump: reading schemaspg_dump: reading user-defined tablespg_dump: reading extensionspg_dump: reading user-defined functionspg_dump: reading user-defined typespg_dump: reading procedural languagespg_dump: reading user-defined aggregate functionspg_dump: reading user-defined operatorspg_dump: reading user-defined operator classespg_dump: reading user-defined operator familiespg_dump: reading user-defined text search parserspg_dump: reading user-defined text search templatespg_dump: reading user-defined text search dictionariespg_dump: reading user-defined text search configurationspg_dump: reading user-defined foreign-data wrapperspg_dump: reading user-defined foreign serverspg_dump: reading default privilegespg_dump: reading user-defined collationspg_dump: reading user-defined conversionspg_dump: reading type castspg_dump: reading table inheritance informationpg_dump: reading rewrite rulespg_dump: finding extension memberspg_dump: finding inheritance relationshipspg_dump: reading column info for interesting tablespg_dump: finding the columns and types of table \"test_1\"pg_dump: flagging inherited columns in subtablespg_dump: reading indexespg_dump: reading constraintspg_dump: reading triggerspg_dump: reading large objectspg_dump: reading dependency datapg_dump: saving encoding = UTF8pg_dump: saving standard_conforming_strings = onpg_dump: saving database definitionpg_dump: [archiver] WARNING: requested compression not available in this installation -- archive will be uncompressedpg_dump: dumping contents of table test_1 备注：查看日志输出倒数第二行，”pg_dump: [archiver] WARNING: requested compression not available in this installation “ ， 表示不支持压缩，而这个功能是备份数据库时压缩比高，非常有用。 总结Readline 和 zlib 属性非常重要，在编译 PostgreSQL 时，强烈推荐启用。 附: Readline 和 Zlib4.1 Readline 解释 The GNU Readline library is used by default. It allows psql (the PostgreSQL command line SQL interpreter) to remember each command you type, and allows you to use arrow keys to recall and edit previous commands. This is very helpful and is strongly recommended. If you do not want to use it then you must specify the –without-readline option to configure. As an alternative, you can often use the BSD-licensed libedit library, originally developed on NetBSD. The libedit library is GNU Readline-compatible and is used if libreadline is not found, or if –with-libedit-preferred is used as an option to configure. If you are using a package-based Linux distribution, be aware that you need both the readline and readline-devel packages, if those are separate in your distribution. 4.2 zlib 解释 The zlib compression library is used by default. If you do not want to use it then you must specify the –without-zlib option to configure. Using this option disables support for compressed archives in pg_dump and pg_restore. 参考 Requirements Installation Procedure RedHat Enterprise 5上安装 Postgresql","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：编译安装常见问题","slug":"20131118144309","date":"2013-11-18T06:43:09.000Z","updated":"2018-09-04T01:34:05.778Z","comments":true,"path":"20131118144309.html","link":"","permalink":"https://postgres.fun/20131118144309.html","excerpt":"","text":"PostgreSQL 生产环境一般使用编译安装方式，今天刚好有个测试库需要重新安装，重新梳理了安装过程中遇到的常见问题，觉得有必要整理一下，首先看下安装步骤，以下摘自手册，以 9.1版本 CentOS 5.5 平台安装为例，其它版本安装过程类似。 这篇 BLOG 不详细介绍安装过程，关于安装步骤，参考：RedHat Enterprise 5上安装 Postgresql 安装简版1234567891011121) ./configure2) gmake3) su4) gmake install5) adduser postgres6) mkdir /usr/local/pgsql/data7) chown postgres /usr/local/pgsql/data8) su - postgres9) /usr/local/pgsql/bin/initdb -D /usr/local/pgsql/data10) /usr/local/pgsql/bin/postgres -D /usr/local/pgsql/data &gt;logfile 2&gt;&amp;1 &amp;11) /usr/local/pgsql/bin/createdb test12) /usr/local/pgsql/bin/psql test 备注：上面 12 步包括了 PostgreSQL 软件的安装，PG 服务初始化以及建库操作。其中 1-4 是软件安装过程，在第一步 configure 过程中遇到的问题较多。 安装需求2.1 必须的系统组件 需要 3.8 版本或更新的 GNU make version 需要一个ISO/ANSIC编译器（至少兼容C89），推荐使用最近版本的GCC 需要tar来解压，也需要 gzip 或者 bzip2 GNU Readline 库: 这可以记住 psql 历史命令 PG默认安装时会使用 zlib压缩库 ，这个工具支持 pg_dump 和 pg_restore 的压缩选项。 2.2 可选的系统组件 以下的包是可选的，在默认安装时并不需要，当某些特定的选项打开时才可能需要。 PL/Perl PL/Python PL/Tcl Native Language Support (NLS) 如果要支持认证或加密，可能需要 Kerberos, OpenSSL, OpenLDAP, 或者 PAM,备注：可选组件根据需求选择安装。 编译常见问题 下载介质并解压后，进行文件目录执行 configure 操作，这步操作遇到的问题较多，例如： 问题1：gcc 没安装12345678910111213141516171819[root@host postgresql-9.1.9]# ./configure --prefix=/opt/pgsql9.1.9 --with-pgport=1921 --with-wal-blocksize=16checking build system type... x86_64-unknown-linux-gnuchecking host system type... x86_64-unknown-linux-gnuchecking which template to use... linuxchecking whether to build with 64-bit integer date/time support... yeschecking whether NLS is wanted... nochecking for default port number... 1921checking for block size... 8kBchecking for segment size... 1GBchecking for WAL block size... 16kBchecking for WAL segment size... 16MBchecking for gcc... nochecking for cc... noconfigure: error: in `/opt/soft_bak/postgresql-9.1.9':configure: error: no acceptable C compiler found in $PATHSee `config.log' for more details. [root@host postgresql-9.1.9]# gcc -version-bash: gcc: command not found 解决方法安装gcc包，如下：1234567[root@host172-16-3-215 ~]# yum install gcc [root@host172-16-3-215 ~]# gcc --versiongcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-54)Copyright (C) 2006 Free Software Foundation, Inc.This is free software; see the source for copying conditions. There is NOwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. 问题2：readline 没安装12345678910[root@host postgresql-9.1.9]# ./configure --prefix=/opt/pgsql9.1.9 --with-pgport=1921 --with-wal-blocksize=16....省略部分....checking for library containing gethostbyname_r... none requiredchecking for library containing shmget... none requiredchecking for library containing readline... noconfigure: error: readline library not foundIf you have readline already installed, see config.log for details on thefailure. It is possible the compiler isn t looking in the proper directory.Use --without-readline to disable readline support. 解决方法安装 readline 包，如下：1root@host ~]# yum install readline-devel.x86_64 备注：安装 readline 组件开发包。 问题3：zlib 没安装123456789[root@host postgresql-9.1.9]# ./configure --prefix=/opt/pgsql9.1.9 --with-pgport=1921 --with-wal-blocksize=16....省略部分....checking for library containing fdatasync... none requiredchecking for library containing gethostbyname_r... none requiredchecking for library containing shmget... none requiredchecking for library containing readline... -lreadline -ltermcapchecking for inflate in -lz... noconfigure: error: zlib library not found 解决方法安装 zlib 开发包，如下：1[root@host ~]# yum install zlib-devel.x86_64 备注：安装 zlib 组件开发包，暂时先总结这些，之后遇到新问题再补充。 参考 Short Version Requirements Installation Procedure RedHat Enterprise 5上安装 Postgresql","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL：什么情况会触发 WAL 日志归档?","slug":"20131017100802","date":"2013-10-17T02:08:02.000Z","updated":"2018-09-04T01:34:05.715Z","comments":true,"path":"20131017100802.html","link":"","permalink":"https://postgres.fun/20131017100802.html","excerpt":"","text":"这几天bbs里提了几个关于 WAL 归档的问题，提的问题越来越深入了，其中有个问题大概是这样的:在开启归档的情况下，PostgreSQL 什么情况会触发 WAL 日志归档? 对于这个问题，简单的实验了下，稍后会总结，先来看配置情况。 环境信息1.1 PostgreSQL 版本PostgreSQL 9.3.0 1.2 postgresql.conf123wal_level = hot_standbyarchive_mode = onarchive_command = 'test ! -f /archive/pg93/archive_active || cp %p /archive/pg93/%f' 1.3 归档目录12345678[pg93@redhatB pg93]$ pwd/archive/pg93 [pg93@redhatB pg93]$ lltotal 32M-rw-------. 1 pg93 pg93 16M Oct 16 11:05 00000001000000000000007A-rw-------. 1 pg93 pg93 16M Oct 16 11:07 00000001000000000000007B-rw-rw-r--. 1 pg93 pg93 0 Oct 16 11:05 archive_active --归档标识文件 备注：归档目录为 “/archive/pg93”，归档脚本为 archive_command 设置的内容，回到本文的问题：什么情况下会触发归档？ 经观察只要发生了 WAL 日志切换时，就会触发归档，更进一步分析，目前想到了以下三种切换 WAL 日志的方法。 方法一: 手动切换 WAL 日志 PostgreSQL 提供 pg_switch_xlog() 函数可以手工切换 WAL 日志，如下： 手动归档123456789[pg93@redhatB ~]$ psqlshpsql (9.3.0)Type \"help\" for help. postgres=# select pg_switch_xlog();pg_switch_xlog----------------0/87000000(1 row) 备注：执行 pg_switch_xlog() 后，WAL 会切换到新的日志，这时会将老的 WAL日志归档，这里归档到 /archive/pg93 目录。 方法二: WAL 日志写满后触发归档WAL 日志被写满后会触发归档，文档在说明配置参数 archive_command 时的第一句说就说明了这点，WAL 日志文件默认为 16MB，这个值可以在编译 PostgreSQL 时通过参数 “–with-wal-segsize” 更改，编译后不能修改。 方法三: 设置 archive_timeout另外可以设置archive 超时参数 archive_timeout ，假如设置 archive_timeout=60 ，那么每 60 s ，会触发一次 WAL 日志切换，同时触发日志归档，这里有个隐含的假设: 当前 WAL 日志中仍有未归档的 WAL 日志内容，有兴趣的朋友可以自己测试下。 参考 pg_switch_xlog() archive_command (string) archive_timeout (integer) PostgreSQL：关于 archive_command 归档命令 How to estimate total number of WAL segments ?","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：关于 \"Numeric Field Overflow\" 报错","slug":"20130929160500","date":"2013-09-29T08:05:00.000Z","updated":"2018-09-04T01:34:05.637Z","comments":true,"path":"20130929160500.html","link":"","permalink":"https://postgres.fun/20130929160500.html","excerpt":"","text":"今天发现一生产库报出大量 “numeric field overflow” 错误，详细报错日志如下： 数据库日志取数据库报错日志中的一行，如下。12013-09-29 15:12:05.830 CST,\"db_name\",\"db_name\",7195,\"XXXX.XXX.XXX.XX51344\",5247d21f.1c1b,19,\"INSERT\",2013-09-29 15:09:19 CST,512/17000814,0,ERROR,22003,\"numeric field overflow\",\"A field with precision 10, scale 0 must round to an absolute value less than 10^10.\" 备注：这里仅取一行，而且还省略了日志中的 SQL 语句。这个报错说明是整型字段超出指定精度，接着模拟下。 模拟测试创建测试表12345678rancs=&gt; create table test_num (rate numeric(4,1));CREATE TABLE francs=&gt; \\d test_num Table \"francs.test_num\"Column | Type | Modifiers--------+--------------+-----------rate | numeric(4,1) | 插入数据测试12345678francs=&gt; insert into test_num (rate) values (100);INSERT 0 1 francs=&gt; insert into test_num (rate) values (100.3);INSERT 0 1 francs=&gt; insert into test_num (rate) values (1000.3);ERROR: numeric field overflow 备注：预期错误出现。 数据库报错 2013-09-29 15:41:55.471 CST,&quot;francs&quot;,&quot;francs&quot;,27916,&quot;[local]&quot;,5247d912.6d0c,4,&quot;INSERT&quot;,2013-09-29 15:38:58 CST,3/1036,0,ERROR,22003,&quot;numeric field overflow&quot;,&quot;A field with precision 4, scale 1 must round to an absolute value less than 10^3.&quot;,,,,,&quot;insert into test_num (rate) values (1000.3);&quot;,,,&quot;psql&quot; 备注：关于 NUMERIC(precision, scale)，文档中有详细说明，precision 表示整个 numeric 的长度，scale 表示小数部分的长度，手册中说明如下： 手册中说明 NUMERIC(precision, scale) We use the following terms below: The scale of a numeric is the count of decimal digits in the fractional part, to the right of the decimal point. The precision of a numeric is the total count of significant digits in the whole number, that is, the number of digits to both sides of the decimal point. So the number 23.5141 has a precision of 6 and a scale of 4. Integers can be considered to have a scale of zero. 备注：原因已经很清楚了，接下来联系项目组，修复这个错误。 参考 http://www.postgresql.org/docs/9.3/static/datatype-numeric.html","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL：如何查询库中包含某个字段的所有表。","slug":"20130929144713","date":"2013-09-29T06:47:13.000Z","updated":"2018-09-04T01:34:05.575Z","comments":true,"path":"20130929144713.html","link":"","permalink":"https://postgres.fun/20130929144713.html","excerpt":"","text":"今天开发组同事过来，需要从数据库端拉一份含有某个字段的表的清单出来，因为这个日志库中表很多，开发人员不方便统计，想了想，可以通过查询 pg_attribute 系统表完成。 数据库信息版本: PostgreSQL 9.1.9表类型：按日分区表 ( 2 套 )；按月分区表 ( 几十套 ) 查询包含 name 字段的表1234567select b.oid, b.relname, att.attname, b.relkind,attinhcount, atttypmod from pg_attribute att, pg_class bwhere b.oid = att.attrelid and att.attname = 'name' and attinhcount in (0) and b.relkind in ('r')order by b.relname, atttypmod; 备注：以上系统表详细信息参考 pg_attribute 和 pg_class，这个 SQL 不需要超级权限执行。 参考 pg_attribute pg_class","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：如何查看安装时的编译信息?","slug":"20130918142212","date":"2013-09-18T06:22:12.000Z","updated":"2018-09-04T01:34:05.512Z","comments":true,"path":"20130918142212.html","link":"","permalink":"https://postgres.fun/20130918142212.html","excerpt":"","text":"PostgreSQL 安装分为二进制包安装和编译安装方式，如果是编译安装，有时希望查看编译安装时的配置参数，例如编译安装时时设置的 WAL segsize 日志文件大小，或者 wal-blocksize 的大小，这些信息非常有用，这时可以使用pg_config查看。 查看编译信息12postgres@db&gt; pg_config --configure'--prefix=/opt/pgsql9.2.4' '--with-pgport=1921' '--with-perl' '--with-tcl' '--with-python' '--with-openssl' '--with-pam' '--without-ldap' '--with-libxml' '--with-libxslt' '--enable-thread-safety' 'with-wal-blocksize=16' 备注：更多选项参考 pg_config。 附: pg_config 命令123456789101112131415pg_configNamepg_config -- retrieve information about the installed version of PostgreSQL Synopsispg_config [option...] DescriptionThe pg_config utility prints configuration parameters of the currently installed version of PostgreSQL. It is intended, for example, to be used by software packages that want to interface to PostgreSQL to facilitate finding the required header files and libraries. OptionsTo use pg_config, supply one or more of the following options: --bindir... 参考 pg_config Installation Procedure","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：关于 archive_command 归档命令","slug":"20130916160003","date":"2013-09-16T08:00:03.000Z","updated":"2018-09-04T01:34:05.465Z","comments":true,"path":"20130916160003.html","link":"","permalink":"https://postgres.fun/20130916160003.html","excerpt":"","text":"archive_command是用来设置归档行为的命令，重要的库在存储空间允许的情况下会一直开启归档，而有些相对不是很重要的库可能偶尔开启归档，对于第二种情况，应该如何编写 archive_command 命令呢？这里提供两种方式 方法一: 编写 archive_command这种方法编写两条 archive_command 命令，不归档时启用 /bin/date，真正需要归档时启用下面这条，如下： 1.1 归档参数123456# - Archiving - archive_mode = on # allows archiving to be done # (change requires restart)archive_command = '/bin/date' # (change requires restart)#archive_command='cp %p /archive/pg93/%f' 备注：这种归档的禁用或启用需要通过修改 postgresql.conf 文件来控制，并且修改之后还需要 reload 操作，步骤较烦琐。 方法二: 使用逻辑或运算 看下面这个配置： 2.1 归档参数12345# - Archiving - archive_mode = on # allows archiving to be done # (change requires restart)archive_command = 'test ! -f /archive/pg93/archive_active || cp %p /archive/pg93/%f' 备注：这里使用了逻辑或运算，当文件标识 /archive/pg93/archive_active 存在时，则会运行之后的归档命令。逻辑或命令参考 2.3 的规则。 2.2 查看归档目录1234567[pg93@redhatB pg93]$ ll /archive/pg93total 64M-rw-------. 1 pg93 pg93 16M Sep 16 15:29 000000010000000100000077-rw-------. 1 pg93 pg93 16M Sep 16 15:30 000000010000000100000078-rw-------. 1 pg93 pg93 16M Sep 16 15:40 00000001000000010000007A-rw-------. 1 pg93 pg93 16M Sep 16 15:49 00000001000000010000007C-rw-rw-r--. 1 pg93 pg93 0 Sep 16 15:49 archive_active -- 归档标识文件 备注：归档标识文件archive_active 需要手工创建。 2.3 逻辑或，逻辑与运算 命令1 &amp;&amp; 命令2 命令1执行成功后才会执行命令2 命令1 || 命令2 命令1执行失败后才会执行命令2 备注：逻辑运算较第一种方法更为简洁，启用归档时只需 touch 一个标识文件，关闭归档时只需要删除标识文件即可，个人推荐这种方法。如果觉得逻辑运算比较难懂，也可以直接通过 shell 判断语句执行，简单粗暴： 2.4 归档参数12345# - Archiving - archive_mode = on # allows archiving to be done # (change requires restart)archive_command = 'if [ -f \"/archive/pg93/archive_active\" ]; then cp %p /archive/pg93/%f; fi' 备注：这种方法脚本量稍多点。 2.5 每天生成一个归档目录 archive_command = &apos;DIR=/pgarch/arch/`date +%F`; sudo test ! -d $DIR &amp;&amp; sudo mkdir $DIR; sudo test ! -f $DIR/%f &amp;&amp; sudo cp %p $DIR/%f&apos; 参考 Linux shell笔记 archive_command PostgreSQL：流复制环境清除 pg_xlog 日志 How to estimate total number of WAL segments ?","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：主备自动切换 HA 脚本","slug":"20130916103404","date":"2013-09-16T02:34:04.000Z","updated":"2018-09-04T01:34:05.387Z","comments":true,"path":"20130916103404.html","link":"","permalink":"https://postgres.fun/20130916103404.html","excerpt":"","text":"德哥编写的 PostgreSQL HA 脚本，结合 PostgreSQL 流复制完成自动主备库切换。 详见：https://github.com/digoal/sky_postgresql_cluster","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/tags/PG高可用性/"}]},{"title":"PostgreSQL：PostgreSQL 9.3 正式版发布","slug":"20130910095751","date":"2013-09-10T01:57:51.000Z","updated":"2018-09-04T01:34:05.340Z","comments":true,"path":"20130910095751.html","link":"","permalink":"https://postgres.fun/20130910095751.html","excerpt":"","text":"PostgreSQL 9.3 终于发布了，详见官网 ，之前对 PostgreSQL 9.3 新特性进行了学习，如下： PostgreSQL 9.3 新特性 Waiting for PostgreSQL9.3：增加物化视图 (MATERIALIZED VIEW) PostgreSQL9.3Beta1：新增 postgres_fdw 外部模块 PostgreSQL9.3Beta1：支持可写的外部表(Writeable Foreign Tables) PostgreSQL9.3Beta1：JSON 功能增强 PostgreSQL9.3Beta1：pg_dump 新增并行参数 (Parallel pg_dump) PostgreSQL9.3Beta1：新增 pg_isready 测试工具 PostgreSQL9.3Beta1：视图新增可更新功能（ Updatable Views ） PostgreSQL9.3Beta1：新增 Lock_timeout 参数 cancel 超长等待 SQL PostgreSQL9.3Beta1：支持事件触发器 ( Event Triggers ) PostgreSQL9.3Beta1：ALTER ROLE 新增 ALL SET 选项设置所有用户参数 PostgreSQL9.3Beta1：建表时废弃 “Implicit Index” 和 Sequence 的提示信息 PostgreSQL9.3Beta1：新增 Array_remove() 和 Array_replace() 数组函数 PostgreSQL9.3Beta1：反斜杠命令( Backslash Commands ) 的改变 PostgreSQL9.3Beta1：新增 pg_xlogdump 模块 PostgreSQL9.3Beta1：LATERAL JOIN Downloadshttp://www.postgresql.org/download/ Release Noteshttp://www.postgresql.org/docs/9.3/static/release-9-3.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：修改配置文件 pg_hba.conf、postgresql.conf 注意点","slug":"20130909173548","date":"2013-09-09T09:35:48.000Z","updated":"2018-09-04T01:34:05.278Z","comments":true,"path":"20130909173548.html","link":"","permalink":"https://postgres.fun/20130909173548.html","excerpt":"","text":"在维护 PostgreSQL 库时，有两个配置文件修改的情况比较多，第一是认证文件 pg_hba.conf，另一个是配置文件 postgresql.conf。 其中 pg_hba.conf 的更改更频繁些，因为业务服务器经常出现调整，或增加应用服务器，此时需要增加 pg_hba.conf 的 IP 签权信息，因为目前我们的生产库对 IP 实行严格控制，只有指定的 IP 权限才被开放，下面是一个生产库的 pg_hba.conf 文件。 pg_hba.conf 文件12345678910# TYPE DATABASE USER ADDRESS METHODhost postgres all 0.0.0.0/0 rejecthost template0 all 0.0.0.0/0 rejecthost template1 all 0.0.0.0/0 reject #Aplication Ip listhost all all xxx.xxx.xxx.xxx/32 md5......省略部分 备注：前三行的认证策略为 reject ，表示不允许任何远程的连接连接系统库，之后才是应用服务器的IP 列表，没列入文件的 IP 将不允许连接，这就是常见的 “no pg_hba.conf entry” 错误。 $ “no pg_hba.conf entry” 错误1psql: FATAL: no pg_hba.conf entry for host \"192.168.101.40\", user \"francs\", database \"francs\", SSL off 备注：更多的时候，有时不小心在配置文件 pg_hba.conf 增加了非法字符，也不会报错，但会带来问题，举个简单的例子。 pg_hba.conf 添加一行123#Aplication Ip listhost all all xxx.xxx.xxx.xxx/32 md5host francs francs 192.168.1.25 md5 备注：最后一行为新增的行，这条策略的本意是允许 192.168.1.25 以 francs 用户连接 francs 库，但不小心漏了子网排码。 重载配置文件12[pg93@redhatB ~]$ pg_ctl reload -D $PGDATAserver signaled 备注： reload 时并不报错，更多信息需要查看日志文件。 查看数据库日志122013-09-09 17:20:47.034 CST,,,2504,,51bfaf13.9c8,92,,2013-06-18 08:51:31 CST,,0,LOG,F0000,\"configuration file \"\"/database/pg93/pg_root/postgresql.conf\"\" contains errors; unaffected changes were applied\",,,,,,,,,\"\"2013-09-09 17:20:47.049 CST,,,2504,,51bfaf13.9c8,93,,2013-06-18 08:51:31 CST,,0,LOG,F0000,\"invalid IP mask \"\"md5\"\": Name or service not known\",,,,,\"line 99 of configuration file \"\"/database/pg93/pg_root/pg_hba.conf\"\"\",,,,\"\" 备注：日志信息显示了 pg_hba.conf 文件报错的行号，估计 PG 的初学者很容易掉这坑里，因此，在更改完 pg 的任何配置文件时，建议查看日志文件，勤快点，别偷懒。当然这里有两点需要说明： pg_hba.conf 文件的更改对当前连接不影响，仅影响更改配置之后的新的连接； 上面的例子仅是一个小的格式错误，如果更严重点的格式问题，会影响这个 pg_hba.conf 文件的认证策略， 从而影响业务。 总结 pg_hba.conf 文件的更改对当前连接不影响，仅影响更改配置之后的新的连接； 上面的例子仅是一个小的格式错误，如果更严重点的格式问题，会影响这个 pg_hba.conf 文件的认证策略， 从而影响业务。 PostgreSQL.conf 配置文件的更改情况类似，这里不演示了。 参考 The pg_hba.conf File","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PGCon：2013 PostgreSQL 全国大会 ( 第三届 杭州 )","slug":"20130818105556","date":"2013-08-18T02:55:56.000Z","updated":"2018-09-04T01:34:05.215Z","comments":true,"path":"20130818105556.html","link":"","permalink":"https://postgres.fun/20130818105556.html","excerpt":"","text":"今年 PostgreSQL 大会在杭州举行，欢迎正在使用 PostgreSQL 或将来计划使用 PostgreSQL 的个人或公司参加，大会信息以 wiki 为准：https://wiki.postgresql.org/wiki/Pgconf_cn2013 Pgconf cn2013PostgreSQL中国用户会全国大会是由PostgreSQL中国用户会(CPUG)主办的旨在促进PostgreSQL在中国发展的非营利性的会议年度大会， 大会是分享PostgreSQL的最佳使用实践、学习最新的功能特性、结交本领域的朋友的最佳场所。 2013年度大会将于2013年10月26日-27日在杭州举行 Contents[hide] 1大会基本信息 2演讲嘉宾征集 3大会议程 4候选议题 5嘉宾介绍 6大会预算 7地图信息 8其他信息 9欢迎赞助本次大会 10鸣谢大会基本信息 主办 PostgreSQL中国用户会 时间 2013-10-26：培训专场 2013-10-27：大会演讲 地点 主会场-杭州:杭州市紫荆花路2号联合大厦B座10楼 内容 第一天培训, 内核和DBA的免费培训分两场. 第二天大会, (9.3新特性介绍, 9.4开发版功能预览, 数据库架构设计, 内核开发, 集群, 水平扩展, 大数据应用, 地理位置应用, 运维经验交流等方面.) 报名方式 http://bbs.pgsqldb.com/client/bm.php(报名时请提供上衣尺寸s,m,xl,xxl) 报名截止时间：2013年10月10日 参加人数：100人(名额满则报名截止) 费用 9月30日前交费: 100元每人 （含场地费及大会纪念T恤费用，大会不提供发票） 10月1日后交费：200元每人 报名费支付方法 支付宝地址 chenaisheng@126.com 支付主页https://me.alipay.com/chenaisheng 说明：支付时付款说明输入你姓名+手机号+报名时收到的认证号码，如(陈爱声 13828026658 6688). 特别要注意的是，只有我们确认收到报名费，你的报名手续才能最终完成，名额才会保留 我们在确认收到报名费之后会发一封确认邮件给你们告知报名成功，请同学们注意查收 报名相关咨询: 陈爱声 13828026658 联系信息 QQ群：3336901 组织者: 周正中(德哥)(负责 议程安排及嘉宾联系 场地组织 报名组织 礼物 会议组织) 电话：18657125281 QQ：276732431 邮箱：digoal at 126.com 报名负责： 陈爱声(阿弟) 电话：13828026658 邮箱：4893310 at qq.com 其他工作人员: 阿弟，永收,少聪,泥鳅,孙鹏,francs,galy(galylee at gmail.com 微博 @李元佳Postgres). 赞助商 特别鸣谢杭州斯凯网络科技有限公司 合作媒体演讲嘉宾征集 大会现在正征集演讲嘉宾，如有分享话题，请联系德哥digoal at 126.com 大会议程 第一天 周六 培训 培训报到 DBA培训专场 9:00 - 12:00 入门培训 13:00 - 17:00 高级管理培训 内核培训专场 9:00 - 12:00 内核原理、架构 13:00 - 17:00 社区结构、特性开发、插件开发 第二天大会, (9.3新特性介绍, 9.4开发版功能预览, 数据库架构设计, 内核开发, 集群, 水平扩展, 大数据应用, 地理位置应用, 运维经验交流等方面.) 8:00 - 8:45 大会报到 9:00 - 9:15 开场介绍 9:15 - 10:00 主题演讲1 10:00 - 10:45 主题演讲2 10:45 - 11:00 茶歇 11:00 - 11:45 演讲 午餐时间 12:00 - 13:00 13:00 - 13:30 演讲 13:30 - 14:00 自由交流 14:00 - 14:30 演讲 14:30 - 15:00 自由交流 15:00 - 15:30 演讲 15:30 - 16:00 演讲 16:00 - 17:00 圆桌论坛候选议题 应用案例 罗翼(去哪儿) - 去哪儿的Postgres应用 周正中(斯凯) - 斯凯网络的Postgres应用,systemtap性能分析工具 沈镔(斯凯) - 斯凯PostgreSQL中json的应用案例 田文罡(华为) - PostgreSQL的企业级HA实践 陈立群(通策) - PostgreSQL医疗行业应用案例 新特性 周正中(斯凯) - 9.3新特性+9.4CF介绍 李元佳(华为) 2013 PostgreSQL全球开发者大会参会报告 应用技术 黄坚(从兴电子) - pgpool-II 逯永收 - Java 自动分表框架 陈爱声 - PG全文搜索,plpgsql开发,体系结构 大数据 金华峰 - 大数据实时分析 萧少聪(神舟立诚) alan(腾讯) - 腾讯PostgreSQL应用分享 内核 李海翔 - PostgreSQL的查询优化器探讨 嘉宾介绍 周正中 罗翼 田文罡 陈爱声 逯永收 黄坚 李元佳 金华峰 萧少聪 陈立群 alan 大会预算由于大会是社区志愿者组织的, 大会的资金来源除了门票以外就没有其他来源了. 从前2届的情况来看, 大会是只赔不赚的, 这里像资助的同学表示感谢. 今年以一切从简为原则，不邀请国外嘉宾，缩减人数，场地使用公司赞助的。今年的费用预计如下: 支出1万元 场地: 2000元(由于场地今年有望可以使用公司赞助的场地，免去场租，场地费用主要是现场的布置及小点心、饮料) 纪念品: 40元 * 100人 = 4000 印刷品: 1000元 其他杂费: 3000元 收入1万元 门票： 100人 x 100元=1万元 地图信息 会场位置和附近的公交站点","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL: UNION 和 UNION ALL 操作的不同","slug":"20130816161857","date":"2013-08-16T08:18:57.000Z","updated":"2018-09-04T01:34:05.153Z","comments":true,"path":"20130816161857.html","link":"","permalink":"https://postgres.fun/20130816161857.html","excerpt":"","text":"今天读到老外的一篇关于 UNION 和 UNION ALL 操作对比的文章觉得很受用，原文 http://www.cybertec.at/common-mistakes-union-vs-union-all/ ， 有必要实验下。 UNION 操作一般用来合并多个结果集，尽管如此，仍然有细节需要注意，接着看以下测试。 PostgreSQL 9.3 中测试1.1 测试123456789[pg93@redhatB ~]$ psql francs francspsql (9.3beta1)Type \"help\" for help. francs=&gt; select 1 union select 1;?column?---------- 1(1 row) 备注：这里预期是 2 条记录，结果却为 1 条。 1.2 执行计划123456789101112francs=&gt; explain analyze select 1 union select 1; QUERY PLAN ------------------------------------------------------------------------------------------------------Unique (cost=0.05..0.06 rows=2 width=0) (actual time=0.062..0.070 rows=1 loops=1) -&gt; Sort (cost=0.05..0.06 rows=2 width=0) (actual time=0.058..0.061 rows=2 loops=1) Sort Key: (1) Sort Method: quicksort Memory: 17kB -&gt; Append (cost=0.00..0.04 rows=2 width=0) (actual time=0.006..0.017 rows=2 loops=1) -&gt; Result (cost=0.00..0.01 rows=1 width=0) (actual time=0.003..0.005 rows=1 loops=1) -&gt; Result (cost=0.00..0.01 rows=1 width=0) (actual time=0.002..0.003 rows=1 loops=1)Total runtime: 0.121 ms(8 rows) 备注：根据以上，知道先进行排序，然后有个 Unique 操作去掉重复的行。 1.3 pg 手册中的解释 [ { UNION | INTERSECT | EXCEPT } [ ALL | DISTINCT ] select ] Using the operators UNION, INTERSECT, and EXCEPT, the output of more than one SELECT statement can be combined to form a single result set. The UNION operator returns all rows that are in one or both of the result sets. The INTERSECT operator returns all rows that are strictly in both result sets. The EXCEPT operator returns the rows that are in the first result set but not in the second. In all three cases, duplicate rows are eliminated unless ALL is specified. The noise word DISTINCT can be added to explicitly specify eliminating duplicate rows. Notice that DISTINCT is the default behavior here, even though ALL is the default for SELECT itself. (See UNION Clause, INTERSECT Clause, and EXCEPT Clause below.) UNION Clause The result of UNION does not contain any duplicate rows unless the ALL option is specified. ALL prevents elimination of duplicates. (Therefore, UNION ALL is usually significantly quicker than UNION; use ALL when you can.) DISTINCT can be written to explicitly specify the default behavior of eliminating duplicate rows. 备注： 上面解释得很清楚， UNION 默认使用 DISTINCT 模式，会删除重复的行。 1.4 union distinct12345francs=&gt; select 1 union distinct select 1;?column?---------- 1(1 row) 1.5 union all123456francs=&gt; select 1 union all select 1;?column?---------- 1 1(2 rows) 备注：”union distinct” 会删除重复的行， “union all” 是预期的结果，不会删除重复的行。 Oracle 10g 中测试2.1 测试1234515:38:03 SKYTF@skytf&gt; select 1 from dual union select 1 from dual; 1---------- 1 备注： union 操作和 PG 的测试结果一样。 2.2 执行计划123456789101112131415161718192021222324252627282930313233343515:38:13 SKYTF@skytf&gt; set autotrace on;15:38:30 SKYTF@skytf&gt; select 1 from dual union select 1 from dual; 1---------- 1 执行计划----------------------------------------------------------Plan hash value: 4080566713 -----------------------------------------------------------------| Id | Operation | Name | Rows | Cost (%CPU)| Time |-----------------------------------------------------------------| 0 | SELECT STATEMENT | | 2 | 6 (67)| 00:00:01 || 1 | SORT UNIQUE | | 2 | 6 (67)| 00:00:01 || 2 | UNION-ALL | | | | || 3 | FAST DUAL | | 1 | 2 (0)| 00:00:01 || 4 | FAST DUAL | | 1 | 2 (0)| 00:00:01 |----------------------------------------------------------------- 统计信息---------------------------------------------------------- 0 recursive calls 0 db block gets 0 consistent gets 0 physical reads 0 redo size 401 bytes sent via SQL*Net to client 385 bytes received via SQL*Net from client 2 SQL*Net roundtrips to/from client 1 sorts (memory) 0 sorts (disk) 1 rows processed 备注：从 PLAN 中看出，oracle 的 union 和 pg 的 union 操作内部处理过程是类似的。 2.3 union all12345615:44:32 SKYTF@skytf&gt; select 1 from dual union all select 1 from dual; 1---------- 1 1 备注： union all 操作和 PG 的测试结果一样。 2.4 oracle 手册中的解释 UNION ALL Example The UNION operator returns only distinct rows that appear in either result, while the UNION ALL operator returns all rows. The UNION ALL operator does not eliminate duplicate selected rows: 总结 PG 和 ORACLE 对 UNION 的处理是一样的，即 union 操作会删除重复的行，而 union all 操作不会删除重复的行。 由于 union 操作会对多个结果集进行排序然后删除重复的行，因此效率会比 union all 低很多。 参考 common-mistakes: union vs union-all SELECT COMMAND The Usage of Combining Queries","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：版本支持周期","slug":"20130809153223","date":"2013-08-09T07:32:23.000Z","updated":"2018-09-04T01:34:05.090Z","comments":true,"path":"20130809153223.html","link":"","permalink":"https://postgres.fun/20130809153223.html","excerpt":"","text":"PostgreSQL 大版本支持周期为 5 年，如下： End Of Life (EOL) dates 备注: 官方建议使用每个大版本的最新小版本。 参考 PostgreSQL Versioning policy","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：快速生成大字段值的方法","slug":"20130726095431","date":"2013-07-26T01:54:31.000Z","updated":"2018-09-04T01:34:05.028Z","comments":true,"path":"20130726095431.html","link":"","permalink":"https://postgres.fun/20130726095431.html","excerpt":"","text":"今天bbs里有朋友问如下问题 “ 请问创建一个表，只插20条左右的记录，使其大小在1G以上，怎么实现？”，我们可以这样理解，如何快速生成一条大记录。在 PostgreSQL 中，最大字段值是有限制的，限制为 1 GB。那么如何来构造一个大字段呢，相信方法很多，这里提供一个简单的方式，计划使用 row_to_json 函数生成一个 array 值，SQL 如下： 生成大字段的SQL12345WITH query_1 as(select a, a||'_francs', clock_timestamp() from generate_series(1,5000000) a)insert into test_array(name) select array_agg(col1) from (select row_to_json(c) as col1 from query_1 c ) d ; 测试: 生成大字段表1.1 创建表12francs=&gt; create table test_array(name json[]);CREATE TABLE 1.2 插入一条大记录12345678910111213francs=&gt; with query_1 asfrancs-&gt; (francs(&gt; select a, a||'_francs', clock_timestamp() from generate_series(1,5000000) afrancs(&gt; )francs-&gt; insert into test_array(name) select array_agg(col1) from (select row_to_json(c) as col1 from query_1 c ) d ;INSERT 0 1francs=&gt; \\dt+ test_array List of relationsSchema | Name | Type | Owner | Size | Description--------+------------+-------+--------+-------+-------------francs | test_array | table | francs | 66 MB |(1 row) 备注：这里先通过 generate_series 函数生成 5000000 条记录，然后将这些记录转换成 JSON，最后转换成数组并插入到表 test_array 表中，如果机器性能好，可以把这个数搞大点，这样，插入数条记录后数据很容易超过 1 GB。 参考 Maximum Field Size Aggregate Functions PostgreSQL: 如何连接 “ Group By “ 结果集的行","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL ERROR: $2 is declared CONSTANT","slug":"20130725162934","date":"2013-07-25T08:29:34.000Z","updated":"2018-09-04T01:34:04.965Z","comments":true,"path":"20130725162934.html","link":"","permalink":"https://postgres.fun/20130725162934.html","excerpt":"","text":"今天有开发的同事咨询在执行函数报错的问题，主要问题出现在更改入参上，仿照这个函数，创建以下测试函数，用来重现错误信息，数据库版本 8.4。 创建测试函数1234567891011121314151617 CREATE OR REPLACE FUNCTION func_create_daily_table(table_name character varying, table_num integer) RETURNS void LANGUAGE plpgsqlAS $function$declare today char(8); v_childtable character varying;beginIF table_num is null then table_num :=1;END IF; for i in 0 .. table_num loop v_childtable := table_name || '_' || to_char(current_date + i, 'YYYYMMDD') ; execute 'create table ' || v_childtable || '(like ' || table_name || ' including all ) inherits (' || table_name || ')'; end loop;end;$function$; 报错信息1psql:creaet_func.sql:129: ERROR: \"$2\" is declared CONSTANT 备注：这里提示入参 $1 被定义成常量，且不能修改，报错的数据库版本为 8.4，接下来在 9.0 版本测试。 9.0 版本测试123456789101112131415161718192021222324[pg90@redhatB ~]$ psql francs francspsql (9.0.9)Type \"help\" for help. francs=&gt; CREATE OR REPLACE FUNCTION func_create_daily_table(table_name character varying, table_num integer)francs-&gt; RETURNS voidfrancs-&gt; LANGUAGE plpgsqlfrancs-&gt; AS $function$francs$&gt; declarefrancs$&gt; today char(8);francs$&gt; v_childtable character varying;francs$&gt; beginfrancs$&gt; francs$&gt; IF table_num is null thenfrancs$&gt; table_num :=1;francs$&gt; END IF;francs$&gt; francs$&gt; for i in 0 .. table_num loopfrancs$&gt; v_childtable := table_name || '_' || to_char(current_date + i, 'YYYYMMDD') ;francs$&gt; execute 'create table ' || v_childtable || '(like ' || table_name || ' including all ) inherits (' || table_name || ')';francs$&gt; end loop;francs$&gt; end;francs$&gt; $function$;CREATE FUNCTION 备注：在9.0 版本测试是正常的，猜测可能是 8.4 版本的一个 bug ，但在官网邮件列表里没有查到明确定位是bug 的信息，唯一相关的内容是以下这个帖子，遇到的问题类似，但下面用到了参数 ALIAS ，情况不太一样，有兴趣的朋友可以看下。 解决方法主要有两种解决方法： 1. 在函数中额外使用中转变量，将入参的值先赋给中转变量，然后再去修改中转变量的值。 2. 升级数据库版本。 参考 http://www.postgresql.org/message-id/5.1.0.14.0.20020508000544.02653960@hermes.tassie.net.au","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL：ERROR,0A000,\"cached plan must not change result type\"","slug":"20130724155037","date":"2013-07-24T07:50:37.000Z","updated":"2018-09-04T01:34:04.902Z","comments":true,"path":"20130724155037.html","link":"","permalink":"https://postgres.fun/20130724155037.html","excerpt":"","text":"今天发现数据库日志报大量如下错误，数据库版本: PostgreSQL 9.1.9。数据库错误日志12013-07-24 15:11:35.895 CST,\"db_test\",\"db_test\",17526,\"192.168.100.221:40188\",51ed7b10.4476,1555,\"BIND\",2013-07-23 02:33:52 CST,236/820620,0,ERROR,0A000,\"cached plan must not change result type\",,,,,,\"SELECT pkg_name FROM tbl_test WHERE channel_no = $1 AND store_id = $2\",,\"RevalidateCachedPlan, plancache.c:589\",\"\" 备注：网上查了下资料，这个错误与 DDL 操作有关，并且在使用了PREPARE语句的情况下会发生，接下来模拟下这个错误。 模拟异常1.1 创建测试表并插入数据12345678910111213francs=&gt; create table test_cache (id int4, name character varying(32));CREATE TABLE francs=&gt; insert into test_cache values (1,'a'),(2,'b'),(3,'c');INSERT 0 3 francs=&gt; select * from test_cache ;id | name----+------ 1 | a 2 | b 3 | c(3 rows) 备注：创建完表后，接着开启两个会话，顺序按以下操作进行。 1.2 会话1: 创建 PREPARE SQL12345678910francs=&gt; PREPARE select_1 (character varying) ASfrancs-&gt; select * From test_cache where name=$1;PREPARE francs=&gt; EXECUTE select_1('a');id | name----+------ 1 | a(1 row)备注：正常使用 EXECUTE 语句。 1.3 会话2: DDL 更改表结构123456789francs=&gt; \\d test_cache Table \"francs.test_cache\"Column | Type | Modifiers--------+-----------------------+-----------id | integer |name | character varying(32) | francs=&gt; alter table test_cache alter column name type character varying;ALTER TABLE 备注：这里更改字段长度。 1.4 再次回到会话1:12francs=&gt; EXECUTE select_1('a');ERROR: cached plan must not change result type 1.5 数据库日志如下：12013-07-24 15:19:36.139 CST,\"francs\",\"francs\",23261,\"[local]\",51ef7f33.5add,1,\"EXECUTE\",2013-07-24 15:16:03 CST,3/10083,0,ERROR,0A000,\"cached plan must not change result type\",,,,,,\"EXECUTE select_1('a');\",,,\"psql\" 备注：正好重现了标题出现的错误。 解决方法由于 PREPARE 语句在会话结束后会自动消失，因此解决方式有多种，可以断开会话，重启应用程序，也可以使用 DEALLOCATE 命令取消 PREPARE 语句，然后重新生成 PREPARE 语句，今天出现的异常后来通知项目组重启这个模块后，问题解决。 2.1 DEALLOCATE123456789101112francs=&gt; DEALLOCATE select_1;DEALLOCATE francs=&gt; PREPARE select_1 (character varying) ASfrancs-&gt; select * From test_cache where name=$1;PREPARE francs=&gt; EXECUTE select_1('a');id | name----+------ 1 | a(1 row) 备注：DEALLOCATE 语句只对本身单个会话生效，如果是应用程序批量报错，这个方法显然不合适。 总结 这里演示了使用 prepared 语句后，如果涉及的表字段长度更改了会报错。我这里还测试了增，减字段的情况，得到的结果是一样的。 在数据库维护过程中，对于 DDL 操作需要格外小心，一般在表执行 DDL 后，应用会升级重启， 这时问题不大，如果应用程序不涉及升级重启，之后就会遇到之前的错，个人暂时没有发现在数据库端有很好的规避这个错误的方法。 参考 PREPARE Postgres 8.3: “ERROR: cached plan must not change result type” PostgreSQL Error Codes","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL：如何限制一个表的多个字段不能同时为空？","slug":"20130709151551","date":"2013-07-09T07:15:51.000Z","updated":"2018-09-04T01:34:04.840Z","comments":true,"path":"20130709151551.html","link":"","permalink":"https://postgres.fun/20130709151551.html","excerpt":"","text":"今天有开发组的同事咨询数据库是否有方法可以限制一个表中的多个字段不能同时为空，起始想到的是创建 trigger 的方法，但 trigger 比较费性能，不建议使用，后来想到可以通过创建约束轻松实现，如下： 创建测试表123456[pg93@redhatB pg_root]$ psql francs francspsql (9.3beta1)Type \"help\" for help. francs=&gt; create table test_con (col_a character varying(32),col_b character varying(32),col_c character varying(32));CREATE TABLE 创建约束123456789101112francs=&gt; alter table test_con add constraint con_1 check (col_a is not null or col_b is not null or col_c is not null);ALTER TABLE francs=&gt; \\d test_con Table \"francs.test_con\"Column | Type | Modifiers--------+-----------------------+-----------col_a | character varying(32) |col_b | character varying(32) |col_c | character varying(32) |Check constraints: \"con_1\" CHECK (col_a IS NOT NULL OR col_b IS NOT NULL OR col_c IS NOT NULL) 备注：三个字段不能同时为空。 插入测试数据插入测试数据验证约束，如下：123456789101112131415161718192021francs=&gt; insert into test_con (col_a,col_b,col_c) values ('a','a','a');INSERT 0 1francs=&gt; insert into test_con (col_a,col_b,col_c) values ('a',null,null);INSERT 0 1francs=&gt; insert into test_con (col_a,col_b,col_c) values (null,'a',null);INSERT 0 1francs=&gt; insert into test_con (col_a,col_b,col_c) values (null,null,'a');INSERT 0 1 francs=&gt; select * from test_con;col_a | col_b | col_c-------+-------+-------a | a | aa | | | a | | | a(4 rows)francs=&gt; insert into test_con (col_a,col_b,col_c) values (null,null,null);ERROR: new row for relation \"test_con\" violates check constraint \"con_1\"DETAIL: Failing row contains (null, null, null). 备注：创建约束能轻松满足需求，当然也可以创建触发器实现此功能，这里不演示了。 参考 CREATE TABLE","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：FATAL: requested WAL segment 0000000800002A0000000000 has already been removed ","slug":"20130702145542","date":"2013-07-02T06:55:42.000Z","updated":"2018-09-04T01:34:04.777Z","comments":true,"path":"20130702145542.html","link":"","permalink":"https://postgres.fun/20130702145542.html","excerpt":"","text":"昨天，一重要生产环境的备库主机由于硬件故障需要停机做硬件检测，由于是流复制环境，备库可以停，停机检测大概花了 2 小时左右，之后再次启动备库时，报了如下错误： 数据库日志1232013-07-01 13:25:29.430 CST,,,27738,,51d112c8.6c5a,1,,2013-07-01 13:25:28 CST,,0,LOG,00000,\"streaming replication successfully connected to primary\",,,,,,,,\"libpqrcv_connect, libpqwalreceiver.c:171\",\"\"2013-07-01 13:25:29.430 CST,,,27738,,51d112c8.6c5a,2,,2013-07-01 13:25:28 CST,,0,FATAL,XX000,\"could not receive data from WAL stream: FATAL: requested WAL segment 0000000800002A0000000000 has already been removed\",,,,,,,,\"libpqrcv_receive, libpqwalreceiver.c:389\",\"\" 备注：根据报错信息，很容易知道是由于停机时间的过程中备库所需的 WAL 已经被主库循环使用覆盖了，而在备库停机维护过程中，主库并未打开归档，这时这个备库需要重做了。也许有人会问，为何不一直打开主库的归档，我想说的是，这个库在 TB 级而且比较繁忙，忙的时候一天的归档 600 GB左右，这么大的归档需要大量的存储。但是在备库停机维护过程中，建议主库打开归档，只要不把归档目录撑满，那么在备库重新恢复后，有了主库的归档，那么备库依然能够跟上主库，为了加深理解，下面模拟这个错误，并演示规避方法： 环境信息主机： 笔记本虚拟机系统： Red Hat Enterprise Linux Server release 6.2版本： PostgreSQL 9.3beta1主库IP： 192.168.1.36 主机名：redhatB备库IP： 192.168.1.35 主机名: redhat6备注： 流复制搭建过程略，参考 PostgreSQL：使用 pg_basebackup 搭建流复制环境 模拟过程2.1 设置主库 postgresql.conf 为了容易出演示效果，设置以下参数，其它参数根据需求设置：123456checkpoint_segments = 3archive_mode = onarchive_command = 'cp %p /archive/pg93/%f'max_wal_senders = 3wal_keep_segments = 3 max_wal_senders = 3 备注：归档目录 /archive/pg93/ 需要创建好并赋相应权限。 2.2 重载配置文件12[pg93@redhatB pg_root]$ pg_ctl reload -D $PGDATAserver signaled 备注：参数修改好后需要 reload 。 2.3 测试前: 主库数据1234567891011[pg93@redhatB ~]$ psqlpsql (9.3beta1)Type \"help\" for help.postgres=# select * from test_1; id | create_time ----+--------------------- 1 | 2013-07-01 21:15:34 2 | 2013-07-01 21:55:37 3 | 2013-07-01 22:01:18(3 rows) 2.4 测试前: 备库数据12345678910[pg93@redhat6 ~]$ psqlpsql (9.3beta1)Type \"help\" for help.postgres=# select * from test_1; id | create_time ----+--------------------- 1 | 2013-07-01 21:15:34 2 | 2013-07-01 21:55:37 3 | 2013-07-01 22:01:18(3 rows) 备注：这里先标记下数据，以便后面做对比。 2.5 停备库123[pg93@redhat6 ~]$ pg_ctl stop -m fast -D $PGDATAwaiting for server to shut down.... doneserver stopped 2.6 在主库上执行以下操作12345678910111213141516171819202122postgres=# insert into test_1 values (5,now());INSERT 0 1postgres=# select pg_switch_xlog(); pg_switch_xlog ---------------- 1/310000AC(1 row)postgres=# insert into test_1 values (5,now());INSERT 0 1postgres=# select pg_switch_xlog(); pg_switch_xlog ---------------- 1/320004D0(1 row)postgres=# insert into test_1 values (5,now());INSERT 0 1postgres=# select pg_switch_xlog(); pg_switch_xlog ---------------- 1/330000AC(1 row)..... 便于显示，重复内容就不贴了。 2.7 查看归档目录12[pg93@redhatB pg93]$ ll /archive/pg93/ | wc -l36 备注：这时归档目录就产生了一些归档的 WAL 日志。 2.8 启动备库12[pg93@redhat6 ~]$ pg_ctl start -D $PGDATAserver starting 备注：数据库能启来，但查看日志，报以下错误： 2.9 csv 日志1234[pg93@redhat6 pg_log]$ tail -f postgresql-2013-07-02_062642.csv\",,,,,,,,,\"\"2013-07-02 06:27:17.672 CST,,,3704,,51d20245.e78,1,,2013-07-02 06:27:17 CST,,0,LOG,00000,\"started streaming WAL from primary at 1/22000000 on timeline 1\",,,,,,,,,\"\"2013-07-02 06:27:17.674 CST,,,3704,,51d20245.e78,2,,2013-07-02 06:27:17 CST,,0,FATAL,XX000,\"could not receive data from WAL stream: ERROR: requested WAL segment 000000010000000100000022 has already been removed 备注：目标的错误已重现，在主库的 $PGDATA/pg_xlog 目录里已找不到 000000010000000100000022 文件了,因为 XLOG 文件已被循环使用覆盖了，但在归档目录 /archive/pg93 里可以找到。 2.10 复制主库归档目录的 WAL 到备节点1[pg93@redhatB pg93]$ scp /archive/pg93/* pg93@192.168.1.35:/archive/pg93 备注：这时把主节点上归档目录的 WAL 文件复制到备节点的归档目录。 2.11 修改备库 recovery.conf 文件的以下参数1restore_command = 'cp /archive/pg93/%f %p' 备注：其它参数略, 参考 PostgreSQL：使用 pg_basebackup 搭建流复制环境 2.12 重启备库12345[pg93@redhat6 pg_root]$ pg_ctl stop -m fast -D $PGDATAwaiting for server to shut down.... doneserver stopped[pg93@redhat6 pg_root]$ pg_ctl start -D $PGDATAserver starting 2.13 查看备库日志123456789101112[pg93@redhat6 pg_log]$ tail -f postgresql-2013-07-02_063606.csv2013-07-02 06:36:08.123 CST,,,4008,,51d20456.fa8,4,,2013-07-02 06:36:06 CST,1/0,0,LOG,00000,\"redo starts at 1/200002BC\",,,,,,,,,\"\"2013-07-02 06:36:08.592 CST,,,4008,,51d20456.fa8,5,,2013-07-02 06:36:06 CST,1/0,0,LOG,00000,\"restored log file \"\"000000010000000100000021\"\" from archive\",,,,,,,,,\"\"2013-07-02 06:36:08.618 CST,,,4008,,51d20456.fa8,6,,2013-07-02 06:36:06 CST,1/0,0,LOG,00000,\"consistent recovery state reached at 1/22000000\",,,,,,,,,\"\"2013-07-02 06:36:08.630 CST,,,4006,,51d20456.fa6,1,,2013-07-02 06:36:06 CST,,0,LOG,00000,\"database system is ready to accept read only connections\",,,,,,,,,\"\"2013-07-02 06:36:10.014 CST,,,4008,,51d20456.fa8,7,,2013-07-02 06:36:06 CST,1/0,0,LOG,00000,\"restored log file \"\"000000010000000100000022\"\" from archive\",,,,,,,,,\"\"2013-07-02 06:36:10.800 CST,,,4008,,51d20456.fa8,8,,2013-07-02 06:36:06 CST,1/0,0,LOG,00000,\"restored log file \"\"000000010000000100000023\"\" from archive\",,,,,,,,,\"\"2013-07-02 06:36:12.139 CST,,,4008,,51d20456.fa8,9,,2013-07-02 06:36:06 CST,1/0,0,LOG,00000,\"restored log file \"\"000000010000000100000024\"\" from archive\",,,,,,,,,\"\"2013-07-02 06:36:13.937 CST,,,4008,,51d20456.fa8,10,,2013-07-02 06:36:06 CST,1/0,0,LOG,00000,\"restored log file \"\"000000010000000100000025\"\" from archive\",,,,,,,,,\"\"....省略部分内容2013-07-02 06:36:47.400 CST,,,4059,,51d2047f.fdb,1,,2013-07-02 06:36:47 CST,,0,LOG,00000,\"started streaming WAL from primary at 1/43000000 on timeline 1\",,,,,,,,,\"\" 备注：此时备库从归档目录取到所需的 WAL 后开始拼命追主库，直到看到上面日志的最后一行信息时表示已完全追上主库。 验证3.1 主库123456789101112131415[pg93@redhatB ~]$ psqlpsql (9.3beta1)Type \"help\" for help.postgres=# select * from test_1 order by create_time desc limit 3; id | create_time ----+--------------------- 5 | 2013-07-02 06:38:07 5 | 2013-07-02 06:26:11 5 | 2013-07-02 06:26:09(3 rows)postgres=# select count(*) from test_1; count ------- 39(1 row) 3.2 备库123456789101112131415[pg93@redhat6 pg_root]$ psql psql (9.3beta1)Type \"help\" for help.postgres=# select * from test_1 order by create_time desc limit 3; id | create_time ----+--------------------- 5 | 2013-07-02 06:38:07 5 | 2013-07-02 06:26:11 5 | 2013-07-02 06:26:09(3 rows)postgres=# select count(*) from test_1; count ------- 39(1 row) 备注：此时在备库停掉过程中新增的数据也已经同步到备库了，恢复成功。 总结 对于比较繁忙的库，建议给 pg_xlog 分配较大的存储，从而能保留较多的 WAL 文件，在备节点停机维护后，能够获得更多的停库时间。 对于更繁忙的大数据库，例如 TB 级，如果没有足够的存储长期开启归档，那么至少在备节点需要停库维护时的这段时间把归档开启，否则当备库启来后追不上主库时，TB 级库重做 standby 的代价是可想而知。 参考 Archive Recovery Settings System Administration Functions PostgreSQL：使用 pg_basebackup 搭建流复制环境 How to estimate total number of WAL segments ?","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"PostgreSQL：使用 pg_basebackup 搭建流复制环境","slug":"20130702100240","date":"2013-07-02T02:02:40.000Z","updated":"2018-09-04T01:34:04.715Z","comments":true,"path":"20130702100240.html","link":"","permalink":"https://postgres.fun/20130702100240.html","excerpt":"","text":"早在 PostgreSQL 9.1 版就已新出 pg_basebackup 工具，用来搭建流复制备库，之前一直没有实践，今天补上。 传统的搭建流复制备库步骤为以下: select pg_start_backup(); 复制数据文件; select pg_stop_backup(); 而 pg_basebackup 则省略以上步骤，一步搞定，对于有多个数据目录的库来说，pg_basebackup 工具比上面步骤要简单多了，并且可以在线操作，下面演示下。 环境信息主机： 笔记本虚拟机系统： Red Hat Enterprise Linux Server release 6.2版本： PostgreSQL 9.3beta1主库IP： 192.168.1.36 主机名：redhatB备库IP： 192.168.1.35 主机名 redhat6备注： PostgreSQL 安装略。 主库上操作2.1 创建复制用户12345CREATE USER repuser REPLICATION LOGIN CONNECTION LIMIT 2 ENCRYPTED PASSWORD 'rep123us345er'; 2.2 设置 pg_hba.conf，添加以下1host replication repuser 192.168.1.35/32 md5 2.3 设置主库 postgresql.conf123456checkpoint_segments = 16archive_mode = onarchive_command = '/bin/date'max_wal_senders = 3wal_keep_segments = 16max_wal_senders = 3 备注：仅列出主要参数，其它参数根据实际情况设置。 2.4 重载配置文件12[pg93@redhatB ~]$ pg_ctl reload -D $PGDATAserver signaled 2.5 查看表空间目录123456789postgres=# \\db List of tablespaces Name | Owner | Location ---------------+----------+-------------------------------------pg_default | postgres |pg_global | postgres |tbs_francs | postgres | /database/pg93/pg_tbs/tbs_francstbs_source_db | postgres | /database/pg93/pg_tbs/tbs_source_db(4 rows) 2.6 查看数据目录12[pg93@redhatB pg_xlog]$ echo $PGDATA/database/pg93/pg_root 备注：先查看表空间目录和数据目录，因为这些目录需要在备库主机上手工创建。 备库上操作3.1 创建目录并赋权12345678[root@redhat6 pgsql9.3beta1]# mkdir -p /database/pg93/pg_tbs/tbs_francs[root@redhat6 pgsql9.3beta1]# mkdir -p /database/pg93/pg_tbs/tbs_source_db[root@redhat6 pgsql9.3beta1]# mkdir -p /database/pg93/pg_root [root@redhat6 pgsql9.3beta1]# chown -R pg93:pg93 /database/pg93/pg_tbs/tbs_francs[root@redhat6 pgsql9.3beta1]# chown -R pg93:pg93 /database/pg93/pg_tbs/tbs_source_db[root@redhat6 pgsql9.3beta1]# chown -R pg93:pg93 /database/pg93/pg_root[root@redhat6 pgsql9.3beta1]# chmod 0700 /database/pg93/pg_root 3.2 创建 .pgpass12345[pg93@redhat6 ~]$ cat .pgpass192.168.1.36:1925:replication:repuser:rep123us345er [pg93@redhat6 ~]$ chmod 0600 .pgpass备注：注意 .pgpass文件权限为 0600。 3.3 使用 pg_basebackup 生成备库12345678[pg93@redhat6 pg93]$ pg_basebackup -D /database/pg93/pg_root -Fp -Xs -v -P -h 192.168.1.36 -p 1925 -U repuser transaction log start point: 1/1B000024 on timeline 1pg_basebackup: starting background WAL receiver651493/651493 kB (100%), 3/3 tablespaces transaction log end point: 1/1B0000DCpg_basebackup: waiting for background process to finish streaming ...pg_basebackup: base backup completed 备注：这时表空间目录，$PGDATA 目录已经复制过来了，这里使用了 -X 参数，在备份完成之后，会到主库上收集 pg_basebackup 执行期间产生的 WAL 日志，在 9.2 版本之后支持 -Xs 即stream 形式，这种模式不需要收集主库的 WAL 文件，而能以 stream 复制方式直接追赶主库。 3.4 设置从库 postgresql.conf1hot_standby = on 3.5 设置从库 recovery.conf 3.5.1 生成 recovery.conf1[pg93@redhat6 pg_root]$ cp /opt/pgsql9.3beta1/share/recovery.conf.sample recovery.conf 3.5.2 修改以下参数123standby_mode = onprimary_conninfo = 'host=192.168.1.36 port=1925 user=repuser'trigger_file = '/database/pg93/pg_root/postgresql.trigger.1925' 3.6 启服务12[pg93@redhat6 pg_root]$ pg_ctl start -D $PGDATAserver starting 3.7 查看备库进程12345678[pg93@redhat6 pg_xlog]$ ps -ef | grep pg93pg93 31398 1 0 21:09 pts/0 00:00:00 /opt/pgsql9.3beta1/bin/postgres -D /database/pg93/pg_rootpg93 31399 31398 0 21:09 ? 00:00:00 postgres: logger process pg93 31400 31398 0 21:09 ? 00:00:00 postgres: startup process waiting for 00000001000000010000001Apg93 31401 31398 0 21:09 ? 00:00:00 postgres: checkpointer process pg93 31402 31398 0 21:09 ? 00:00:00 postgres: writer process pg93 31403 31398 0 21:09 ? 00:00:00 postgres: stats collector process pg93 31404 31398 0 21:09 ? 00:00:00 postgres: wal receiver process 3.8 查看主库进程12345678910[pg93@redhatB pg_xlog]$ ps -ef | grep pg93pg93 2504 1 0 Jun28 ? 00:00:26 /opt/pgsql9.3beta1/bin/postgres -D /database/pg93/pg_rootpg93 2505 2504 0 Jun28 ? 00:00:00 postgres: logger process pg93 2507 2504 0 Jun28 ? 00:00:08 postgres: checkpointer process pg93 2508 2504 0 Jun28 ? 00:00:28 postgres: writer process pg93 2509 2504 0 Jun28 ? 00:00:08 postgres: wal writer process pg93 2510 2504 0 Jun28 ? 00:00:19 postgres: autovacuum launcher process pg93 2511 2504 0 Jun28 ? 00:00:00 postgres: archiver process last was 000000010000000100000019.00000024.backuppg93 2512 2504 0 Jun28 ? 00:00:44 postgres: stats collector process pg93 31898 2504 0 21:09 ? 00:00:00 postgres: wal sender process repuser 192.168.1.35(39545) idle 测试4.1 主库123456789101112131415[pg93@redhatB ~]$ psqlpsql (9.3beta1)Type \"help\" for help. postgres=# create table test_1 (id int4,create_time timestamp(0) without time zone);CREATE TABLE postgres=# insert into test_1 values (1,now());INSERT 0 1 postgres=# select * from test_1;id | create_time ----+--------------------- 1 | 2013-07-01 21:15:34(1 row) 4.2 备库1234567891011[pg93@redhat6 pg_xlog]$ psqlpsql (9.3beta1)Type \"help\" for help. postgres=# select * from test_1 postgres=# select * from test_1 ;id | create_time ----+--------------------- 1 | 2013-07-01 21:15:34(1 row) 备注：流复制搭建完成。 附: pg_basebackup 参数12345678910111213141516171819202122232425262728293031323334353637[pg93@redhat6 pg_xlog]$ pg_basebackup --helppg_basebackup takes a base backup of a running PostgreSQL server. Usage: pg_basebackup [OPTION]... Options controlling the output: -D, --pgdata=DIRECTORY receive base backup into directory -F, --format=p|t output format (plain (default), tar) -R, --write-recovery-conf write recovery.conf after backup -x, --xlog include required WAL files in backup (fetch mode) -X, --xlog-method=fetch|stream include required WAL files with specified method -z, --gzip compress tar output -Z, --compress=0-9 compress tar output with given compression level General options: -c, --checkpoint=fast|spread set fast or spread checkpointing -l, --label=LABEL set backup label -P, --progress show progress information -v, --verbose output verbose messages -V, --version output version information, then exit -?, --help show this help, then exit Connection options: -d, --dbname=CONNSTR connection string -h, --host=HOSTNAME database server host or socket directory -p, --port=PORT database server port number -s, --status-interval=INTERVAL time between status packets sent to server (in seconds) -U, --username=NAME connect as specified database user -w, --no-password never prompt for password -W, --password force password prompt (should happen automatically) Report bugs to &lt;pgsql-bugs@postgresql.org&gt;. 参考 pg_basebackup PG: Setting up streaming log replication (Hot Standby )","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"PostgreSQL：客户端会话中断后，会话中的语句会中断吗？","slug":"20130627133934","date":"2013-06-27T05:39:34.000Z","updated":"2018-12-04T00:27:54.792Z","comments":true,"path":"20130627133934.html","link":"","permalink":"https://postgres.fun/20130627133934.html","excerpt":"","text":"今天 BBS 有朋友问到这么一个问题：假如在客户端执行某一查询语句需要 20 秒时间，而在执行过程中由于某种原因客户端掉线了，此时查询语句在数据库服务端是被终止还是继续运行？ 带着这个问题，先做个简单的测试，看看结果如何？ 客户端异常掉线测试测试严格按以下步骤进行： 1.1 会话1123456789101112131415161718[pg93@redhatB ~]$ psql francs francspsql (9.3beta1)Type \"help\" for help. francs=&gt; begin;BEGINfrancs=&gt; select pg_backend_pid(); pg_backend_pid ---------------- 30079(1 row) francs=&gt; timingTiming is on. francs=&gt; select pg_sleep(120);... 备注：这里还在执行过程中。 1.2 另开一窗口查看会话1 状态123 [pg93@redhatB ~]$ ps -ef | grep 30079pg93 30079 2504 0 11:47 ? 00:00:00 postgres: francs francs [local] SELECT pg93 30104 29293 0 11:48 pts/1 00:00:00 grep 30079 1.3 中断会话1 这里模拟客户端异常，直接右键将会话1 的 SecureCrt 客户端关闭。 1.4 继续查看会话1 状态123 [pg93@redhatB ~]$ ps -ef | grep 30079pg93 30079 2504 0 11:47 ? 00:00:00 postgres: francs francs [local] SELECT pg93 30104 29293 0 11:48 pts/1 00:00:00 grep 30079 备注：说明客户端会话被关闭后，查询 SQL 依然在数据库服务器上执行，而没被中断。 总结 以上测试的结果是：客户端中断后，语句依然在数据库端执行，有兴趣的朋友可以模拟更多的场景。 当语句在客户端发送到数据库后，数据库忙着跑语句，而不会检查客户端的状态，因此客户端掉了，语句依然能执行，在程序中需要加入连接异常时的处理代码。 当维护人员通过客户端 (psql, pgAdmin3) 时发出大的统计查询时，如果会话异常中断，此时最好联系 DBA ，查下会话是否还在跑，大的统计 SQL 可能要跑好几小时，会严重影响数据库性能。 参考 Clients disconnect but query still runs","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL：流复制环境清除 Pg_xlog 日志","slug":"20130621155115","date":"2013-06-21T07:51:15.000Z","updated":"2018-09-04T01:34:04.574Z","comments":true,"path":"20130621155115.html","link":"","permalink":"https://postgres.fun/20130621155115.html","excerpt":"","text":"今天一流复制( Streaming Replication) 数据库一目录空间告警，经查是 pg_xlog 目录 WAL 日志文件过多，已超过阀值。 pg_xlog 是数据库重做日志，非常重要，不能直接清理，否则给数据库带来巨大灾难，以下是流复制环境下清理 pg_xlog 的方法。 查看数据库参数查看当前 checkpoint_segments 和 wal_keep_segments123456789101112131415postgres@db&gt; psql -h 127.0.0.1psql (9.2.4)Type \"help\" for help. postgres=&gt; show checkpoint_segments;checkpoint_segments---------------------128(1 row postgres=&gt; show wal_keep_segments ;wal_keep_segments-------------------4096(1 row) 查看 pg_xlog 文件数量123postgres@db&gt; cd $PGDATApostgres@db&gt; ll pg_xlog/ | wc -l4156 备注：这比 wal_keep_segments 设置值大些，但这是正常的，因为 4156 &lt; 4096 +128(checkpoint_segments)+1 至于 pg_xlog 最大文件数量的估算参考之前 blog: How to estimate total number of WAL segments ? 修改 postgreql.conf1wal_keep_segments = 2048 备注：将参数 wal_keep_segments 由原来的 4096 调整到 2048。 重载配置文件12postgres@db&gt; pg_ctl reload -D $PGDATAserver signaled 再次查看 wal_keep_segments12345postgres=# show wal_keep_segments ;wal_keep_segments-------------------2048(1 row) 查看 pg_xlog 文件数量123postgres@db&gt; cd $PGDATApostgres@db&gt; ll pg_xlog/ | wc -l4156 备注：此时 pg_xlog 日志文件依然没被删除，接着往下操作。 执行 Checkpoint12postgres=# checkpoint;CHECKPOINT 再次查看 pg_xlog 文件数量12postgres@db&gt; ll $PGDATA/pg_xlog/ | wc -l2310 备注：手动执行 checkpoint 后，大概一半 pg_xlog 日志已被删除，空间使用率降下去了，也可以不手动操作，因为 checkpoint 操作数据库会自动执行，执行频率由参数 checkpoint_timeout 控制。 参考 How to estimate total number of WAL segments ? formula about the number of WAL files","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"},{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/tags/PG高可用性/"}]},{"title":"PostgreSQL: 关于中文排序问题","slug":"20130621112126","date":"2013-06-21T03:21:26.000Z","updated":"2018-09-04T01:34:04.512Z","comments":true,"path":"20130621112126.html","link":"","permalink":"https://postgres.fun/20130621112126.html","excerpt":"","text":"今天有同事问到中文排序问题，中文排序问题没有怎么深究，主要参考 bbs 和德哥的帖子,做些测试记录下。 环境信息1.1 数据库信息12345francs=&gt; select datname,pg_encoding_to_char(encoding),datcollate,datctype from pg_database where datname='francs';datname | pg_encoding_to_char | datcollate | datctype---------+---------------------+------------+----------francs | UTF8 | C | C(1 row) 备注：测试库字符集 UTF8，数据库排序规则：C，测试版本: PostgreSQL 9.2。 1.2 创建测试表123456789101112CREATE TABLE tbl_area ( id character varying(32),name character varying(1024) ); INSERT INTO tbl_area (id,name) VALUES ('000001', '香港');INSERT INTO tbl_area (id,name) VALUES ('000002', '澳门');INSERT INTO tbl_area (id,name) VALUES ('0100', '呼和浩特');INSERT INTO tbl_area (id,name) VALUES ('011200', '苏尼特右旗');INSERT INTO tbl_area (id,name) VALUES ('011300', '苏尼特左旗');INSERT INTO tbl_area (id,name) VALUES ('011600', '清水河县');INSERT INTO tbl_area (id,name) VALUES ('011700', '武川县');INSERT INTO tbl_area (id,name) VALUES ('0118', '乌兰察布');INSERT INTO tbl_area (id,name) VALUES ('011800', '四王子旗');INSERT INTO tbl_area (id,name) VALUES ('012000', '集宁区'); 1.3 根据 name 排序1234567891011121314francs=&gt; select * From tbl_area order by name; id | name --------+------------0118 | 乌兰察布0100 | 呼和浩特011800 | 四王子旗011700 | 武川县011600 | 清水河县000002 | 澳门011200 | 苏尼特右旗011300 | 苏尼特左旗012000 | 集宁区000001 | 香港(10 rows) 备注：根据中文字段 name 排序的结果不是我们想要的，参考 bbs 帖子，可以使用 convert_to ()函数转换成目标编码的 bytea 形式解决. 方法一: 使用 Convert_to 函数2.1 convert_to 函数123456789101112francs=&gt; \\df convert_to List of functions Schema | Name | Result data type | Argument data types | Type ------------+------------+------------------+---------------------+--------pg_catalog | convert_to | bytea | text, name | normal(1 row) francs=&gt; select convert_to('香港','gbk');convert_to------------xcfe3b8db(1 row 备注：输出为二进制形式。 2.2 根据 convert_to() 函数输出排序1234567891011121314francs=&gt; select * From tbl_area order by convert_to(name,'gbk'); id | name --------+------------000002 | 澳门0100 | 呼和浩特012000 | 集宁区011600 | 清水河县011800 | 四王子旗011200 | 苏尼特右旗011300 | 苏尼特左旗0118 | 乌兰察布011700 | 武川县000001 | 香港(10 rows) 备注：使用 convert_to() 函数转换后得到了想要的结果。 方法二: 排序时指定字段的 Collate由于创建数据库后不支持更改数据库的 LC_COLLATE （排序规则），但在 order by 时可以指定字段的 Collate 属性，如下： 3.1 排序时指定字段的 collate1234567891011121314151617181920212223242526272829303132333435363738394041424344francs=&gt; select * From tbl_area order by name; id | name --------+------------0118 | 乌兰察布0100 | 呼和浩特011800 | 四王子旗011700 | 武川县011600 | 清水河县000002 | 澳门011200 | 苏尼特右旗011300 | 苏尼特左旗012000 | 集宁区000001 | 香港(10 rows) francs=&gt; select * From tbl_area order by name collate \"C\"; id | name --------+------------0118 | 乌兰察布0100 | 呼和浩特011800 | 四王子旗011700 | 武川县011600 | 清水河县000002 | 澳门011200 | 苏尼特右旗011300 | 苏尼特左旗012000 | 集宁区000001 | 香港(10 rows) francs=&gt; select * From tbl_area order by name collate \"zh_CN.utf8\"; id | name --------+------------000002 | 澳门0100 | 呼和浩特012000 | 集宁区011600 | 清水河县011800 | 四王子旗011200 | 苏尼特右旗011300 | 苏尼特左旗0118 | 乌兰察布011700 | 武川县000001 | 香港(10 rows) 备注：排序规则“C” 得到的结果不是想要的， “zh_CN.utf8” 得到了想要的结果，中文字段排序建议使用 “zh_CN.utf8” 排序规则。 参考中文排序涉及到字符集，是一个比较复杂的话题，这里不研究了， 以下几篇帖子对中文排序有 更深入的描述，强烈推荐。 http://blog.163.com/digoal@126/blog/static/163877040201173003547236/ http://www.postgresql.org/docs/9.2/static/collation.html","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL: 关于 psql 的 Prompting 设置 ","slug":"20130620165441","date":"2013-06-20T08:54:41.000Z","updated":"2018-09-04T01:34:04.449Z","comments":true,"path":"20130620165441.html","link":"","permalink":"https://postgres.fun/20130620165441.html","excerpt":"","text":"今天在维护一个开发库时发现个奇怪的现象，psql 登陆到数据库后显示如下： psql 连接示例12345[postgres@xxx_xx_x_xxx ~]$ psqlpsql (8.4.3)Type \"help\" for help. SQL&gt;postgres@ postgres 15:47:47=# 备注：红色字体部分为特殊的部分，一般不会显示这么多信息，这一串是什么内容？表示的是什么？在哪里可以设置呢？ 之后会做分析。 psql 常规显示12345[pg92@redhatB ~]$ psqlpsql (9.2.1)Type \"help\" for help. postgres=# 备注：默认情况 psql 连接库后不会显示之前那么多信息，猜想这可能可以通过 psql 选项设置，带着这个问题查阅了 psql 相关文档，文档上关于 psql 的内容很多，高兴的是每次查阅 psql 文档都能学到点新东西。 查看 .psqlrc 文件12[postgres@xxx_xx_x_xxx src]$ cat ~/.psqlrcset PROMPT1 'SQL&gt;%n@ %/ %`date +%H:%M:%S`%R%#' 备注：在查看了手册后，知道可以在 .psqlrc 文件中保存设置(安装的开发人员挺有才的。)，以便不用 每次设置。那这串表示什么意思呢？ 参考下面的解释就明白了。 常见 Prompting 参数Prompting 参数以百分号 % 打头，主要有如下： Prompting 参数 含义 %M 数据库主机全名，如果通过 UNIX Socket 连接则显示为 [local] %m 也表示数据库主机名，会截断第一个 . 后的内容 %&gt; 数据库端口号 %n 会话的用户名 %/ 当前数据库名 %# 如果是超级用户显示为 #，否则显示为 &gt; %R 在prompt 1中，通常显示 = ，单用户模式显示为 ^，如果会话被断开显示为 !, 等等。 备注： 默认的 prompt 的设置为 ‘%/%R%# ‘。 5 更改成默认 prompt 形式:123456789101112[postgres@xxx_xx_x_xxx ~]$ psqlpsql (8.4.3)Type \"help\" for help. SQL&gt;postgres@ postgres 16:23:09=#select current_database(),current_user;current_database | current_user------------------+--------------postgres | postgres(1 row) SQL&gt;postgres@ postgres 16:23:17=#set PROMPT1 '%/%R%# 'postgres=# 备注：更改成我们熟悉的默认形式了，看起来舒服多了，再来看看其它的设置。 显示用户名和主机名123456[pg92@redhatB ~]$ psql -h 192.168.1.36psql (9.2.1)Type \"help\" for help. postgres=# set PROMPT1 '%n@%M %~%R%# 'postgres@192.168.1.36 ~=# 备注：可以根据自己的喜好调整输出的内容，我个人还是偏好默认的设置。 参考 psql","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"psql","slug":"psql","permalink":"https://postgres.fun/tags/psql/"}]},{"title":"PGCN OpenCamp 201304: 深圳 ","slug":"20130620142957","date":"2013-06-20T06:29:57.000Z","updated":"2018-09-04T01:34:04.387Z","comments":true,"path":"20130620142957.html","link":"","permalink":"https://postgres.fun/20130620142957.html","excerpt":"","text":"第一次在 PostgreSQL 官网的事件列表上看到关于中国 PostgreSQL 活动的情况( 不知道以前确实没有写，还是我没注意 )，挺高兴的，原文如下：来自：http://www.postgresql.org/about/event/1588/ PGCN OpenCamp 2013#4 Date:2013-06-22Location: ShenZhen, ShenZhen, ChinaLanguage: ChinesePGCN OpenCamp is a Chinese conference for anyone interested in PostgreSQL. We will be traveling across China conference to promote PostgreSQL in China, each will be invited to 1-2 technical experts to share technical topics. You can through our website: http://www.postgres.org.cn to learn more with our relevant information and event details.PGCN OpenCamp 2013 Topics: 1. PostgreSQL Architecture 2. Planing your backup and recovery 3. PITR DEMOPosted by Postgres China (scott.siu@postgres.org.cn).Note:The PostgreSQL Global Development Group does not endorse any events run by third parties. No guarantee of the quality of events is offered whatsoever. 这次会有来自社区的阿弟和泥鳅 给大家分享，议题如下： 报名地址：http://www.headin.cn/Themes/Activity/Details?activityId=51a70568869d082330f46e0a 深圳的朋友有机会与这两位大神近距离交流了。","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL：关于 \" Select MAX(id) From Table_name \" 的优化与分析","slug":"20130618171615","date":"2013-06-18T09:16:15.000Z","updated":"2018-09-04T01:34:04.324Z","comments":true,"path":"20130618171615.html","link":"","permalink":"https://postgres.fun/20130618171615.html","excerpt":"","text":"今天在一重要生产库上发现一个 SQL 执行较慢，严重影响数据库性能，以下是优化过程： 一 优化过程1.1 出现性能问题的 SQL1SELECT max(duser2) FROM mpt_table; 1.2 老的 PLAN1234567ndroid_market=&gt; explain analyze SELECT max(duser2) FROM mpt_table; QUERY PLAN -----------------------------------------------------------------------------------------------------------------------------------Aggregate (cost=45159.95..45159.96 rows=1 width=4) (actual time=1010.232..1010.233 rows=1 loops=1) -&gt; Seq Scan on mpt_table (cost=0.00..40891.56 rows=1707356 width=4) (actual time=0.019..413.291 rows=1707488 loops=1)Total runtime:1010.278 ms(3 rows) 备注：走了全表扫，执行时间为 1010.278 ms. 并且字段 duser2 上没加索引，经分析 duser2 的选择性较好，于是创建索引. 1.3 创建索引1create index concurrently idx_mpt_table_duser2 on mpt_table using btree (duser2); 1.4 新的 PLAN12345678910android_market=&gt; explain analyze SELECT max(duser2) FROM mpt_table; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------Result (cost=0.03..0.04 rows=1 width=0) (actual time=0.093..0.093 rows=1 loops=1) InitPlan 1 (returns $0) -&gt; Limit (cost=0.00..0.03 rows=1 width=4) (actual time=0.091..0.091 rows=1 loops=1) -&gt;Index Scan Backwardusing idx_mpt_table_duser2 on mpt_table (cost=0.00..58384.34 rows=1707488 width=4) (actual time=0.090..0.090 rows=1 loops=1) Index Cond: (duser2 IS NOT NULL)Total runtime: 0.134 ms(6 rows) 备注：在 duser2 字段上创建索引后，走了 Index Scan Backward 扫描，并且执行时间为 0.134 ms，非常的快，接着想测试下 oracle 对 max() 的处理。 二 Oracle 测试2.1 创建测试表并导入数据1234567891011SQL&gt; create table test_max_oracle(id integer);Table created SQL&gt; begin 2 for i in 1..5000000 loop 3 insert into test_max_oracle (id) values (i); 4 end loop; 5 commit; 6 end; 7 /PL/SQL procedure successfully completed 2.2 创建索引并表分析12345SQL&gt; create index idx_test_max_id on test_max_oracle (id);Index created SQL&gt; execute dbms_stats.gather_table_stats(ownname=&gt;'skytf',tabname=&gt;'test_max_oracle',cascade=&gt; true);PL/SQL procedure successfully completed 2.3 执行时间12345678910111213SQL&gt; set timi on; SQL&gt; select max(id) from test_max_oracle; MAX(ID)---------- 5000000Executed in 0.031 secondsSQL&gt; select max(id) from test_max_oracle; MAX(ID)---------- 5000000Executed in0.031 seconds 备注：最短执行时间为 31 ms 左右。 2.4 执行计划1234567891011121314151617181920212223242526272829303115:34:24 SKYTF@skytf&gt; select max(id) from test_max_oracle; MAX(ID)---------- 5000000已用时间: 00: 00: 00.01 执行计划----------------------------------------------------------Plan hash value: 1160081309 ----------------------------------------------------------------------------------------------| Id | Operation | Name | Rows | Bytes | Cost (%CPU)| Time |----------------------------------------------------------------------------------------------| 0 | SELECT STATEMENT | | 1 | 5 | 1705 (3)| 00:00:21 || 1 | SORT AGGREGATE | | 1 | 5 | | || 2 | INDEX FULL SCAN(MIN/MAX)| IDX_TEST_MAX_ID | 5000K| 23M| | |---------------------------------------------------------------------------------------------- 统计信息---------------------------------------------------------- 0 recursive calls 0 db block gets 3 consistent gets 0 physical reads 0 redo size 407 bytes sent via SQL*Net to client 385 bytes received via SQL*Net from client 2 SQL*Net roundtrips to/from client 0 sorts (memory) 0 sorts (disk) 1 rows processed 备注：走的是 INDEX FULL SCAN，全索引扫描会根据索引的顺序访问所有的索引 block, 这种扫描方式类似全表扫，效率不高。下面引用这段描述： 2.5 关于 INDEX FULL SCAN In a full index scan, the database reads the entire index in order. A full index scan is available if a predicate (WHERE clause) in the SQL statement references a column in the index, and in some circumstances when no predicate is specified. A full scan can eliminate sorting because the data is ordered by index key. 三 PostgreSQL 测试3.1 创建测试表并插入数据1234567891011francs=&gt; create table test_max_pg(id int4);CREATE TABLE francs=&gt; insert into test_max_pg select generate_series(1,5000000);INSERT 0 5000000francs=&gt; create index idx_test_max_id on test_max_pg using btree (id);CREATE INDEX francs=&gt; analyze test_max_pg;ANALYZE 3.2 执行时间1234567891011121314151617181920212223francs=&gt; select max(id) from test_max_pg; max ---------5000000(1 row) Time: 13.960 ms francs=&gt; select max(id) from test_max_pg; max ---------5000000(1 row) Time: 2.493 ms francs=&gt; select max(id) from test_max_pg; max ---------5000000(1 row) Time: 1.233 ms 备注：最短执行时间只花了 1.233 ms 左右，比 oracle 测试结果要好些，这里不是对比 oracle 和 pg 性能，只是争对 max() 函数应用场景分析两种数据库的处理方式。 3.3 执行计划123456789francs=&gt; explain select max(id) from test_max_pg; QUERY PLAN --------------------------------------------------------------------------------------------------------------------------Result (cost=0.03..0.04 rows=1 width=0) InitPlan 1 (returns $0) -&gt; Limit (cost=0.00..0.03 rows=1 width=4) -&gt; Index Only Scan Backwardusing idx_test_max_id on test_max_pg (cost=0.00..151011.64 rows=5000000 width=4) Index Cond: (id IS NOT NULL)(5 rows) 备注：这里走的是 Index Only Scan Backward，由于索引是顺序排序的，这种扫描方式直接后向读取索引项，而不需要扫描整个索引 page，因此效率比”INDEX FULL SCAN “要高些。 四 补充 这里的测试版本： Oracle 10.2.0.1 ; PostgreSQL 9.2。 Oracle 的索引扫描方式很多，例如 index rang scan，index unique scan，index fast full scan等，比 PostgreSQL 的索引访问方式多些，这里不再描述。","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"PGCon：2013 Ottawa PostgreSQL 大会","slug":"20130609155223","date":"2013-06-09T07:52:23.000Z","updated":"2018-09-04T01:34:04.262Z","comments":true,"path":"20130609155223.html","link":"","permalink":"https://postgres.fun/20130609155223.html","excerpt":"","text":"以下是 2013 年 5 月 在 Ottawa 举行的 PostgreSQL 大会的部分 PPT，有兴趣的朋友可以下载看看，更多内容访问： http://www.pgcon.org/2013/schedule/ A Tour of PostgreSQL Data Typeshttp://www.pgcon.org/2013/schedule/attachments/269_tour-of-postgresql-data-types.pdf SQL HINTS, TIPS, TRICKSAND TUNINGhttp://www.pgcon.org/2013/schedule/attachments/281_SQL_tips_tricks_tuning.pdf One step forward true json data type.Nested hstore with arrays supporthttp://www.pgcon.org/2013/schedule/attachments/280_hstore-pgcon-2013.pdf PostgreSQL 9 High Availability With Linux-HAhttp://www.pgcon.org/2013/schedule/attachments/279_PostgreSQL_9_and_Linux_HA.pdf","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL：Oid2name 介绍","slug":"20130605111619","date":"2013-06-05T03:16:19.000Z","updated":"2018-09-04T01:34:04.215Z","comments":true,"path":"20130605111619.html","link":"","permalink":"https://postgres.fun/20130605111619.html","excerpt":"","text":"PostgreSQL 提供 oid2name 客户端程序，用来解析数据目录里的文件，平常用得比较少，这里简单介绍下。 Oid2name 手册介绍1.1 手册介绍12345678910Name oid2name -- resolve OIDs and file nodes in a PostgreSQL data directory Synopsis oid2name [option...] Description oid2name is a utility program that helps administrators to examine the file structure used by PostgreSQL. To make use of it, you need to be familiar with the database file structure, which is described in Chapter 56. 1.2 oid2name 参数12345678910111213141516171819202122232425[pg93@redhatB pg_tblspc]$ oid2name --helpoid2name helps examining the file structure used by PostgreSQL. Usage: oid2name [OPTION]... Options: -d DBNAME database to connect to -f FILENODE show info for table with given file node -H HOSTNAME database server host or socket directory -i show indexes and sequences too -o OID show info for table with given OID -p PORT database server port number -q quiet (dont show headers) -s show all tablespaces -S show system objects too -t TABLE show info for named table -U NAME connect as specified database user -V, --version output version information, then exit -x extended (show additional columns) -?, --help show this help, then exit The default action is to show all database OIDs. Report bugs to &lt;pgsql-bugs@postgresql.org&gt;. Oid2name 使用2.1 列出所有库123456789[pg93@redhatB pg_tblspc]$ oid2nameAll databases: Oid Database Name Tablespace------------------------------------- 16386 francs tbs_francs 12895 postgres pg_default 16390 source_db tbs_source_db 12890 template0 pg_default 1 template1 pg_default 2.2 列出所有表空间12345678[pg93@redhatB pg_tblspc]$ oid2name -sAll tablespaces: Oid Tablespace Name------------------------ 1663 pg_default 1664 pg_global 16385 tbs_francs 16389 tbs_source_db 2.3 进入数据目录123456789101112[pg93@redhatB 16386]$ cd $PGDATA/pg_tblspc/16385[pg93@redhatB 16385]$ cd PG_9.3_201305061/16386/ [pg93@redhatB 16386]$ ll | tail -n 8-rw-------. 1 pg93 pg93 35M Jun 3 17:47 16702-rw-------. 1 pg93 pg93 161M Jun 3 16:22 16703-rw-------. 1 pg93 pg93 64K Jun 3 15:45 16703_fsm-rw-------. 1 pg93 pg93 8.0K Jun 3 16:14 16703_vm-rw-------. 1 pg93 pg93 35M Jun 3 15:45 16704-rw-------. 1 pg93 pg93 512 May 15 10:30 pg_filenode.map-rw-------. 1 pg93 pg93 98K Jun 3 10:26 pg_internal.init-rw-------. 1 pg93 pg93 4 May 15 10:30 PG_VERSION 2.4 查看 16703 文件是什么12345[pg93@redhatB 16386]$ oid2name -d francs -f 16703From database \"francs\": Filenode Table Name------------------------- 16703 test_not_full 2.5 显示更多信息12345[pg93@redhatB 16386]$ oid2name -d francs -f 16703 -xFrom database \"francs\": Filenode Table Name Oid Schema Tablespace---------------------------------------------------- 16703 test_not_full 16688 francs tbs_francs 2.6 根据 oid 查对表信息1234567891011[pg93@redhatB ~]$ psql francs francs -c \"select oid,relname from pg_class where relname='test_1'\"; oid | relname-------+---------16457 | test_1(1 row) [pg93@redhatB ~]$ oid2name -d francs -o 16457From database \"francs\": Filenode Table Name---------------------- 16457 test_1 参考 oid2name","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：浅析 ACL 权限","slug":"20130603104434","date":"2013-06-03T02:44:34.000Z","updated":"2018-09-04T01:34:04.152Z","comments":true,"path":"20130603104434.html","link":"","permalink":"https://postgres.fun/20130603104434.html","excerpt":"","text":"在 PostgreSQL 维护过程中，表，视图的权限查看是很平常的工作，本文从一张视图的权限开始，简单的介绍权限查看的几种方法。 一张视图1234567891011121314[pg93@redhatB ~]$ psql francs francspsql (9.3beta1)Type \"help\" for help. francs=&gt; \\d+ view1_test View \"francs.view1_test\"Column | Type | Modifiers | Storage | Description--------+-----------------------+-----------+----------+-------------id | integer | | plain |name | character varying(64) | | extended |View definition:SELECT test_view1.id, test_view1.name FROM test_view1; 备注：假设系统有如上视图，如何查看这张视图的权限呢？大概有以下几种方法： 方法一: 通过 \\dp 元子命令1234567francs=&gt; \\dp view1_test Access privilegesSchema | Name | Type | Access privileges | Column access privileges--------+------------+------+-----------------------+--------------------------francs | view1_test | view | francs=arwdDxt/francs+| | | | select_only=rw/francs |(1 row) 备注：\\dp 后接表名或视图名查看对像权限，个人推荐这种方法，因为简单，但要会看输出的格式。例如，什么是 “arwd…？” ， 详见附一： ACL 权限列表。 方法二: 通过 aclexplode 函数2.1 查看 acl12345francs=&gt; select oid,relname ,relacl from pg_class where relname='view1_test'; oid | relname | relacl -------+------------+-----------------------------------------------16492 | view1_test | &#123;francs=arwdDxt/francs,select_only=rw/francs&#125;(1 row) 备注： pg_class.relacl 输出实际上和 dp 显示的值类似。 2.2 解析 acl 权限12345678910111213francs=&gt; select * From aclexplode('&#123;francs=arwdDxt/francs,select_only=rw/francs&#125;');grantor | grantee | privilege_type | is_grantable---------+---------+----------------+-------------- 16384 | 16384 | INSERT | f 16384 | 16384 | SELECT | f 16384 | 16384 | UPDATE | f 16384 | 16384 | DELETE | f 16384 | 16384 | TRUNCATE | f 16384 | 16384 | REFERENCES | f 16384 | 16384 | TRIGGER | f 16384 | 16496 | SELECT | f 16384 | 16496 | UPDATE | f(9 rows) 备注：aclexplode 函数将 acl 转换成易读的格式。 2.3 找到对应用户123456francs=&gt; select oid,rolname from pg_roles where oid in (16384,16496); oid | rolname -------+-------------16384 | francs16496 | select_only(2 rows) 方法三: 通过 table_privileges 视图3.1 查看 information_schema.table_privileges1234567891011121314francs=&gt; select grantor,grantee,table_schema,table_name,privilege_type from information_schema.table_privilegesfrancs-&gt; where table_name='view1_test';grantor | grantee | table_schema | table_name | privilege_type---------+-------------+--------------+------------+----------------francs | francs | francs | view1_test | INSERTfrancs | francs | francs | view1_test | SELECTfrancs | francs | francs | view1_test | UPDATEfrancs | francs | francs | view1_test | DELETEfrancs | francs | francs | view1_test | TRUNCATEfrancs | francs | francs | view1_test | REFERENCESfrancs | francs | francs | view1_test | TRIGGERfrancs | select_only | francs | view1_test | SELECTfrancs | select_only | francs | view1_test | UPDATE(9 rows) 备注：上面格式简单多了，用户 select_only 对视图 view1_test 只有 SELECT，UPDATE 权限。 附: ACL 权限列表 Relacl Code Privilege Name a INSERT r SELECT w UPDATE d DELETE D TRUNCATE x REFERENCES t TRIGGER arwdDxt ALL 备注：建议牢记以上权限代码，才能完全看懂 dp 输出。 参考 Securing Tables WAITING FOR 9.3 – IMPLEMENT SQL-STANDARD LATERAL SUBQUERIES table_privileges grant","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：也谈 pg_dumpall","slug":"20130531133448","date":"2013-05-31T05:34:48.000Z","updated":"2018-09-04T01:34:04.090Z","comments":true,"path":"20130531133448.html","link":"","permalink":"https://postgres.fun/20130531133448.html","excerpt":"","text":"pg_dumpall 可以导出所有数据库，类似 pg_dump，但用得非常少，原因可能是多方面的，例如以下： pg_dumpall 导出所有数据库到一个脚本文件，维护不方便; pg_dumpall 依次导出所有库，总耗时比并行 pg_dump 各个库要多; pg_dumpall 仅支持导出文件格式，还原时不能使用 pg_restore 有效使用并行等参数。 诸如以上不足之处，使得 .pg_dumpall. 使用非常有限; 尽管如此，这两天看文档时，发现 pg_dumpall 在取全局对像时非常方便，例如取数据库上的表空间(tablespace)，用户(roles)等，这在数据库迁移时非常有用，下面以 9.2 版本为例简单演示下： 导出所有 Role 和 Tablespace1.1 导出 role 和 tablespace1[pg92@redhatB tf]$ pg_dumpall -g &gt; pg92_globle.sql 1.2 查看 pg92_globle 文件123456789101112131415161718192021222324252627282930313233[pg92@redhatB tf]$ view pg92_globle.sql ---- PostgreSQL database cluster dump-- SET client_encoding = 'UTF8';SET standard_conforming_strings = on; Roles CREATE ROLE francs;ALTER ROLE francs WITH NOSUPERUSER NOINHERIT NOCREATEROLE NOCREATEDB LOGIN NOREPLICATION PASSWORD 'md53dd797adaf7158f9625ccda805c4d881';CREATE ROLE londiste_reader;ALTER ROLE londiste_reader WITH NOSUPERUSER INHERIT NOCREATEROLE NOCREATEDB NOLOGIN NOREPLICATION;.... Tablespaces CREATE TABLESPACE tbs_skytf OWNER postgres LOCATION '/database/pg92/pgdata/pg_tbs/tbs_skytf';REVOKE ALL ON TABLESPACE tbs_skytf FROM PUBLIC;REVOKE ALL ON TABLESPACE tbs_skytf FROM postgres;GRANT ALL ON TABLESPACE tbs_skytf TO postgres;GRANT ALL ON TABLESPACE tbs_skytf TO skytf;...CREATE TABLESPACE tbs_francs OWNER postgres LOCATION '/database/pg92/pgdata/pg_tbs/tbs_francs';REVOKE ALL ON TABLESPACE tbs_francs FROM PUBLIC;REVOKE ALL ON TABLESPACE tbs_francs FROM postgres;GRANT ALL ON TABLESPACE tbs_francs TO postgres;GRANT ALL ON TABLESPACE tbs_francs TO francs; 备注：上面省略部分输出。 仅导出 Role1[pg92@redhatB tf]$ pg_dumpall -r &gt; pg92_roles.sql 仅导出 Tablespace1[pg92@redhatB tf]$ pg_dumpall -t &gt; pg92_tablespace.sql 备注： 这里的 -t 是指导出所有表空间，和 pg_dump 的 -t 不一样。 参考 pg_dumpall pg_dump","categories":[{"name":"PG备份与恢复","slug":"PG备份与恢复","permalink":"https://postgres.fun/categories/PG备份与恢复/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"PostgreSQL9.3：新特性汇总","slug":"20130531101317","date":"2013-05-31T02:13:17.000Z","updated":"2018-09-04T01:34:04.027Z","comments":true,"path":"20130531101317.html","link":"","permalink":"https://postgres.fun/20130531101317.html","excerpt":"","text":"这段时间简单的熟悉了 PostgreSQL9.3 新特性，汇总如下： Waiting for PostgreSQL9.3：增加物化视图 (MATERIALIZED VIEW) PostgreSQL9.3Beta1：新增 postgres_fdw 外部模块 PostgreSQL9.3Beta1：支持可写的外部表(Writeable Foreign Tables) PostgreSQL9.3Beta1：JSON 功能增强 PostgreSQL9.3Beta1：pg_dump 新增并行参数 (Parallel pg_dump) PostgreSQL9.3Beta1：新增 pg_isready 测试工具 PostgreSQL9.3Beta1：视图新增可更新功能（ Updatable Views ） PostgreSQL9.3Beta1：新增 lock_timeout 参数 cancel 超长等待 SQL PostgreSQL9.3Beta1：支持事件触发器 ( event triggers ) PostgreSQL9.3Beta1：ALTER ROLE 新增 ALL SET 选项设置所有用户参数 PostgreSQL9.3Beta1：建表时废弃 “implicit index” 和 sequence 的提示信息 PostgreSQL9.3Beta1：新增 array_remove() 和 array_replace() 数组函数 PostgreSQL9.3Beta1：反斜杠命令( Backslash Commands ) 的改变 PostgreSQL9.3Beta1：新增 pg_xlogdump 模块 PostgreSQL9.3Beta1：LATERAL JOIN 备注：目前先总结这些，有新内容再补充。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.3Beta1：LATERAL JOIN","slug":"20130530100110","date":"2013-05-30T02:01:10.000Z","updated":"2018-09-04T01:34:03.965Z","comments":true,"path":"20130530100110.html","link":"","permalink":"https://postgres.fun/20130530100110.html","excerpt":"","text":"LATERAL JOIN 是 PostgreSQL 9.3 新特性之一，LATERAL 一词意思为 “侧面的，横向的”，LATERAL JOIN 允许 from 语句后面的子查询 subquery2 直接引用 subquery1 的字段，描述比较费力，参照文档，给出例子如下： 例一: LATERAL JOIN报错 SQL123456789francs=&gt; SELECT base.nr,francs-&gt; multiples.multiplefrancs-&gt; FROM (SELECT generate_series(1,10) AS nr) basefrancs-&gt; JOIN (SELECT generate_series(1,5) AS b_nr, base.nr * 2 AS multiple) multiplesfrancs-&gt; ON multiples.b_nr = base.nr ;ERROR: invalid reference to FROM-clause entry for table \"base\"LINE 4: JOIN (SELECT generate_series(1,5) AS b_nr, base.nr * 2 AS ... ^HINT: There is an entry for table \"base\", but it cannot be referenced from this part of the query. 使用 LATERAL JOIN12345678SELECT base.nr, multiples.multiple FROM (SELECT generate_series(1,10) AS nr) base,LATERAL ( SELECT multiples.multiple FROM ( SELECT generate_series(1,5) AS b_nr, base.nr * 2 AS multiple ) multiples WHERE multiples.b_nr = base.nr ) multiples; 查询结果12345678910111213141516francs=&gt; SELECT base.nr,francs-&gt; multiples.multiplefrancs-&gt; FROM (SELECT generate_series(1,10) AS nr) base,francs-&gt; LATERAL (francs(&gt; SELECT multiples.multiple FROMfrancs(&gt; ( SELECT generate_series(1,5) AS b_nr, base.nr * 2 AS multiple ) multiplesfrancs(&gt; WHERE multiples.b_nr = base.nrfrancs(&gt; ) multiples;nr | multiple----+---------- 1 | 2 2 | 4 3 | 6 4 | 8 5 | 10(5 rows) 此外，函数也可直接引用前面子查询的字段，而不需要使用 LATERAL JOIN。 例二: 函数测试9.3 版本123456789101112131415161718192021222324252627francs=&gt; CREATE FUNCTION multiply(INT, INT)francs-&gt; RETURNS INTfrancs-&gt; LANGUAGE SQLfrancs-&gt; ASfrancs-&gt; $$francs$&gt; SELECT $1 * $2;francs$&gt; $$francs-&gt; ;CREATE FUNCTION francs=&gt; SELECT base.nr,francs-&gt; multiplefrancs-&gt; FROM (SELECT generate_series(1,10) AS nr) base,francs-&gt; multiply(base.nr, 2) AS multiple;nr | multiple----+---------- 1 | 2 2 | 4 3 | 6 4 | 8 5 | 10 6 | 12 7 | 14 8 | 16 9 | 1810 | 20(10 rows) 9.2 版12345678910111213141516171819[pg92@redhatB ~]$ psql francs francspsql (9.2.1)Type \"help\" for help. francs=&gt; CREATE FUNCTION multiply(INT, INT)francs-&gt; RETURNS INTfrancs-&gt; LANGUAGE SQLfrancs-&gt; ASfrancs-&gt; $$francs$&gt; SELECT $1 * $2;francs$&gt; $$;CREATE FUNCTION francs=&gt; SELECT base.nr,francs-&gt; multiplefrancs-&gt; FROM (SELECT generate_series(1,10) AS nr) base,francs-&gt; multiply(base.nr, 2) AS multiple;ERROR: function expression in FROM cannot refer to other relations of same query levelLINE 4: multiply(base.nr, 2) AS multiple; 备注： 9.3 之前版本报错。 参考 What’s new in PostgreSQL 9.3 Waiting for 9.3: Implement SQL standard lateral subqueries Documentation: SELECT","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：如何传递参数到 SQL 脚本?","slug":"20130528154353","date":"2013-05-28T07:43:53.000Z","updated":"2018-09-04T01:34:03.902Z","comments":true,"path":"20130528154353.html","link":"","permalink":"https://postgres.fun/20130528154353.html","excerpt":"","text":"昨天bbs论坛里有网友提到一个有关参数调用问题，这个问题应该很多人遇到过，这里记录下。 网友提的问题 PostgreSQL数据库中，\\i 1.sql 表示执行这个SQL脚本，如何将变量传递给 1.sql 脚本？ 备注：这位网友想在 psql 调用 sql 脚本时传递参数到 sql 文件中，这个需求很普通，因为在 shell 脚本执行某些维护工作时非常有用。这时可以使用 psql 的 -v 参数，如下： 测试环境准备1234567891011121314151617[pg93@redhatB ~]$ psql francs francspsql (9.3beta1)Type \"help\" for help. francs=&gt; create table test_3(id int4,name varchar(32));CREATE TABLE francs=&gt; insert into test_3 values (1,'a'),(2,'b'),(3,'c');INSERT 0 3 francs=&gt; select * from test_3;id | name----+------ 1 | a 2 | b 3 | c(3 rows) 带变量的 sql 脚本12[pg93@redhatB tf]$ cat 1.sqlselect * from test_3 where id=:v_id; 测试：psql客户端传递变量psql 客户端使用 -v 选项传递变更，同时使用 -f 选项执行脚本，如下：1234567891011[pg93@redhatB tf]$ psql francs francs -v v_id=1 -f 1.sqlid | name----+------ 1 | a(1 row) [pg93@redhatB tf]$ psql francs francs -v v_id=2 -f 1.sqlid | name----+------ 2 | b(1 row) 备注：满足需求。这里通过 -f 直接调用 sql 文件，也可以通过 -c 命令调用 sql 文件，如下： psql 客户端使用 -v 选项传递变更，同时使用 -c 选项执行脚本，如下：1234567891011[pg93@redhatB tf]$ psql francs francs -v v_id=1 -c 'i 1.sql'id | name----+------ 1 | a(1 row) [pg93@redhatB tf]$ psql francs francs -v v_id=2 -c 'i 1.sql'id | name----+------ 2 | b(1 row) 备注：结果一样，个人建议使用前面的方法。另外，如果是比较复杂的逻辑，建议写在 function 中。 参考 psql","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"psql","slug":"psql","permalink":"https://postgres.fun/tags/psql/"}]},{"title":"PostgreSQL9.3Beta1：新增 pg_xlogdump 模块","slug":"20130527160209","date":"2013-05-27T08:02:09.000Z","updated":"2018-09-04T01:34:03.840Z","comments":true,"path":"20130527160209.html","link":"","permalink":"https://postgres.fun/20130527160209.html","excerpt":"","text":"PostgreSQL 9.3 新增 pg_xlogdump 模块，这个模块的的功能是将 WAL 日志展现成易读的格式，大家知道 WAL 日志是二进制文件，是了解数据库内部机制的重要文件，新版本的这个模块主要是从 debugging 或 educational 角度出发，下面简单演示下： 手册上的解释12345678910111213Name pg_xlogdump -- Display a human-readable rendering of the write-ahead log of a PostgreSQL database cluster Synopsis pg_xlogdump [option...] [startseg [endseg] ] Description pg_xlogdump displays the write-ahead log (WAL) and is mainly useful for debugging or educational purposes. This utility can only be run by the user who installed the server, because it requires read-only access to the data directory. pg_xlogdump 测试2.1 展现 startseg 到 startseg 的 WAL 的事务日志12345678910[pg93@redhatB pg_xlog]$ lltotal 65M-rw-------. 1 pg93 pg93 16M May 15 12:04 000000010000000000000001-rw-------. 1 pg93 pg93 16M May 18 15:25 000000010000000000000002-rw-------. 1 pg93 pg93 16M May 24 13:46 000000010000000000000003-rw-------. 1 pg93 pg93 16M May 27 13:18 000000010000000000000004drwx------. 2 pg93 pg93 4.0K May 24 13:47 archive_status [pg93@redhatB pg_xlog]$pg_xlogdump -b -n1 000000010000000000000001 000000010000000000000004rmgr: XLOG len (rec/tot): 68/ 96, tx: 0, lsn: 0/01000024, prev 0/00000000, bkp: 0000, desc: checkpoint: redo 0/1000024; tli 1; prev tli 1; fpw true; xid 0/3; oid 10000; multi 1; offset 0; oldest xid 3 in DB 1; oldest multi 1 in DB 1; oldest 备注：这里展现的是 000000010000000000000001 到 000000010000000000000004 之间的 WAL 日志，-n 表示显示的行数。 2.2 展现 WAL 中所有的 resource manager 名称123456789101112131415161718[pg93@redhatB pg_xlog]$pg_xlogdump --rmgr=listXLOGTransactionStorageCLOGDatabaseTablespaceMultiXactRelMapStandbyHeap2HeapBtreeHashGinGistSequenceSPGist 备注：pg_xlogdump 展现的所有资源信息为以上。 2.3 仅展现 WAL 中资源名称 Sequence 的日志12[pg93@redhatB pg_xlog]$ pg_xlogdump -rSequence-n1 000000010000000000000001 000000010000000000000004rmgr: Sequence len (rec/tot): 158/ 186, tx: 1852, lsn: 0/024DCD7C, prev 0/024DCCC8, bkp: 0000, desc: log: rel 16385/16386/16438 备注：上面指定的 resource manager 为 Sequence，字段信息解释如下： - rmgr: 资源名称 - len: rmgr记录长度 - tx: 事务ID (transaction ID) - lsn: log sequence numbe，包括之前的 LSN - bkp: backup blocks (具体意思还没弄懂) - desc: 一些描述信息，包括 relation 的相关页信息（重要信息） 2.3.1 关于 desc 信息再来看上面的 des 信息，有条“rel 16385/16386/16438” 记录，这是指什么？很容易猜到表示: {reltablespace}/{databaseid}/{relfilenode}*， 接着验证： tabespace12345francs=&gt; select oid,spcname from pg_tablespace where oid=16385; oid | spcname -------+------------16385 | tbs_francs(1 row) database12345francs=&gt; select oid,datname from pg_database where oid=16386; oid | datname-------+---------16386 | francs(1 row) relation1234francs=&gt; select oid,relkind,relname from pg_class where oid=16438; oid | relkind | relname -------+---------+-------------------16438 | S | test_json1_id_seq 备注：可见，最终是个序列。 2.4 仅展现 WAL 中资源名称 Database 的日志123456[pg93@redhatB pg_xlog]$ pg_xlogdump -r Database -b 000000010000000000000001 000000010000000000000004rmgr: Database len (rec/tot): 16/ 44, tx: 1797, lsn: 0/0176C9D4, prev 0/0176C974, bkp: 0000, desc: create db: copy dir 1/1663 to 12890/1663rmgr: Database len (rec/tot): 16/ 44, tx: 1804, lsn: 0/0178332C, prev 0/017832CC, bkp: 0000, desc: create db: copy dir 1/1663 to 12895/1663rmgr: Database len (rec/tot): 16/ 44, tx: 1808, lsn: 0/0178441C, prev 0/017843BC, bkp: 0000, desc: create db: copy dir 12890/1663 to 16386/16385rmgr: Database len (rec/tot): 16/ 44, tx: 1814, lsn: 0/0178619C, prev 0/0178613C, bkp: 0000, desc: create db: copy dir 12890/1663 to 16390/16389pg_xlogdump: FATAL: error in WAL record at 0/40019F8: record with zero length at 0/4001A58 备注：上面指定的 resource manager 为 Database，也可以指定LSN，如下。 2.5 指定 LSN12345[pg93@redhatB pg_xlog]$ pg_xlogdump -r Databasestart 0/0178332C000000010000000000000001 000000010000000000000004rmgr: Database len (rec/tot): 16/ 44, tx: 1804, lsn: 0/0178332C, prev 0/017832CC, bkp: 0000, desc: create db: copy dir 1/1663 to 12895/1663rmgr: Database len (rec/tot): 16/ 44, tx: 1808, lsn: 0/0178441C, prev 0/017843BC, bkp: 0000, desc: create db: copy dir 12890/1663 to 16386/16385rmgr: Database len (rec/tot): 16/ 44, tx: 1814, lsn: 0/0178619C, prev 0/0178613C, bkp: 0000, desc: create db: copy dir 12890/1663 to 16390/16389pg_xlogdump: FATAL: error in WAL record at 0/40019F8: record with zero length at 0/4001A58 2.6 关于 LSN2.6.1 查看当前 LSN12345postgres=# select pg_current_xlog_location();pg_current_xlog_location--------------------------0/4001A58(1 row) 2.6.2 查看 LSN 位于哪个 WAL 文件12345postgres=# select pg_xlogfile_name('0/4001A58'); pg_xlogfile_name --------------------------000000010000000000000004(1 row) 总结 从上来看， pg_xlogdump 有点像 Oracle 的 LogMiner，但目前 pg_xlogdump 还不能生成历史 SQL 信息。 目前 pg_xlogdump 主要是出于从 debugging 或 educational 角度使用，生产上使用需谨慎。 pg_xlogdump 展现的信息有限，希望在之后版本加强。 参考 pg_xlogdump System Administration Functions Postgres 9.3 feature highlight: pg_xlogdump Write-Ahead Log","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.3Beta1：反斜杠命令( Backslash Commands ) 的改变","slug":"20130527102829","date":"2013-05-27T02:28:29.000Z","updated":"2018-09-04T01:34:03.777Z","comments":true,"path":"20130527102829.html","link":"","permalink":"https://postgres.fun/20130527102829.html","excerpt":"","text":"PostgreSQL 9.3 版本的反斜杠命令有少量变化，这里列出几点介绍下： Add psql \\watch command to repeatedly execute commands (Will Leinweber) Add psql command \\gset to store query results in psql variables (Pavel Stehule) Allow psql \\l to accept a database name pattern (Peter Eisentraut) Add “Security” label to \\psql df+ output (Jon Erdman) 测试一: watchwatch 命令可以重复执行缓冲区的指令，如下： 12345678910111213141516171819202122232425262728293031[pg93@redhatB ~]$ psql francs francspsql (9.3beta1)Type \"help\" for help. francs=&gt; select now()::timestamp(0); now ---------------------2013-05-27 10:06:20(1 row) francs=&gt; watch 2Watch every 2s Mon May 27 10:06:22 2013 now ---------------------2013-05-27 10:06:22(1 row) Watch every 2s Mon May 27 10:06:24 2013 now ---------------------2013-05-27 10:06:24(1 row) Watch every 2s Mon May 27 10:06:26 2013 now ---------------------2013-05-27 10:06:26(1 row) 备注： watch 后接的参数为时间参数，表示时间间隔（s），上面时间间隔为 2 秒。 测试二: gsetgset 命令可以存储 psql 的执行结果到 psql 变量中，执行结果必须返回一行，否则变量不会被设置。 12345678910francs=&gt; select 'francs' as name, '2013-05-27' as create_time; name | create_time--------+-------------francs | 2013-05-27(1 row) francs=&gt; gset francs=&gt; echo :name :create_timefrancs 2013-05-27 备注： gset 命令后可以接前缀参数，如下：12345678910francs=&gt; select 'francs' as name, '2013-05-27' as create_time; name | create_time--------+-------------francs | 2013-05-27(1 row) francs=&gt; gset user_ francs=&gt; echo :user_name :user_create_timefrancs 2013-05-27 测试三: \\l psql 的 \\l 命令支持数据库名匹配参数，如下：9.3 版本 \\l 测试12345678910111213[pg93@redhatB ~]$ psqlpsql (9.3beta1)Type \"help\" for help. postgres=# \\l francs List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges --------+----------+----------+---------+-------+------------------------francs | postgres | UTF8 | C | C | =Tc/postgres + | | | | | postgres=CTc/postgres + | | | | | francs=C*T*c*/postgres+ | | | | | select_only=c/francs(1 row) 备注：之前版本不支持库名匹配参数。第 4 点是关于 df+ 命令增加显示函数的 SECURITY 属性，这里不演示了。但是函数的 SECURITY 属性很有意思，和 Linux 系统 SUID 类似，以后可能做下测试。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"psql","slug":"psql","permalink":"https://postgres.fun/tags/psql/"}]},{"title":"PostgreSQL：Psql 客户端无法使用方向建查看历史命令","slug":"20130523174449","date":"2013-05-23T09:44:49.000Z","updated":"2018-09-04T01:34:03.714Z","comments":true,"path":"20130523174449.html","link":"","permalink":"https://postgres.fun/20130523174449.html","excerpt":"","text":"今天在一开发环境上维护时，发现 使用 psql 连接到数据库后，无法使用方向键查看历史命令，如下所示。 问题现象psql 命令台无法使用方向键123456789101112-bash-3.2$ psqlpsql (9.2.2)Type \"help\" for help. postgres=# select now(); now -------------------------------2013-05-23 16:54:16.278334+08(1 row) postgres=# ^[[A^[[A^[[A^[[A^[[A --这里尝试使用方向键，出现这个字符。postgres-# 备注：方向键无法使用，非常不方便，这个与 readline 有关。 排查过程检查系统12[root@skycac44 ~]# rpm -qa | grep readlinereadline-5.1-3.el5 备注：系统上已安装 readline。 检查 PostgreSQL 客户端1-bash-3.2$ pg_config --configure '--prefix=/usr/local/postgres' '--without-readline' 备注： 以上命令显示编译 PostgreSQL 时的配置参数列表，上面果然没有安装 readline。 configure 时手册上的解释 without-readline Prevents use of the Readline library (and libedit as well). This option disables command-line editingand history in psql, so it is not recommended. 备注：手册上不建议禁用 readline。 对于已安装 readline 的 PostgreSQL 客户端，也有参数可以控制是否session 级禁用 readline。 测试查看编译信息12[pg93@redhatB ~]$ pg_config --configure'--prefix=/opt/pgsql9.3beta1' '--with-pgport=1925' '--with-segsize=8' '--with-wal-segsize=16' '--with-wal-blocksize=64' '--with-perl' '--with-python' '--with-openssl' '--with-pam' '--with-ldap' '--with-libxml' '--with-libxslt' '--enable-thread-safety' 备注：这个是我笔记本上 9.3 的库，编译时已安装 readline。 psql 测试1: 启用 readline123456789101112131415[pg93@redhatB ~]$ psqlpsql (9.3beta1)Type \"help\" for help. postgres=# select now(); now ------------------------------2013-05-23 17:00:09.76174+08(1 row) postgres=# select now(); 这里使用方向键滚动历史命令，正常。 now -------------------------------2013-05-23 17:00:13.901787+08(1 row) 备注：方向键滚动历史命令正常。 psql 测试2：session 级禁用 readline123456789101112[pg93@redhatB ~]$ psql --no-readlinepsql (9.3beta1)Type \"help\" for help. postgres=# select now(); now -------------------------------2013-05-23 17:01:48.875906+08(1 row) postgres=# ^[[A^[[Apostgres-# 备注：psql 连接时加了参数“–no-readline”，设置后，方向键又抽风了，更进一步，TAB 补全功能也被禁用了。 总结 建议开启 readline 功能，便于数据库管理，维护。 readline 功能与 psql 客户端相关，而与服务端无关。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"psql","slug":"psql","permalink":"https://postgres.fun/tags/psql/"}]},{"title":"PostgreSQL9.3Beta1：新增 Array_remove() 和 Array_replace() 数组函数","slug":"20130522104957","date":"2013-05-22T02:49:57.000Z","updated":"2018-09-04T01:34:03.652Z","comments":true,"path":"20130522104957.html","link":"","permalink":"https://postgres.fun/20130522104957.html","excerpt":"","text":"PostgreSQL 9.3 新增 array_remove() 和 array_replace() 数组函数，增强了对数组元素的处理功能，下面演示下。 Array_remove()array_remove()：删除数组中指定的元素。1.1 示例123456789101112131415[pg93@redhatB ~]$ psql francs francspsql (9.3beta1)Type \"help\" for help. francs=&gt; select array_remove(array[1,2,3,4],1);array_remove--------------&#123;2,3,4&#125;(1 row) francs=&gt; select array_remove(array[1,2,3,4,1],1);array_remove--------------&#123;2,3,4&#125;(1 row) 备注：重复的元素也被删除。 Array_replace()array_replace(): 替换数组中的指定元素值为新值。2.1 示例1234567891011francs=&gt; select array_replace(array[1,2,3,4],4,5);array_replace---------------&#123;1,2,3,5&#125;(1 row) francs=&gt; select array_replace(array[1,2,3,4,4],4,5);array_replace---------------&#123;1,2,3,5,5&#125;(1 row 备注：重复的元素也被替换。 附: 新增的数组函数 Function Return Type Description array_remove(anyarray, anyelement) anyarray remove all elements equal to the given value from the array (array must be one-dimensional) array_replace(anyarray, anyelement, anyelement) anyarray replace each array element equal to the given value with a new value 备注： 有了 array_remove()，array_replace()，array_append() 函数，PostgreSQL 中对数组元素的处理功能在很多场合应该够用了。 参考 Array Functions and Operators PostgreSQL: Array 数组类型添加元素 PostgreSQL: 数组类型(Array) 的使用","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.3Beta1：建表时废弃 \"Implicit Index\" 和 Sequence 的提示信息","slug":"20130521154938","date":"2013-05-21T07:49:38.000Z","updated":"2018-09-04T01:34:03.589Z","comments":true,"path":"20130521154938.html","link":"","permalink":"https://postgres.fun/20130521154938.html","excerpt":"","text":"9.3 版本之前，当创建表时，如果有隐式的索引或序列，那么会抛出 NOTICE 信息，新版本后这些 NOTICE 信息不再抛出，个人觉得这些信息在建表时不出现为好，日志看上去简捷点。 文档中说明 Suppress messages about implicit index and sequence creation (Robert Haas)These messages now appear at DEBUG1 verbosity, so that they will not be shown by default. 备注：接着演示下。 9.2 版本创建表测试123456789101112131415161718192021222324[pg92@redhatB ~]$ psql francs francspsql (9.2.1)Type \"help\" for help. francs=&gt; show client_min_messages;client_min_messages---------------------notice(1 row) francs=&gt; create table test_notice (id serial primary key ,name varchar(32));NOTICE: CREATE TABLE will create implicit sequence \"test_notice_id_seq\" for serial column \"test_notice.id\"NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_notice_pkey\" for table \"test_notice\"CREATE TABLE francs=&gt; \\d test_notice Table \"francs.test_notice\"Column | Type | Modifiers --------+-----------------------+----------------------------------------------------------id | integer | not null default nextval('test_notice_id_seq'::regclass)name | character varying(32) |Indexes: \"test_notice_pkey\" PRIMARY KEY, btree (id), tablespace \"tbs_francs_idx\"Tablespace: \"tbs_francs_idx\" 备注：9.2 版本建表时抛出了隐式索引和序列信息。 9.3 版本创建表测试123456789101112131415161718192021[pg93@redhatB ~]$ psql francs francspsql (9.3beta1)Type \"help\" for help. francs=&gt; show client_min_messages;client_min_messages---------------------notice(1 row) francs=&gt; create table test_notice (id serial primary key ,name varchar(32));CREATE TABLE francs=&gt; \\d test_notice Table \"francs.test_notice\"Column | Type | Modifiers --------+-----------------------+----------------------------------------------------------id | integer | not null default nextval('test_notice_id_seq'::regclass)name | character varying(32) |Indexes: \"test_notice_pkey\" PRIMARY KEY, btree (id) 备注：9.3 版本建表时不再抛出隐式索引和序列信息，这类信息出现在 debug1 日志模式，因此也有方法查看这些信息。 设置 client_min_messages=’debug1’123456789101112131415161718192021222324[pg93@redhatB ~]$ psql francs francspsql (9.3beta1)Type \"help\" for help.francs=&gt; show client_min_messages;client_min_messages---------------------notice(1 row) francs=&gt; set client_min_messages='debug1';SET francs=&gt; show client_min_messages;client_min_messages---------------------debug1(1 row) francs=&gt; create table test_notice2 (id serial primary key ,name varchar(32));DEBUG: CREATE TABLE will create implicit sequence \"test_notice2_id_seq\" for serial column \"test_notice2.id\"DEBUG: CREATE TABLE / PRIMARY KEY will create implicit index \"test_notice2_pkey\" for table \"test_notice2\"DEBUG: building index \"test_notice2_pkey\" on table \"test_notice2\"CREATE TABLE 备注：设置 client_min_messages=’debug1’ 后，这类信息可以查看到了。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Sequence","slug":"Sequence","permalink":"https://postgres.fun/tags/Sequence/"}]},{"title":"PostgreSQL9.3Beta1：ALTER ROLE 新增 ALL SET 选项设置所有用户参数","slug":"20130521143149","date":"2013-05-21T06:31:49.000Z","updated":"2018-09-04T01:34:03.527Z","comments":true,"path":"20130521143149.html","link":"","permalink":"https://postgres.fun/20130521143149.html","excerpt":"","text":"PostgreSQL9.3 的 ALTER ROLE 命令有如下改进：1Add ALTER ROLE ALL SET to add settings to all users (Peter Eisentraut) 此命令支持一个命令设置所有用户(role) 的配置参数，而之前版本不支持这个功能。 ALTER ROLE 命令语法PostgreSQL 9.2 版本 ALTER ROLE1234ALTER ROLE name [ IN DATABASE database_name ] SET configuration_parameter &#123; TO | = &#125; &#123; value | DEFAULT &#125;ALTER ROLE name [ IN DATABASE database_name ] SET configuration_parameter FROM CURRENTALTER ROLE name [ IN DATABASE database_name ] RESET configuration_parameterALTER ROLE name [ IN DATABASE database_name ] RESET ALL PostgreSQL 9.3 版本 ALTER ROLE1234ALTER ROLE name [ IN DATABASE database_name ] SET configuration_parameter &#123; TO | = &#125; &#123; value | DEFAULT &#125;ALTER ROLE &#123; name | ALL &#125; [ IN DATABASE database_name ] SET configuration_parameter FROM CURRENTALTER ROLE &#123; name | ALL &#125; [ IN DATABASE database_name ] RESET configuration_parameterALTER ROLE &#123; name | ALL &#125; [ IN DATABASE database_name ] RESET ALL 备注：接下来简单测试下。 ALTER ROLE 命令测试查看 role 信息12345678910111213141516[pg93@redhatB ~]$ psql francs postgrespsql (9.3beta1)Type \"help\" for help. francs=# \\du List of roles Role name | Attributes | Member of-------------+------------------------------------------------+-----------francs | No inheritance | &#123;&#125;postgres | Superuser, Create role, Create DB, Replication | &#123;&#125;select_only | No inheritance | &#123;&#125;source_db | No inheritance | &#123;&#125;francs=# select * from pg_db_role_setting ;setdatabase | setrole | setconfig-------------+---------+-----------(0 rows) 备注：此时还没有争对 role 级设置参数。 设置所有 role 的 log_min_duration_statement 参数12345678francs=# alter role ALL set log_min_duration_statement=200;ALTER ROLE francs=# select * from pg_db_role_setting ;setdatabase | setrole | setconfig -------------+---------+---------------------------------- 0 | 0 | &#123;log_min_duration_statement=200&#125;(1 row) 备注：setrole 值为 0 表示没有指定数据库，即对所有库生效，setdatabase 值同理。 参考 ALTER ROLE pg_db_role_setting PostgreSQL：如何查询基于用户(role)设置的参数","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.3Beta1：支持事件触发器 ( Event Triggers ) ","slug":"20130521125609","date":"2013-05-21T04:56:09.000Z","updated":"2018-09-04T01:34:03.464Z","comments":true,"path":"20130521125609.html","link":"","permalink":"https://postgres.fun/20130521125609.html","excerpt":"","text":"PostgreSQL 9.3 新增事件触发器，事件触发器为全局触发器，影响范围为指定的某个库，并且可以捕获 DDL 事件，而传统的触发器是基于表级别，并且只能捕获 DML 事件。 事件触发器概述1.1 事件触发器语法12345678910111213141516Name CREATE EVENT TRIGGER -- define a new event trigger Synopsis CREATE EVENT TRIGGER name ON event [ WHEN filter_variable IN (filter_value [, ... ]) [ AND ... ] ] EXECUTE PROCEDURE function_name()Description CREATE EVENT TRIGGER creates a new event trigger. Whenever the designated event occurs and theWHEN condition associated with the trigger, if any, is satisfied, the trigger function will beexecuted. For a general introduction to event triggers, see Chapter 37. The user who creates anevent triggerbecomes its owner. 1.2 关于触发器 EVENT 当指定 EVENT 发生时触发器才会被触发，目前支持三种 EVENT: ddl_command_start，ddl_command_end ，sql_drop。 ddl_command_start : 在 CREATE, ALTER, 或 DROP 命令执行之前触发，例外的情况是: 这不会在共享对象上触发，例如 database，roles， tablespace。 ddl_command_end： ddl_command_end 事件在以类似同样命令执行后触发。 sql_drop: sql_drop 事件在 ddl_command_end 触发之前触发，要查看删除的数据库对像，可以通过函数 pg_event_trigger_dropped_objects() 查看，当数据库对像在 catalog 表中被删除时，触发器被触发。 1.3 event trigger 使用场景 逻辑复制 审计 限制用户的 DDL 命令 备注：这里列出主要的应用场景，可能还有更多。x以上是 PostgreSQL 9.3 事件触发器的基本知识，接下来通过一个示例进一步了解 EVENT trigger。 审计数据库的 Drop 命令这个示例将创建 sql_drop 事件触发器，用来记录指定数据库中 drop 命令历史记录。 2.1 创建 drop 命令审计表12345678910111213141516171819[pg93@redhatB ~]$ psql francs postgrespsql (9.3beta1)Type \"help\" for help. francs=# create table tbl_ddl_drop_log (francs(# login_role text,francs(# db_name character varying(64),francs(# client_ip inet,francs(# ddl_type character varying(32),francs(# schema_name text,francs(# object_type text,francs(# object_name text,francs(# object_identity text,francs(# drop_time timestamp with time zonefrancs(# );CREATE TABLE francs=# grant insert on tbl_ddl_drop_log to francs;GRANT 备注：这里将审计表的 INSERT 权限赋给应用用户 francs。 2.2 创建 sql_drop 触发器函数1234567891011121314151617181920212223242526272829303132francs=# CREATE OR REPLACE FUNCTION fun_log_drop_command()francs-# RETURNS event_trigger LANGUAGE plpgsql AS $$francs$# DECLAREfrancs$# obj record;francs$# BEGINfrancs$# FOR obj IN (SELECT * FROM pg_event_trigger_dropped_objects() t where t.object_typefrancs$# in ('table','sequence','index','function','view'))francs$# LOOPfrancs$# insert into tbl_ddl_drop_log (francs$# login_role,francs$# db_name, francs$# client_ip,francs$# ddl_type,francs$# schema_name, francs$# object_type, francs$# object_name, francs$# object_identity,francs$# drop_time)francs$# values (francs$# current_user,francs$# current_database(),francs$# inet_client_addr(),francs$# tg_tag,francs$# obj.schema_name,francs$# obj.object_type,francs$# obj.object_name,francs$# obj.object_identity,francs$# now()); francs$# END LOOP;francs$# END; francs$# $$; CREATE FUNCTION 备注：这个函数需要返回 event_trigger 类型，用于触发器调用; 另外函数中调用内置函数 pg_event_trigger_dropped_objects() 返回被删除的数据库对像，我这里做了过滤，关于 pg_event_trigger_dropped_objects 的返回类型，参考文本末尾的附。 2.3 创建触发器123456789francs=# CREATE EVENT TRIGGER trg_log_drop_command ON sql_dropfrancs-# EXECUTE PROCEDURE fun_log_drop_command();CREATE EVENT TRIGGERfrancs=# \\dy List of event triggers Name | Event | Owner | Enabled | Procedure | Tags----------------------+----------+----------+---------+----------------------+------trg_log_drop_command | sql_drop | postgres | enabled | fun_log_drop_command |(1 row) 备注：\\dy 用来查看触发器。 2.4 测试2.4.1 创建测试表和测试函数12345678910111213141516pg91@redhat6 ~]$ psql -h 192.168.1.36 -p 1925 francs francsPassword for user francs:psql (9.1.2, server 9.3beta1)WARNING: psql version 9.1, server version 9.3. Some psql features might not work.Type &quot;help&quot; for help. francs=&gt; create table test_trigger(id serial primary key);CREATE TABLE francs=&gt; CREATE or replace FUNCTION fun_ins_test_1(i int4) RETURNS INTEGER AS $$francs$&gt; BEGINfrancs$&gt; return $1::text;francs$&gt; END;francs$&gt; $$ LANGUAGE &apos;plpgsql&apos;;CREATE FUNCTION 备注：这步在客户端（192.168.1.35） 上操作。 2.4.2 删除测试表和测试函数12345francs=&gt; drop table test_trigger;DROP TABLE francs=&gt; drop function fun_ins_test_1(i int4);DROP FUNCTION 备注：这步在客户端（192.168.1.35） 上操作。删除测试表 test_trigger 和测试函数 fun_ins_test_1(i int4) 之后，接下来看下审计表。 2.4.3 服务端查看审计表备注：这步在服务端（192.168.1.36） 上操作，测试成功，以上只是简单的演示 EVENT TRIGGER，生产上的审计比这复杂得多。 附: Event Trigger Functionspg_event_trigger_dropped_objects 返回以下列: 参考 CREATE EVENT TRIGGER Event Trigger Functions Event Trigger Firing Matrix Overview of Event Trigger Behavior http://wiki.postgresql.org/wiki/Event_Triggers","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.3Beta1：新增 Lock_timeout 参数 cancel 超长等待 SQL","slug":"20130520142939","date":"2013-05-20T06:29:39.000Z","updated":"2018-12-04T00:27:40.915Z","comments":true,"path":"20130520142939.html","link":"","permalink":"https://postgres.fun/20130520142939.html","excerpt":"","text":"PostgreSQL 9.3 新增服务端配置参数 lock_timeout，此参数用来 cancel 超过指定时间的等待 SQL，由于这个参数会主动探测长时间处于等待状态(通常因为申请不到相关锁)的SQL，并终止超过指定时间的 SQL，故不推荐使用，这里仅简单说明这个参数，在某些场合确实需要时可以考虑使用。 手册上的解释 lock_timeout (integer) Abort any statement that waits longer than the specified number of milliseconds while attempting to acquire a lock on a table, index, row, or other database object. The time limit applies separately to each lock acquisition attempt. The limit applies both to explicit locking requests (such as LOCK TABLE, or SELECT FOR UPDATE without NOWAIT) and to implicitly-acquired locks. If log_min_error_statement is set to ERROR or lower, the statement that timed out will be logged. A value of zero (the default) turns this off.备注：手册上解释的比较清楚了，下面通过简单的例子演示下。 测试SQL超时场景2.1 修改参数设置 postgresql.conf 中的以下参数，并 reload。1lock_timeout = 10000 设置好后，到数据库里查下：12345francs=&gt; show lock_timeout; lock_timeout -------------- 10s(1 row) 备注：这里设置了 10 秒，便于测试。 2.2 创建测试表123456[pg93@redhatB ~]$ psql francs francspsql (9.3beta1)Type \"help\" for help. francs=&gt; create table test_lock (id int4 primary key ,name character varying (32),create_time timestamp without time zone );CREATE TABLE 2.3 开启会话 1123456francs=&gt; begin;BEGINfrancs=&gt; insert into test_lock values (1,'a',clock_timestamp());INSERT 0 1francs=&gt; .....事务未提交 备注：此时会话 1 仍然还在事务中，事务未结束。 2.4 开启会话 2 12345678910111213[pg93@redhatB tf]$ psql francs francspsql (9.3beta1)Type \"help\" for help. francs=&gt; select pg_backend_pid(); pg_backend_pid ---------------- 22962(1 row francs=&gt; alter table test_lock alter column name type character varying(64);ERROR: canceling statement due to lock timeoutTime: 10017.376 ms 备注：会话 2 尝试给表 test_lock 的 name 字段扩长，这时处于等待状态，并且在等待超过 10 秒时 PostgreSQL 会 cancel 这个 SQL，关于 PostgreSQL 锁分, 什么情况下被锁，参考之前 BLOG: Postgresql 锁浅析 2.5 查看进程1234[pg93@redhatB ~]$ ps -ef | grep 22962pg93 22962 2758 0 13:46 ? 00:00:00 postgres: francs francs [local] idle pg93 23202 23088 0 13:51 pts/4 00:00:00 grep 22962[pg93@redhatB ~]$ 备注：OS 系统上进程 22962 还存在，说明之前 PostgreSQL 仅 cancel SQL 语句，而没有杀掉会话。 2.6 查看PostgreSQL 日志1 2013-05-20 13:48:52.626 CST,\"francs\",\"francs\",22962,\"[local]\",5199b8b7.59b2,2,\"ALTER TABLE\",2013-05-20 13:46:31 CST,3/4214,1891,ERROR,57014,\"canceling statement due to lock timeout\",,,,,,\"alter table test_lock alter column name type character varying(64);\",,,\"psql\" 备注：如果 log_min_error_statement 参数设置成 ERROR 或更低级别，则这些信息会记录到日志中。以上 lock_timeout 参数实验完成了，此时你可能还会想到另一个类似参数，该参数为 statement_timeout ，不要混淆这两参数，因为 statement_timeout 参数用来控制已运行 SQL ，可以杀掉运行时间超长的 SQL ，关于该参数参考BLOG： PostgreSQL: Autoabort user’s statement that takes over the specified time 参考 PostgreSQL 锁浅析 PostgreSQL: Autoabort user’s statement that takes over the specified time Lock_timeout (integer) http://www.postgresql.org/docs/9.3/static/runtime-config-logging.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.3Beta1：视图新增可更新功能（ Updatable Views ）","slug":"20130518160750","date":"2013-05-18T08:07:50.000Z","updated":"2018-09-04T01:34:03.339Z","comments":true,"path":"20130518160750.html","link":"","permalink":"https://postgres.fun/20130518160750.html","excerpt":"","text":"PostgreSQL 9.3 版本支持视图更新操作，也就是说可以在 views 上执行 UPDATE/INSERT/DELETE 操作，但这种视图必须是简单的而且还有其它限制条件，例如视图创建中只允许引用单张表，等等，接下来会介绍这些限制条件，先通过一个简单的实验验证下： 可更新视图测试1.1 创建测试表12345678910111213francs=&gt; create table test_view1 (id int4 primary key ,name character varying(64),creat_time timestamp without time zone);CREATE TABLE francs=&gt; insert into test_view1 select generate_series(1,100000),'a_' || generate_series(1,100000),clock_timestamp();INSERT 0 100000 francs=&gt; select * from test_view1 limit 3;id | name | creat_time ----+------+---------------------------- 1 | a_1 | 2013-05-18 15:25:25.815398 2 | a_2 | 2013-05-18 15:25:25.816195 3 | a_3 | 2013-05-18 15:25:25.816219(3 rows) 1.2 创建视图12francs=&gt; create view view1_test as select id,name from test_view1;CREATE VIEW 1.3 查看表，视图大小1234567891011francs=&gt; select pg_relation_size('test_view1');pg_relation_size------------------ 4825088(1 row) francs=&gt; select pg_relation_size('view1_test');pg_relation_size------------------ 0(1 row) 备注：视图占用 0 字节，说明本身不存数据，这与物化视图不同。 1.4 更新视图1234567891011121314francs=&gt; select * from view1_test where id=1;id | name----+------ 1 | a_1(1 row) francs=&gt; update view1_test set name='a_111' where id=1;UPDATE 1 francs=&gt; select * from view1_test where id=1;id | name ----+------- 1 | a_111(1 row) 备注：视图 view1_test 果然可以更新，接下来看表中的数据是否被更新。 1.5 验证表数据12345francs=&gt; select * from test_view1 where id=1;id | name | creat_time ----+-------+---------------------------- 1 | a_111 | 2013-05-18 15:25:25.815398(1 row) 备注：更新视图后，表中对应的数据被更新了，原理：当视图被更新时，PostgreSQL 会将视图上的 INSERT/UPDATE/DELETE 语句传送到视图引用的基表。回到本文开始的问题，只有简单的视图才支持可更新操作，并且有很多限制，如下： 可更新视图的限制 The view must have exactly one entry in its FROM list, which must be a table or another updatable view. The view definition must not contain WITH, DISTINCT, GROUP BY, HAVING, LIMIT, or OFFSET clauses at the top level. The view definition must not contain set operations (UNION, INTERSECT or EXCEPT) at the top level. All columns in the view’s select list must be simple references to columns of the underlying relation. They cannot be expressions, literals or functions. System columns cannot be referenced, either. No column of the underlying relation can appear more than once in the view’s select list. The view must not have the security_barrier property. 备注：以上来自手册，不翻译了。 做这个实验时想到一个问题，假如赋给一个用户对这张视图的 select, update 权限，而不赋予这个户对这张视图所引用的表的 select ,update 权限，那么这个用户是否能更新视图呢？接着实验。 可更新视图权限2.1 创建测试用户并赋权1234567891011121314postgres=# create role user1 LOGIN encrypted password 'user1' NOSUPERUSER NOCREATEDB NOCREATEROLE NOINHERIT ;CREATE ROLE postgres=# \\c francs francsYou are now connected to database \"francs\" as user \"francs\".francs=&gt; grant connect on database francs to user1 ;GRANT francs=&gt; grant select,update on view1_test to user1;GRANT francs=&gt; grant usage on schema francs to user1;GRANT 2.2 测试 user1 权限1234567891011francs=&gt; \\c francs user1;You are now connected to database \"francs\" as user \"user1\". francs=&gt; select * from francs.test_view1 limit 1;ERROR: permission denied for relation test_view1 francs=&gt; select * from francs.view1_test limit 1;id | name----+------ 2 | a_2(1 row) 备注：user1 能查询视图，但不能查询基表。 1234567891011121314151617francs=&gt; update francs.test_view1 set name='update' where id=3;ERROR: permission denied for relation test_view1 francs=&gt; update francs.view1_test set name='update' where id=2;UPDATE 1 francs=&gt; select * from francs.view1_test where id=2;id | name ----+-------- 2 | update(1 row) francs=&gt; select * from francs.view1_test where id=2;id | name ----+-------- 2 | update(1 row) 备注：user1 能更新视图，但不能直接更新基表。 总结上面简单的演示了可更新视图，非常重要的特性，在使用过程中可能会碰到更多问题，以后补充。 参考 http://www.postgresql.org/docs/9.3/static/sql-createview.html http://www.depesz.com/2012/12/11/waiting-for-9-3-support-automatically-updatable-views/","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.3Beta1：新增 pg_isready 测试工具","slug":"20130518092803","date":"2013-05-18T01:28:03.000Z","updated":"2018-09-04T01:34:03.277Z","comments":true,"path":"20130518092803.html","link":"","permalink":"https://postgres.fun/20130518092803.html","excerpt":"","text":"PostgreSQL9.3 将新增 pg_isready 客户端工具，用来测试 PostgreSQL 服务连接状态。运行 pg_isready 命令后，产生以下四种返回结果之一： 0: 服务能正常响应连接; 1: 服务拒绝连接(比如 PostgreSQL 启动过程中); 2: 服务收不到连接响应； 3: 没有尝试连接(例如 输入非法连接参数) 根据以上返回结果，简单测试下。 pg_isready 参数12345678910111213141516171819[pg93@server1 tf]$ pg_isready --helppg_isready issues a connection check to a PostgreSQL database. Usage: pg_isready [OPTION]... Options: -d, --dbname=DBNAME database name -q, --quiet run quietly -V, --version output version information, then exit -?, --help show this help, then exit Connection options: -h, --host=HOSTNAME database server host or socket directory -p, --port=PORT database server port -t, --timeout=SECS seconds to wait when attempting connection, 0 disables (default: 3) -U, --username=USERNAME database username Report bugs to &lt;pgsql-bugs@postgresql.org&gt;. 连接正常12345[pg93@server1 tf]$ pg_isready -h 127.0.0.1 -p 1925127.0.0.1:1925 - accepting connections [pg93@server1 tf]$ echo $?0 连接无响应12345[pg93@server1 tf]$ pg_isready -h 127.0.0.1 -p 1922127.0.0.1:1922 - no response [pg93@server1 tf]$ echo $?2 连接参数不对[pg93@server1 tf]$ pg_isready -h 127.0.0.1 -a pg_isready: invalid option -- a Try &quot;pg_isready --help&quot; for more information. [pg93@server1 tf]$ echo $? 3 总结显然用 pg_isready 探测 PostgreSQL 服务状态非常简单，容易，这比其它探测方式，例如 select 1 要好些。 参考 http://wiki.postgresql.org/wiki/What’s_new_in_PostgreSQL_9.3 http://www.postgresql.org/docs/9.3/static/app-pg-isready.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.3Beta1：pg_dump 新增并行参数 (Parallel pg_dump) ","slug":"20130517145907","date":"2013-05-17T06:59:07.000Z","updated":"2018-09-04T01:34:03.214Z","comments":true,"path":"20130517145907.html","link":"","permalink":"https://postgres.fun/20130517145907.html","excerpt":"","text":"9.3 版本开始，pg_dump 增加 -j njobs 参数支持并行导出功能，而之前版本的 pg_dump 备份时不支持这一功能，当然这个并行也有很多限制，之后会介绍到。 环境信息主机： 8 核 8GB数据库: PostgreSQL9.3beta1系统：CentOS release 5.4 1.1 数据库信息12345678910111213141516171819202122[pg93@server1 ~]$ psqlpsql (9.3beta1)Type \"help\" for help. francs=&gt; select pg_size_pretty(pg_database_size('francs'));pg_size_pretty----------------5678 MB(1 row) francs=&gt; \\dt+ List of relationsSchema | Name | Type | Owner | Size | Description--------+------------+-------+--------+------------+-------------francs | big_table1 | table | francs | 1613 MB |francs | big_table2 | table | francs | 1074 MB |francs | big_table3 | table | francs | 535 MB |francs | big_table4 | table | francs | 265 MB |francs | big_table5 | table | francs | 1073 MB |francs | test_1 | table | francs | 8192 bytes |francs | test_json1 | table | francs | 16 kB |(7 rows) 并行备份测试2.1 普通备份1pg_dump -h 127.0.0.1 -E UTF8 -Fc -U postgres -v francs &gt; francs.dmp 备注：耗时 9分51秒; 备份文件大小 1046 MB 。 2.2 并行备份: -j 22.2.1 备份脚本1pg_dump -h 127.0.0.1 -E UTF8 -Fd -j 2 -U postgres -v francs -f francs_j2 备注：耗时 4分59秒; 备份目录大小 1045 MB。 2.2.2 备份过程中有两个 copy 进程。123[pg93@server1 tf]$ ps -ef | grep COPYpg93 15813 32478 42 13:53 ? 00:00:02 postgres: postgres francs 127.0.0.1(59247) COPY pg93 15814 32478 40 13:53 ? 00:00:02 postgres: postgres francs 127.0.0.1(59248) COPY 2.2.3 备份目录的文件1234567891011[pg93@server1 tf]$ ll francs_j2total 1.1G -rw-rw-r-- 1 pg93 pg93 52 May 17 13:57 2751.dat.gz-rw-rw-r-- 1 pg93 pg93 74 May 17 13:57 2753.dat.gz-rw-rw-r-- 1 pg93 pg93 368M May 17 13:56 2754.dat.gz-rw-rw-r-- 1 pg93 pg93 246M May 17 13:55 2755.dat.gz-rw-rw-r-- 1 pg93 pg93 124M May 17 13:57 2756.dat.gz-rw-rw-r-- 1 pg93 pg93 62M May 17 13:58 2757.dat.gz-rw-rw-r-- 1 pg93 pg93 246M May 17 13:57 2758.dat.gz-rw-rw-r-- 1 pg93 pg93 7.4K May 17 13:53 toc.dat 备注：pg_dump 加了 -j 参数后，会对每张表生成一个已压缩的备份文件。 2.3 并行备份: -j 42.3.1 备份脚本1pg_dump -h 127.0.0.1 -E UTF8 -Fd -j 4 -U postgres -v francs -f francs_j4 备注：耗时 4分16秒 ; 备份目录大小 1045 MB。 2.3.2 对应有 4 个 COPY 进程12345[pg93@server1 tf]$ ps -ef | grep COPYpg93 16280 32478 39 14:00 ? 00:00:03 postgres: postgres francs 127.0.0.1(55852) COPY pg93 16283 32478 30 14:00 ? 00:00:03 postgres: postgres francs 127.0.0.1(55853) COPY pg93 16285 32478 34 14:00 ? 00:00:03 postgres: postgres francs 127.0.0.1(55854) COPY pg93 16286 32478 33 14:00 ? 00:00:03 postgres: postgres francs 127.0.0.1(55855) COPY 备注：开启 4 个并行进程时，服务器明显负载升高，负载达到 6 左右。 2.4 总结从上面测试来看，开启并行备份产生的文件和不开启并行的文件大小差不多大，但开启并行的总体时间要短些，服务器负载也高些。 数据库大小: 5678 MB 并行度 消耗时间 备份文件大小 -j 2 4分59秒 1045 MB -j 4 4分16秒 1045 MB 恢复测试加了 -j 参数的 pg_dump 最后产生的是目录，并且目录里存放的是已压缩的表文件，那么如何利用这个目录恢复数据库呢，接着测试。 3.1 创建新库 francs21234567891011postgres=# create database francs2;CREATE DATABASE postgres=# grant all on database francs2 to francs with grant option;GRANT postgres=# \\c francs2 francsYou are now connected to database \"francs2\" as user \"francs\". francs2=&gt; create schema francs;CREATE SCHEMA 3.2 将备份文件导入库 francs21pg_restore -h 127.0.0.1 -d francs2 -U postgres -v -j 2 francs_j4 备注：francs_j4 是数据目录，之前版本这里需要指定文件，而这里指定的是目录。 3.3 验证1234567891011121314francs=&gt; \\c francs2 francsYou are now connected to database \"francs2\" as user \"francs\". francs2=&gt; \\dt List of relationsSchema | Name | Type | Owner --------+------------+-------+--------francs | big_table1 | table | francsfrancs | big_table2 | table | francsfrancs | big_table3 | table | francsfrancs | big_table4 | table | francsfrancs | big_table5 | table | francsfrancs | test_1 | table | francsfrancs | test_json1 | table | francs 备注：导入正常。 pg_dump 开启并行注意事项 要开启 pg_dump 的并行参数，归档格式必须为 -Fd 。 开启并行后，会产生一个数据目录，并在这个目录中产生已压缩的数据文件。 开启并行后，运行过程中数据库负载会增加，但总体运行时间要短些。 使用 9.3 版本的 pg_dump 并行备份 pre-9.2 版本需要特别注意，建议不要这么做。 参考 http://wiki.postgresql.org/wiki/What’s_new_in_PostgreSQL_9.3 http://www.postgresql.org/docs/9.3/static/app-pgdump.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"PostgreSQL 编译安装报错：\"Undefined reference to gzopen64\"","slug":"20130517091459","date":"2013-05-17T01:14:59.000Z","updated":"2018-09-04T01:34:03.152Z","comments":true,"path":"20130517091459.html","link":"","permalink":"https://postgres.fun/20130517091459.html","excerpt":"","text":"昨天在编译安装 PostgreSQL 9.3 Beta1 时报错，这个错困扰了好长时间，记录下。 环境信息OS： CentOS release 5.4 (Final)PostgreSQL: 9.3Beta1 备注：开始一直怀疑与平台或 PG 版本有关，后来发现不是这样。 安装过程configure1./configure --prefix=/opt/pgsql9.3beta1 --with-pgport=1925 --with-wal-segsize=16 --with-wal-blocksize=64 --with-perl --with-python --with-openssl --with-pam --with-ldap --with-libxml --with-libxslt --enable-thread-safety 备注： configure 通过，无报错。 gmake1gmake world 备注：这个命令抛出以下错误信息：只显示最后一段。123456789101112131415....gmake[4]: Leaving directory `/opt/soft_bak/postgresql-9.3beta1/src/common'gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wmissing-format-attribute -Wformat-security -fno-strict-aliasing -fwrapv pg_dump.o common.o pg_dump_sort.o keywords.o kwlookup.o pg_backup_archiver.o pg_backup_db.o pg_backup_custom.o pg_backup_null.o pg_backup_tar.o pg_backup_directory.o pg_backup_utils.o parallel.o compress_io.o dumputils.o -L../../../src/port -lpgport -L../../../src/common -lpgcommon -L../../../src/interfaces/libpq -lpq -L../../../src/port -L../../../src/common -L/usr/lib -Wl,-rpath,'/opt/pgsql9.3beta1/lib',--enable-new-dtags -lpgport -lpgcommon -lxslt -lxml2 -lpam -lssl -lcrypto -lz -lreadline -ltermcap -lcrypt -ldl -lm -o pg_dumppg_backup_archiver.o: In function `SetOutput':pg_backup_archiver.c:(.text+0x4117): undefined reference to `gzopen64'compress_io.o: In function `cfopen':compress_io.c:(.text+0x202): undefined reference to `gzopen64'collect2: ld returned 1 exit statusgmake[3]: * [pg_dump] Error 1gmake[3]: Leaving directory `/opt/soft_bak/postgresql-9.3beta1/src/bin/pg_dump'gmake[2]: * [all-pg_dump-recurse] Error 2gmake[2]: Leaving directory `/opt/soft_bak/postgresql-9.3beta1/src/bin'gmake[1]: * [all-bin-recurse] Error 2gmake[1]: Leaving directory `/opt/soft_bak/postgresql-9.3beta1/src'gmake: * [world-src-recurse] Error 2 备注：网上查了些资料，说是要安装 zlib 和 zlib-devel 包， 当然也有方法规避这个问题，比如configure 时加上 “–without-zlib”，这个功能是需要的，故不建议这么做，接着检查以下。 检查是否安装 zlib 和 zlib-devel123456[root@server1 ~]# rpm -qa | grep zlibzlib-devel-1.2.3-7.el5jzlib-demo-1.0.7-4jpp.1jzlib-javadoc-1.0.7-4jpp.1jzlib-1.0.7-4jpp.1zlib-1.2.3-7.el5 备注：说明已安装 1.2.3 版本的 zlib，后来咨询了系统平台比较熟悉的同事，终于发现了问题。问题是：系统安装了多个版本的 zlib。 查看 1.2.7 版本的 zlib123456789[root@server1 ~]# ll /usr/local/include/zlib.h-rw-r--r-- 1 root root 86717 Nov 6 2012 /usr/local/include/zlib.h[root@mpchat-server1 ~]# [root@server1 ~]# head -n 5 /usr/local/include/zlib.h/* zlib.h -- interface of the 'zlib' general purpose compression library version 1.2.7, May 2nd, 2012 Copyright (C) 1995-2012 Jean-loup Gailly and Mark Adler 查看 1.2.3 版本的 zlib12345678[root@server1 ~]# ll /usr/include/zlib.h-rw-r--r-- 1 root root 66188 Jul 17 2012 /usr/include/zlib.h [root@server1 ~]# head -n 5 /usr/include/zlib.h/* zlib.h -- interface of the 'zlib' general purpose compression library version 1.2.3, July 18th, 2005 Copyright (C) 1995-2005 Jean-loup Gailly and Mark Adler 备注：可见系统安装了两个版本的 zlib，而且 1.2.7 版本通过 “rpm -qa “ 命令根本查不到，应该是源码安装的，并且 /usr/local 目录的优先级更高，所以在编译时默认找的的 1.2.7 版本的 zlib。故编译通不过。 解决方法临时将 /usr/local/include/zlib.h 移动其它地方 1 [root@server1 ~]# mv /usr/local/include/zlib.h /root 2 cd /opt/soft_bak/postgresql-9.3beta1 &amp;&amp; make clean 3 configure 4 gmake world ( 这时终于不报这个错了。) 5 把文件 /root/zlib.h 移回去 [root@server1 ~]# mv /root/zlib.h /usr/local/include/ 6 接下来安装软件，建库。。。。 参考 http://www.postgresql.org/message-id/CAFrxt0hEtGddnTLqjRGdsLRZZtNgG3=sCVu_ji6ac33Hs34MCw@mail.gmail.com http://www.zlib.net/","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL9.3Beta1：JSON 功能增强","slug":"20130516133630","date":"2013-05-16T05:36:30.000Z","updated":"2018-09-04T01:34:03.089Z","comments":true,"path":"20130516133630.html","link":"","permalink":"https://postgres.fun/20130516133630.html","excerpt":"","text":"在 PostgreSQL 9.2 版本中已经支持 JSON 类型，不过支持的操作非常有限，仅支持以下函数 array_to_json(anyarray [, pretty_bool])row_to_json(record [, pretty_bool]) 如果想扩充 JSON 其它函数，得另外安装一些外部模块，好在 9.3 版本 JSON 增加了多种函数和操作符，这增强了 JSON 的功能，接下来演示下。 JSON 操作符介绍1.1 创建 json 表并插入数据1234567891011121314151617181920212223242526272829303132francs=&gt; create table test_json1 (id serial primary key,name json);CREATE TABLE francs=&gt; insert into test_json1 (name) values ('&#123;\"col1\":1,\"col2\":\"francs\",\"col3\":\"male\"&#125;');INSERT 0 1francs=&gt; insert into test_json1 (name) values ('&#123;\"col1\":2,\"col2\":\"fp\",\"col3\":\"female\"&#125;');INSERT 0 1 francs=&gt; select * From test_json1;id | name ----+------------------------------------------ 1 | &#123;\"col1\":1,\"col2\":\"francs\",\"col3\":\"male\"&#125; 2 | &#123;\"col1\":2,\"col2\":\"fp\",\"col3\":\"female\"&#125;(2 rows) francs=&gt; create table test_1 (id int4,name varchar(32),flag char(1));CREATE TABLE francs=&gt; insert into test_1 values (1,'a','f');INSERT 0 1francs=&gt; insert into test_1 values (2,'b','f');INSERT 0 1francs=&gt; insert into test_1 values (3,'c','t');INSERT 0 1 francs=&gt; select * from test_1;id | name | flag----+------+------ 1 | a | f 2 | b | f 3 | c | t(3 rows) 备注：创建两张测试表，其中第一张为 json 表。 1.2 操作符 -&gt; 介绍操作符 -&gt; 可以返回 json 类型的字段值，例如：12345francs=&gt; select id ,name -&gt;'col1' col1, name -&gt; 'col2' col2, name -&gt; 'col3' col3 from test_json1 where id=1;id | col1 | col2 | col3 ----+------+----------+-------- 1 | 1 | \"francs\" | \"male\"(1 row) 1.3 操作符 -&gt;&gt; 介绍操作符 -&gt;&gt; 与之前的 -&gt; 类似，只不过返回的是 text 类型，例如：12345francs=&gt; select id ,name -&gt;&gt;'col1' col1, name -&gt;&gt; 'col2' col2, name -&gt;&gt; 'col3' col3 from test_json1 where id=1;id | col1 | col2 | col3----+------+--------+------ 1 | 1 | francs | male(1 row) 1.4 操作符 #&gt; 介绍操作符 #&gt; 返回 json 数据字段指定的元素，如下：1234567891011francs=&gt; select '&#123;\"a\":[1,2,3],\"b\":[4,5,6]&#125;'::json#&gt;'&#123;b,1&#125;';?column?----------5(1 row) francs=&gt; select '&#123;\"a\":[1,2,3],\"b\":[4,5,6]&#125;'::json#&gt;'&#123;a,2&#125;';?column?----------3(1 row) JSON 函数介绍2.1 json_each(json) 函数1234567891011121314151617181920francs=&gt; select * from test_json1 where id=1;id | name ----+------------------------------------------ 1 | &#123;\"col1\":1,\"col2\":\"francs\",\"col3\":\"male\"&#125;(1 row) francs=&gt; select * from json_each((select name from test_json1 where id=1));key | value ------+----------col1 | 1col2 | \"francs\"col3 | \"male\"(3 rows) francs=&gt; select * from json_each('&#123;\"a\":\"foo\", \"b\":\"bar\"&#125;');key | value-----+-------a | \"foo\"b | \"bar\"(2 rows) 2.2 json_each_text(json) 函数1234567francs=&gt; select * from json_each_text((select name from test_json1 where id=1));key | value ------+--------col1 | 1col2 | francscol3 | male(3 rows) 2.3 row_to_json 函数12345678910111213francs=&gt; select row_to_json(test_1) from test_1; row_to_json --------------------------------&#123;\"id\":1,\"name\":\"a\",\"flag\":\"f\"&#125;&#123;\"id\":2,\"name\":\"b\",\"flag\":\"f\"&#125;&#123;\"id\":3,\"name\":\"c\",\"flag\":\"t\"&#125;(3 rows) francs=&gt; select row_to_json(test_1) from test_1 where id=1; row_to_json --------------------------------&#123;\"id\":1,\"name\":\"a\",\"flag\":\"f\"&#125;(1 row) 备注：这个函数在 9.2 就有，将结果集转换成 json，这里也记录下。 聚合函数 Json_agg(record)最后介绍新增加的聚合函数 json_agg(record)，此函数用来将结果集转换成 JSON 数组。 3.1 例1123456789101112131415francs=&gt; alter table test_json1 add grade int4 default '6';ALTER TABLE francs=&gt; select * from test_json1;id | name | grade----+------------------------------------------+------- 1 | &#123;\"col1\":1,\"col2\":\"francs\",\"col3\":\"male\"&#125; | 6 2 | &#123;\"col1\":2,\"col2\":\"fp\",\"col3\":\"female\"&#125; | 6(2 rows) francs=&gt; select json_agg(name) from test_json1 group by grade; json_agg ------------------------------------------------------------------------------------[&#123;\"col1\":1,\"col2\":\"francs\",\"col3\":\"male\"&#125;, &#123;\"col1\":2,\"col2\":\"fp\",\"col3\":\"female\"&#125;](1 row) 备注：结果很明显。 3.2 例2123456789101112131415francs=&gt; select * from test_1;id | name | flag----+------+------ 1 | a | f 2 | b | f 3 | c | t(3 rows) francs=&gt; select json_agg(a) from test_1 a; json_agg -----------------------------------[&#123;\"id\":1,\"name\":\"a\",\"flag\":\"f\"&#125;, + &#123;\"id\":2,\"name\":\"b\",\"flag\":\"f\"&#125;, + &#123;\"id\":3,\"name\":\"c\",\"flag\":\"t\"&#125;](1 row) 备注：JSON 函数还是比较复杂的，更多内容请参考手册。 参考 http://www.postgresql.org/docs/9.3/static/functions-json.html http://www.postgresql.org/docs/9.3/static/functions-aggregate.html http://michael.otacoo.com/postgresql-2/postgres-9-3-feature-highlight-json-data-generation/ http://michael.otacoo.com/postgresql-2/postgres-9-3-feature-highlight-json-parsing-functions/ waiting-for-9-3-json-generation-improvements/","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"JSON/JSONB","slug":"JSON-JSONB","permalink":"https://postgres.fun/tags/JSON-JSONB/"}]},{"title":"PostgreSQL 9.3Beta1：支持可写的外部表(Writeable Foreign Tables)","slug":"20130515135934","date":"2013-05-15T05:59:34.000Z","updated":"2018-09-04T01:34:03.011Z","comments":true,"path":"20130515135934.html","link":"","permalink":"https://postgres.fun/20130515135934.html","excerpt":"","text":"PostgreSQL 9.3 新增的一个主要特性是支持可写的外部表 (foreign tables)，而在 9.3 版本之前外部表只读，关于外部表的使用可以参考之前的 blog： pgsql_fdw oracle_fdw mysql_fdw file_fdw 在之前版本要实现跨库操作，一般通过 dblink 或者 pgsql_fdw ，而 dblink 用起来非常不方便；9.3 版本支持外部表可写，对跨库操作的支持进一步提升，越来越像 oracle 的 dblink 了，接下来演示下： 源库数据准备123456789101112131415161718192021[pg93@redhatB pg_root]$ psql source_db source_dbpsql (9.3beta1)Type \"help\" for help. source_db=&gt; create table test_1 (id int4 primary key ,name varchar(32),amount numeric);CREATE TABLE source_db=&gt; insert into test_1 values (1,'a',100);INSERT 0 1source_db=&gt; insert into test_1 values (2,'b',200);INSERT 0 1source_db=&gt; insert into test_1 values (3,'c',300);INSERT 0 1 source_db=&gt; select * from test_1; id | name | amount ----+------+-------- 1 | a | 100 2 | b | 200 3 | c | 300(3 rows) 安装并配置 postgres_fdw2.1 安装 postgres_fdw12francs=# create extension postgres_fdw;CREATE EXTENSION 备注：详细内容参考之前的 blog: PostgreSQL 9.3Beta1: 新增 postgres_fdw 外部模块。 2.2 creater server 并赋权1234567source_db=&gt; \\c francs postgresfrancs=# CREATE SERVER srv_source_db FOREIGN DATA WRAPPER postgres_fdw francs-# OPTIONS (host '127.0.0.1', port '1925', dbname 'source_db');CREATE SERVER francs=# grant usage on foreign server srv_source_db to francs;GRANT 2.3 CREATE MAPPING USER123francs=# CREATE USER MAPPING FOR francs SERVER srv_source_dbfrancs-# OPTIONS (user 'source_db', password 'source_db');CREATE USER MAPPING 2.4 创建外部表1234567francs=&gt; CREATE FOREIGN TABLE ft_test_1 (francs(&gt; id integer , francs(&gt; name character varying(32), francs(&gt; amount numeric ) francs-&gt; SERVER srv_source_dbfrancs-&gt; OPTIONS (schema_name 'source_db',table_name 'test_1'); CREATE FOREIGN TABLE 2.5 查询测试1234francs=&gt; select * from ft_test_1;ERROR: password is requiredDETAIL: Non-superuser cannot connect if the server does not request a password.HINT: Target server s authentication method must be changed. 备注：如果遇到这个错误，在 pg_hba.conf 增加一条策略：红色那条为新增，文件内容未全部列出。修改之后执行 pg_ctl reload 命令。 123# IPv4 local connections:host source_db source_db 127.0.0.1/32 md5host all all 127.0.0.1/32 trust 2.6 再次查询测试1234567 francs=&gt; select * from ft_test_1; id | name | amount ----+------+-------- 1 | a | 100 2 | b | 200 3 | c | 300(3 rows) 备注：以上已经安装并配置好了 postgres_fdw ，接着测试外部表的可写情况。 外部表可写测试3.1 插入测试3.1.1 francs 库增加一条数据1234567891011francs=&gt; insert into ft_test_1 values (4,'d',400);INSERT 0 1 francs=&gt; select * from ft_test_1; id | name | amount ----+------+-------- 1 | a | 100 2 | b | 200 3 | c | 300 4 | d | 400(4 rows) 3.1.2 source_db 库验证1234567891011francs=&gt; \\c source_db source_dbYou are now connected to database \"source_db\" as user \"source_db\". source_db=&gt; select * from test_1; id | name | amount ----+------+-------- 1 | a | 100 2 | b | 200 3 | c | 300 4 | d | 400(4 rows) 备注：验证 OK。 3.2 删除测试3.2.1 francs 库删除一条数据12345678910francs=&gt; delete from ft_test_1 where id=1;DELETE 1 francs=&gt; select * from ft_test_1 ; id | name | amount ----+------+-------- 2 | b | 200 3 | c | 300 4 | d | 400(3 rows) 3.2.2 source_db 库验证12345678910francs=&gt; \\c source_db source_db;You are now connected to database \"source_db\" as user \"source_db\". source_db=&gt; select * from test_1 ; id | name | amount ----+------+-------- 2 | b | 200 3 | c | 300 4 | d | 400(3 rows) 备注：验证 OK。同理 update 测试也正常，这里不介绍了。 事务测试123456789101112131415161718192021222324francs=&gt; begin;BEGINfrancs=&gt; insert into ft_test_1 values (5,'e',500);INSERT 0 1 francs=&gt; select * from ft_test_1; id | name | amount ----+------+-------- 3 | c | 300 4 | d | 400 2 | b | 2000 5 | e | 500(4 rows) francs=&gt; rollback;ROLLBACK francs=&gt; select * from ft_test_1; id | name | amount ----+------+-------- 3 | c | 300 4 | d | 400 2 | b | 2000(3 rows) 备注：远程操作依然支持事务。 参考 postgres_fdw PostgreSQL: Using pgsql_fdw connect remote PostgteSQL DB WAITING FOR 9.3 – SUPPORT WRITABLE FOREIGN TABLES","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.3Beta1：新增 postgres_fdw 外部模块","slug":"20130515134001","date":"2013-05-15T05:40:01.000Z","updated":"2018-09-04T01:34:02.964Z","comments":true,"path":"20130515134001.html","link":"","permalink":"https://postgres.fun/20130515134001.html","excerpt":"","text":"在 PostgreSQL 9.3 已将 postgres_fdw 合并到源码中，因此可以直接安装，不需要从其它网站下载安装。postgres_fdw 的模块用来访问外部 postgresql 数据库，功能类于之前版本的 pgsql_fdw，接下来演示下。 查看 postgres_fdw 文件12345[pg93@redhatB extension]$ cd /opt/pgsql9.3beta1/share/extension/ [pg93@redhatB extension]$ ll *postgres_fdw*-rw-r--r--. 1 root root 507 May 15 10:10 postgres_fdw--1.0.sql-rw-r--r--. 1 root root 172 May 15 10:10 postgres_fdw.control 安装 postgres_fdw123456[pg93@redhatB pg_root]$ psql francs postgrespsql (9.3beta1)Type \"help\" for help. francs=# create extension postgres_fdw;CREATE EXTENSION 备注：由于 postgres_fdw 模块已合并到 postgreql 源码中，只需通过 “create extension “ 命令安装即可。 查看已安装的模块1234567francs=# \\dx List of installed extensions Name | Version | Schema | Description --------------+---------+------------+----------------------------------------------------plpgsql | 1.0 | pg_catalog | PL/pgSQL procedural languagepostgres_fdw | 1.0 | public | foreign-data wrapper for remote PostgreSQL servers(2 rows) postgres_fdw 的用法postgres_fdw 用法是常规外部表的用法，在接下来 PostgreSQL 9.3Beta1：支持可写的外部表(foreign tables) blog 中会详细介绍。 参考 PostgreSQL: Using pgsql_fdw connect remote PostgteSQL DB postgres_fdw","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.3 Beta 1 Released","slug":"20130513172216","date":"2013-05-13T09:22:16.000Z","updated":"2018-09-04T01:34:02.902Z","comments":true,"path":"20130513172216.html","link":"","permalink":"https://postgres.fun/20130513172216.html","excerpt":"","text":"PostgreSQL 的版本更新非常快，今天注意到 9.3 beta1 版已经可以下载使用了。 主要特性 Writeable Foreign Tables, enabling pushing data to other databases pgsql_fdw driver for federation of PostgreSQL databases Automatically updatable VIEWs MATERIALIZED VIEW declaration LATERAL JOINs Additional JSON constructor and extractor functions Indexed regular expression search Disk page checksums to detect filesystem failures 更多特性 Fast failover to replicas for high availability Streaming-only remastering of replicas Performance and locking improvements for Foreign Key locks Parallel pg_dump for faster backups Directories for configuration files pg_isready database connection checker COPY FREEZE for reduced IO bulk loading User-defined background workers for automating database tasks Recursive view declaration lock_timeout directive 下载地址http://www.postgresql.org/ftp/source/v9.3beta1/ Release Notehttp://www.postgresql.org/docs/9.3/static/release-9-3.html 接下来好好学习 PostgreSQL 9.3 的功能和变化。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：使用 \"do ...\" 命令执行代码段","slug":"20130508173342","date":"2013-05-08T09:33:42.000Z","updated":"2018-09-04T01:34:02.839Z","comments":true,"path":"20130508173342.html","link":"","permalink":"https://postgres.fun/20130508173342.html","excerpt":"","text":"我们知道可以使用 function 来实现较复杂的业务逻辑，有时候在数据库维护过程中需要执行稍复杂点的维护，但又不想写 function ，这时可以使用 “do “命令了， 这里介绍 “do “命令执行代码段的用法。 在介绍 “do” 命令之前先来看一个函数: 函数 fun_ins_test_3()1234567891011121314CREATE or replace FUNCTION fun_ins_test_3() RETURNS INTEGER AS $$ DECLARE i INTEGER; BEGIN perform 1 from pg_tables where schemaname = 'francs' and tablename = 'test_3'; if not found then create table test_3(id int4); end if; for i in 1 .. 10000 loop insert into test_3 values (i); end loop; return 1; END;$$ LANGUAGE 'plpgsql'; 备注：函数代码非常简单，向一张表插入数据。 执行函数12345francs=&gt; select count(*) from test_3;count-------10000(1 row) 备注：上面通过创建一个函数向表 test_3 中插入数据，有时觉得一个简单的操作不想再创建一个函数，这时就可以使用 do 命令执行代码断了，简单的说 do 命令的代码段就是没有入参并且返回值为空的函数的语句体， 使用 DO 命令12345678910111213do $$ DECLARE i INTEGER; BEGIN perform 1 from pg_tables where schemaname = 'francs' and tablename = 'test_3'; if not found then create table test_3(id int4); end if; for i in 1 .. 10000 loop insert into test_3 values (i); end loop; END;$$ LANGUAGE 'plpgsql'; 执行测试12345678910111213141516171819202122232425francs=&gt; drop table test_3;DROP TABLE francs=&gt; do $$ francs$&gt; DECLAREfrancs$&gt; i INTEGER;francs$&gt; BEGINfrancs$&gt; perform 1 from pg_tables where schemaname = 'francs' and tablename = 'test_3';francs$&gt; francs$&gt; if not found thenfrancs$&gt; create table test_3(id int4);francs$&gt; end if;francs$&gt; francs$&gt; for i in 1 .. 10000 loopfrancs$&gt; insert into test_3 values (i);francs$&gt; end loop;francs$&gt; END;francs$&gt; $$ LANGUAGE 'plpgsql';DOfrancs=&gt; select count(*) from test_3;count-------10000(1 row) 备注：结果和执行函数一样。 do 命令可以很好的和 shell 脚本结合，在维护过程中非常有用，下面给个简单的例子。 一个包含DO命令的 Shell 脚本123456789101112131415161718192021[pg92@redhatB tf]$ vim insert_test_03.sh #!/bin/bash psql -h 127.0.0.1 -d francs -U francs &lt;&lt; EOFdo $$ DECLARE i INTEGER; BEGIN perform 1 from pg_tables where schemaname = 'francs' and tablename = 'test_3'; if not found then create table test_3(id int4); end if; for i in 1 .. 10000 loop insert into test_3 values (i); end loop; END;$$ LANGUAGE 'plpgsql';EOF 备注：$$ 符加需要转义，否则会报错。 测试12[pg92@redhatB tf]$ ./insert_test_03.shDO 附: DO 命令语法1234567891011Name DO -- execute an anonymous code block Synopsis DO [ LANGUAGE lang_name ] code Description DO executes an anonymous code block, or in other words a transient anonymous function in a procedural language.The code block is treated as though it were the body of a function with no parameters, returning void. It is parsed and executed a single time.The optional LANGUAGE clause can be written either before or after the code block. 参考 http://www.postgresql.org/docs/9.2/static/sql-do.html http://www.postgresql.org/docs/9.2/static/view-pg-tables.html http://blog.163.com/digoal@126/blog/static/163877040201312241028667/","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：测试 Maintenance_work_mem 参数对索引创建的影响","slug":"20130427204348","date":"2013-04-27T12:43:48.000Z","updated":"2018-09-04T01:34:02.777Z","comments":true,"path":"20130427204348.html","link":"","permalink":"https://postgres.fun/20130427204348.html","excerpt":"","text":"索引创建是数据库管理员的基础工作之一，有时由于业务需求需要对生产库的大表新建索引，Oracle 创建索引时可以开启并行 目前 PostgreSQL 本身并不提供并行创建索引功能，如何能加快索引的创建呢？本文测试 maintenance_work_mem 参数对索引创建的影响。 创建测试表1234567891011121314francs=&gt; create table test_1(id int4 primary key, name character varying(64),create_time timestamp without time zone);NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_1_pkey\" for table \"test_1\"CREATE TABLE francs=&gt; insert into test_1 select generate_series(1,10000000),generate_series(1,10000000)||'_francs',clock_timestamp();INSERT 0 10000000 francs=&gt; select * From test_1 limit 3;id | name | create_time ----+----------+---------------------------- 1 | 1_francs | 2013-04-27 16:28:11.465584 2 | 2_francs | 2013-04-27 16:28:11.465928 3 | 3_francs | 2013-04-27 16:28:11.465943(3 rows) 备注：创建一张 1 千万数据的表做测试。 测试一 设置 maintenance_work_mem 为 1MB1234567891011121314francs=&gt; show maintenance_work_mem;maintenance_work_mem----------------------256MB(1 row) francs=&gt; set maintenance_work_mem='1MB';SET francs=&gt; show maintenance_work_mem;maintenance_work_mem----------------------1MB(1 row) 创建索引1234567francs=&gt; create index concurrently idx_test_1_name on test_1 using btree (name);CREATE INDEXTime: 72676.986 ms francs=&gt; create index concurrently idx_test_1_ctime on test_1 using btree (create_time);CREATE INDEXTime: 40593.181 ms 测试二 设置 maintenance_work_mem 为 10 MB1234567891011121314francs=&gt; show maintenance_work_mem ;maintenance_work_mem----------------------1MB(1 row) francs=&gt; set maintenance_work_mem ='10MB';SET francs=&gt; show maintenance_work_mem ;maintenance_work_mem----------------------10MB(1 row) 创建索引（先删除之前的两个索引）123456francs=&gt; create index concurrently idx_test_1_name on test_1 using btree (name);Time: 83767.538 ms francs=&gt; create index concurrently idx_test_1_ctime on test_1 using btree (create_time);CREATE INDEXTime: 47679.868 ms 测试三 设置 maintenance_work_mem 为 1 GB1234567891011121314francs=&gt; show maintenance_work_mem ;maintenance_work_mem----------------------10MB(1 row) francs=&gt; set maintenance_work_mem ='1GB';SET francs=&gt; show maintenance_work_mem ;maintenance_work_mem----------------------1GB(1 row) 创建索引（先删除之前的两个索引）1234567francs=&gt; create index concurrently idx_test_1_name on test_1 using btree (name);CREATE INDEXTime: 47657.969 ms francs=&gt; create index concurrently idx_test_1_ctime on test_1 using btree (create_time);CREATE INDEXTime: 18953.915 ms 测试四 设置 maintenance_work_mem 为 2 GB1234567891011francs=&gt; show maintenance_work_mem;maintenance_work_mem----------------------1GB(1 row) francs=&gt; show maintenance_work_mem;maintenance_work_mem----------------------2047MB(1 row) 创建索引（先删除之前的两个索引） francs=&gt; create index concurrently idx_test_1_name on test_1 using btree (name); CREATE INDEX Time: 50843.263 ms francs=&gt; create index concurrently idx_test_1_ctime on test_1 using btree (create_time); CREATE INDEX Time: 13304.179 ms 总结备注：从以上看出，增加 maintenance_work_mem 参数能够加速索引创建，当然不建议在 postgresql.conf 中设置较大的 maintenance_work_mem，因为 autovacuum 进程消耗的内存受这个参数影响， 一般仅在创建索引的 session 设置 session 级maintenance_work_mem 参数。","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"[阿弟] 分享 plpgsql 中各种使用方法性能测试对比 ","slug":"20130425211256","date":"2013-04-25T13:12:56.000Z","updated":"2018-09-04T01:34:02.714Z","comments":true,"path":"20130425211256.html","link":"","permalink":"https://postgres.fun/20130425211256.html","excerpt":"","text":"以下是 Chinese bbs 阿弟哥总结的 plpgsql 的性能相关的帖子，非常有用，供日后参考： 直接函数调用跟select into付值性能差别采用直接函数调用性能是select into的n倍以上，具体见http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57522 plpgsql中case与if的性能对比case的执行效率比if高出了10%有多，具体见http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57606 plpgsql中采用exception，分步检查数据重复，合并执行性能测试合并执行的综合性能是最佳的，实际生产环境中应该是重复机率不会很高的,整体来说提高个20%左右，具体见http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57607 plpgsql函数提取数据付值不同方式的性能差别直接在sql中付值明显的高出10%以上的性能，具体见http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57608 plpgsql中采用perform 和 select执行函数或句子时的性能差别如果执行结果是不需要返回值时则采用perform的性能会更好，大约有3%提升，具体见http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57609 分享pg中数据类型的转换对性能的影响类型的转换对于性能的影响大约是15%左右的性能开销，具体见http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57610 在plpgsql中使用cursor和直接select的性能差别我们可以看到同样的work_mem,采用cursor的话,order by 需要 temp read=32474 written=32472，而用select直接提取数据的话，temp read=16237 written=16236，少了一半，所以性能提高明显，如果提高work_mem后，性能比较接近了一些，但还是select 直接提取数据来得好，具体见http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57611 plpgsql中返回与不返回数据的性能差别在自定义函数里，如果不需要返回值的话，则直接return void能获得30%性能提升http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57612 plpgsql自定义函数常用返回数据类型测试从测试的结果来看，数字形是integer跟boolean是一个筹级的，numeric次之，smallint,bigint比较差,float是最差的。字符类型相差不大，跟smallint,bigint差不多 ，具体见http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57613 分享plpgsql返回明确和不明确的数据类型时性能差别返回数据里要明确的告诉pg的解释器，否则性能将会降低30%，具体见http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57614 plpgsql中CURRENT OF cur更新方式性能测试CURRENT OF cur的性能只有明确的指定条件的1/4,方便使用了，但要付出性能的代价http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=57615","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"PostgreSQL：关于 Socket 文件 \"/tmp/.s.PGSQL.nnnn\" 丢失处理","slug":"20130425095258","date":"2013-04-25T01:52:58.000Z","updated":"2018-09-04T01:34:02.652Z","comments":true,"path":"20130425095258.html","link":"","permalink":"https://postgres.fun/20130425095258.html","excerpt":"","text":"在学习 PostgreSQL 的过程中，相信很多人都遇到过下面这个错误，错误代码如下： 问题描述错误代码1234[pg92@redhatB ~]$ psqlpsql: could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket \"/tmp/.s.PGSQL.1921\"? 备注：报错信息已经很显示，找不到文件 /tmp/.s.PGSQL.1921。 查看 /tmp123[root@redhatB tmp]# ll -alrt /tmp | grep \"1921\"-rw-------. 1 pg92 pg92 56 4月 24 20:42 .s.PGSQL.1921.lock备注：可以看到，文件 /tmp/.s.PGSQL.1921 不存在。 “/tmp/.s.PGSQL.1921” 文件先来看看 socket 文件 “/tmp/.s.PGSQL.1921”，其中 1921 是 pg 的端口号; socket 文件可以通过postgresql.conf 文件以下参数配置：12#unix_socket_directory = ''#unix_socket_permissions = 0777 备注：其中参数 unix_socket_directory 用来配置 socket 文件的目录，默认是 /tmp 目录，参数unix_socket_permissions 用来设置 socket 文件的权限。回到开头的问题，如何解决呢？这里提供两种方法。 方法一: 使用 -h 连接参数12345678910pg92@redhatB ~]$ psqlpsql: could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket &quot;/tmp/.s.PGSQL.1921&quot;? [pg92@redhatB ~]$ psql -h 127.0.0.1psql (9.2.1)Type &quot;help&quot; for help. postgres=# q 备注：指定 -h 参数后，不再报之前的错。 方法二: 重启数据库重启数据库123456[pg92@redhatB ~]$ pg_ctl stop -m fast -D $PGDATAwaiting for server to shut down.... doneserver stopped [pg92@redhatB ~]$ pg_ctl start -D $PGDATAserver starting 再次查看 socket 文件123[root@redhatB tmp]# ll -alrt /tmp | grep \"1921\"-rw-------. 1 pg92 pg92 56 4月 24 21:06 .s.PGSQL.1921.locksrwxrwxrwx. 1 pg92 pg92 0 4月 24 21:06 .s.PGSQL.1921 备注：数据库重启后，在 /tmp 目录下会产生 .s.PGSQL.1921 文件。 连接测试12345[pg92@redhatB ~]$ psqlpsql (9.2.1)Type \"help\" for help. postgres=# 备注：这时 psql 连接时不再报之前的错。 疑问: 有没有不重启，也不带 -h 参数的方法解决这个问题？目前还没找到不重启，也不带 -h 参数就能解决的方法，本人也测试过复制文件 .s.PGSQL.1921 到 /tmp 目录，但不带 -h 参数的连接依然会报这个错，有兴趣的朋友可以自己试一试。 参考http://www.postgresql.org/docs/9.2/static/runtime-config-connection.html","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL：如何查询基于用户(role)设置的参数","slug":"20130422154754","date":"2013-04-22T07:47:54.000Z","updated":"2018-09-04T01:34:02.589Z","comments":true,"path":"20130422154754.html","link":"","permalink":"https://postgres.fun/20130422154754.html","excerpt":"","text":"PostgreSQL 可以针对服务级别，数据库级别，用户级别进行参数设置，例如在 postgresql.conf 设置的参数是针对整个服务级别，当然偶尔有这样的需求：需要对指定用户设置相关的参数，设置好用户的参数后，如何查询呢？下面演示下： 方法一: 查询 pg_user 视图1.1 设置用户的 log_statement 参数12postgres=# alter role francs set log_statement=\"all\";ALTER ROLE 1.2 验证1234567891011postgres=# select * From pg_user where usename='francs';-[ RECORD 1 ]--------------------usename | francsusesysid | 24920usecreatedb | fusesuper | fusecatupd | fuserepl | fpasswd | valuntil |useconfig | &#123;log_statement=all&#125; 1.3 设置用户的 maintenance_work_mem 参数12postgres=# alter role francs set maintenance_work_mem=\"1GB\";ALTER ROLE 1.4 再次验证1234567891011postgres=# select * From pg_user where usename='francs';-[ RECORD 1 ]---------------------------------------------usename | francsusesysid | 24920usecreatedb | fusesuper | fusecatupd | fuserepl | fpasswd | valuntil |useconfig | &#123;log_statement=all,maintenance_work_mem=1GB&#125; 备注：上面是通过 pg_user.useconfig 查询。 方法二: 查询 pg_db_role_setting catalog 系统表2.1 pg_db_role_setting 表结构12345678Table \"pg_catalog.pg_db_role_setting\" Column | Type | Modifiers-------------+--------+-----------setdatabase | oid | not nullsetrole | oid | not nullsetconfig | text[] |Indexes:\"pg_db_role_setting_databaseid_rol_index\" UNIQUE, btree (setdatabase, setrole), tablespace \"pg_global\" 备注：可见 pg_db_role_setting 会针对数据库，用户级别进行记录。 2.2 验证12345678910postgres=# select oid,rolname from pg_authid where rolname='francs'; oid | rolname-------+---------24920 | francs(1 row) postgres=# select * From pg_db_role_setting where setrole=24920;setdatabase | setrole | setconfig -------------+---------+---------------------------------------------- 0 | 24920 | &#123;log_statement=all,maintenance_work_mem=1GB&#125; 备注：因为之前的设置只针对用户 francs 设置，而没指定相应的数据库，所以 setdatabase 值为 0。接着往下看。 2.3 用户 francs 登陆 francs 库时，设置 client_min_messages 参数12345678postgres=# alter role francs in database francs set client_min_messages='warning';ALTER ROLE postgres=# select * From pg_db_role_setting where setrole=24920;setdatabase | setrole | setconfig -------------+---------+---------------------------------------------- 0 | 24920 | &#123;log_statement=all,maintenance_work_mem=1GB&#125; 24922 | 24920 | &#123;client_min_messages=warning&#125; 备注：此时 setdatabase 值为 24922, 结果很明显了，不多解释。 2.4 再次查询 pg_user1234567891011postgres=# select * From pg_user where usename='francs';-[ RECORD 1 ]---------------------------------------------usename | francsusesysid | 24920usecreatedb | fusesuper | fusecatupd | fuserepl | fpasswd | valuntil |useconfig | &#123;log_statement=all,maintenance_work_mem=1GB&#125; 备注：之前针对数据库 francs 设置的 client_min_messages 参数在 pg_user 里是查不到的。这是与 pg_user 比较大的区别。 方法三: 通过 show 命令查看 这种方法使用某个用户登陆到相应数据库后，通过 show 命令查看，如下： 1234567891011121314151617[pg92@redhatB ~]$ psqlpsql (9.2.1)Type \"help\" for help. postgres=# show log_statement;log_statement---------------none(1 row) postgres=# \\c francs francsYou are now connected to database \"francs\" as user \"francs\".francs=&gt; show log_statement;log_statement---------------all(1 row) 备注：这种方法最直接，但很不方便。 取消用户级参数设置4.1 取消对用户 francs 的 maintenance_work_mem 设置12postgres=# alter role francs reset maintenance_work_mem;ALTER ROLE 4.2 验证1234567891011postgres=# select * From pg_user where usename='francs';-[ RECORD 1 ]--------------------usename | francsusesysid | 24920usecreatedb | fusesuper | fusecatupd | fuserepl | fpasswd | valuntil |useconfig | &#123;log_statement=all&#125; 备注：如果想取消多个参数，则使用”RESET ALL” 语法。 4.3 RESET ALL 语法1ALTER ROLE name [ IN DATABASE database_name ] RESET ALL 备注：比较简单，不再演示。 总结 pg_user 只能查询针对用户的设置，如果设置了用户登陆到某个库的参数设置，则查不到。 用户级别的参数设置建议查询 pg_db_role_setting 系统表，因为这记录了 “数据库 + 用户” 的参数设置。 参考 pg_user pg_db_role_setting ALTER ROLE","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：关于参数 Superuser_reserved_connections","slug":"20130420164016","date":"2013-04-20T08:40:16.000Z","updated":"2018-09-04T01:34:02.526Z","comments":true,"path":"20130420164016.html","link":"","permalink":"https://postgres.fun/20130420164016.html","excerpt":"","text":"PostgreSQL 的参数 superuser_reserved_connections 一直没怎么关注，这两天遇到个与之相关的错误，才仔细查了下文档，有了进一步了解。 简单的说这个参数的意思是：为具有超级用户权限的用户预留的连接数，就像售火车票一样，不会全部售光，总会预留点，闲话少说，接下来的演示，相信你会对superuser_reserved_connections参数更加熟悉。 superuser_reserved_connections 参数配置设置参数为了便于演示，配置 postgresql.conf 文件，并设置以下参数，其它参数这里没列出。12max_connections = 15superuser_reserved_connections = 3 备注：max_connections 参数默认值为 100, superuser_reserved_connections 参数默认值为 3，我们调整如上，这两个参数修改之后需要重启才能生效。 重启数据库123456[pg92@redhatB pg_root]$ pg_ctl stop -m fast -D $PGDATAwaiting for server to shut down.... doneserver stopped [pg92@redhatB pg_root]$ pg_ctl start -D $PGDATAserver starting 查看参数是否生效123456789101112131415[pg92@redhatB pg_root]$ psqlspsql (9.2.1)Type \"help\" for help. postgres=# show max_connections ;max_connections-----------------15(1 row) postgres=# show superuser_reserved_connections ;superuser_reserved_connections--------------------------------3(1 row) 备注：修改好参数后，再次连到数据库里查看参数的值是否生效是个好习惯，因为有可能参数没设对，然后 reload 后也不会提示错误；如果谨慎些 ，还应查看数据库日志是否有报错。 pgbench 测试1[pg92@redhatB load_test]$ nohup pgbench -c 10 -T 60 -j 2 -n -d skytf -U skytf -f update_1.sql &gt; update_1.out &amp; 备注： update_1.sql 脚本略，这里开 10 个长连接，如下图： 重新打开 session12[pg92@redhatB londiste]$ psql francs francspsql: FATAL: remaining connection slots are reserved for non-replication superuser connections 备注：再次以普通用户连接时，报了以上错，提示剩余的连接是预留给 non-replication 超级用户的。 尝试超级用户连接12345[pg92@redhatB londiste]$ psql francs postgrespsql (9.2.1)Type \"help\" for help. francs=# 备注：第5,第6 步都是在第 4 步的 pgbench 脚本执行过程中的测试，这步超级用户则可以连接，但如果超过了 max_connection 设置，则会报以下错误。 psql: FATAL: sorry, too many clients already 总结 普通用户的最大连接数为 max_connections - superuser_reserved_connections; superuser_reserved_connections 参数建议根据实际需求设置，一般设置成 10 就够用了。 superuser_reserved_connections 参数设置的是给 non-replication 超级用户预留连接数。当普通用户连接数达到 max_connections - superuser_reserved_connections 时，会报第 5 步抛出的错。 当数据库连接数（普通用户+超级用户）达到 max_connections 时，则会报第 6 步抛出的错。 参考 http://www.postgresql.org/docs/9.2/static/runtime-config-connection.html#GUC-MAX-CONNECTIONS","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 序列无法删除一例","slug":"20130411204058","date":"2013-04-11T12:40:58.000Z","updated":"2018-09-04T01:34:02.464Z","comments":true,"path":"20130411204058.html","link":"","permalink":"https://postgres.fun/20130411204058.html","excerpt":"","text":"今天在开发环境上维护一个序列时，由于命名不规范，带来维护上的难度问题，简单的来看下今天遇到的问题。 遇到的问题库中的一个奇特序列1234567891011[pg92@redhatB ~]$ psql francs francspsql (9.2.1)Type \"help\" for help. francs=&gt; \\ds List of relationsSchema | Name | Type | Owner --------+---------------------------------+----------+--------francs | seq_test_1 | sequence | francspublic | \"public\".\"seq_log_op_logininfo\" | sequence | francs(2 rows) 备注：注意序列 “public”.”seq_log_op_logininfo”， 这个命名非常奇特，我们起初目标是将这个序列的 schema 转换成 francs。 尝试更改序列的 schema123francs=&gt; alter schema \"public\".\"seq_log_op_logininfo\" set schema francs;ERROR: syntax error at or near \".\"LINE 1: alter schema \"public\".\"seq_log_op_logininfo\" set schema fran... 备注：遇到错误，接着往下看。 无法引用，也无法删除12345francs=&gt; \\ds \"public\".\"seq_log_op_logininfo\"No matching relations found. francs=&gt; drop sequence \"public\".\"seq_log_op_logininfo\";ERROR: sequence \"seq_log_op_logininfo\" does not exist 备注：可见直接引用序列 “public”.”seq_log_op_logininfo” 是行不通的，这个问题困扰了我些许时间，因为我一直想通过 OID 删除它，快下班了，一时也想不到办法。后来在回家的路上，突然想到了一个非常简单的方法。 处理过程dump DDL1[pg92@redhatB tf]$ pg_dump -h 127.0.0.1 -E UTF8 -s -v francs &gt; francs.ddl 备注：导出 francs 库的表结构到文件 francs.ddl ，然后在文件 francs.ddl 看看这个序列到底是如何表示的。2.2 在文件 francs.ddl 中，找到如下代码12345678CREATE SEQUENCE \"\"\"public\"\".\"\"seq_log_op_logininfo\"\"\" START WITH 1600 INCREMENT BY 1 NO MINVALUE MAXVALUE 99999999999999999 CACHE 20; ALTER TABLE public.\"\"\"public\"\".\"\"seq_log_op_logininfo\"\"\" OWNER TO francs; 备注：原来这个序列需要这样引用: “””public””.””seq_log_op_logininfo””” 接着尝试：123456francs=&gt; \\ds \"\"\"public\"\".\"\"seq_log_op_logininfo\"\"\" List of relationsSchema | Name | Type | Owner --------+---------------------------------+----------+--------public | \"public\".\"seq_log_op_logininfo\" | sequence | francs(1 row) 备注：终于可以查看这个序列了。 2.4 更改序列 schema12345678910francs=&gt; alter sequence \"\"\"public\"\".\"\"seq_log_op_logininfo\"\"\" set schema francs;ALTER SEQUENCE francs=&gt; \\ds List of relationsSchema | Name | Type | Owner --------+---------------------------------+----------+--------francs | \"public\".\"seq_log_op_logininfo\" | sequence | francsfrancs | seq_test_1 | sequence | francs(2 rows) 删除序列12francs=&gt; drop sequence \"\"\"public\"\".\"\"seq_log_op_logininfo\"\"\";DROP SEQUENCE 备注：一切正常。 总结这是开发环境上遇到的一个问题，至于这个序列如何生成的不得而知，但有一点是需要明白的，在 PostgreSQL 中创建对像时应尽量不使用双引号，这会带来管理上的难度。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"Sequence","slug":"Sequence","permalink":"https://postgres.fun/tags/Sequence/"}]},{"title":"londiste3：常用管理命令","slug":"20130401163803","date":"2013-04-01T08:38:03.000Z","updated":"2018-09-04T01:34:02.401Z","comments":true,"path":"20130401163803.html","link":"","permalink":"https://postgres.fun/20130401163803.html","excerpt":"","text":"之前 Blog 介绍了 Londsite 的搭建和以及 Londsite 的压力测试，今天介绍下 Londsite 常用管理命令，权当整理。 Londiste 程序的起停1.1 语法12345678910DAEMON OPTIONS -r, --reload:: Reload config (send SIGHUP). -s, --stop:: Stop program safely (send SIGINT). -k, --kill:: Kill program immediately (send SIGTERM). 1.2 安全停止 Londsite1[pg91@redhat6 londiste]$ londiste3 -s s_skytf.ini 1.3 立即停止1[pg91@redhat6 londiste]$ londiste3 -k s_skytf.ini 1.4 启动 Londsite1[pg91@redhat6 londiste]$ londiste3 -d s_skytf.ini worker 1.5 重新加载配置文件1[pg91@redhat6 londiste]$ londiste3 -r s_skytf.ini Londiste 节点管理命令2.1 语法12345678pause Pause the consumer: the replication of the events is stopped and can be resumed later. resume When the consumer has been paused, let it replay again. change-provider &lt;tonode&gt; Make &lt;tonode&gt; become the new provider for the current node. 2.2 暂停复制 pause1234[pg91@redhat6 londiste]$ londiste3 s_skytf.ini pause2013-04-01 15:05:37,256 14808 INFO [node_s] Consumer job_skytf tagged as paused2013-04-01 15:05:37,260 14808 INFO Waiting for worker to accept2013-04-01 15:05:38,269 14808 INFO Consumer 'job_skytf' on node 'node_s' paused 2.3 继续开启复制 resume1234[pg91@redhat6 londiste]$ londiste3 s_skytf.ini resume2013-04-01 15:07:14,132 14831 INFO [node_s] Consumer job_skytf tagged as resumed2013-04-01 15:07:14,139 14831 INFO Waiting for worker to accept2013-04-01 15:07:15,152 14831 INFO Consumer 'job_skytf' on node 'node_s' resumed 2.4 更改 provider 节点 change-provider 暂没实验，以后补上。 复制管理命令3.1 语法12345678910111213141516add-table &lt;table&gt; [args] Add the table to the replication. See ADD ARGUMENTS below for the list of possible arguments. remove-table &lt;table&gt; Remove the table from the replication. add-seq &lt;seq&gt; [args] Add the sequence to the replication. See ADD ARGUMENTS below for the list of possible arguments. remove-seq &lt;seq&gt; Remove the sequence from the replication. resync &lt;table&gt; Do full copy of the table, again. 3.2 添加表到复制队列12[pg91@redhat6 londiste]$ londiste3 s_skytf.ini add-table skytf.test_lond12013-04-01 14:00:59,608 14036 INFO Table added: skytf.test_lond1 3.3 从复制队列删除表12[pg91@redhat6 londiste]$ londiste3 s_skytf.ini remove-table skytf.test_lond12013-04-01 14:00:52,437 14030 INFO Table removed: skytf.test_lond1 3.4 添加序列到复制队列12[pg92@redhatB londiste]$ londiste3 p_skytf.ini add-seq skytf.seq_test_12013-04-01 14:06:26,565 20599 INFO Sequence added: skytf.seq_test_1 3.5 从复制队列删除序列12[pg92@redhatB londiste]$ londiste3 p_skytf.ini remove-seq skytf.seq_test_12013-04-01 14:13:19,371 20886 INFO Sequence removed: skytf.seq_test_1 3.6 重新订阅表数据（全量同步）123456789[pg91@redhat6 londiste]$ londiste3 s_skytf.ini resync skytf.test_lond32013-04-01 15:16:07,572 15047 INFO Table skytf.test_lond3 state set to NULL [pg91@redhat6 londiste]$ londiste3 s_skytf.ini tablesTables on nodetable_name merge_state table_attrs---------------- --------------- ---------------skytf.test_lond2 ok skytf.test_lond3 in-copy 备注： resync 会从新从源节点同步数据，因此会带来较大的 IO 操作。 显示复制相关信息4.1 显示复制相关节点信息123456789[pg91@redhat6 londiste]$ londiste3 s_skytf.ini statusQueue: testing Local node: node_s node_p (root) | Tables: 3/0/0 | Lag: 45s, Tick: 1944 +--node_s (leaf) Tables: 2/1/0 Lag: 45s, Tick: 1944 4.2 显示节点成员（节点名，状态，连接信息）123456[pg91@redhat6 londiste]$ londiste3 s_skytf.ini membersMember info on node_s@testing:node_name dead node_location--------------- --------------- ---------------------------------------------------------------------------node_p False host=192.168.1.36 port=1921 user=postgres password=postgres123 dbname=skytfnode_s False host=192.168.1.35 port=1923 user=postgres password=postgres123 dbname=skytf 4.3 显示已加入复制队列的表123456[pg91@redhat6 londiste]$ londiste3 s_skytf.ini tablesTables on nodetable_name merge_state table_attrs---------------- --------------- ---------------skytf.test_lond2 ok skytf.test_lond3 ok 4.4 显示已加入复制队列的序列12345[pg91@redhat6 londiste]$ londiste3 s_skytf.ini seqsSequences on nodeseq_name local last_value--------------- --------------- ---------------skytf.seq_1 True 30101 4.5 列出订阅节点尚未加入复制队列的对象 [pg91@redhat6 londiste]$ londiste3 s_skytf.ini missing Missing objects on node obj_kind obj_name --------------- ---------------- r skytf.test_lond1 参考 Londiste3：搭建简单的基于表的复制 Londiste3：压力测试","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Londiste","slug":"Londiste","permalink":"https://postgres.fun/tags/Londiste/"}]},{"title":"Londiste3：压力测试","slug":"20130330121744","date":"2013-03-30T04:17:44.000Z","updated":"2018-09-04T01:34:02.339Z","comments":true,"path":"20130330121744.html","link":"","permalink":"https://postgres.fun/20130330121744.html","excerpt":"","text":"上篇 blog 介绍了搭建简单的基于表的 Londiste 复制，接下来使用 pgbench 测试下性能，看看在较大 update 压力下，复制是否依然正常。 关于 pgbench这里使用 pgbench 做压力测试，当然也可以选择其它工具，关于 pgbench 的使用可以参考之前的 blog: 使用 pgbench 进行数据库压力测试 Londiste 压力测试2.1 创建测试表并插入测试数据 (源库和目标库)12345678910111213141516171819202122232425skytf=&gt; create table test_lond2(id serial primary key ,amount int8,create_time timestamp(0) without time zone);NOTICE: CREATE TABLE will create implicit sequence \"test_lond2_id_seq\" for serial column \"test_lond2.id\"NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_lond2_pkey\" for table \"test_lond2\"CREATE TABLE skytf=&gt; \\d test_Lond2 Table \"skytf.test_lond2\" Column | Type | Modifiers -------------+--------------------------------+---------------------------------------------------------id | integer | not null default nextval('test_lond2_id_seq'::regclass)amount | bigint |create_time | timestamp(0) without time zone |Indexes: \"test_lond2_pkey\" PRIMARY KEY, btree (id)skytf=&gt; insert into test_lond2 (amount) select 0 from generate_series(1,10000);INSERT 0 10000 skytf=&gt; select * from test_lond2 limit 3;id | amount | create_time----+--------+------------- 1 | 0 | 2 | 0 | 3 | 0 |(3 rows) 2.2 添加表到 Londsite 复制列表 （源库）12[pg92@redhatB londiste]$ londiste3 p_skytf.ini add-table skytf.test_lond22013-03-30 10:53:19,964 29596 INFO Table added: skytf.test_lond2 2.3 添加表到 Londsite 复制列表 （目标库）12[pg91@redhat6 londiste]$ londiste3 s_skytf.ini add-table skytf.test_lond22013-03-30 10:54:04,596 8629 INFO Table added: skytf.test_lond2 2.4 检查源库和目标库表同步状态123456[pg92@redhatB londiste]$ londiste3 p_skytf.ini tablesTables on nodetable_name merge_state table_attrs---------------- --------------- ---------------skytf.test_lond1 ok skytf.test_lond2 ok 备注：两节点 merge_state 状态都显示为 ok 时表示完成同步，尤其是目标节点，merge_state 状态变化为以下： none -&gt; in-copy -&gt; catching-up -&gt; ok 。 2.5 编写压力测试 SQL 脚本 update_1.sql123setrandom v_amount 1 10000 update skytf.test_lond2 set amount=repeat(:v_amount::text,3)::int8,create_time=clock_timestamp() where id=:v_amount; 2.6 pgbench 压力测试(源库)123456789101112nohup pgbench -c 10 -T 30 -j 5 -n -d skytf -U skytf -f update_1.sql &gt; update_1.out &amp; pghost: pgport: 1921 nclients: 10 duration: 30 dbName: skytftransaction type: Custom queryscaling factor: 1query mode: simplenumber of clients: 10number of threads: 5duration: 30 snumber of transactions actually processed: 37403tps = 1246.423194 (including connections establishing)tps = 1251.730479 (excluding connections establishing) 2.7 源库数据验证1234567891011[pg92@redhatB ~]$ psql skytf skytfpsql (9.2.1)skytf=&gt; select * from test_lond2 where id &gt; 100 and id &lt; 106 order by id;id | amount | create_time -----+-----------+---------------------101 | 101101101 | 2013-03-30 11:16:34102 | 102102102 | 2013-03-30 11:16:30103 | 103103 | 2013-03-30 11:13:59104 | 104104104 | 2013-03-30 11:16:36105 | 105105105 | 2013-03-30 11:16:35(5 rows) 2.8 目标库数据验证12345678910111213[pg91@redhat6 ~]$ psql skytf skytfpsql (9.1.2)Type \"help\" for help. skytf=&gt; select * from test_lond2 where id &gt; 100 and id &lt; 106 order by id;id | amount | create_time -----+-----------+---------------------101 | 101101101 | 2013-03-30 11:16:34102 | 102102102 | 2013-03-30 11:16:30103 | 103103 | 2013-03-30 11:13:59104 | 104104104 | 2013-03-30 11:16:36105 | 105105105 | 2013-03-30 11:16:35(5 rows) 备注：可见数据已经同步过来了。 2.9 compare 比较记录条数12345678910111213141516171819[pg91@redhat6 londiste]$ londiste3 s_skytf.ini compare2013-03-30 11:23:07,946 9164 INFO Checking if node_p can be used for copy2013-03-30 11:23:07,950 9164 INFO Node node_p seems good source, using it2013-03-30 11:23:07,951 9164 INFO skytf.test_lond1: Using node node_p as provider2013-03-30 11:23:08,024 9164 INFO Provider: node_p (root)2013-03-30 11:23:08,045 9164 INFO Locking skytf.test_lond12013-03-30 11:23:08,052 9164 INFO Syncing skytf.test_lond12013-03-30 11:23:10,623 9164 INFO Counting skytf.test_lond12013-03-30 11:23:10,769 9164 INFO srcdb: 701 rows, checksum=4555416322013-03-30 11:23:10,783 9164 INFO dstdb: 701 rows, checksum=4555416322013-03-30 11:23:10,855 9164 INFO Checking if node_p can be used for copy2013-03-30 11:23:10,866 9164 INFO Node node_p seems good source, using it2013-03-30 11:23:10,867 9164 INFO skytf.test_lond2: Using node node_p as provider2013-03-30 11:23:10,953 9164 INFO Provider: node_p (root)2013-03-30 11:23:10,972 9164 INFO Locking skytf.test_lond22013-03-30 11:23:10,979 9164 INFO Syncing skytf.test_lond22013-03-30 11:23:13,564 9164 INFO Counting skytf.test_lond22013-03-30 11:23:13,690 9164 INFO srcdb: 10000 rows, checksum=-932274149532013-03-30 11:23:13,754 9164 INFO dstdb: 10000 rows, checksum=-93227414953 参考 使用 pgbench 进行数据库压力测试 Londiste3：搭建简单的基于表的复制 pgbench 使用手册 londiste3：常用管理命令","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Londiste","slug":"Londiste","permalink":"https://postgres.fun/tags/Londiste/"}]},{"title":"Londiste3：搭建简单的基于表的复制","slug":"20130330120557","date":"2013-03-30T04:05:57.000Z","updated":"2018-09-04T01:34:02.276Z","comments":true,"path":"20130330120557.html","link":"","permalink":"https://postgres.fun/20130330120557.html","excerpt":"","text":"londiste3(后面简称 Londiste ) 是 skype 公司开发的一款异步模式的复制工具，用的是触发器模式，适用于一个库中部分表的同步，可以跨 PostgreSQL 大版本实现表复制，接下来先安装，随后做些测试。 Londiste 组件 PgQ用来处理队列消息的机制，在 skytool 安装包中，关于 PgQ 更详细的介绍请参考 PGQ Tutorial。 Londiste用 Python 写的复制工具，使用 PgQ 事件处理器。关于 Londiste 详细介绍请参考 Londiste Tutorial Psycopg2Python-Postgres driver ，可以在这个链接下载：( http://initd.org/psycopg/ ) 环境信息 节点 IP PORT DATNAME VERSION 源库 192.168.1.36 1921 skytf PostgreSQL 9.2.1 目标库 192.168.1.35 1923 skytt PostgreSQL 9.1.2 测试样例： 一张表。备注： 实验环境为两台虚拟机。 Londisite 安装3.1 下载在 pgfoundry 中下载 skytools 3.1.3，链接为：http://pgfoundry.org/frs/?group_id=1000206 3.2 解压1[root@redhatB soft_bak]# tar zxvf skytools-3.1.3.tar.gz 3.3 编译并安装 (源库)123./configure --prefix=/opt/skytools-3.1.3 --with-pgconfig=/opt/pgsql9.2.1/bin/pg_config make make install 备注：安装成功后，目录 /opt/skytools-3.1.3 会生成相应文件。 3.4 安装其它模块 12345[root@redhat6 skytools-3.1.3]# python setup_pkgloader.py build[root@redhat6 skytools-3.1.3]# python setup_pkgloader.py install [root@redhat6 skytools-3.1.3]# python setup_skytools.py build[root@redhat6 skytools-3.1.3]# python setup_skytools.py install 3.5 创建 londiste 相关目录123[pg92@redhatB pgdata]$ mkdir -p /database/pg92/pgdata/londiste[pg92@redhatB pgdata]$ mkdir -p /database/pg92/pgdata/londiste/log[pg92@redhatB pgdata]$ mkdir -p /database/pg92/pgdata/londiste/pid 3.6 配置 p_skytf.ini (源库)123456[londiste3]job_name = job_skytfdb = dbname=skytf user=postgres port=1921 host=127.0.0.1pgq_queue_name = testinglogfile = ./log/p_skytf.logpidfile = ./pid/p_fskytf.pid 3.7 初始化 Londiste 源库:12345678910111213141516[pg92@redhatB londiste]$ londiste3 p_skytf.ini create-root node_p 'host=192.168.1.36 port=1921 user=postgres password=postgres123 dbname=skytf' 2013-03-28 20:26:03,109 4162 INFO plpgsql is installed2013-03-28 20:26:03,113 4162 INFO Installing pgq2013-03-28 20:26:03,115 4162 INFO Reading from /usr/share/skytools3/pgq.sql2013-03-28 20:26:03,378 4162 INFO pgq.get_batch_cursor is installed2013-03-28 20:26:03,380 4162 INFO Installing pgq_ext2013-03-28 20:26:03,381 4162 INFO Reading from /usr/share/skytools3/pgq_ext.sql2013-03-28 20:26:03,487 4162 INFO Installing pgq_node2013-03-28 20:26:03,487 4162 INFO Reading from /usr/share/skytools3/pgq_node.sql2013-03-28 20:26:03,622 4162 INFO Installing londiste2013-03-28 20:26:03,622 4162 INFO Reading from /usr/share/skytools3/londiste.sql2013-03-28 20:26:03,831 4162 INFO londiste.global_add_table is installed2013-03-28 20:26:03,843 4162 INFO Initializing node2013-03-28 20:26:03,850 4162 INFO Location registered2013-03-28 20:26:03,987 4162 INFO Node \"node_p\" initialized for queue \"testing\" with type \"root\"2013-03-28 20:26:03,993 4162 INFO Done 备注：这步会在对应数据库中安装模式 londiste，pgq，pgq_ext，pgq_node 和相关系统表，出于权限问题，推荐使用数据库超级用户安装组件。 3.8 开启源库节点的 worker 进程1 [pg92@redhatB londiste]$ londiste3 -d p_skytf.ini worker 查看进程123[pg92@redhatB londiste]$ ps -ef | grep londistepg92 8773 1 0 15:38 ? 00:00:00 /usr/bin/python /opt/skytools-3.1.3/bin/londiste3 -d p_skytf.ini workerpg92 8799 17600 0 15:38 pts/1 00:00:00 grep londiste 3.9 创建 PgQ ticker 配置文件 pgqd.ini123 [pgqd]logfile = ./log/pgqd.logpidfile = ./pid/pgqd.pid 3.10 启动 ticker daemon:1 [pg92@redhatB londiste]$ pgqd -d /database/pg92/pgdata/londiste/pgqd.ini 3.11 编译并安装 (目标库)123 ./configure --prefix=/opt/skytools-3.1.3 with-pgconfig=/opt/pgsql9.1/bin/pg_config make make install 备注：在目标库创建相同目录，解压介质步步骤参考之前的 3.1- 3.5 小节，这里注意参数 “–with-pgconfig” 的不同。 3.12 配置 s_skytf.ini 文件123456[londiste3]job_name = job_skytfdb = dbname=skytf user=postgres port=1923 host=127.0.0.1pgq_queue_name = testinglogfile = ./log/s_skytf.logpidfile = ./pid/s_skytf.pid 3.13 初始化 Londiste 目标库:1234567891011121314151617181920[pg91@redhat6 londiste]$ londiste3 s_skytf.ini create-leaf node_s 'host=192.168.1.35 port=1923 user=postgres password=postgres123 dbname=skytf' --provider='host=192.168.1.36 port=1921 user=postgres dbname=skytf password=postgres123' 2013-03-28 20:28:11,294 3233 INFO plpgsql is installed2013-03-28 20:28:11,302 3233 INFO Installing pgq2013-03-28 20:28:11,304 3233 INFO Reading from /usr/share/skytools3/pgq.sql2013-03-28 20:28:11,612 3233 INFO pgq.get_batch_cursor is installed2013-03-28 20:28:11,617 3233 INFO Installing pgq_ext2013-03-28 20:28:11,618 3233 INFO Reading from /usr/share/skytools3/pgq_ext.sql2013-03-28 20:28:11,774 3233 INFO Installing pgq_node2013-03-28 20:28:11,775 3233 INFO Reading from /usr/share/skytools3/pgq_node.sql2013-03-28 20:28:11,906 3233 INFO Installing londiste2013-03-28 20:28:11,907 3233 INFO Reading from /usr/share/skytools3/londiste.sql2013-03-28 20:28:12,078 3233 INFO londiste.global_add_table is installed2013-03-28 20:28:12,094 3233 INFO Initializing node2013-03-28 20:28:12,195 3233 INFO Location registered2013-03-28 20:28:12,210 3233 INFO Location registered2013-03-28 20:28:12,227 3233 INFO Subscriber registered: node_s2013-03-28 20:28:12,232 3233 INFO Location registered2013-03-28 20:28:12,239 3233 INFO Location registered2013-03-28 20:28:12,248 3233 INFO Node \"node_s\" initialized for queue \"testing\" with type \"leaf\"2013-03-28 20:28:12,258 3233 INFO Done 3.14 运行目标库 worker 订阅进程1 londiste3 -d s_skytf.ini worker 备注：到了这里 londiste 的安装基本完成了，接下来看看配置信息。 3.15 查看节点状态123456789 [pg92@redhatB londiste]$ londiste3 p_skytf.ini statusQueue: testing Local node: node_p node_p (root) | Tables: 1/0/0 | Lag: 22s, Tick: 221 +--node_s (leaf) Tables: 1/0/0 Lag: 22s, Tick: 221 3.16 查看节点成员123456 [pg92@redhatB londiste]$ londiste3 p_skytf.ini membersMember info on node_p@testing:node_name dead node_location--------------- --------------- ---------------------------------------------------------------------------node_p False host=192.168.1.36 port=1921 user=postgres password=postgres123 dbname=skytfnode_s False host=192.168.1.35 port=1923 user=postgres password=postgres123 dbname=skytf 备注：3.15, 3.16 的信息应该在源库和目标库都能显示。 Londiste 测试4.1 创建测试表(源库和目标库)123skytf=&gt; create table test_lond1 (id int4 primary key,name character varying(32));NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_lond1_pkey\" for table \"test_lond1\"CREATE TABLE 4.2 add tables (源库)12345678[pg92@redhatB londiste]$ londiste3 p_skytf.ini add-table skytf.test_lond12013-03-28 16:34:14,270 11487 INFO Table added: skytf.test_lond1 [pg92@redhatB londiste]$ londiste3 p_skytf.ini tablesTables on nodetable_name merge_state table_attrs---------------- --------------- ---------------skytf.test_lond1 ok 备注： merge_state 值为 ok 表示正常。 4.3 add tables (目标库)1234567891011121314 [pg91@redhat6 londiste]$ londiste3 s_skytf.ini add-table skytf.test_lond12013-03-28 20:36:20,538 3498 INFO Table added: skytf.test_lond1 [pg91@redhat6 londiste]$ londiste3 s_skytf.ini tablesTables on nodetable_name merge_state table_attrs---------------- --------------- ---------------skytf.test_lond1 None [pg91@redhat6 londiste]$ londiste3 s_skytf.ini tablesTables on nodetable_name merge_state table_attrs---------------- --------------- ---------------skytf.test_lond1 ok 备注：在目标库加完表后， merge_state 值开始为 None, 随后变成 ok，表示正常。 4.4 表 INSERT 测试源库123456789101112[pg92@redhatB londiste]$ psql skytf skytfpsql (9.2.1)Type \"help\" for help. skytf=&gt; insert into test_lond1 select generate_series(1,1000),generate_series(1,1000) || 'a';INSERT 0 1000 skytf=&gt; select count(*) from test_lond1 ; count ------- 1000(1 row) 备注：先在源库的表 test_lond1 插入 1000 条数据，看看目标库是否会同步。 目标库123456789[pg91@redhat6 londiste]$ psql skytf skytfpsql (9.1.2)Type \"help\" for help. skytf=&gt; select count(*) from test_lond1 ; count ------- 1000(1 row) 备注：可见目标库数据迅速同步了。 4.5 也可以通过 compare 比较1234567891011[pg91@redhat6 londiste]$ londiste3 s_skytf.ini compare skytf.test_lond12013-03-28 20:49:26,271 3642 INFO Checking if node_p can be used for copy2013-03-28 20:49:26,275 3642 INFO Node node_p seems good source, using it2013-03-28 20:49:26,276 3642 INFO skytf.test_lond1: Using node node_p as provider2013-03-28 20:49:26,350 3642 INFO Provider: node_p (root)2013-03-28 20:49:26,371 3642 INFO Locking skytf.test_lond12013-03-28 20:49:26,378 3642 INFO Syncing skytf.test_lond12013-03-28 20:49:29,429 3642 INFO Counting skytf.test_lond12013-03-28 20:49:29,564 3642 INFO srcdb: 1000 rows, checksum=15111876742013-03-28 20:49:29,575 3642 INFO dstdb: 1000 rows, checksum=1511187674 备注：最后两行显示源库，目标库记录条数，但目标节点对应表数据不能修改，只能订阅， 否则出现以下错误。4.6 删除目标节点对应表数据12skytf=&gt; delete from test_lond1 where id=100;ERROR: Table 'skytf.test_lond1' to queue 'testing': change not allowed (D) 备注：目标节点复制表 test_lond1 数据不能修改。 Londiste 复制机制为什么能复制？londiste3 使用的是触发器机制，看看以下 12345678910 Table \"skytf.test_lond1\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null name | character varying(32) | Indexes: \"test_lond1_pkey\" PRIMARY KEY, btree (id)Triggers: _londiste_testing AFTER INSERT OR DELETE OR UPDATE ON test_lond1 FOR EACH ROW EXECUTE PROCEDURE pgq.logutriga('testing') _londiste_testing_truncate AFTER TRUNCATE ON test_lond1 FOR EACH STATEMENT EXECUTE PROCEDURE pgq.sqltriga('testing') 特殊情况如果节点没配置好，同时又删除不掉(drop-node)，怎么办？个比推荐一个较暴力的做法，直接删除 Londsite 的几个模式即可。 删除 londiste 相关模式 drop schema londiste cascade; drop schema pgq cascade; drop schema pgq_ext cascade; drop schema pgq_node cascade; 压力测试 压力测试参考下一篇 blog ：Londiste3：压力测试 参考 wiki SkyTools PGQ_Tutorial Londiste_Tutorial Londiste 3 replicate case - 1 上节(德哥) Londiste3：压力测试 londiste3：常用管理命令","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Londiste","slug":"Londiste","permalink":"https://postgres.fun/tags/Londiste/"}]},{"title":"PostgreSQL：PostGIS 2.0 安装","slug":"20130329174805","date":"2013-03-29T09:48:05.000Z","updated":"2018-09-04T01:34:02.214Z","comments":true,"path":"20130329174805.html","link":"","permalink":"https://postgres.fun/20130329174805.html","excerpt":"","text":"之前 blog 介绍了 postgis 1.5 的安装， Postgis 2.0 已经 release 有一段时间了，和之前版本相比，需要多安装很多组件，一直没整理，这次需要在测试环境下装一套 Postgis 2.0 ，抽时间整理下。 所需组件 PostgreSQL 8.4 or higher. Proj4 reprojection library, version 4.6.0 or greater GEOS geometry library, version 3.2.2 or greater, but GEOS 3.3.2+ is recommended. LibXML2, version 2.5.x or higher. LibXML2 is currently used in some imports functions JSON-C, version 0.9 or higher. JSON-C is currently used to import GeoJSON via the function ST_GeomFromGeoJson. GDAL, version 1.6 or higher 环境信息Postgresql: 9.2.3Postgis: 2.0.2OS : CentOS release 5.4 (Final) 安装 PostGIS安装 PostGIS 首先安装 pro、GEOS、JSON-C、GDAL、libXML2 插件，以下是详细安装步骤。 安装 Proj-4.8.0下载地址： http://trac.osgeo.org/proj/ 简要步骤1234# mkdir -p /usr/local/pg_tool/proj#./configure --prefix=/usr/local/pg_tool/proj# make# make install 备注：安装过程中没报错，并且 /usr/local/pg_tool/proj 目录下有文件，说明 proj 安装成功。 安装 Geos-3.3.8下载地址: http://trac.osgeo.org/geos/ 简要步骤12345# mkdir -p /usr/local/pg_tool/geos# ./configure --prefix=/usr/local/pg_tool/geos --enable-python --enable-ruby# make# make check# make install 安装 JSON-C 0.9下载地址： http://oss.metaparadigm.com/json-c/简要步骤123456#wget http://oss.metaparadigm.com/json-c/json-c-0.9.tar.gz # mkdir -p /usr/local/pg_tool/json #sh autogen.sh# ./configure --prefix=/usr/local/pg_tool/json# make# make install 安装 GDAL-1.9.2 下载地址：http://trac.osgeo.org/gdal/wiki/DownloadSource 简要步骤1234# mkdir -p /usr/local/pg_tool/gdal# ./configure --prefix=/usr/local/pg_tool/gdal# make# make install 安装 LibXML21# yum install libxml2 备注：也可以在 http://www.linuxfromscratch.org/blfs/view/svn/general/libxml2.html 下载。 安装 PostGIS-2.0.23.6.1 .bash_profile 新增以下环境变量12345export PROJ_HOME=/usr/local/pg_tool/projexport GEOS_HOME=/usr/local/pg_tool/geosexport GDAL_HOME=/usr/local/pg_tool/gdalexport JSON_HOME=/usr/local/pg_tool/json export LD_LIBRARY_PATH=$GDAL_HOME/lib:$JSON_HOME/lib:$PROJ_HOME/lib:$GEOS_HOME/lib 备注：编缉完成后，执行.bash_profile 立即生效。 3.6.2 下载 postgis-2.0.2 下载地址: http://postgis.refractions.net/download/ 3.6.3 configure1# ./configure --with-pgconfig=/opt/pgsql9.2.3/bin/pg_config --with-projdir=/usr/local/pg_tool/proj --with-geosconfig=/usr/local/pg_tool/geos/bin/geos-config --with-gdalconfig=/usr/local/pg_tool/gdal/bin/gdal-config --with-jsondir=/usr/local/pg_tool/json 备注：执行 configure 之后，如果没报错，会出现以下信息： 3.6.4 configure 结果信息12345678910111213141516171819202122232425262728293031PostGIS is now configured for i686-pc-linux-gnu -------------- Compiler Info ------------- C compiler: gcc -g -O2 C++ compiler: g++ -g -O2 SQL preprocessor: /usr/bin/cpp -traditional-cpp -P -------------- Dependencies -------------- GEOS config: /usr/local/pg_tool/geos/bin/geos-config GEOS version: 3.3.8 GDAL config: /usr/local/pg_tool/gdal/bin/gdal-config GDAL version: 1.9.2 PostgreSQL config: /opt/pgsql9.2.3/bin/pg_config PostgreSQL version: PostgreSQL 9.2.3 PROJ4 version: 48 Libxml2 config: /usr/bin/xml2-config Libxml2 version: 2.6.26 JSON-C support: yes PostGIS debug level: 0 Perl: /usr/bin/perl --------------- Extensions --------------- PostGIS Raster: enabled PostGIS Topology: enabled -------- Documentation Generation -------- xsltproc: /usr/bin/xsltproc xsl style sheets: dblatex: convert: /usr/bin/convert mathml2.dtd: http://www.w3.org/Math/DTD/mathml2/mathml2.dtd 3.6.5 编译并安装123# make# make check# make install 3.6.6 安装完成后，在 $PGHOME/share/extension 下会产生以下文件1234567891011121314151617181920212223242526272829303132333435363738[pg92@mpchat-server1 extension]$ pwd/opt/pgsql9.2.3/share/extension [pg92@mpchat-server1 extension]$ ll postgis*-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0alpha1--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0alpha2--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0alpha3--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0alpha4--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0alpha5--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0alpha6--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0beta1--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0beta2--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0beta3--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0beta4--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0rc1--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.0rc2--2.0.2.sql-rw-r--r-- 1 root root 486K Mar 29 16:27 postgis--2.0.1--2.0.2.sql-rw-r--r-- 1 root root 4.0M Mar 29 16:27 postgis--2.0.2.sql-rw-r--r-- 1 root root 185 Mar 29 16:27 postgis.control-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0alpha1--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0alpha2--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0alpha3--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0alpha4--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0alpha5--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0alpha6--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0beta1--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0beta2--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0beta3--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0beta4--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0rc1--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.0rc2--2.0.2.sql-rw-r--r-- 1 root root 281K Mar 29 16:27 postgis_topology--2.0.1--2.0.2.sql-rw-r--r-- 1 root root 278K Mar 29 16:27 postgis_topology--2.0.2.sql-rw-r--r-- 1 root root 169 Mar 29 16:27 postgis_topology.control-rw-r--r-- 1 root root 8.0K Mar 29 16:27 postgis_topology--unpackaged--2.0.2.sql-rw-r--r-- 1 root root 77K Mar 29 16:27 postgis--unpackaged--2.0.2.sql 备注：此时说明 Postgis 安装成功。 测试 PostGIS4.1 安装 postgis 和 postgis_topology 模块1234567891011postgres=# \\c francsYou are now connected to database \"francs\" as user \"postgres\". francs=# create extension postgis;CREATE EXTENSION francs=# create extension postgis_topology;CREATE EXTENSION francs=#CREATE EXTENSION fuzzystrmatch;CREATE EXTENSION 4.2 创建测试表12345678910111213141516171819202122232425francs=# \\dT geometry List of data typesSchema | Name | Description --------+----------+-----------------------------------------public | geometry | postgis type: Planar spatial data type.(1 row) francs=# CREATE TABLE geom_test ( gid int4, geom geometry, name varchar(25) );CREATE TABLE francs=# INSERT INTO geom_test ( gid, geom, name )francs-# VALUES ( 1, 'POLYGON((0 0 0,0 5 0,5 5 0,5 0 0,0 0 0))', '3D Square');INSERT 0 1 francs=# SELECT * from geom_test WHERE geom &amp;&amp; 'BOX3D(2 2 0,3 3 0)'::box3d;gid | geom | name -----+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------- 1 | 01030000800100000005000000000000000000000000000000000000000000000000000000000000000000000000000000000014400000000000000000000000000000144000000000000014400000000000000000000000000000144000000000000000000000000000000000000000000000000000000000000000000000000000000000 | 3D Square(1 row) 参考 https://postgres.fun/20100822190943.html http://blog.163.com/digoal@126/blog/static/1638770402012513055140/ http://postgis.net/docs/manual-2.0/postgis_installation.html#id600468","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostGIS","slug":"PostGIS","permalink":"https://postgres.fun/tags/PostGIS/"}]},{"title":"PostgreSQL：如何在另一个 SQL 中使用 INSERT...RETURNING 返回的值 ","slug":"20130313102200","date":"2013-03-13T02:22:00.000Z","updated":"2018-09-04T01:34:02.151Z","comments":true,"path":"20130313102200.html","link":"","permalink":"https://postgres.fun/20130313102200.html","excerpt":"","text":"今天 bbs里有人问到如何使用 “INSERT…RETURNING” 语句值的问题，目标想使用 array_append 数组函数追加 “INSERT…RETURNING” 返回的值。 目标结果123francs=&gt; select array_append(array[1,2,3],insert into test_return (id) values (3) returning id);ERROR: syntax error at or near \"into\"LINE 1: select array_append(array[1,2,3],insert into test_return (id... 备注：当然这是行不通的，因为语法不支持，接下来测试下，看看是否有其它方法。 在介绍之前，先复习下 “INSERT…RETURNING” 的用法。 创建测试表创建测试表，并测试返回插入的值1234567891011121314151617181920212223242526francs=&gt; create table test_return (id int4 primary key,name character(32));NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_return_pkey\" for table \"test_return\"CREATE TABLE francs=&gt; insert into test_return (id) values (1) returning id;id---- 1(1 row) INSERT 0 1 francs=&gt; insert into test_return (id,name) values (2,'a') returning *;id | name ----+---------------------------------- 2 | a (1 row) INSERT 0 1 francs=&gt; select * from test_return ;id | name ----+---------------------------------- 1 | 2 | a (2 rows) 备注：上面的结果已经很清晰了， returning 属性不但能返回指定的字段，也能返回插入的整行。且返回的数据类型和插入的数据类型对应。回到开头的问题，如何使用 “INSERT…RETURNING” 语句返回的值呢， 使用 CTE12345678francs=&gt; with temp_row as (francs(&gt; insert into test_return (id) values (3) returning idfrancs(&gt; )francs-&gt; select array_append(array[1,2,3],id) from temp_row;array_append--------------&#123;1,2,3,3&#125;(1 row) 备注：使用 “WITH QUERY” 语句先保存中间结果到表 temp_row,接下来可以使用 temp_row 的值进行 SELECT ,UPDATE,DELETE,INSERT 操作了。 也能使用 “INSERT…RETURNING” 返回的值插入到另一个表1234567891011121314francs=&gt; truncate table test_1;TRUNCATE TABLE francs=&gt; with temp_row as (francs(&gt; insert into test_return (id) values (6) returning idfrancs(&gt; )francs-&gt; insert into test_1 (id) select id from temp_row;INSERT 0 1 francs=&gt; select * From test_1;id | name----+------ 6 |(1 row) 参考 http://www.postgresql.org/docs/9.2/static/queries-with.html http://www.postgresql.org/docs/9.2/static/sql-insert.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：\"ORDER BY Multiple Columns \" SQL 优化一例","slug":"20130312155601","date":"2013-03-12T07:56:01.000Z","updated":"2018-09-04T01:34:02.073Z","comments":true,"path":"20130312155601.html","link":"","permalink":"https://postgres.fun/20130312155601.html","excerpt":"","text":"今天监控数据库时，一慢 SQL 引起了我的注意，这个 SQL 运行时间在 474 ms 左右，经分析，创建合适的索引后，最终将 SQL 优化到 4 ms 左右，以下是优化过程，记录下。 数据库日志12013-03-12 14:14:15.266 CST,\"francs\",\"francs\",10729,\"192.168.100.114:32996\",513e742a.29e9,759,\"SELECT\",2013-03-12 08:17:46 CST,1424/3734,0,LOG,00000,\"duration: 474.781 ms execute &lt;unnamed&gt;: select this_.id as id0_0_, this_.create_time as create2_0_0_, this_.create_user as create3_0_0_, this_.deleted as deleted0_0_, this_.modify_time as modify5_0_0_, this_.modify_user as modify6_0_0_, this_.apk_id as apk7_0_0_, this_.apk_md5 as apk8_0_0_, this_.apk_size as apk9_0_0_, this_.app_class_id as app10_0_0_, this_.app_id as app11_0_0_, this_.package as package0_0_, this_.app_show_ver as app13_0_0_, this_.app_type as app14_0_0_, this_.app_ver as app15_0_0_, this_.authentic as authentic0_0_, this_.cn_name as cn17_0_0_, this_.content_provider as content18_0_0_, this_.eng_name as eng19_0_0_, this_.on_off as on20_0_0_, this_.promotion as promotion0_0_, this_.sdk_ver as sdk22_0_0_, this_.visible as visible0_0_ from tbl_test this_ where this_.app_class_id in ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13) and this_.app_type in ($14, $15) and this_.on_off=$16 and this_.visible=$17 and this_.deleted=$18 order by this_.app_type desc, this_.create_time desc limit $19\",\"parameters: $1 = '3', $2 = '4', $3 = '5', $4 = '6', $5 = '7', $6 = '8', $7 = '9', $8 = '10', $9 = '11', $10 = '12', $11 = '13', $12 = '14', $13 = '15', $14 = '0', $15 = '1', $16 = '1', $17 = '1', $18 = '0', $19 = '80'\",,,,,,,\"exec_execute_message, postgres.c:2025\",\"\" 备注：SQL 的详细信息可以从数据库日志得到。 格式化 SQL12345678910111213141516171819202122232425select this_.id as id0_0_, this_.create_time as create2_0_0_, this_.create_user as create3_0_0_, this_.deleted as deleted0_0_, this_.modify_time as modify5_0_0_, this_.modify_user as modify6_0_0_, this_.app_type as app14_0_0_, this_.app_ver as app15_0_0_, this_.authentic as authentic0_0_, this_.cn_name as cn17_0_0_, this_.content_provider as content18_0_0_, this_.eng_name as eng19_0_0_, this_.on_off as on20_0_0_, this_.promotion as promotion0_0_, this_.sdk_ver as sdk22_0_0_, this_.visible as visible0_0_ ....省略部分字段 from tbl_test this_where this_.app_class_id in (3,4,5,6,7,8,9,10,11,12,13,14,15) and this_.app_type in (0, 1) and this_.on_off = 1 and this_.visible = 1 and this_.deleted = '0'order by this_.app_type desc, this_.create_time desc limit 80; 备注：这个 SQL 非常简单，没有关联查询，运行时间在 474 ms 确实比较糟糕。 优化前的执行计划123456789101112--------------------------------------------------------------------------------------------------------------------------------Limit (cost=32253.40..32253.60 rows=80 width=449) (actual time=500.618..500.638 rows=80 loops=1) -&gt; Sort (cost=32253.40..32484.00 rows=92241 width=449) (actual time=500.617..500.624 rows=80 loops=1) Sort Key: app_type, create_time Sort Method: top-N heapsort Memory: 46kB -&gt; Bitmap Heap Scan on tbl_test this_ (cost=1998.46..28876.49 rows=92241 width=449) (actual time=69.868..360.386 rows=125997 loops=1) Recheck Cond: ((app_class_id = ANY ('&#123;3,4,5,6,7,8,9,10,11,12,13,14,15&#125;'::numeric[])) AND (on_off = 1::numeric) AND (app_type = ANY ('&#123;0,1&#125;'::numeric[]))) Filter: ((visible = 1::numeric) AND (deleted = 0::numeric)) -&gt; Bitmap Index Scan on idx_tbl_test_muti (cost=0.00..1975.40 rows=112718 width=0) (actual time=67.843..67.843 rows=152951 loops=1) Index Cond: ((app_class_id = ANY ('&#123;3,4,5,6,7,8,9,10,11,12,13,14,15&#125;'::numeric[])) AND (on_off = 1::numeric) ANY (app_type = ANY ('&#123;0,1&#125;'::numeric[])))Total runtime: 500.717 ms(10 rows) 备注：重新运行这个 SQL ，花了 500 ms 左右，时间主要花在通过索引 “ idx_tbl_test_muti ” 读取记录环节，并且 PLAN 中显示了排序步骤，接下来看下表结构。 表信息表结构123456789101112131415161718192021222324 Table \"francs.tbl_test\" Column | Type | Modifiers ------------------+-----------------------------+------------------------id | numeric(19,0) | not nullapk_id | numeric(19,0) | not nullapp_ver | numeric(19,0) | not nullapp_show_ver | character varying(32) | not nullcn_name | character varying(128) | not nullauthentic | numeric(10,0) | not nullapp_class_id | numeric(19,0) | not nulldescription | character varying(1024) |create_user | character varying(32) | not nullcreate_time | timestamp without time zone | not null default now()deleted | numeric(5,0) | not null default 0on_off | numeric(1,0) | not null default 1app_type | numeric(2,0) | not null default 0visible | numeric(2,0) | not null default 1.....部分字段略Indexes: \"pk_op_app\" PRIMARY KEY, btree (id) \"idx_tbl_test_muti\" btree (app_class_id, app_type DESC, create_time DESC), tablespace \"tbs_francs_03\" \"tbl_test_app_type_index\" btree (app_type), tablespace \"tbs_francs_idx\" \"tbl_test_create_time_index\" btree (create_time), tablespace \"tbs_francs_idx\" 备注：重点看下索引 idx_tbl_test_muti 的定义。 查询字段 distinct 值12345678910francs=&gt; select tablename,attname,n_distinct from pg_stats where tablename='tbl_test'francs-&gt; and attname in ('app_class_id','app_type','on_off','visible','deleted','create_time');tablename | attname | n_distinct------------+--------------+------------tbl_test | app_class_id | 21tbl_test | create_time | -1tbl_test | deleted | 2tbl_test | on_off | 2tbl_test | app_type | 6tbl_test | visible | 2 备注：pg_stats 视图可以查看表字段的统计信息，这里查看字段的 distinct 值，从上面看出，where 条件的几个字段的选择性都非常不好，但 order by 字段的 create_time 的n_distinct 为 -1 ，表示字段 create_time 是唯一的，选择性好。 创建索引12set default_tablespace='tbs_francs_03';create index concurrently idx_tbl_test_muti2 on tbl_test using btree (app_type desc,create_time desc ); 备注：尝试仅在 order by 字段上创建索引。 优化后的执行计划1234567 QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------------Limit (cost=0.00..28.83 rows=80 width=448) (actual time=2.342..4.065 rows=80 loops=1) -&gt; Index Scan using idx_tbl_test_muti2 on tbl_test this_ (cost=0.00..33145.06 rows=91966 width=448) (actual time=2.342..4.051 rows=80 loops=1) Filter: ((app_type = ANY ('&#123;0,1&#125;'::numeric[])) AND (on_off = 1::numeric) AND (visible = 1::numeric) AND (deleted = 0::numeric) AND (app_class_id = ANY ('&#123;3,4,5,6,7,8,9,10,11,12,13,14,15&#125;'::numeric[])))Total runtime: 4.128 ms(4 rows) 备注：优化后，执行时间下降到 4 ms 左右，仅原来的百分之一。 索引引使用情况1234567francs=&gt; select relname,indexrelname ,idx_scan from pg_stat_user_indexes where relname='tbl_test' order by idx_scan; relname | indexrelname | idx_scan------------+------------------------------+----------tbl_test | idx_tbl_test_muti2 | 48tbl_test | tbl_test_app_type_index | 1205tbl_test | tbl_test_create_time_index | 13688tbl_test | pk_op_app | 66520 备注：上面显示索引 “idx_tbl_test_muti2” 已经被使用了，这时可以删除老索引 “idx_tbl_test_muti” 了。 参考 http://www.postgresql.org/docs/9.1/static/indexes-ordering.html http://www.postgresql.org/docs/9.1/static/catalog-pg-statistic.html https://postgres.fun/20120621210555.html","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"PostgreSQL：函数稳定性( IMMUTABLE | STABLE | VOLATILE )","slug":"20130306141014","date":"2013-03-06T06:10:14.000Z","updated":"2018-09-04T01:34:02.026Z","comments":true,"path":"20130306141014.html","link":"","permalink":"https://postgres.fun/20130306141014.html","excerpt":"","text":"关于函数稳定性(IMMUTABLE | STABLE | VOLATILE) 一直比较模糊，看了手册相关解释后也不太明白，德哥有两篇 Blog 解释提比较清楚，先收藏下，慢慢消化。 Retalk PostgreSQL function’s ( volatile|stable|immutable) Thinking PostgreSQL Function’s Volatility Categories 德哥下面这篇 BLOG 用到了 IMMUTABLE 函数对 SQL 进行优化。 PostgreSQL multi-column concate query condition performance tuning","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Waiting for PostgreSQL9.3：增加物化视图 (MATERIALIZED VIEW)","slug":"20130305164239","date":"2013-03-05T08:42:39.000Z","updated":"2018-09-04T01:34:01.948Z","comments":true,"path":"20130305164239.html","link":"","permalink":"https://postgres.fun/20130305164239.html","excerpt":"","text":"今天在访问 depesz 博客时，发现了一个令人兴奋的消息，在未来的 PostgreSQL 9.3 版本中将带来物化视图，很多 PostgreSQL 爱好者对这个功能期待已久，闲话少说，先体验一下。 物化视图创建语法123456789Command: CREATE MATERIALIZED VIEWDescription: define a new materialized viewSyntax:CREATE [ UNLOGGED ] MATERIALIZED VIEW table_name [ (column_name [, ...] ) ] [ WITH ( storage_parameter [= value] [, ... ] ) ] [ TABLESPACE tablespace_name ] AS query [ WITH [ NO ] DATA ] 创建物化视图1.1 创建测试表12345678910111213141516171819202122232425francs=# select version(); version ----------------------------------------------------------------------------------------------------------PostgreSQL 9.3devel on i686-pc-linux-gnu, compiled by gcc (GCC) 4.4.6 20110731 (Red Hat 4.4.6-3), 32-bit(1 row) francs=&gt; create table test_1 (id int4,name character varying(32));CREATE TABLEfrancs=&gt; insert into test_1 select generate_series(1,10),generate_series(1,10)||'a';INSERT 0 10 francs=&gt; select * from test_1;id | name----+------ 1 | 1a 2 | 2a 3 | 3a 4 | 4a 5 | 5a 6 | 6a 7 | 7a 8 | 8a 9 | 9a10 | 10a(10 rows) 1,2 创建普通视图和物化视图12345francs=&gt; create view v_test_1 as select * from test_1 where pg_sleep(5) is null;CREATE VIEW francs=&gt; create materialized view mv_test_1 as select * from test_1 where id &gt; 5;SELECT 5 1.3 查看PLAN123456789101112131415francs=&gt; explain analyze select * From v_test_1; QUERY PLAN ----------------------------------------------------------------------------------------------------Seq Scan on test_1 (cost=0.00..18.75 rows=233 width=86) (actual time=0.032..0.041 rows=5 loops=1) Filter: (id &gt; 5) Rows Removed by Filter: 5Total runtime: 0.153 ms(4 rows) francs=&gt; explain analyze select * from mv_test_1; QUERY PLAN -------------------------------------------------------------------------------------------------------Seq Scan on mv_test_1 (cost=0.00..17.00 rows=700 width=86) (actual time=0.222..0.231 rows=5 loops=1)Total runtime: 0.296 ms(2 rows) 备注：上面看出访问普通视图时，依然是访问基表; 而访问物化视图时，不需要访问基表，而是直接从物化视图读数据。 1.4 查看大小12345678910111213francs=&gt; \\dm+ List of relationsSchema | Name | Type | Owner | Size | Description--------+-----------+-------------------+--------+-------+-------------francs | mv_test_1 | materialized view | francs | 16 kB |(1 row) francs=&gt; \\dv+ List of relationsSchema | Name | Type | Owner | Size | Description--------+----------+------+--------+---------+-------------francs | v_test_1 | view | francs | 0 bytes |(1 row) 备注: \\dm 元子命令显示物化视图大小，mv_test_1 大小为 16 k，这也说明了物化视图直接存储数据。 刷新物化视图2.1 物化视图刷新语法123456francs=&gt; \\h refresh materialized viewCommand: REFRESH MATERIALIZED VIEWDescription: replace the contents of a materialized viewSyntax:REFRESH MATERIALIZED VIEW name [ WITH [ NO ] DATA ] 2.2 刷新前12345678910111213141516171819202122francs=&gt; select * from mv_test_1;id | name----+------ 6 | 6a 7 | 7a 8 | 8a 9 | 9a10 | 10a(5 rows) francs=&gt; delete from test_1 where id=10;DELETE 1 francs=&gt; select * from mv_test_1;id | name----+------ 6 | 6a 7 | 7a 8 | 8a 9 | 9a10 | 10a(5 rows) 2.3 刷新物化视图12francs=&gt; refresh materialized view mv_test_1 ;REFRESH MATERIALIZED VIEW 2.4 再次查看物化视图12345678francs=&gt; select * from mv_test_1;id | name----+------ 6 | 6a 7 | 7a 8 | 8a 9 | 9a(4 rows) 2.5 “WITH NO DATE” 刷新模式12345678910111213francs=&gt; refresh materialized view mv_test_1 with no data;REFRESH MATERIALIZED VIEW francs=&gt; select * from mv_test_1;ERROR: materialized view \"mv_test_1\" has not been populatedHINT: Use the REFRESH MATERIALIZED VIEW command. francs=&gt; \\dm+ mv_test_1 List of relationsSchema | Name | Type | Owner | Size | Description--------+-----------+-------------------+--------+---------+-------------francs | mv_test_1 | materialized view | francs | 0 bytes |(1 row) 备注：刷新物化视图时有两种模式，”WITH DATA” 模式表示重新加载数据到物化视图，命令执行后物化视图可以访问；而”WITH NO DATE” 模式表示不刷新物化视图，命令执行之后物化视图不可访问。 总结 以上只是 PostgreSQL 9.3 deve 版目前提供的功能，可能之后会有较大改变。 目前物化视图功能并不完善，例如自动刷新、增量刷新、跨库刷新等功能暂不支持，期待后续完善。 参考 http://www.postgresql.org/docs/devel/static/sql-creatematerializedview.html http://www.postgresql.org/docs/devel/static/sql-refreshmaterializedview.html http://blog.163.com/digoal@126/blog/static/16387704020132581138434/?newFollowBlog http://www.depesz.com/2013/03/04/waiting-for-9-3-add-a-materialized-view-relations/comment-page-1/#comment-37694","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"C语言学习：数组和指针","slug":"20130304161712","date":"2013-03-04T08:17:12.000Z","updated":"2018-09-04T01:34:01.886Z","comments":true,"path":"20130304161712.html","link":"","permalink":"https://postgres.fun/20130304161712.html","excerpt":"","text":"今天学习了指针章节的数组部分，一开始觉得指针和组数组没啥必然的联系，通过书中的例子，了解了一些，通过例子学习： 例一：一维数组测试： array_point.c1234567891011121314151617181920#include &lt;stdio.h&gt;int main(void)&#123; long mult[]=&#123;15L,25L,35L,45L&#125;; long *p=mult; int i=0; /* 查看数组 mult 的值和地址 */ printf(\"nmult= %ld\", *mult ); printf(\"nmult= %d\", mult );/* 列出数组元素值 */for ( i=0;i&lt;4;i++) &#123; printf(\"nmult[%d]=%d\", i, mult[i] ); printf(\"naddress p+%d (&amp;muti[%d]): %d \", i,i,p+i); &#125; printf(\"n\"); return 0;&#125; 1.1 编译执行123456789101112[pg92@redhatB point]$ gcc -o array_point array_point.c[pg92@redhatB point]$ ./array_pointmult= 15mult= -1077617976mult[0]=15address p+0 (&amp;muti[0]): -1077617976mult[1]=25address p+1 (&amp;muti[1]): -1077617972mult[2]=35address p+2 (&amp;muti[2]): -1077617968mult[3]=45address p+3 (&amp;muti[3]): -1077617964 备注：可见数组 mult 表示第一个数组元素的地址。 *mult 表示第一个数组元素的值。 例二：二维数组：muti_array.cs123456789101112131415161718192021222324252627/*Program:muti_array.c*/ #include &lt;stdio.h&gt; int main(void)&#123; char board[3][3]=&#123;&#123;'1','2','3'&#125;,&#123;'4','5','6'&#125;,&#123;'7','8','9'&#125;&#125;; int i=0; printf(\"n values of board[0][0]: %c\",board[0][0]); printf(\"n values of board[0]: %p\",board[0]); printf(\"n values of *board[0]: %c\",*board[0]); printf(\"n values of *board: %p\",*board); printf(\"n values of board: %c\",board); printf(\"n values of *board[0]: %c\",*board[0]); printf(\"n values of *board[1]: %c\",*board[1]); printf(\"n values of *board[2]: %c\",*board[2]); /*List all elements of the array 注意: *board 为数组的第一个元素的地址,board 为数组第一个元素的值*/ for(i=0;i&lt;9;i++) &#123; printf(\"nboard: %p\",*board+i); printf(\"nboard: %c\",*(*board+i)); &#125; printf(\"n\"); return 0;&#125; 2.1 编译执行1234567891011121314151617181920212223242526values of board[0][0]: 1values of board[0]: 0xbfa14103values of *board[0]: 1values of *board: 0xbfa14103values of board: 1values of *board[0]: 1values of *board[1]: 4values of *board[2]: 7board: 0xbfa14103board: 1board: 0xbfa14104board: 2board: 0xbfa14105board: 3board: 0xbfa14106board: 4board: 0xbfa14107board: 5board: 0xbfa14108board: 6board: 0xbfa14109board: 7board: 0xbfa1410aboard: 8board: 0xbfa1410bboard: 9 备注：这个例子引用的是二维数组并打印一系列的值，用于理解各变量的意义。 board[0][0]：表示直接引用第一个数组的第一个元素值。 board[0]: 表示二维数组第一个数组的起始地址。 board[0]: 指明了第一个数组，但没指明哪个元素，省略元素是指引用第一个元素。 board: 表示二维数组第一个数组的第一个元素地址。 board： 表示二维数组第一个数组的第一个元素值。 board[i]: 表示二唯数组第 i 个数组第一个元素的值。 board+i: 获得数组所有元素的地址。 (*board+i) 获得数组所有元素的值。 总结 数组和指针的关系比较复杂，暂时也总结这些，以后再补上。 附：访问数组元素的指针表达式","categories":[{"name":"C语言","slug":"C语言","permalink":"https://postgres.fun/categories/C语言/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"https://postgres.fun/tags/C语言/"}]},{"title":"PostgreSQL: 如何计算 Character 数据类型物理占用多少字节？ ","slug":"20130227215522","date":"2013-02-27T13:55:22.000Z","updated":"2018-09-04T01:34:01.839Z","comments":true,"path":"20130227215522.html","link":"","permalink":"https://postgres.fun/20130227215522.html","excerpt":"","text":"今天有朋友问到在 PostgreSQL 中，character 字符类型物理占用字节数是如何计算的？这方面关注得比较少，于是查下手册，发现有系统函数 octet_length 可以查询字符类型数据所占的字节数。 octet_length 函数 Function Return Type Description Example Result octet_length(string) int Number of bytes in string octet_length(‘jose’) 4 备注：接下来测试下。 占用字节数测试1.1 创建测试表12345678910111213francs=&gt; create table test_bytes1(id int4,name character varying);CREATE TABLE francs=&gt; insert into test_bytes1 values (1,'a'),(2,'ab'),(3,'abc');INSERT 0 3 francs=&gt; select * from test_bytes1;id | name----+------ 1 | a 2 | ab 3 | abc(3 rows) 1.2 查看 name 字段所占字节数1234567francs=&gt; select id,name,octet_length(name),pg_column_size(name) from test_bytes1;id | name | octet_length | pg_column_size----+------+--------------+---------------- 1 | a | 1 | 2 2 | ab | 2 | 3 3 | abc | 3 | 4(3 rows) 备注：发现 pg_column_size 计算的数值比 octet_length 要多 1 个字节，这是在 strings 比较小的情况下，如果字符比较大，情况又不一样，pg_column_size 用来计算列物理所占字符数。 1.3 测试较大字符1234567891011francs=&gt; insert into test_bytes1 values (4,repeat('aaa',100));INSERT 0 1 francs=&gt; select id,octet_length(name),pg_column_size(name) from test_bytes1;id | octet_length | pg_column_size----+--------------+---------------- 1 | 1 | 2 2 | 2 | 3 3 | 3 | 4 4 | 300 | 304(4 rows) 备注：从上看出，当存储较大字节时，pg_column_size 算出的字节数要比 octet_length 算出的字节数多 4。 1.4 测试更大字符123456789101112francs=&gt; insert into test_bytes1 values (5,repeat('aaa',10000));INSERT 0 1 francs=&gt; select id,octet_length(name),pg_column_size(name) from test_bytes1;id | octet_length | pg_column_size----+--------------+---------------- 1 | 1 | 2 2 | 2 | 3 3 | 3 | 4 4 | 300 | 304 5 | 30000 | 353(5 rows) 备注：这时 octet_length 计算的字节数反而超过 pg_column_size 了，这是由于大字段发生了TOAST。 附: pg_column_size 函数 Name Return Type Description pg_column_size(any) int Number of bytes used to store a particular value (possibly compressed) 备注： pg_column_size 与 octet_length 的区别是，前者计算的是压缩后的值。 参考 http://www.postgresql.org/docs/9.2/static/datatype-character.html http://www.postgresql.org/docs/9.2/static/functions-string.html http://www.postgresql.org/docs/9.2/static/functions-admin.html http://www.postgresql.org/docs/9.2/static/storage-toast.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"C语言学习：常量指针和指向常量的指针","slug":"20130222160826","date":"2013-02-22T08:08:26.000Z","updated":"2018-12-04T00:20:19.875Z","comments":true,"path":"20130222160826.html","link":"","permalink":"https://postgres.fun/20130222160826.html","excerpt":"","text":"今天在学习常量指针和指向常量的指针两个概念时，开始总感觉很混乱，后来实验后，总算明白了，记录下，以后忘记了还能看看复习。 一 常量指针 常量指针是指指针中存储的地址不能改变，但能更改指针指向的值，声明时格式如下： 1int *const pcount=&amp;count; 编写以下脚本测试：1.1 常量指针测试脚本12345678910111213141516171819[pg92@redhatB point]$ vim test_1.c /*Program test_1.c 常量指针: 指针存储的地址不能改变,但能改变指针指向的值.*/ #include &lt;stdio.h&gt; int main(void)&#123;int count=43;int sum=45;int *const pcount=&amp;count; /*Defines a constant, means \"pcount\" read-only variable */ printf(\"n step1 count=%d\",count);*pcount=143;pcount=&amp;sum;printf(\"n step2 count=%d\",count);printf(\"nthe address of pcount is %p\",pcount);printf(\"nthe value of pcount is %dn\",*pcount);&#125; 1.2 编译123[pg92@redhatB point]$ gcc -o test_1 test_1.ctest_1.c: In function ‘main’:test_1.c:13: error: assignment of read-only variable ‘pcount’ 备注：更改常量指针地址时，报错。 1.3 去掉 &quot;pcount=&amp;sum;&quot; 行时，正常执行1234567[pg92@redhatB point]$ gcc -o test_1 test_1.c[pg92@redhatB point]$ ./test_1 step1 count=43step2 count=143the address of pcount is 0xbfc7b224the value of pcount is 143 备注：从上面看出常量指针 pcount 存储的地址不能被修改，但能更改它指向的值。 二 指向常量的指针 指向常量的指针是指该指针指向的值不能通过指针改变，但能更改指针存储的地址。 声明如下：1const int *pcount=&amp;count; 编写脚本测试：2.1 指向常量的指针测试脚本。1234567891011121314151617[pg92@redhatB point]$ vim test_2.c /*Program test_2.c 指向常量指针: 指针指向的值不能改变,但能改变指针地址.*/#include &lt;stdio.h&gt; int main(void)&#123;int count=43;int sum=45;const int *pcount=&amp;count; /*Defines a pointer to a constant,means \"*pcount\" is read-only. */ printf(\"n step1 *pcount=%d\",*pcount);printf(\"n step1 The address of pcount is %p\",pcount);pcount=&amp;sum;printf(\"n step2 *pcount=%d\",*pcount);printf(\"n step2 The address of pcount is %pn\",pcount);&#125; 2.2 编译执行1234567[pg92@redhatB point]$ gcc -o test_2 test_2.c[pg92@redhatB point]$ ./test_2step1 *pcount=43step1 The address of pcount is 0xbfab4928step2 *pcount=45step2 The address of pcount is 0xbfab4924[pg92@redhatB point]$ 备注：上面测试说明指针存储的地址改变了。 如果脚本中增加“ *pcount=&sum;” 代码，则会报以下错：2.3 ERROR123[pg92@redhatB point]$ gcc -o test_2 test_2.ctest_2.c: In function ‘main’:test_2.c:14: error: assignment of read-only location ‘*pcount 备注：说明不能通过 pcount 更改指向变量的值;从以上实验看出，指向常量的指针指向的值并是保持不变，因为虽然不能通过 pcount 来更改指针指向的值，但更改指针地址后，它指向的值依然被改变。","categories":[{"name":"C语言","slug":"C语言","permalink":"https://postgres.fun/categories/C语言/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"https://postgres.fun/tags/C语言/"}]},{"title":"C语言学习：指针","slug":"20130221154322","date":"2013-02-21T07:43:22.000Z","updated":"2018-12-04T00:20:41.591Z","comments":true,"path":"20130221154322.html","link":"","permalink":"https://postgres.fun/20130221154322.html","excerpt":"","text":"今天重温了C 语言指针相关的内容，简单的说指针也是变量，可以是 int，char，float等类型，指针存储的是地址，下面的这段代码能充分说明指针的特性，记录下： Point.c 代码123456789101112131415161718192021/*Program: pointer.c*/#include &lt;stdio.h&gt; int main(void)&#123;int number=10;int *p1=NULL; printf(\"nnumber's address: %p\",&amp;number);printf(\"nnumber's values: %dn\",number); p1=&amp;number; /*Store the address of number in pointer*/ printf(\"npointer's address: %p \",&amp;p1); /*Output the address*/printf(\"npointer's values: %p\",p1); /*Output the value (an address )*/printf(\"npointer's size: %d bytes\",sizeof(p1)); /*Output the size*/ printf(\"nvalues pointed to: %dn\",*p1); return 0;&#125; 编译并执行12345678910[pg92@redhatB point]$ gcc -o points points.c[pg92@redhatB point]$ ./points number's address: 0xbf865bacnumber's values: 10 pointer's address: 0xbf865ba8pointer's values: 0xbf865bacpointer's size: 4 bytesvalues pointed to: 10 备注：指针 p1 本身的地址为 0xbf865ba8，它存储的值为 0xbf865bac， 这个值正是整形变量 number 的地址。 指针用法图备注： 这个图能形象的说明指针。 参考C语言入门经典（第四版）","categories":[{"name":"C语言","slug":"C语言","permalink":"https://postgres.fun/categories/C语言/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"https://postgres.fun/tags/C语言/"}]},{"title":"Linux: 字体安装","slug":"20130220164952","date":"2013-02-20T08:49:52.000Z","updated":"2018-09-04T01:34:01.635Z","comments":true,"path":"20130220164952.html","link":"","permalink":"https://postgres.fun/20130220164952.html","excerpt":"","text":"今天接触了 Linux 下字体安装方面的内容，本实验以安装宋体( simsun.ttf ) 为例，记录下安装过程。 下载字体可以在网上下载，也可以在 windows 目录下找到对应字体，这里是从另一套系统上 copy simsun.ttf 文件。 查看已安装的中文字体123456789101112[root@redhat6 ~]# fc-list :lang=zhAR PL UMing TW:style=LightAR PL UMing HK:style=LightAR PL UMing CN:style=LightAR PL UKai TW MBE:style=BookAR PL UKai CN:style=BookAR PL UKai HK:style=BookAR PL UKai TW:style=BookWenQuanYi Zen Hei,文泉驛正黑,文泉驿正黑:style=RegularWenQuanYi Zen Hei Mono,文泉驛等寬正黑,文泉驿等宽正黑:style=RegularAR PL UMing TW MBE:style=LightWenQuanYi Zen Hei Sharp,文泉驛點陣正黑,文泉驿点阵正黑:style=Regular 备注：这个命令显示已安装的中文字体，从输出来看，宋体没有安装。 复制 Simsun.ttf 字体文件1[root@redhat6 my_fonts]# mkdir -p /usr/share/fonts/my_fonts 并从另一台已安装 simsun.ttf 字体的系统复制这个文件到这个目录。 生成字体索引信息123456789[root@redhat6 fonts]# cd my_fonts/[root@redhat6 my_fonts]# mkfontscale [root@redhat6 my_fonts]# mkfontsdir[root@redhat6 my_fonts]# lltotal 10276-rw-r--r--. 1 root root 189 Jan 31 22:33 fonts.dir-rw-r--r--. 1 root root 189 Jan 31 22:33 fonts.scale-rw-r--r--. 1 root root 10512288 Jan 31 22:33 simsun.ttf 备注：生成相应的 fonts.scale 和 fonts.dir 文件。 再次查看已安装的字体1234567891011121314[root@redhat6 my_fonts]# fc-list :lang=zhAR PL UMing TW:style=LightAR PL UMing HK:style=LightNSimSun,新宋体:style=RegularAR PL UMing CN:style=LightSimSun,宋体:style=RegularAR PL UKai TW MBE:style=BookAR PL UKai CN:style=BookAR PL UKai HK:style=BookAR PL UKai TW:style=BookWenQuanYi Zen Hei,文泉驛正黑,文泉驿正黑:style=RegularWenQuanYi Zen Hei Mono,文泉驛等寬正黑,文泉驿等宽正黑:style=RegularAR PL UMing TW MBE:style=LightWenQuanYi Zen Hei Sharp,文泉驛點陣正黑,文泉驿点阵正黑:style=Regular 备注：此时宋体字体已安装完成，也可以在图形化界面 “System –&gt; Preferences –&gt; Appearance –&gt;fonts “ 面板查看已安装的字体。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"Ubuntu：Unrar 的使用 ","slug":"20131216131717","date":"2013-02-16T05:17:17.000Z","updated":"2018-09-04T01:34:07.247Z","comments":true,"path":"20131216131717.html","link":"","permalink":"https://postgres.fun/20131216131717.html","excerpt":"","text":"由于工作需要，有时需要解压 rar 文件， 在 ubuntu 系统上安装 unrar 工具即可． 安装 Unrar 工具1sudo apt-get install unrar 解压到当前目录12345francs@francs:~/Desktop/冒泡助手$ unrar e update.rarUNRAR 4.00 beta 3 freeware Copyright (c) 1993-2010 Alexander RoshalExtracting from update.rarExtracting update.txt OK All OK 备注：参数 e 表示解压到当前目录． 解压到指定目录 123456francs@francs:~/Desktop/冒泡助手$ unrar x update.rar update/UNRAR 4.00 beta 3 freeware Copyright (c) 1993-2010 Alexander RoshalExtracting from update.rarCreating update OKExtracting update/update.txt OK All OK 备注：参数 x 表示解压到绝对路径． 压缩12345678francs@francs:~/Downloads$ rar a pg_healthcheck.rar1 pg_healthcheck/RAR 4.20 Copyright (c) 1993-2012 Alexander Roshal 9 Jun 2012Trial version Type RAR -? for helpEvaluation copy. Please register.Creating archive pg_healthcheck.rar1Adding pg_healthcheck/pg_healthcheck.sh OK Adding pg_healthcheck/pg_healthcheck.report OK Done Unrar 部分选项1234567891011francs@francs:~/Desktop/冒泡助手$ unrar --helpUNRAR 4.00 beta 3 freeware Copyright (c) 1993-2010 Alexander RoshalUsage: unrar &lt;command&gt; -&lt;switch 1&gt; -&lt;switch N&gt; &lt;archive&gt; &lt;files...&gt; &lt;@listfiles...&gt; &lt;path_to_extract\\&gt;&lt;Commands&gt; e Extract files to current directory l[t,b] List archive [technical, bare] p Print file to stdout t Test archive files v[t,b] Verbosely list archive [technical,bare] x Extract files with full path","categories":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/categories/Ubuntu/"}],"tags":[{"name":"Ubuntu","slug":"Ubuntu","permalink":"https://postgres.fun/tags/Ubuntu/"}]},{"title":"PostgreSQL: psql 命令 ON_ERROR_STOP 选项讲解 ","slug":"20130206110026","date":"2013-02-06T03:00:26.000Z","updated":"2018-09-04T01:34:01.573Z","comments":true,"path":"20130206110026.html","link":"","permalink":"https://postgres.fun/20130206110026.html","excerpt":"","text":"psql是连接 PostgreSQL 库的命令行交互式工具，相当于 oracle 的 sqlplus ，其中特性非常丰富，今天无意间查看到 ON_ERROR_STOP 特性，简单演示下。 ON_ERROR_STOP 属于 psql 的高级特性设置，更改后会影响 psql 的行为，一般不轻易更改，除非是有特殊需求。 关于 ON_ERROR_STOP 的演示，首先得从脚本中命令执行说起，在 PostgreSQL 中的 DB 脚本(假如是 100 条 INSERT )执行时会从上往下依次执行，当然，如果这个脚本全部正常，当然OK，那么如果其中有条由于某种原因执行报错怎么办？psql 默认的特性是继续处理这个 ERROR 之后的 SQL，接着演示： psql 默认的行为1.1 创建测试表12345678skytf=&gt; select version(); version -------------------------------------------------------------------------------------------------------PostgreSQL 9.2.1 on i686-pc-linux-gnu, compiled by gcc (GCC) 4.4.6 20110731 (Red Hat 4.4.6-3), 32-bit(1 row) skytf=&gt; create table test_stop(id int4,name varchar(32));CREATE TABLE 1.2 创建测试 shell 脚本12345678910[pg92@redhatB tf]$ cat insert.sh#!/bin/bash psql -h 127.0.0.1 -d skytf -U skytf -f insert_1.sqlecho $? [pg92@redhatB tf]$ cat insert_1.sqlinsert into test_stop values (1,'a');insert into test_stop values (2,b); --注意：这条 INSERT 将会报错insert into test_stop values (3,'c'); 备注：通过 insert.sh 脚本主要是想查看下返回状态。 1.3 执行脚本1234567[pg92@redhatB tf]$ ./insert.shINSERT 0 1psql:insert_1.sql:2: ERROR: column \"b\" does not existLINE 1: insert into test_stop values (2,b); ^INSERT 0 10 备注：脚本返回的是 0。 1.4 查看数据123456skytf=&gt; select * From test_stop;id | name----+------ 1 | a 3 | c(2 rows) 备注：insert_1.sql 脚本中原本想插入 3 条数据，其中第二条报错了，但第二条之后的 SQL 会被执行。如果想让 SQL 报错时停住不往下执行怎么办？接着测试。 设置 ON_ERROR_STOP 选项2.1 更改 insert_1.sql 脚本123456[pg92@redhatB tf]$ cat insert_1.sqlset ON_ERROR_STOP on insert into test_stop values (1,'a');insert into test_stop values (2,b);insert into test_stop values (3,'c'); 备注：设置 ON_ERROR_STOP 2.2 清理数据12skytf=&gt; truncate table test_stop;TRUNCATE TABLE 2.3 执行脚本123456[pg92@redhatB tf]$ ./insert.shINSERT 0 1psql:insert_1.sql:4: ERROR: column \"b\" does not existLINE 1: insert into test_stop values (2,b); ^3 备注：这时脚本返回的错误代码为 3。 2.4 验证数据 skytf=&gt; select * From test_stop; id | name ----+------ 1 | a (1 row) 备注：可见脚本执行到第二条 INSERT 报错时没有往下继续执行，这就是 ON_ERROR_STOP 的作用，根据字面很好理解，遇到错误就停止。 附: Exit Status psql returns 0 to the shell if it finished normally,1 if a fatal error of its own occurs (e.g. out of memory, file not found),2 if the connection to the server went bad and the session was not interactive,3 if an error occurred in a script and the variable ON_ERROR_STOP was set. 备注：上面是 psql 在不同情况下返回给操作系统的返回值。 参考 http://www.postgresql.org/docs/9.2/static/app-psql.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"psql","slug":"psql","permalink":"https://postgres.fun/tags/psql/"}]},{"title":"C 语言学习：使用寻址运算符","slug":"20130125171350","date":"2013-01-25T09:13:50.000Z","updated":"2018-12-04T00:21:18.448Z","comments":true,"path":"20130125171350.html","link":"","permalink":"https://postgres.fun/20130125171350.html","excerpt":"","text":"程序中的变量会占用一定量的内存，不同变量占用的类存大小（字节）不同，今天学习计算变量类型占用内存量的方法以及如何获得不同变量在内存中的地址。 获取 Int，Double 变量地址1.1 程序 mem_addr.c12345678910111213141516171819202122232425262728/*Program: mem_addr.c*/#include &lt;stdio.h&gt; int main(void)&#123; /*declare some integer variables*/ long a=1L; long b=2L; long c=3L; int g=4; /*declare some floating-point variables */ double d=4.0; double e=5.0; double f=6.0; printf(\"A varible of type long occupies %d byte.\",sizeof(long)); printf(\"nHere are the addresses of some variables of type long:\"); printf(\"nThe address of a is :%p, The address of b is :%p\",&amp;a,&amp;b); printf(\"nThe address of c is :%p\",&amp;c); printf(\"nnA varible of type double occupies %d bytes.\", sizeof(double)); printf(\"nHere are the addresses of somae varibles of type double:\"); printf(\"nThe address of d is :%p, The address of e is :%p\",&amp;d,&amp;e); printf(\"nThe address of f is :%p\",&amp;f); printf(\"nnA varible of type int occupies %d bytes.n\", sizeof(int)); return 0;&#125; 1.2 编译 mem_addr.c1[pg92@redhatB array]$ gcc -o mem_addr mem_addr.c 1.3 执行 mem_addr.c123456789101112[pg92@redhatB array]$ ./mem_addrA varible of type long occupies 4 byte.Here are the addresses of some variables of type long:The address of a is :0xbfe26418, The address of b is :0xbfe26414The address of c is :0xbfe26410 A varible of type double occupies 8 bytes.Here are the addresses of somae varibles of type double:The address of d is :0xbfe26408, The address of e is :0xbfe26400The address of f is :0xbfe263f8 A varible of type int occupies 4 bytes. 备注： long int 类型占用的字节为 4，并且输出了变量 a,b,c 在内存中的地址； double 类型占用的字节为 8，并且输出了变量 d,e,f 在内存中的地址。 获取数组变量地址2.1 array_addr.c12345678910111213141516/*Program: array_addr.c*/ #include &lt;stdio.h&gt; int main(void)&#123;int numbers[5];int i=0; for(;i&lt;5;i++)&#123; numbers[i]=12*(i+1); printf(\"n numbers[%d] Address: %p Contents: %d\",i,&amp;numbers[i],numbers[i]);&#125;return 0;&#125; 2.2 编译 array_addr.c1[pg92@redhatB array]$ gcc -o array_addr array_addr.c 2.3执行 array_addr [pg92@redhatB array]$ ./array_addr numbers[0] Address: 0xbfe4f3f8 Contents: 12 numbers[1] Address: 0xbfe4f3fc Contents: 24 numbers[2] Address: 0xbfe4f400 Contents: 36 numbers[3] Address: 0xbfe4f404 Contents: 48 numbers[4] Address: 0xbfe4f408 Contents: 60 备注：可以看出每个元素的地址都要比前一个元地址多 4，因为 int 类型变量在内存中占用四个节字。","categories":[{"name":"C语言","slug":"C语言","permalink":"https://postgres.fun/categories/C语言/"}],"tags":[{"name":"C语言","slug":"C语言","permalink":"https://postgres.fun/tags/C语言/"}]},{"title":"PostgreSQL：如何查询表的字段信息？","slug":"20130105211101","date":"2013-01-05T13:11:01.000Z","updated":"2018-12-04T00:27:21.696Z","comments":true,"path":"20130105211101.html","link":"","permalink":"https://postgres.fun/20130105211101.html","excerpt":"","text":"今天 PostgreSQL bbs里有人问到如何通过查询数据字典获得表字段信息的问题，虽然比较基础，依然总结下。 首先，在 PostgreSQL 中，提供一种称为元子命令的命令，可以方便的查看数据库对像信息，包括表结构，表索引等信息，如下： 方法一: 通过元子命令查看通过元子命令 \\d 查看表结构，如下：12345678910111213skytf=&gt; \\d tbl_role Table \"skytf.tbl_role\" Column | Type | Modifiers -----------+-----------------------+-------------------------------------------------------id | integer | not null default nextval('tbl_role_id_seq'::regclass)role_name | character varying(32) |exp | bigint |wealth | bigint |status | character(1) |attr | hstore |Indexes: \"tbl_role_pkey\" PRIMARY KEY, btree (id) \"idx_tbl_role_attr\" gist (attr), tablespace \"tbs_skytf_idx\" 备注：\\d 加上表名，就能非常容易的显示表字段信息和索引信息，当然这不是本文开头问题的答案，提问的同学是想通过系统数据字典来查看这些信息，方法为以下。 方法二: 查看 catalog 基表1234567891011121314skytf=&gt; select attrelid ::regclass, attname, atttypid ::regtype, attnotnull, attnumskytf-&gt; from pg_attributeskytf-&gt; where attrelid = 'tbl_role' ::regclassskytf-&gt; and attnum &gt; 0skytf-&gt; and attisdropped = 'f';attrelid | attname | atttypid | attnotnull | attnum----------+-----------+-------------------+------------+--------tbl_role | id | integer | t | 1tbl_role | role_name | character varying | f | 2tbl_role | exp | bigint | f | 3tbl_role | wealth | bigint | f | 4tbl_role | status | character | f | 5tbl_role | attr | hstore | f | 6(6 rows) 备注：系统表 pg_attribute 存储表的每一个列信息，包括系统列，首先通过条件“attnum&gt;0” 排除系统列 xmin,ctid 等; 接着通过条件“attisdropped=’f’” 排除已被删除的列，因为在 pg 中被删除的列并没有物理删除，只是标记，可以通过这个字段过滤。 方法三: 查看系统视图查看 information_schema 模式的视图，如下：1234567891011121314151617skytf=&gt; select table_schema,skytf-&gt; table_name,skytf-&gt; column_name,skytf-&gt; data_type,skytf-&gt; column_default,skytf-&gt; is_nullableskytf-&gt; from information_schema.columnsskytf-&gt; where table_name = 'tbl_role';table_schema | table_name | column_name | data_type | column_default | is_nullable--------------+------------+-------------+-------------------+--------------------------------------+-------------skytf | tbl_role | id | integer | nextval('tbl_role_id_seq'::regclass) | NOskytf | tbl_role | role_name | character varying | | YESskytf | tbl_role | exp | bigint | | YESskytf | tbl_role | wealth | bigint | | YESskytf | tbl_role | status | character | | YESskytf | tbl_role | attr | USER-DEFINED | | YES(6 rows) 备注：information_schema.columns 视图存储表和视图的字段信息，与前者不同的是，它并不存储系统字段信息，关于这个视图的其它字段，可以参考本文的参考部分。方法二，方法三是通过查看系统表或视图达到目标的，接下来介绍另一种方法，这种方法能非常全面的获得表定义，包括字段，索引，权限，甚至是序列信息，而不仅仅是字段信息。 方法四: 使用 pg_dump使用 pg_dump 导出表 skytf.tbl_role 定义，如下：1pg_dump -h 127.0.0.1 -E UTF8 -t \"skytf.tbl_role\" -s -v skytf &gt; skytf.tbl_role.ddl 备注：-s 表示仅导出表定义，不导表数据，最后查看文件 skytf.tbl_role 确认下。 参考 http://www.postgresql.org/docs/9.2/static/catalog-pg-attribute.html http://www.postgresql.org/docs/9.2/static/infoschema-columns.html http://archives.postgresql.org/pgsql-admin/2008-09/msg00240.php 附一: 查询表定义的SQL补充一个查询表结构的ＳＱＬ123456789101112131415161718192021SELECT col.table_schema , col.table_name , col.ordinal_position, col.column_name , col.data_type , col.character_maximum_length, col.numeric_precision, col.numeric_scale, col.is_nullable, col.column_default , des.descriptionFROM information_schema.columns col LEFT JOIN pg_description des ON col.table_name::regclass = des.objoid AND col.ordinal_position = des.objsubidWHERE table_schema = 'XXXX' AND table_name = 'XXXX'ORDER BY ordinal_position; 附二: 查询 Table 列表1234567891011121314SELECTn.nspname ,relnameFROMpg_class c ,pg_namespace nWHEREc.relnamespace = n.oidAND nspname='schema 名称'AND relkind = 'r'AND relhassubclassORDER BYnspname ,relname;","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 流复制：Archive_mode 参数设置问题","slug":"20130105135949","date":"2013-01-05T05:59:49.000Z","updated":"2018-09-04T01:34:01.385Z","comments":true,"path":"20130105135949.html","link":"","permalink":"https://postgres.fun/20130105135949.html","excerpt":"","text":"今天PostgreSQL bbs群里有人问到在做流复制环境时 archive_mode 的设置问题，贴子的意思是：将 archive_mode 设置成 off 时，依然能搭建成功流复制环境，而网上大多数资料都是将该参数设置成 on。 的确，在配置 PostgreSQL 流复制时，我们通常开启 archive_mode，但并不一定会配置 archive_command shell 脚本归档 WAL，因为 PostgreSQL 的归档非常大，而且保留归档 WAL 需要很大的存储空间，当然有条件的情况下建议开启并且设置保留策略。 针对群里提出的问题，接下来测试下，版本 PostgreSQL 9.1.0，关于流复制的搭建本文略，可以参考 PostgreSQL: Setting up streaming log replication (Hot Standby ) 这里不同的是 pg 版本，但这影响不大，另外不同的是 archive_mode 参数设置成 off，并且注释 archive_command 参数，分以下两种情况测试： 测试场景一以开启 archive_mode 方式搭建流复制环境，搭建完成后更改主库，备库的 achive_mode 参数为 off，并重启服务; 测试结果：流复制正常。 测试场景二重新搭建流复制环境，并且搭建时关闭主库，备库的 archive_mode 参数（off）。测试结果：流复制正常。 结论根据以上测试，似乎说明在搭建流复制环境时，并不必须设置 archive_mode 参数为 on ，很多资料在介绍搭建流复制环境时设置这个参数为 on ，可能是出于开启 WAL 归档更安全的原因， 因为在主库宕机并且较长时间不能恢复时，从库依然可以读取归档目录的 WAL，从而保证不丢数据; 另一方面，如果主库设置了较大的 wal_keep_segments ，也可以不用开启 archive_mode，因为主库保留了足够的 WAL，从而大大降低了因从库所需要的 WAL 被主库覆盖而需要从归档去取 WAL 的情况。所以从这方面说，archive_mode 参数的设置与搭建流复制并没有直接关系。 提示对于比较繁忙的库，在搭建流复制从库时，建议主库设置较大的 wal_keep_segments 参数。 参考 http://www.postgresql.org/docs/9.1/static/runtime-config-wal.html#RUNTIME-CONFIG-WAL-ARCHIVING http://wiki.postgresql.org/wiki/Hot_Standby http://deepakmurthy.wordpress.com/2011/06/03/postgresql-hot-standby-replication/ PostgreSQL: Setting up streaming log replication (Hot Standby )","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"MongoDB：锁机制 (Concurrency)","slug":"20121228112608","date":"2012-12-28T03:26:08.000Z","updated":"2018-09-04T01:34:01.307Z","comments":true,"path":"20121228112608.html","link":"","permalink":"https://postgres.fun/20121228112608.html","excerpt":"","text":"今天了解了 MongoDB 的锁机制，与传统 RDMBS MVCC 机制相差较大，它在一些方面的特性比较特殊，下面简单介绍下： 1 MongoDB 使用的锁MongoDB 使用的是“readers-writer”锁，可以支持并发但有很大的局限性，当一个读锁存在,许多读操作可以使用这把锁，然而, 当一个写锁的存在，一个单一的写操作会 exclusively 持有该锁，同时其它读，写操作不能使用共享这个锁；举个例子，假设一个集合里有 10 个文档，多个 update 操作不能并发在这个集合上，即使是更新不同的文档。 2 锁的粒度在 2.2 版本以前，mongod 只有全局锁；在 2.2 版本开始，大部分读写操作只锁一个库，相对之前版本这个粒度已经下降，例如如果一个 mongod 实例上有 5 个库，如果只对一个库中的一个集合执行写操作，那么在写操作过程中，这个库被锁；而其它 5 个库不影响。相比 RDBMS 来说，这个粒度已经算很大了！ 3 如何查看锁的状态12345db.serverStatus() db.currentOp() mongotop mongostat the MongoDB Monitoring Service (MMS) 4 哪些操作会对数据库产生锁？下表列出了常见数据库操作产生的锁。12345678910111213141516171819202122OperationLock TypeIssue a queryRead lockGet more data from a[cursor](http://docs.mongodb.org/manual/reference/glossary/#term-cursor)Read lockInsert dataWrite lockRemove dataWrite lockUpdate dataWrite lock[Map-reduce](http://docs.mongodb.org/manual/reference/glossary/#term-map-reduce)Read lock and write lock, unless operations are specified as non-atomic. Portions of map-reduce jobs can run concurrently.Create an indexBuilding an index in the foreground, which is the default, locks the database for extended periods of time.[db.eval()](http://docs.mongodb.org/manual/reference/method/db.eval/#db.eval \"db.eval\")Write lock.[db.eval()](http://docs.mongodb.org/manual/reference/method/db.eval/#db.eval \"db.eval\")blocks all other JavaScript processes.[eval](http://docs.mongodb.org/manual/reference/commands/#eval \"eval\")Write lock. If used with thenolocklock option, the[eval](http://docs.mongodb.org/manual/reference/commands/#eval \"eval\")option does not take a write lock and cannot write data to the database.[aggregate()](http://docs.mongodb.org/manual/reference/method/db.collection.aggregate/#db.collection.aggregate \"db.collection.aggregate\")Read lock 5 哪些数据库管理操作会锁数据库？某些数据库管理操作会 exclusively 锁住数据库，以下命令需要申请 exclusively 锁，并锁定一段时间 1234567db.collection.ensureIndex(), reIndex, compact, db.repairDatabase(), db.createCollection(), when creating a very large (i.e. many gigabytes) capped collection, db.collection.validate(), db.copyDatabase().This operation may lock all databases 以下命令需要申请 exclusively 锁，但锁定很短时间。1234567db.collection.dropIndex(), db.collection.getLastError(), db.isMaster(), rs.status() (i.e. replSetGetStatus,) db.serverStatus(), db.auth(), and db.addUser(). 备注：可见，一些查看命令也会锁库，在比较繁忙的生产库中，也会有影响的。 6 锁住多个库的操作以下数据库操作会锁定多个库。123456db.copyDatabase() must lock the entire mongod instance at once. Journeying, which is an internal operation, locks all databases for short intervals. All databases share a single journal. User authentication locks the admin database as well as the database the user is accessing. All writes to a replica set’s primary lock both the database receiving the writes and the local database. The lock for the local database allows the mongod to write to the primary’s oplog. 7 参考http://docs.mongodb.org/manual/faq/concurrency/http://blog.163.com/dazuiba_008/blog/static/36334981201211112221314/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"Sharding：Balancer 操作","slug":"20121221153558","date":"2012-12-21T07:35:58.000Z","updated":"2018-12-04T00:27:07.758Z","comments":true,"path":"20121221153558.html","link":"","permalink":"https://postgres.fun/20121221153558.html","excerpt":"","text":"在 sharded cluster 体系结构中，Balancer 进程的作用是转移数据，当一个 shard 中的数据比其它 shard 的数据多并达到一定条件时，Balancer 进程触发。为了减少 Balancer 进程对性能的消耗，当拥有最多 Chunks 的 shard 节点和拥有最少 Chunks 的 shard 节点 chunks 差着达到阀值时才触发 Balancer 进程，如下。 1 Migration Thresholds Number of Chunks Migration Threshold Less than 20 2 21-80 4 Greater than 80 8 备注：当 Balancer 进程开始执行时，会一直执行下去，直到当拥有最多 Chunks 的 shard 节点和拥有最少 Chunks 的 shard 节点的差值小于上表阀值时结束。 2 查看 Balancer 进程信息12345678910111213[shard@redhatB ~]$ mongo 127.0.0.1:7282/config MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:7282/configmongos&gt; db.locks.find( &#123; _id : \"balancer\" &#125; ).pretty(); &#123; \"_id\" : \"balancer\", \"process\" : \"redhatB.example.com:7282:1355471954:1804289383\", \"state\" : 0, \"ts\" : ObjectId(\"50d11adc2f9d8a6566923017\"), \"when\" : ISODate(\"2012-12-19T01:39:40.448Z\"), \"who\" : \"redhatB.example.com:7282:1355471954:1804289383:Balancer:846930886\", \"why\" : \"doing balance round\" &#125; 3 查看 Balancer 进程是否开启1mongos&gt; sh.getBalancerState();true 备注：连接到 config 库操作。 4 停 Balancer 进程12345mongos&gt; sh.stopBalancer();Waiting for active hosts... Waiting for the balancer lock... Waiting again for active hosts after balancer is off...mongos&gt; sh.getBalancerState(); false 备注：连接到 config 库操作。 5 开启 Balancer 进程123mongos&gt; sh.startBalancer(); mongos&gt; sh.getBalancerState(); true 备注：连接到 config 库操作。 6 设置 Balancer 进程运行时间窗口默认情况下Balancing 进程时时在运行 为了降低 Balancing 进程对系统的影响，也可以设置 Balancer 进程的运行时间窗口，让 Balancer 进程在指定时间窗口操作。123mongos&gt; db.settings.update(&#123; _id : \"balancer\" &#125;, &#123; $set : &#123; activeWindow : &#123; start : \"23:00\", stop : \"6:00\" &#125; &#125; &#125;, true ) ;mongos&gt; db.settings.find(); &#123; \"_id\" : \"balancer\", \"activeWindow\" : &#123; \"start\" : \"23:00\", \"stop\" : \"6:00\" &#125;, \"stopped\" : false &#125; 备注：以上设置 balancer 进程在 23:00 到 6:00 时间窗口执行，如果要设置时间窗口，确保在指定时间段内能够完成数据分布。 7删除 Balancer 进程运行时间窗口1234mongos&gt; db.settings.update(&#123; \"_id\" : \"balancer\" &#125;, &#123; $unset : &#123; activeWindow : 1 &#125;&#125;);mongos&gt; db.settings.find(); &#123; \"_id\" : \"chunksize\", \"value\" : 10 &#125; &#123; \"_id\" : \"balancer\", \"stopped\" : false &#125; 8 参考http://docs.mongodb.org/manual/administration/sharding/#balancer-operationshttp://docs.mongodb.org/manual/core/sharding/#sharding-balancing","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"Sharding：Shard Cluster 删除 Shard 节点","slug":"20121218160448","date":"2012-12-18T08:04:48.000Z","updated":"2018-12-04T00:32:02.841Z","comments":true,"path":"20121218160448.html","link":"","permalink":"https://postgres.fun/20121218160448.html","excerpt":"","text":"上篇Blog学习了给 Shard Cluster 增加 shard 节点，接下来学习下删除节点。 一 删除 Shard 节点基本思路 移走要删除 Shard 节点的数据； 确保要删除的 shard 节点不是任何库的 Primary 节点，如果是，将这些库的 Primary 节点移到其它 shard 节点； 从 Shard Cluster 配置中删除 shard 节点信息。 二 删除 Shard 节点步骤2.1 查看当前 Shard Cluster 状态123456789101112131415161718192021222324252627mongos&gt; sh.status(); --- Sharding Status --- sharding version: &#123; \"_id\" : 1, \"version\" : 3 &#125; shards: &#123; \"_id\" : \"shard0000\", \"host\" : \"redhatB.example.com:5281\" &#125; &#123; \"_id\" : \"shard0001\", \"host\" : \"redhatB.example.com:5282\" &#125; &#123; \"_id\" : \"shard0002\", \"host\" : \"redhatB.example.com:5283\" &#125; databases: &#123; \"_id\" : \"admin\", \"partitioned\" : false, \"primary\" : \"config\" &#125; &#123; \"_id\" : \"test\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; &#123; \"_id\" : \"francs\", \"partitioned\" : true, \"primary\" : \"shard0000\" &#125; francs.test_1 chunks: shard0000 2 shard0001 2 shard0002 3 &#123; \"id\" : &#123; $minKey : 1 &#125; &#125; --&gt;&gt; &#123; \"id\" : 1 &#125; on : shard0000 Timestamp(5000, 0) &#123; \"id\" : 1 &#125; --&gt;&gt; &#123; \"id\" : 7282 &#125; on : shard0001 Timestamp(5000, 1) &#123; \"id\" : 7282 &#125; --&gt;&gt; &#123; \"id\" : 15651 &#125; on : shard0001 Timestamp(4000, 3) &#123; \"id\" : 15651 &#125; --&gt;&gt; &#123; \"id\" : 29868 &#125; on : shard0002 Timestamp(4000, 4) &#123; \"id\" : 29868 &#125; --&gt;&gt; &#123; \"id\" : 37149 &#125; on : shard0002 Timestamp(5000, 2) &#123; \"id\" : 37149 &#125; --&gt;&gt; &#123; \"id\" : 48037 &#125; on : shard0002 Timestamp(5000, 3) &#123; \"id\" : 48037 &#125; --&gt;&gt; &#123; \"id\" : &#123; $maxKey : 1 &#125; &#125; on : shard0000 Timestamp(4000, 1) &#123; \"_id\" : \"records\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; &#123; \"_id\" : \"fracns\", \"partitioned\" : false, \"primary\" : \"shard0002\" &#125;mongos&gt; db.test_1.count(); 100000 备注：当前有三个 shard 节点，其中 test_1 集合有 10000 个文档。目标删除 “redhatB.example.com:5283” 节点。 2.2 从目标 shard 中转移数据使用 removeShard 命令就能将 shard 节点的数据转移到其它节点，前提是已开启balancer 进程。 2.2.1 查看 balancer 进程是否已开启12mongos&gt; sh.getBalancerState(); true 2.2.2 转移数据12345678910111213mongos&gt; use admin; switched to db adminmongos&gt; db.runCommand(&#123;removeshard:\"redhatB.example.com:5283\"&#125;);&#123; \"msg\" : \"draining started successfully\", \"state\" : \"started\", \"shard\" : \"shard0002\", \"note\" : \"you need to drop or movePrimary these databases\", \"dbsToMove\" : [ \"fracns\" ], \"ok\" : 1 &#125; 备注：这个步骤会将目标删除的 shard 节点数据转移到其它 shard 节点，数据量越大，耗时越长。 2.3 核查迁移状态1234567891011121314mongos&gt; db.runCommand(&#123;removeshard:\"redhatB.example.com:5283\"&#125;); &#123; \"msg\" : \"draining ongoing\", \"state\" : \"ongoing\", \"remaining\" : &#123; \"chunks\" : NumberLong(0), \"dbs\" : NumberLong(1) &#125;, \"note\" : \"you need to drop or movePrimary these databases\", \"dbsToMove\" : [ \"fracns\" ], \"ok\" : 1 &#125; 备注： 提示所删除的 shard 节点是某个库的 Primary 节点，再根据 2.1 红色字体信息，发现 “redhatB.example.com:5283” 节点是 fracns 库的主节点。 2.4 转移 Unsharded Databases12345mongos&gt; use admin; switched to db adminmongos&gt; db.runCommand( &#123; movePrimary: \"fracns\", to: \"redhatB.example.com:5281\" &#125;) ; &#123; \"primary \" : \"shard0000:redhatB.example.com:5281\", \"ok\" : 1 &#125; 备注：这个命令将 fracns 库的主节点转移到 “redhatB.example.com:5281” shard 节点。 2.5 查看 Shard Cluster 状态1234567891011121314151617181920212223mongos&gt; sh.status(); --- Sharding Status --- sharding version: &#123; \"_id\" : 1, \"version\" : 3 &#125; shards: &#123; \"_id\" : \"shard0000\", \"host\" : \"redhatB.example.com:5281\" &#125; &#123; \"_id\" : \"shard0001\", \"host\" : \"redhatB.example.com:5282\" &#125; &#123; \"_id\" : \"shard0002\", \"draining\" : true, \"host\" : \"redhatB.example.com:5283\" &#125; databases: &#123; \"_id\" : \"admin\", \"partitioned\" : false, \"primary\" : \"config\" &#125; &#123; \"_id\" : \"test\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; &#123; \"_id\" : \"francs\", \"partitioned\" : true, \"primary\" : \"shard0000\" &#125; francs.test_1 chunks: shard0000 4 shard0001 3 &#123; \"id\" : &#123; $minKey : 1 &#125; &#125; --&gt;&gt; &#123; \"id\" : 1 &#125; on : shard0000 Timestamp(5000, 0) &#123; \"id\" : 1 &#125; --&gt;&gt; &#123; \"id\" : 7282 &#125; on : shard0001 Timestamp(5000, 1) &#123; \"id\" : 7282 &#125; --&gt;&gt; &#123; \"id\" : 15651 &#125; on : shard0001 Timestamp(4000, 3) &#123; \"id\" : 15651 &#125; --&gt;&gt; &#123; \"id\" : 29868 &#125; on : shard0000 Timestamp(6000, 0) &#123; \"id\" : 29868 &#125; --&gt;&gt; &#123; \"id\" : 37149 &#125; on : shard0001 Timestamp(7000, 0) &#123; \"id\" : 37149 &#125; --&gt;&gt; &#123; \"id\" : 48037 &#125; on : shard0000 Timestamp(8000, 0) &#123; \"id\" : 48037 &#125; --&gt;&gt; &#123; \"id\" : &#123; $maxKey : 1 &#125; &#125; on : shard0000 Timestamp(4000, 1) &#123; \"_id\" : \"records\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; &#123; \"_id\" : \"fracns\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; 备注：注意最后一行，库 fracns 的主节点已变为 shard0000 了。 2.6 Finalize the Migration1234567mongos&gt; db.runCommand(&#123;removeshard:\"redhatB.example.com:5283\"&#125;); &#123; \"msg\" : \"removeshard completed successfully\", \"state\" : \"completed\", \"shard\" : \"shard0002\", \"ok\" : 1 &#125; 备注：在转移 shard 节点数据之后，再次执行 removeshard 命令，收到以上信息，表示删除 shard 节点成功。 三：验证3.1 查看 Shard Cluster 状态12345678910111213141516171819202122mongos&gt; sh.status(); --- Sharding Status --- sharding version: &#123; \"_id\" : 1, \"version\" : 3 &#125; shards: &#123; \"_id\" : \"shard0000\", \"host\" : \"redhatB.example.com:5281\" &#125; &#123; \"_id\" : \"shard0001\", \"host\" : \"redhatB.example.com:5282\" &#125; databases: &#123; \"_id\" : \"admin\", \"partitioned\" : false, \"primary\" : \"config\" &#125; &#123; \"_id\" : \"test\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; &#123; \"_id\" : \"francs\", \"partitioned\" : true, \"primary\" : \"shard0000\" &#125; francs.test_1 chunks: shard0000 4 shard0001 3 &#123; \"id\" : &#123; $minKey : 1 &#125; &#125; --&gt;&gt; &#123; \"id\" : 1 &#125; on : shard0000 Timestamp(5000, 0) &#123; \"id\" : 1 &#125; --&gt;&gt; &#123; \"id\" : 7282 &#125; on : shard0001 Timestamp(5000, 1) &#123; \"id\" : 7282 &#125; --&gt;&gt; &#123; \"id\" : 15651 &#125; on : shard0001 Timestamp(4000, 3) &#123; \"id\" : 15651 &#125; --&gt;&gt; &#123; \"id\" : 29868 &#125; on : shard0000 Timestamp(6000, 0) &#123; \"id\" : 29868 &#125; --&gt;&gt; &#123; \"id\" : 37149 &#125; on : shard0001 Timestamp(7000, 0) &#123; \"id\" : 37149 &#125; --&gt;&gt; &#123; \"id\" : 48037 &#125; on : shard0000 Timestamp(8000, 0) &#123; \"id\" : 48037 &#125; --&gt;&gt; &#123; \"id\" : &#123; $maxKey : 1 &#125; &#125; on : shard0000 Timestamp(4000, 1) &#123; \"_id\" : \"records\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; &#123; \"_id\" : \"fracns\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; 备注：上面已经没有 “redhatB.example.com:5283” 节点信息了。 3.2 验证集合 test_1 数据1234mongos&gt; use francs;use francs; switched to db francs mongos&gt; db.test_1.count();db.test_1.count(); 100000 四 停 “redhatB.example.com:5283” 节点12345[shard@redhatB ~]$ ps -ef | grep 5283 shard 18089 17986 0 14:23 pts/2 00:00:00 grep 5283 shard 25864 1 0 02:51 ? 00:04:52 mongod -f /pgdata_xc/shard/shard3/shard3_5283.conf [shard@redhatB ~]$ kill 25864 备注：以上就是完整的删除节点步骤。 五 附primary shard ：如果某个库已开启 sharding 功能，并将未分片的集合存储在一个单一的 shard 节点上，那么这个 shard 节点被称为这个库的 primary shard。 六 参考http://docs.mongodb.org/manual/tutorial/remove-shards-from-cluster/http://docs.mongodb.org/manual/reference/commands/#removeShardhttp://docs.mongodb.org/manual/reference/glossary/#term-primary-shard","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"Sharding：Shard Cluster 增加 Shard 节点","slug":"20121217221548","date":"2012-12-17T14:15:48.000Z","updated":"2018-09-04T01:34:01.120Z","comments":true,"path":"20121217221548.html","link":"","permalink":"https://postgres.fun/20121217221548.html","excerpt":"","text":"上篇 blog 介绍了搭建单节点 shard ，在某些情况下需要增加 shard 节点，接下来介绍下。 现有环境1.1 查看Sharding Cluster 状态1234567891011121314151617mongos&gt; sh.status(); ---Sharding Status --- sharding version: &#123; \"_id\" : 1, \"version\" : 3 &#125; shards: &#123; \"_id\" : \"shard0000\", \"host\" : \"redhatB.example.com:5281\" &#125; &#123; \"_id\" : \"shard0001\", \"host\" : \"redhatB.example.com:5282\" &#125; databases: &#123; \"_id\" : \"admin\", \"partitioned\" : false, \"primary\" : \"config\" &#125; &#123; \"_id\" : \"test\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; &#123; \"_id\" : \"francs\", \"partitioned\" : true, \"primary\" : \"shard0000\" &#125; francs.test_1 chunks: shard0000 2 shard0001 1 &#123; \"id\" : &#123; $minKey : 1 &#125; &#125; --&gt;&gt; &#123; \"id\" : 1 &#125; on : shard0000 Timestamp(2000, 1) &#123; \"id\" : 1 &#125; --&gt;&gt; &#123; \"id\" : 11828 &#125; on : shard0000 Timestamp(1000, 3) &#123; \"id\" : 11828 &#125; --&gt;&gt; &#123; \"id\" : &#123; $maxKey : 1 &#125; &#125; on : shard0001 Timestamp(2000, 0) &#123; \"_id\" : \"records\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; 备注：两单节点的 shard。 1.2 查看当前 test_1 集合状态12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758mongos&gt; db.test_1.stats(); &#123; \"sharded\" : true, \"ns\" : \"francs.test_1\", \"count\" : 50000, \"numExtents\" : 10, \"size\" : 1800024, \"storageSize\" : 5849088, \"totalIndexSize\" : 3131408, \"indexSizes\" : &#123; \"_id_\" : 1684256, \"id_1\" : 1447152 &#125;, \"avgObjSize\" : 36.00048, \"nindexes\" : 2, \"nchunks\" : 4, \"shards\" : &#123; \"shard0000\" : &#123; \"ns\" : \"francs.test_1\", \"count\" : 34350, \"size\" : 1236600, \"avgObjSize\" : 36, \"storageSize\" : 3055616, \"numExtents\" : 5, \"nindexes\" : 2, \"lastExtentSize\" : 2359296, \"paddingFactor\" : 1, \"systemFlags\" : 1, \"userFlags\" : 0, \"totalIndexSize\" : 2125760, \"indexSizes\" : &#123; \"_id_\" : 1144640, \"id_1\" : 981120 &#125;, \"ok\" : 1 &#125;, \"shard0001\" : &#123; \"ns\" : \"francs.test_1\", \"count\" : 15650, \"size\" : 563424, \"avgObjSize\" : 36.00153354632588, \"storageSize\" : 2793472, \"numExtents\" : 5, \"nindexes\" : 2, \"lastExtentSize\" : 2097152, \"paddingFactor\" : 1, \"systemFlags\" : 1, \"userFlags\" : 0, \"totalIndexSize\" : 1005648, \"indexSizes\" : &#123; \"_id_\" : 539616, \"id_1\" : 466032 &#125;, \"ok\" : 1 &#125; &#125;, \"ok\" : 1 &#125; 备注：集合 test_1 的数据分布在两 shard 节点。 新增 Shard3 节点2.1 创建 shard3 数据目录和配置文件123456789[shard@redhatB shard]$ mkdir -p /shard/shard3 [shard@redhatB shard]$ touch /shard/shard3/shard3_5283.conf vim /shard/shard3/shard3_5283.conffork = true port = 5283 dbpath = /shard/shard3 logpath = /shard/shard3/shard3.log logappend = true journal = true 2.2 启动 shard3 节点1234[shard@redhatB shard]$ mongod -f /shard/shard3/shard3_5283.conf forked process: 23241 all output going to: /shard/shard3/shard3.log child process started successfully, parent exiting 2.3 新增 shard 节点1234[shard@redhatB ~]$ mongo 127.0.0.1:7282 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:7282/test mongos&gt; sh.addShard(\"redhatB.example.com:5283\");&#123; \"shardAdded\" : \"shard0002\", \"ok\" : 1 &#125; 备注：通过命令 addShard 新增 shard 节点。 2.4 再次查看 shard cluster 状态 12345678910111213141516171819mongos&gt; sh.status(); --- Sharding Status --- sharding version: &#123; \"_id\" : 1, \"version\" : 3 &#125; shards: &#123; \"_id\" : \"shard0000\", \"host\" : \"redhatB.example.com:5281\" &#125; &#123; \"_id\" : \"shard0001\", \"host\" : \"redhatB.example.com:5282\" &#125; &#123; \"_id\" : \"shard0002\", \"host\" : \"redhatB.example.com:5283\" &#125; databases: &#123; \"_id\" : \"admin\", \"partitioned\" : false, \"primary\" : \"config\" &#125; &#123; \"_id\" : \"test\", \"partitioned\" : false, \"primary\" : \"shard0000\" &#125; &#123; \"_id\" : \"francs\", \"partitioned\" : true, \"primary\" : \"shard0000\" &#125; francs.test_1 chunks: shard0002 1 shard0000 1 shard0001 1 &#123; \"id\" : &#123; $minKey : 1 &#125; &#125; --&gt;&gt; &#123; \"id\" : 1 &#125; on : shard0002 Timestamp(3000, 0) &#123; \"id\" : 1 &#125; --&gt;&gt; &#123; \"id\" : 11828 &#125; on : shard0000 Timestamp(3000, 1) &#123; \"id\" : 11828 &#125; --&gt;&gt; &#123; \"id\" : &#123; $maxKey : 1 &#125; &#125; on : shard0001 Timestamp(2000, 0) &#123; 2.5 查看 test_1 集合状态1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677mongos&gt; db.test_1.stats(); &#123; \"sharded\" : true, \"ns\" : \"francs.test_1\", \"count\" : 50000, \"numExtents\" : 15, \"size\" : 1800048, \"storageSize\" : 8642560, \"totalIndexSize\" : 3147760, \"indexSizes\" : &#123; \"_id_\" : 1692432, \"id_1\" : 1455328 &#125;, \"avgObjSize\" : 36.00096, \"nindexes\" : 2, \"nchunks\" : 4, \"shards\" : &#123; \"shard0000\" : &#123; \"ns\" : \"francs.test_1\", \"count\" : 1964, \"size\" : 70704, \"avgObjSize\" : 36, \"storageSize\" : 3055616, \"numExtents\" : 5, \"nindexes\" : 2, \"lastExtentSize\" : 2359296, \"paddingFactor\" : 1, \"systemFlags\" : 1, \"userFlags\" : 0, \"totalIndexSize\" : 138992, \"indexSizes\" : &#123; \"_id_\" : 73584, \"id_1\" : 65408 &#125;, \"ok\" : 1 &#125;, \"shard0001\" : &#123; \"ns\" : \"francs.test_1\", \"count\" : 15650, \"size\" : 563424, \"avgObjSize\" : 36.00153354632588, \"storageSize\" : 2793472, \"numExtents\" : 5, \"nindexes\" : 2, \"lastExtentSize\" : 2097152, \"paddingFactor\" : 1, \"systemFlags\" : 1, \"userFlags\" : 0, \"totalIndexSize\" : 1005648, \"indexSizes\" : &#123; \"_id_\" : 539616, \"id_1\" : 466032 &#125;, \"ok\" : 1 &#125;, \"shard0002\" : &#123; \"ns\" : \"francs.test_1\", \"count\" : 32386, \"size\" : 1165920, \"avgObjSize\" : 36.000741060952265, \"storageSize\" : 2793472, \"numExtents\" : 5, \"nindexes\" : 2, \"lastExtentSize\" : 2097152, \"paddingFactor\" : 1, \"systemFlags\" : 1, \"userFlags\" : 0, \"totalIndexSize\" : 2003120, \"indexSizes\" : &#123; \"_id_\" : 1079232, \"id_1\" : 923888 &#125;, \"ok\" : 1 &#125; &#125;, \"ok\" : 1 &#125; 备注：数据已分散到节点 shard3 了，这些工作是一个名为 Balancing 的进程完成的，如果数据比较大，这步花的时间越长，这里不详细介绍。 参考http://docs.mongodb.org/manual/tutorial/add-shards-to-shard-cluster/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"Sharding：搭建单节点 Shard 环境","slug":"20121215105236","date":"2012-12-15T02:52:36.000Z","updated":"2018-09-04T01:34:01.057Z","comments":true,"path":"20121215105236.html","link":"","permalink":"https://postgres.fun/20121215105236.html","excerpt":"","text":"今天初步学习 MongoDB 的分片相关的内容，分片是 MongoDB 的一个非常重要的特性，将数据分散到各个子节点，分散IO，具有易分片，易扩展，易集群的特性等，以下在虚拟机环境下搭建最简单的分片环境。 基础信息1.1 分片环境核心组件 shard 节点: 分片节点，存储数据，可以是单个进程，也可以是 replica set；config 节点： 配置节点，仅存储集群的元数据，在生产环境上一般配置 3个 config 节点;monos 进程：路由进程，本身不存储数据，负责将应用发出的SQL分发到各个 shard 节点，monos 进程消耗资源较少，可以部署在应用服务端，也可以部署在数据库端。 1.2 环境信息主机名: redhatB.example.comOS: Red Hat Enterprise Linux Server release 6.2 (Santiago)mongo: db version v2.2.1 ( 软件安装略) 1.3 各节点端口信息shard1 5281shard2 5282Config Server 7281mongos 7282 搭建步骤2.1 增加OS用户12345[root@redhatB ~]# groupadd shard [root@redhatB ~]# useradd -g shard shard [root@redhatB ~]# passwd shard[root@redhatB ]# mkdir -p /shard [root@redhatB ]# chown -R shard:shard /shard 2.2 配置 shard1 节点2.2.1 创建目录和配置文件12345678mkdir -p /shard/shard1 touch /shard/shard1/shard1_5281.conf #(包含以下内容)fork = true port = 5281 dbpath = /shard/shard1 logpath = /shard/shard1/shard1.log logappend = true journal = true 2.2.2 启动 shard11mongod -f /shard/shard1/shard1_5281.conf 2.3 配置 shard2 节点2.3.1 创建目录和配置文件12345678mkdir -p /shard/shard2 touch /shard/shard2/shard2_5282.conf (包含以下内容)fork = true port = 5282 dbpath = /shard/shard2 logpath = /shard/shard2/shard2.log logappend = true journal = true 2.3.2 启动 shard21mongod -f /shard/shard2/shard2_5282.conf 2.4 配置 Config Server2.4.1 创建目录和配置文件12345678mkdir -p /shard/conf touch /shard/conf/conf_7281.conf #(包含以下内容)configsvr = true fork = true port = 7281 dbpath = /shard/conf logpath = /shard/conf/conf.log logappend = true 2.4.2 启动 Config Server1mongod -f /shard/conf/conf_7281.conf 2.5 启动 mongos 进程1mongos --configdb redhatB.example.com:7281 --fork --logpath /shard/mongos.log --chunkSize 1 --port 7282 备注：–chunkSize 选项设置 chunk 的大小，默认64M，这里将它设置为 1M，在接下来的实验更容易看到分片效果。 2.6 增加分片结点到集群1234567891011[shard@redhatB shard]$ mongo 127.0.0.1:7282 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:7282/test mongos&gt; show dbs; config 0.046875GBmongos&gt; sh.addShard( \"redhatB.example.com:5281\" ); &#123; \"shardAdded\" : \"shard0000\", \"ok\" : 1 &#125;mongos&gt; sh.addShard( \"redhatB.example.com:5282\" ); &#123; \"shardAdded\" : \"shard0001\", \"ok\" : 1 &#125; 2.7 查看集群状态123456789mongos&gt; sh.status(); --- Sharding Status --- sharding version: &#123; \"_id\" : 1, \"version\" : 3 &#125; shards: &#123; \"_id\" : \"shard0000\", \"host\" : \"redhatB.example.com:5281\" &#125; &#123; \"_id\" : \"shard0001\", \"host\" : \"redhatB.example.com:5282\" &#125; databases: &#123; \"_id\" : \"admin\", \"partitioned\" : false, \"primary\" : \"config\" &#125; 备注：到了这里，单节点的 Sharded Cluster 搭建完成了，接下来验证下。 测试3.1 开启指定数据库 Sharding 功能123456[shard@redhatB shard]$ mongo 127.0.0.1:7282 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:7282/testmongos&gt; sh.enableSharding(\"francs\"); &#123; \"ok\" : 1 &#125; 3.2 开启指定集合分片12mongos&gt; sh.shardCollection(\"francs.test_1\", &#123; \"id\": 1&#125; ); &#123; \"collectionsharded\" : \"francs.test_1\", \"ok\" : 1 &#125; 3.3 查看已分片集合的索引信息12345678910111213141516171819mongos&gt; db.test_1.getIndexes(); [ &#123; \"v\" : 1, \"key\" : &#123; \"_id\" : 1 &#125;, \"ns\" : \"francs.test_1\", \"name\" : \"_id_\" &#125;, &#123; \"v\" : 1, \"key\" : &#123; \"id\" : 1 &#125;, \"ns\" : \"francs.test_1\", \"name\" : \"id_1\" &#125; ] 备注：在集合上通过命令 shardCollection 开启分片后， 在分片的字段上默认创建了索引。 3.4 测试:插入数据1mongos&gt; for( var i=1; i&lt;50001; i++) db.test_1.save(&#123;id:i,name:'abc'&#125;) 3.5 看集合 test_1 状态123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657mongos&gt; db.test_1.stats(); &#123; \"sharded\" : true, \"ns\" : \"francs.test_1\", \"count\" : 50000, \"numExtents\" : 10, \"size\" : 2400128, \"storageSize\" : 5849088, \"totalIndexSize\" : 3049648, \"indexSizes\" : &#123; \"_id_\" : 1635200, \"id_1\" : 1414448 &#125;, \"avgObjSize\" : 48.00256, \"nindexes\" : 2, \"nchunks\" : 3, \"shards\" : &#123; \"shard0000\" : &#123; \"ns\" : \"francs.test_1\", \"count\" : 11827, \"size\" : 567760, \"avgObjSize\" : 48.005411346918066, \"storageSize\" : 3055616, \"numExtents\" : 5, \"nindexes\" : 2, \"lastExtentSize\" : 2359296, \"paddingFactor\" : 1, \"systemFlags\" : 1, \"userFlags\" : 0, \"totalIndexSize\" : 735840, \"indexSizes\" : &#123; \"_id_\" : 392448, \"id_1\" : 343392 &#125;, \"ok\" : 1 &#125;, \"shard0001\" : &#123; \"ns\" : \"francs.test_1\", \"count\" : 38173, \"size\" : 1832368, \"avgObjSize\" : 48.0016765776858, \"storageSize\" : 2793472, \"numExtents\" : 5, \"nindexes\" : 2, \"lastExtentSize\" : 2097152, \"paddingFactor\" : 1, \"systemFlags\" : 1, \"userFlags\" : 0, \"totalIndexSize\" : 2313808, \"indexSizes\" : &#123; \"_id_\" : 1242752, \"id_1\" : 1071056 &#125;, \"ok\" : 1 &#125; &#125;, \"ok\" : 1 &#125; 备注：”shard0000” 存储 11827条，”shard0001” 存储 38173 条，集合已分片。 参考http://docs.mongodb.org/manual/tutorial/deploy-shard-cluster/http://docs.mongodb.org/manual/administration/sharding/#set-up-a-sharded-clusterhttp://blog.163.com/dazuiba_008/blog/static/36334981201110172191325/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：初识 Capped Collection","slug":"20121207145043","date":"2012-12-07T06:50:43.000Z","updated":"2018-09-04T01:34:00.995Z","comments":true,"path":"20121207145043.html","link":"","permalink":"https://postgres.fun/20121207145043.html","excerpt":"","text":"MongoDB 支持 Capped Collection，一种固定大小的集合，当集合的大小达到指定大小时，新数据覆盖老数据，MongoDB Replica set 中的 oplog 就是 Capped Collection 类型。 1 查看 oplog 是否是 Capped Collection123456789101112131415[mongo@redhatB ~]$ mongo 127.0.0.1:27018 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27018/testrs0:PRIMARY&gt; use local; switched to db local rs0:PRIMARY&gt; show collections; me oplog.rs replset.minvalid slaves system.indexes system.replsetrs0:PRIMARY&gt; db.oplog.rs.isCapped(); true 备注：通过 db.collection.isCapped() 命令可以查看一个集合是否是 Capped Collection 。 Capped Collection 具有以下特性，在使用的时候需要注意： 1 不可以对 Capped Collection 进行分片。 2 在 2.2 版本以后，创建的Capped Collection 默认在 _id 字段上创建索引，而在 2.2 版本或以前没有。 3 在 Capped Collection 插入文档后可以进行更新(update)操作，当更新不能导致原来文档占用 空间增长，否则更新失败。 4 不可以对 capped collection 执行删除文档操作，但可以删除整个集合。 接下来会测试其中的部分特性。 2 创建 Capped Collection12rs0:PRIMARY&gt; db.createCollection(\"mycoll1\",&#123;capped:true,size:1024&#125;); &#123; \"ok\" : 1 &#125; 备注：通过 db.createCollection 命令创建 Capped Collection 集合，创建时必须指定集合大小，用于预先分配空间。 3 查看一个集合是否是 Capped Collection 可以通过以下两种方法查看一个集合是否是 Capped Collection 。12345678910111213141516171819rs0:PRIMARY&gt; db.mycoll1.isCapped();truers0:PRIMARY&gt; db.mycoll1.stats();&#123; \"ns\" : \"test.mycoll1\", \"count\" : 0, \"size\" : 0, \"storageSize\" : 4096, \"numExtents\" : 1, \"nindexes\" : 1, \"lastExtentSize\" : 4096, \"paddingFactor\" : 1, \"systemFlags\" : 1, \"userFlags\" : 0, \"totalIndexSize\" : 8176, \"indexSizes\" : &#123; \"_id_\" : 8176 &#125;, \"capped\" : true, \"max\" : 2147483647, \"ok\" : 1 &#125; 备注：”capped” 属性为 true 表示是 Capped Collection 。 4 测试：插入记录12345678910111213141516171819202122232425rs0:PRIMARY&gt; for (var i = 1; i &lt;= 10000; i++) db.mycoll1.save(&#123;id : i, name : 'francs'&#125;);rs0:PRIMARY&gt; db.mycoll1.find().count(); 56rs0:PRIMARY&gt; db.mycoll1.find(); &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db7f\"), \"id\" : 9945, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db80\"), \"id\" : 9946, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db81\"), \"id\" : 9947, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db82\"), \"id\" : 9948, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db83\"), \"id\" : 9949, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db84\"), \"id\" : 9950, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db85\"), \"id\" : 9951, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db86\"), \"id\" : 9952, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db87\"), \"id\" : 9953, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db88\"), \"id\" : 9954, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db89\"), \"id\" : 9955, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db8a\"), \"id\" : 9956, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db8b\"), \"id\" : 9957, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db8c\"), \"id\" : 9958, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db8d\"), \"id\" : 9959, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db8e\"), \"id\" : 9960, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db8f\"), \"id\" : 9961, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db90\"), \"id\" : 9962, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db91\"), \"id\" : 9963, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db92\"), \"id\" : 9964, \"name\" : \"francs\" &#125; Type \"it\" for more 备注：由于限制了集合大小不小，目标插入 10000 条，结果只插入了 56 条数据，并且老数据被新数据 覆盖。另外不可以删除 Capped Collection 的文档，下面测试下。 5 测试： 删除 capped collection 中的文档12rs0:PRIMARY&gt; db.mycoll1.remove(&#123;id:9956&#125;); canot remove from a capped collection 备注：删除文档时抛出异常。6 测试：更新 capped collection 中的文档1234567rs0:PRIMARY&gt; db.mycoll1.find(&#123;id:9956&#125;); &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db8a\"), \"id\" : 9956, \"name\" : \"francs\" &#125;rs0:PRIMARY&gt; db.mycoll1.update(&#123;id:9956&#125;,&#123;$set:&#123;name:'aaa_francs'&#125;&#125;); failing update: objects in a capped ns cannot growrs0:PRIMARY&gt; db.mycoll1.update(&#123;id:9956&#125;,&#123;$set:&#123;name:'bbb'&#125;&#125;);rs0:PRIMARY&gt; db.mycoll1.find(&#123;id:9956&#125;); &#123; \"_id\" : ObjectId(\"50b811cf68b1911e7096db8a\"), \"id\" : 9956, \"name\" : \"bbb\" &#125; 备注：这里正好验证了特性3，更新后的值不能超过原有空间，否则更新失败。7 参考http://docs.mongodb.org/manual/core/capped-collections/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"MongoDB：查看集合的统计信息","slug":"20121205150855","date":"2012-12-05T07:08:55.000Z","updated":"2018-09-04T01:34:00.932Z","comments":true,"path":"20121205150855.html","link":"","permalink":"https://postgres.fun/20121205150855.html","excerpt":"","text":"和 RDBMS 一样， MongoDB 同样存储集合的统计信息，通过调用命令 db.collection.stats() 可以方便的查看集合的统计信息。 查看集合的统计信息12345678910111213141516171819rs0:PRIMARY&gt; db.things.stats(); &#123; \"ns\" : \"test.things\", \"count\" : 30, \"size\" : 1440, \"avgObjSize\" : 48, \"storageSize\" : 8192, \"numExtents\" : 1, \"nindexes\" : 1, \"lastExtentSize\" : 8192, \"paddingFactor\" : 1, \"systemFlags\" : 1, \"userFlags\" : 0, \"totalIndexSize\" : 8176, \"indexSizes\" : &#123; \"_id_\" : 8176 &#125;, \"ok\" : 1 &#125; 备注： 部分参数解释如下： ns: 集合的命名空间，可以理解为集合名称 count: 集合中的文档总数 size: 集合中数据占用空间大小，不包括索引 ，单位为字节。 avgObjSize： 平均对像占用的空间大小 storageSize： 给整个集合分配的存储空间，当删除集合中的文档时，这个值不会降代。 numExtents： 连续分配的数据块 nindexes: 索引个数，每个集合至少有一个 _id 索引。 lastExtentSize： 最近分配的块的大小 paddingFactor: 这个参数不太清楚，以后补充。 totalIndexSize: 所有索引大小总和 indexSizes: 列出集合的所有索引字段，以及索引大小。 以 KB单位显示123456789101112131415161718rs0:PRIMARY&gt; db.things.stats(1024);&#123; \"ns\" : \"test.things\", \"count\" : 30, \"size\" : 1, \"avgObjSize\" : 0.03333333333333333, \"storageSize\" : 8, \"numExtents\" : 1, \"nindexes\" : 1, \"lastExtentSize\" : 8, \"paddingFactor\" : 1, \"systemFlags\" : 1, \"userFlags\" : 0, \"totalIndexSize\" : 7, \"indexSizes\" : &#123; \"_id_\" : 7 &#125;, \"ok\" : 1 &#125; 备注：db.things.stats() 命令默认以 bytes 为单位，以上是以 KB 为单位。 查看集合占用空间大小12rs0:PRIMARY&gt; db.things.dataSize(); 1440 备注：和前面的命令 db.things.stats() 输出的 size 字段值一样。 参考http://docs.mongodb.org/manual/reference/javascript/#db.collection.statshttp://docs.mongodb.org/manual/reference/collection-statistics/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：Replica Set 之操作日志 Oplog","slug":"20121203151512","date":"2012-12-03T07:15:12.000Z","updated":"2018-09-04T01:34:00.869Z","comments":true,"path":"20121203151512.html","link":"","permalink":"https://postgres.fun/20121203151512.html","excerpt":"","text":"之前的blog 学习了 MongoDB 主从搭建，以及节点管理的内容，接下来学习实现主从复制一个重要角色，即 Oplog。 MongoDB 的复制集是通过 Oplog 来实现的，主库的更改操作会被记录到主库的 Oplog 日志中，然后从库通过异步方式复制主库的 Oplog 文件并且将 Oplog 日志应用到从库，从而实现了与主库的同步。 关于 Oplog 的大小 创建 mongod 服务时可以指定 –oplogSize 参数指定 oplog 大小，如果不指定，不同操作系统上的 oplog 默认大小不同，具体为以下： For 64-bit Linux, Solaris, and FreeBSD systems：可以分配 5% 的剩余空间。如果分配的值仍小于 1GB，那么会分配 1GB。 For 64-bit OS X systems：分配 183MB。 For 32-bit systems：分配 48MB。 查看 Oplog 的内容123456789101112131415161718rs0:PRIMARY&gt; show dbs; local 0.125GB test 0.0625GBrs0:PRIMARY&gt; use local; switched to db localrs0:PRIMARY&gt; show collections; me oplog.rs replset.minvalid slaves system.indexes system.replsetrs0:PRIMARY&gt; db.oplog.rs.find(); &#123; \"ts\" : Timestamp(1354037833000, 1), \"h\" : NumberLong(\"-190625176257847918\"), \"v\" : 2, \"op\" : \"i\", \"ns\" : \"test.test_3\", \"o\" : &#123; \"_id\" : ObjectId(\"50b4fa49f2d598e740e3ee7a\"), \"id\" : 1 &#125; &#125; &#123; \"ts\" : Timestamp(1354038811000, 1), \"h\" : NumberLong(\"6383516783459941672\"), \"v\" : 2, \"op\" : \"i\", \"ns\" : \"test.test_4\", \"o\" : &#123; \"_id\" : ObjectId(\"50b4fe1b15747af472f54831\"), \"id\" : 1 &#125; &#125; 备注：local 库中的集合 oplog.rs 记录了oplog 操作日志内容。 查看 Oplog 的状态 通过 db.printReplicationInfo() 命令查看 oplog 状态。12345678910[mongo@redhatB ~]$ mongo 127.0.0.1:27018 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27018/testrs0:PRIMARY&gt; db.printReplicationInfo(); configured oplog size: 47.6837158203125MB log length start to end: 978secs (0.27hrs) oplog first event time: Wed Nov 28 2012 01:37:13 GMT+0800 (CST) oplog last event time: Wed Nov 28 2012 01:53:31 GMT+0800 (CST) now: Wed Nov 28 2012 20:58:52 GMT+0800 (CST) 备注：输出信息包括 oplog 日志大小，操作日志记录的起始时间。 查看从库同步状态rs0:PRIMARY&gt; db.printSlaveReplicationInfo(); source: redhatB.example.com:27019 syncedTo: Wed Nov 28 2012 01:53:31 GMT+0800 (CST) = 69673 secs ago (19.35hrs) source: redhatB.example.com:27020 syncedTo: Wed Nov 28 2012 01:53:31 GMT+0800 (CST) = 69673 secs ago (19.35hrs) 备注：输出信息包括从库的主机名，port 信息等。 参考http://docs.mongodb.org/manual/core/replication/#replica-set-oplog-sizinghttp://docs.mongodb.org/manual/reference/local-database/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：Replica Set 节点切换和 Failover","slug":"20121128154543","date":"2012-11-28T07:45:43.000Z","updated":"2018-09-04T01:34:00.807Z","comments":true,"path":"20121128154543.html","link":"","permalink":"https://postgres.fun/20121128154543.html","excerpt":"","text":"前面学习了 Replica Set 的搭建和从节点的添加，删除过程，接下来学习Replica Set 节点的切换以及 failover 相关的内容。 Replica Set 节点切换是指当出现故障或者出于维护需要，需要将主节点切换到另一台从节点，例如主节点主机需要硬件扩容时，那么需要停主节点主机， Replica Set 支持节点成员切换，其中使用的是投票竞选机制，对于投票竞选机制看了下文档，还不是非常明白，但主要是通过 priority 参数来控制的，接下来演示下。 节点切换1.1 基础环境 ( 三节点 )1234567891011121314151617181920212223242526272829303132333435363738394041424344rs0:PRIMARY&gt; rs.status(); &#123; \"set\" : \"rs0\", \"date\" : ISODate(\"2012-11-27T16:38:15Z\"), \"myState\" : 1, \"members\" : [ &#123; \"_id\" : 0, \"name\" : \"redhatB.example.com:27018\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 9143, \"optime\" : Timestamp(1354025540000, 1), \"optimeDate\" : ISODate(\"2012-11-27T14:12:20Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-27T16:38:15Z\"), \"pingMs\" : 0 &#125;, &#123; \"_id\" : 1, \"name\" : \"redhatB.example.com:27019\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 8733, \"optime\" : Timestamp(1354025540000, 1), \"optimeDate\" : ISODate(\"2012-11-27T14:12:20Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-27T16:38:15Z\"), \"pingMs\" : 0 &#125;, &#123; \"_id\" : 2, \"name\" : \"redhatB.example.com:27020\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 462929, \"optime\" : Timestamp(1354025540000, 1), \"optimeDate\" : ISODate(\"2012-11-27T14:12:20Z\"), \"self\" : true &#125; ], \"ok\" : 1 &#125; 备注：三节点 replica set 环境，其中最后一个节点为主节点，假如主节点由于某种原因需要切换，假设需要切换到从节点”redhatB.example.com:27018”，可以通过设置节点 priority 参数来实现，节点的 priority 值越大，切换时的优先级越高。 1.2 设置节点的 priority 值12345cfg = rs.conf() cfg.members[0].priority = 2 cfg.members[1].priority = 1 cfg.members[2].priority = 0.5 rs.reconfig(cfg) 1.3 操作日志12345678910111213rs0:PRIMARY&gt; cfg.members[0].priority = 2cfg.members[0].priority = 2 2 rs0:PRIMARY&gt; cfg.members[1].priority = 1cfg.members[1].priority = 1 1 rs0:PRIMARY&gt; cfg.members[2].priority = 0.5cfg.members[2].priority = 0.5 0.5 rs0:PRIMARY&gt; rs.reconfig(cfg) Wed Nov 28 00:57:30 DBClientCursor::init call() failed Wed Nov 28 00:57:30 query failed : admin.$cmd &#123; replSetReconfig: &#123; _id: \"rs0\", version: 9, members: [ &#123; _id: 0, host: \"redhatB.example.com:27018\", priority: 2.0 &#125;, &#123; _id: 1, host: \"redhatB.example.com:27019\", priority: 1.0 &#125;, &#123; _id: 2, host: \"redhatB.example.com:27020\", priority: 0.5 &#125; ] &#125; &#125; to: 127.0.0.1:27020 Wed Nov 28 00:57:30 trying reconnect to 127.0.0.1:27020 Wed Nov 28 00:57:30 reconnect 127.0.0.1:27020 ok reconnected to server after rs command (which is normal) 备注：设置好从节点的 priority 值后，调用一次 rs.reconfig 操作会导致当前主库中断，因为需要重新执行主节点竞选，大概几十秒后，新的主节点产生。 1.4 重新查看状态123456789101112131415161718192021rs0:SECONDARY&gt; rs.conf(); &#123; \"_id\" : \"rs0\", \"version\" : 9, \"members\" : [ &#123; \"_id\" : 0, \"host\" : \"redhatB.example.com:27018\", \"priority\" : 2 &#125;, &#123; \"_id\" : 1, \"host\" : \"redhatB.example.com:27019\" &#125;, &#123; \"_id\" : 2, \"host\" : \"redhatB.example.com:27020\", \"priority\" : 0.5 &#125; ] &#125; 备注：这时可以看到各节点优先级 priority 的值，节点默认的 priority 值为 1，不显示。 1.5 再次查看节点状态12345678910111213141516171819202122232425262728293031323334353637383940414243444546rs0:SECONDARY&gt; rs.status(); &#123; \"set\" : \"rs0\", \"date\" : ISODate(\"2012-11-27T16:58:27Z\"), \"myState\" : 2, \"syncingTo\" : \"redhatB.example.com:27018\", \"members\" : [ &#123; \"_id\" : 0, \"name\" : \"redhatB.example.com:27018\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 47, \"optime\" : Timestamp(1354035450000, 1), \"optimeDate\" : ISODate(\"2012-11-27T16:57:30Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-27T16:58:26Z\"), \"pingMs\" : 0 &#125;, &#123; \"_id\" : 1, \"name\" : \"redhatB.example.com:27019\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 47, \"optime\" : Timestamp(1354035450000, 1), \"optimeDate\" : ISODate(\"2012-11-27T16:57:30Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-27T16:58:26Z\"), \"pingMs\" : 0 &#125;, &#123; \"_id\" : 2, \"name\" : \"redhatB.example.com:27020\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 464141, \"optime\" : Timestamp(1354035450000, 1), \"optimeDate\" : ISODate(\"2012-11-27T16:57:30Z\"), \"errmsg\" : \"syncing to: redhatB.example.com:27018\", \"self\" : true &#125; ], \"ok\" : 1 &#125; 备注：此时主节点已漂移到节点 “redhatB.example.com:27018”，达到目标。 测试 Failover MongoDB 的 replica set 特性支持自动 failover，当主节点由于某种原因掉线时，replica set的其它节点可通过竞选产生新的主节点，接下来测试下这个特性。 2.1 基础信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546rs0:PRIMARY&gt; rs.status(); &#123; \"set\" : \"rs0\", \"date\" : ISODate(\"2012-11-27T17:50:53Z\"), \"myState\" : 1, \"members\" : [ &#123; \"_id\" : 0, \"name\" : \"redhatB.example.com:27018\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 200, \"optime\" : Timestamp(1354037833000, 1), \"optimeDate\" : ISODate(\"2012-11-27T17:37:13Z\"), \"self\" : true &#125;, &#123; \"_id\" : 1, \"name\" : \"redhatB.example.com:27019\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 108, \"optime\" : Timestamp(1354037833000, 1), \"optimeDate\" : ISODate(\"2012-11-27T17:37:13Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-27T17:50:52Z\"), \"pingMs\" : 0, \"errmsg\" : \"syncing to: redhatB.example.com:27018\" &#125;, &#123; \"_id\" : 2, \"name\" : \"redhatB.example.com:27020\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 35, \"optime\" : Timestamp(1354037833000, 1), \"optimeDate\" : ISODate(\"2012-11-27T17:37:13Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-27T17:50:52Z\"), \"pingMs\" : 0, \"errmsg\" : \"syncing to: redhatB.example.com:27018\" &#125; ], \"ok\" : 1 &#125; 备注：此时的主节点为 “redhatB.example.com:27018”。 2.2 异常关闭主节点。12345[mongo@redhatB data03]$ ps -ef | grep 27018 mongo 879 1 0 01:47 ? 00:00:02 mongod -f /pgdata_xc/mongodb/data01/mongodb_27018.conf mongo 1250 21805 0 01:50 pts/0 00:00:00 mongo 127.0.0.1:27018 mongo 1296 23173 0 01:51 pts/1 00:00:00 grep 27018 [mongo@redhatB data03]$ kill -9 879 备注：过一会后，节点 “redhatB.example.com:27019” 竞选成为新主节点。 2.3 查看节点状态123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[mongo@redhatB ~]$ mongo 127.0.0.1:27019 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27019/testrs0:PRIMARY&gt; rs.conf(); &#123; \"_id\" : \"rs0\", \"version\" : 9, \"members\" : [ &#123; \"_id\" : 0, \"host\" : \"redhatB.example.com:27018\", \"priority\" : 2 &#125;, &#123; \"_id\" : 1, \"host\" : \"redhatB.example.com:27019\" &#125;, &#123; \"_id\" : 2, \"host\" : \"redhatB.example.com:27020\", \"priority\" : 0.5 &#125; ] &#125; rs0:PRIMARY&gt; rs.status(); &#123; \"set\" : \"rs0\", \"date\" : ISODate(\"2012-11-27T17:52:41Z\"), \"myState\" : 1, \"members\" : [ &#123; \"_id\" : 0, \"name\" : \"redhatB.example.com:27018\", \"health\" : 0, \"state\" : 8, \"stateStr\" : \"(not reachable/healthy)\", \"uptime\" : 0, \"optime\" : Timestamp(1354037833000, 1), \"optimeDate\" : ISODate(\"2012-11-27T17:37:13Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-27T17:52:01Z\"), \"pingMs\" : 0, \"errmsg\" : \"socket exception [CONNECT_ERROR] for redhatB.example.com:27018\" &#125;, &#123; \"_id\" : 1, \"name\" : \"redhatB.example.com:27019\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 251, \"optime\" : Timestamp(1354037833000, 1), \"optimeDate\" : ISODate(\"2012-11-27T17:37:13Z\"), \"self\" : true &#125;, &#123; \"_id\" : 2, \"name\" : \"redhatB.example.com:27020\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 143, \"optime\" : Timestamp(1354037833000, 1), \"optimeDate\" : ISODate(\"2012-11-27T17:37:13Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-27T17:52:41Z\"), \"pingMs\" : 0, \"errmsg\" : \"syncing to: redhatB.example.com:27019\" &#125; ], \"ok\" : 1 &#125; 备注：可见节点 “redhatB.example.com:27019” 转变成主节点了，此时异常的节点”redhatB.example.com:27018” 不可用，接下来测试，启动异常节点是否能够自动恢复。 2.4 连接新主库，并新表一个集合；1234567891011121314[mongo@redhatB ~]$ mongo 127.0.0.1:27019 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27019/testrs0:PRIMARY&gt; show collections; system.indexes test_1 test_2 test_3 thingsrs0:PRIMARY&gt; db.test_4.save(&#123;id:1&#125;);rs0:PRIMARY&gt; db.test_4.find(); &#123; \"_id\" : ObjectId(\"50b4fe1b15747af472f54831\"), \"id\" : 1 &#125; 2.5 启动原来故障节点1234[mongo@redhatB data03]$ mongod -f /pgdata_xc/mongodb/data01/mongodb_27018.conf forked process: 1692 all output going to: /pgdata_xc/mongodb/data01/mongo.log child process started successfully, parent exiting 备注：三个节点都启用日志模式(journal)，否则恢复时会有异常。 2.6 再次查看节点状态1234567891011121314151617181920212223242526272829303132333435363738394041424344rs0:PRIMARY&gt; rs.status();rs.status(); &#123; \"set\" : \"rs0\", \"date\" : ISODate(\"2012-11-27T18:16:43Z\"), \"myState\" : 1, \"members\" : [ &#123; \"_id\" : 0, \"name\" : \"redhatB.example.com:27018\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 1321, \"optime\" : Timestamp(1354038811000, 1), \"optimeDate\" : ISODate(\"2012-11-27T17:53:31Z\"), \"self\" : true &#125;, &#123; \"_id\" : 1, \"name\" : \"redhatB.example.com:27019\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 1313, \"optime\" : Timestamp(1354038811000, 1), \"optimeDate\" : ISODate(\"2012-11-27T17:53:31Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-27T18:16:42Z\"), \"pingMs\" : 1 &#125;, &#123; \"_id\" : 2, \"name\" : \"redhatB.example.com:27020\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 1313, \"optime\" : Timestamp(1354038811000, 1), \"optimeDate\" : ISODate(\"2012-11-27T17:53:31Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-27T18:16:42Z\"), \"pingMs\" : 0 &#125; ], \"ok\" : 1 &#125; 备注：节点 “name” : “redhatB.example.com:27018” 再次竞选成功。 2.7 测试数据123456789101112[mongo@redhatB ~]$ mongo 127.0.0.1:27018 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27018/test rs0:PRIMARY&gt; show collections; system.indexes test_1 test_2 test_3 test_4 thingsrs0:PRIMARY&gt; db.test_4.find(); &#123; \"_id\" : ObjectId(\"50b4fe1b15747af472f54831\"), \"id\" : 1 &#125; 备注：恢复后的节点可以查到新数据了，说明已自动恢复。 总结以上只演示三节点 Replica set 的 failover 的情况，其它情况并没演示；另外 failover 的关键点为投票机制，这点还不是非常清楚，只知道设置节点的 priority ，还需要查阅相关资料。关于奇数节点和偶数节点 Primary 宕机后 SECONDARY 是否可以接管的问题，可以参考以下帖子：http://www.itpub.net/thread-1740982-1-1.html 四参考http://docs.mongodb.org/manual/administration/replica-sets/http://docs.mongodb.org/manual/core/replication/#replica-set-failover","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：Replica Set 删除节点","slug":"20121127162514","date":"2012-11-27T08:25:14.000Z","updated":"2018-09-04T01:34:00.729Z","comments":true,"path":"20121127162514.html","link":"","permalink":"https://postgres.fun/20121127162514.html","excerpt":"","text":"上一篇学习了 Replica Set 增加节点，继续学习删除节点。 删除节点前最好是先关闭需要删除的节点，之后通过命令 rs.remove 来删除相应的节点，操作如下： 1 查看当前 Replica Set 配置123456789101112131415161718192021222324252627[mongo@redhatB mongodb]$ mongo 127.0.0.1:27018 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27018/test rs0:PRIMARY&gt; rs.conf(); &#123; \"_id\" : \"rs0\", \"version\" : 4, \"members\" : [ &#123; \"_id\" : 0, \"host\" : \"redhatB.example.com:27018\" &#125;, &#123; \"_id\" : 1, \"host\" : \"redhatB.example.com:27019\" &#125;, &#123; \"_id\" : 2, \"host\" : \"redhatB.example.com:27020\" &#125;, &#123; \"_id\" : 3, \"host\" : \"redhatB.example.com:27021\" &#125; ] &#125; 备注：计划删除节点 “_id” : 3。 2 关闭 27021 节点服务12345[mongo@redhatB data04]$ ps -ef | grep 27021 mongo 11733 1 0 21:03 ? 00:00:15 mongod -f /pgdata_xc/mongodb/data04/mongodb_27021.conf mongo 14422 2953 0 21:53 pts/0 00:00:00 mongo 127.0.0.1:27021 mongo 14490 4027 0 21:54 pts/1 00:00:00 grep 27021 [mongo@redhatB data04]$ kill 11733 3 查看 Replica Set 状态123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657rs0:PRIMARY&gt; rs.status(); &#123; \"set\" : \"rs0\", \"date\" : ISODate(\"2012-11-22T13:57:15Z\"), \"myState\" : 1, \"members\" : [ &#123; \"_id\" : 0, \"name\" : \"redhatB.example.com:27018\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 21279, \"optime\" : Timestamp(1353589624000, 1), \"optimeDate\" : ISODate(\"2012-11-22T13:07:04Z\"), \"self\" : true &#125;, &#123; \"_id\" : 1, \"name\" : \"redhatB.example.com:27019\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 18908, \"optime\" : Timestamp(1353589624000, 1), \"optimeDate\" : ISODate(\"2012-11-22T13:07:04Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-22T13:57:13Z\"), \"pingMs\" : 0 &#125;, &#123; \"_id\" : 2, \"name\" : \"redhatB.example.com:27020\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 18900, \"optime\" : Timestamp(1353589624000, 1), \"optimeDate\" : ISODate(\"2012-11-22T13:07:04Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-22T13:57:14Z\"), \"pingMs\" : 0 &#125;, &#123; \"_id\" : 3, \"name\" : \"redhatB.example.com:27021\", \"health\" : 0, \"state\" : 8, \"stateStr\" : \"(not reachable/healthy)\", \"uptime\" : 0, \"optime\" : Timestamp(1353589624000, 1), \"optimeDate\" : ISODate(\"2012-11-22T13:07:04Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-22T13:55:20Z\"), \"pingMs\" : 0, \"errmsg\" : \"socket exception [CONNECT_ERROR] for redhatB.example.com:27021\" &#125; ], \"ok\" : 1 &#125; 备注：最后一个节点 stateStr 状态为 “not reachable/healthy”。 4 删除节点12345678rs0:PRIMARY&gt; rs.remove(\"redhatB.example.com:27021\"); Thu Nov 22 21:58:45 DBClientCursor::init call() failed Thu Nov 22 21:58:45 query failed : admin.$cmd &#123; replSetReconfig: &#123; _id: \"rs0\", version: 5, members: [ &#123; _id: 0, host: \"redhatB.example.com:27018\" &#125;, &#123; _id: 1, host: \"redhatB.example.com:27019\" &#125;, &#123; _id: 2, host: \"redhatB.example.com:27020\" &#125; ] &#125; &#125; to: 127.0.0.1:27018 Thu Nov 22 21:58:45 Error: error doing query: failed src/mongo/shell/collection.js:155 Thu Nov 22 21:58:45 trying reconnect to 127.0.0.1:27018 Thu Nov 22 21:58:45 reconnect 127.0.0.1:27018 ok rs0:SECONDARY&gt; rs0:PRIMARY&gt; 5 再次查看 Replica Set 配置12345678910111213141516171819rs0:PRIMARY&gt; rs.conf(); &#123; \"_id\" : \"rs0\", \"version\" : 5, \"members\" : [ &#123; \"_id\" : 0, \"host\" : \"redhatB.example.com:27018\" &#125;, &#123; \"_id\" : 1, \"host\" : \"redhatB.example.com:27019\" &#125;, &#123; \"_id\" : 2, \"host\" : \"redhatB.example.com:27020\" &#125; ] &#125; 备注：节点 “redhatB.example.com:27021” 已删除。 6 删除旧结点数据目录[mongo@redhatB mongodb]$ rm -rf /mongodb/data04 7 参考http://docs.mongodb.org/manual/administration/replica-sets/#replica-set-admin-procedure-add-memberhttp://docs.mongodb.org/manual/reference/javascript/#rs.remove","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：Replica Set 增加节点","slug":"20121127151527","date":"2012-11-27T07:15:27.000Z","updated":"2018-09-04T01:34:00.666Z","comments":true,"path":"20121127151527.html","link":"","permalink":"https://postgres.fun/20121127151527.html","excerpt":"","text":"前一篇 blog 介绍了三节点 Replica Set 环境的搭建，那么出于各种原因，可能需要增加 mongdb 节点，例如原来是单节点，为了具有高可用，需要增加节点，那么接下来学习增加节点的操作： 一 准备 已经有一套 Replica Set 环境： 具有适当容量的另一套 mongodb 系统，以满足数据存储需求，并且网络通； 二 现有环境信息2.1 数据情况12345678910111213141516171819202122232425262728293031323334[mongo@redhatB mongodb]$ mongo 127.0.0.1:27018 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27018/test rs0:PRIMARY&gt; show collections; system.indexes test_1 thingsrs0:PRIMARY&gt; show dbs; local 0.078125GB test 0.0625GBrs0:PRIMARY&gt; db.things.find(); &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0c6\"), \"id\" : 1, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0c7\"), \"id\" : 2, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0c8\"), \"id\" : 3, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0c9\"), \"id\" : 4, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0ca\"), \"id\" : 5, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0cb\"), \"id\" : 6, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0cc\"), \"id\" : 7, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0cd\"), \"id\" : 8, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0ce\"), \"id\" : 9, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0cf\"), \"id\" : 10, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0d0\"), \"id\" : 11, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0d1\"), \"id\" : 12, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0d2\"), \"id\" : 13, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0d3\"), \"id\" : 14, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0d4\"), \"id\" : 15, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0d5\"), \"id\" : 16, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0d6\"), \"id\" : 17, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0d7\"), \"id\" : 18, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0d8\"), \"id\" : 19, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50ae202524a46399c488c0d9\"), \"id\" : 20, \"name\" : \"aaa\" &#125; 2.2 Replica Set 节点信息12345678910111213141516171819rs0:PRIMARY&gt; rs.conf(); &#123; \"_id\" : \"rs0\", \"version\" : 3, \"members\" : [ &#123; \"_id\" : 0, \"host\" : \"redhatB.example.com:27018\" &#125;, &#123; \"_id\" : 1, \"host\" : \"redhatB.example.com:27019\" &#125;, &#123; \"_id\" : 2, \"host\" : \"redhatB.example.com:27020\" &#125; ] &#125; 备注：从上面看出，目前 Replica Set 共有 3 节点。 三 增加节点3.1 创建数据目录1mkdir -p /mongodb/data04 3.2 创建新从节点配置文件1234567touch /mongodb/data04/mongodb_27021.conf， 写入以下：fork = true port = 27021 dbpath = /mongodb/data04 logpath = /mongodb/data04/mongo.log logappend = true replSet = rs0 3.3 启动新从节点1234[mongo@redhatB mongodb]$ mongod -f /mongodb/data04/mongodb_27021.conf forked process: 11733 all output going to: /mongodb/data04/mongo.log child process started successfully, parent exiting 3.4 连接主节点1234[mongo@redhatB mongodb]$ mongo 127.0.0.1:27018 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27018/test rs0:PRIMARY&gt; 备注：根据“rs0:PRIMARY”标识，即可确认为主节点，也可通过以下命令确认是否是主节点。 3.5 判断当前库是否是主节点123456789101112131415rs0:PRIMARY&gt; rs.isMaster();&#123; \"setName\" : \"rs0\", \"ismaster\" : true, \"secondary\" : false, \"hosts\" : [ \"redhatB.example.com:27018\", \"redhatB.example.com:27020\", \"redhatB.example.com:27019\" ], \"primary\" : \"redhatB.example.com:27018\", \"me\" : \"redhatB.example.com:27018\", \"maxBsonObjectSize\" : 16777216, \"localTime\" : ISODate(\"2012-11-22T13:04:36.501Z\"), \"ok\" : 1 &#125; 3.6 增加新从节点到 Replica Set12rs0:PRIMARY&gt; rs.add(\"redhatB.example.com:27021\"); &#123; \"ok\" : 1 &#125; 3.7 再次查看 Replica Set 配置1234567891011121314151617181920212223rs0:PRIMARY&gt; rs.conf(); &#123; \"_id\" : \"rs0\", \"version\" : 4, \"members\" : [ &#123; \"_id\" : 0, \"host\" : \"redhatB.example.com:27018\" &#125;, &#123; \"_id\" : 1, \"host\" : \"redhatB.example.com:27019\" &#125;, &#123; \"_id\" : 2, \"host\" : \"redhatB.example.com:27020\" &#125;, &#123; \"_id\" : 3, \"host\" : \"redhatB.example.com:27021\" &#125; ] &#125; 备注：新节点已经加入 Replica Set 了，到了这步已完成增加节点所有步骤，接下来验证下新节点。 四 测试4.1 登陆新节点123456789[mongo@redhatB mongodb]$ mongo 127.0.0.1:27021 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27021/test rs0:SECONDARY&gt; show dbs; local 0.125GB test 0.0625GBrs0:SECONDARY&gt; show collections; Thu Nov 22 21:08:20 uncaught exception: error: &#123; \"$err\" : \"not master and slaveOk=false\", \"code\" : 13435 &#125; 4.2 开启从节点只读1rs0:SECONDARY&gt; rs.slars.slaveOk(); 4.3 查看是否有数据123456rs0:SECONDARY&gt; show collections; system.indexes test_1 thingsrs0:SECONDARY&gt; db.things.count(); 30 备注：增加节点后，新节点会自动从主节点复制数据。 四 参考http://docs.mongodb.org/manual/tutorial/expand-replica-set/#procedure-assumption-add-member-rshttp://docs.mongodb.org/manual/administration/replica-sets/#replica-set-admin-procedure-add-member","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：搭建三节点 Replica Set 环境","slug":"20121127134743","date":"2012-11-27T05:47:43.000Z","updated":"2018-09-04T01:34:00.604Z","comments":true,"path":"20121127134743.html","link":"","permalink":"https://postgres.fun/20121127134743.html","excerpt":"","text":"今天学习了搭建 MongDB 复制环境，实验环境是在虚拟机上同一系统，并搭建三节点 Replica Set，根据文档上的描述，mongodb 复制配置简单，并能够自动 failover，这些高级特性以后再实验，这里仅 描述配置过程： 一 环境信息虚拟机: Red Hat Enterprise Linux Server release 6.2主机名： redhatB.example.comMongoDB 版本：v2.2.1 节点 端口 数据目录 主结点 27018 /mongodb/data01 从节点一 27019 /mongodb/data02 从节点二 27020 /mongodb/data03 二 搭建步骤2.1 创建数据目录123[mongo@redhatB mongodb]$ mkdir -p /mongodb/data01 [mongo@redhatB mongodb]$ mkdir -p /mongodb/data02 [mongo@redhatB mongodb]$ mkdir -p /mongodb/data03 备注：三个目录分别为三个节点数据目录，并设置好目录权限。 2.2 创建配置文件2.2.1 主节点配置文件1234567touch /mongodb/data01/mongodb_27018.conf， 写入以下：fork = true port = 27018 dbpath = /mongodb/data01 logpath = /mongodb/data01/mongo.log logappend = true replSet = rs0 2.2.2 从节点一配置1234567touch /mongodb/data02/mongodb_27019.conf， 写入以下：fork = true port = 27019 dbpath = /mongodb/data02 logpath = /mongodb/data02/mongo.log logappend = true replSet = rs0 2.2.3 从节点二配置1234567touch /mongodb/data03/mongodb_27020.conf， 写入以下：fork = true port = 27020 dbpath = /mongodb/data03 logpath = /mongodb/data03/mongo.log logappend = true replSet = rs0 2.3 启动主从节点123mongod -f /mongodb/data01/mongodb_27018.conf mongod -f /mongodb/data02/mongodb_27019.conf mongod -f /mongodb/data03/mongodb_27020.conf 2.4 初始化 Replica Sets2.4.1 登陆结点一初始化123456789[mongo@redhatB mongodb]$ mongo 127.0.0.1:27018 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27018/test&gt; rs.inirs.initiate();&#123; \"info2\" : \"no configuration explicitly specified -- making one\", \"me\" : \"redhatB.example.com:27018\", \"info\" : \"Config now saved locally. Should come online in about a minute.\", \"ok\" : 1 &#125; 备注：使用的是默认配置。 2.5 查看 Replica Sets 配置1234567891011&gt; rs.conf(); &#123; \"_id\" : \"rs0\", \"version\" : 1, \"members\" : [ &#123; \"_id\" : 0, \"host\" : \"redhatB.example.com:27018\" &#125; ] &#125; 备注：只能看到一个节点信息； 2.6 增加从节点12345rs0:PRIMARY&gt; rs.add(\"redhatB.example.com:27019\"); &#123; \"ok\" : 1 &#125; rs0:PRIMARY&gt; rs.add(\"redhatB.example.com:27020\"); &#123; \"ok\" : 1 &#125; rs0:PRIMARY&gt; 2.7 再次查看 Replica Sets 配置123456789101112131415161718rs0:PRIMARY&gt; rs.conf();&#123; \"_id\" : \"rs0\", \"version\" : 3, \"members\" : [ &#123; \"_id\" : 0, \"host\" : \"redhatB.example.com:27018\" &#125;, &#123; \"_id\" : 1, \"host\" : \"redhatB.example.com:27019\" &#125;, &#123; \"_id\" : 2, \"host\" : \"redhatB.example.com:27020\" &#125; ] &#125; 备注：这时已经能看到三个节点信息。 2.8 查看 Replica Sets 状态1234567891011121314151617181920212223242526272829303132333435363738394041424344rs0:PRIMARY&gt; rs.status(); &#123; \"set\" : \"rs0\", \"date\" : ISODate(\"2012-11-22T08:43:21Z\"), \"myState\" : 1, \"members\" : [ &#123; \"_id\" : 0, \"name\" : \"redhatB.example.com:27018\", \"health\" : 1, \"state\" : 1, \"stateStr\" : \"PRIMARY\", \"uptime\" : 2445, \"optime\" : Timestamp(1353573735000, 1), \"optimeDate\" : ISODate(\"2012-11-22T08:42:15Z\"), \"self\" : true &#125;, &#123; \"_id\" : 1, \"name\" : \"redhatB.example.com:27019\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 74, \"optime\" : Timestamp(1353573735000, 1), \"optimeDate\" : ISODate(\"2012-11-22T08:42:15Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-22T08:43:21Z\"), \"pingMs\" : 0 &#125;, &#123; \"_id\" : 2, \"name\" : \"redhatB.example.com:27020\", \"health\" : 1, \"state\" : 2, \"stateStr\" : \"SECONDARY\", \"uptime\" : 66, \"optime\" : Timestamp(1353573735000, 1), \"optimeDate\" : ISODate(\"2012-11-22T08:42:15Z\"), \"lastHeartbeat\" : ISODate(\"2012-11-22T08:43:19Z\"), \"pingMs\" : 0 &#125; ], \"ok\" : 1 &#125; 备注：rs.status() 命令可以显示节点状态具体信息。 三 测试3.1 登陆主节点123456[mongo@redhatB mongodb]$ mongo 127.0.0.1:27018 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27018/testrs0:PRIMARY&gt; db.test_1.save(&#123;id:1&#125;);rs0:PRIMARY&gt; db.test_1.find(); &#123; \"_id\" : ObjectId(\"50ade66efabbde9e747577b1\"), \"id\" : 1 &#125; 备注：主节点可以读写； 3.2 登陆从节点一123456789[mongo@redhatB mongodb]$ mongo 127.0.0.1:27019 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27019/test rs0:SECONDARY&gt; show dbs;show dbs; local 0.125GB test 0.0625GB rs0:SECONDARY&gt; show collections; Thu Nov 22 16:48:15 uncaught exception: error: &#123; \"$err\" : \"not master and slaveOk=false\", \"code\" : 13435 &#125; rs0:SECONDARY&gt; 备注：从节点不可读写，但能看到主库创建的集合，从库需要使用 rs.slaveOk() 命令开启只读。 3.3 开启从库只读模式1234567891011[mongo@redhatB mongodb]$ mongo 127.0.0.1:27019 MongoDB shell version: 2.2.1 connecting to: 127.0.0.1:27019/testrs0:SECONDARY&gt; rs.slaveOk();rs0:SECONDARY&gt; show collections; system.indexes test_1rs0:SECONDARY&gt; db.test_1.find(); &#123; \"_id\" : ObjectId(\"50ade66efabbde9e747577b1\"), \"id\" : 1 &#125;rs0:SECONDARY&gt; db.test_1.save(&#123;id:2&#125;);db. not master 备注：这时从库可读了（不可写），同理设置从库二。 到了这里，三节点 Replica Sets 环境已经全部搭建好了，步骤比较简单。 四 参考http://docs.mongodb.org/manual/core/replication/http://docs.mongodb.org/manual/tutorial/deploy-replica-set/http://docs.mongodb.org/manual/reference/javascript/#rs.initiate","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：用户认证","slug":"20121123152533","date":"2012-11-23T07:25:33.000Z","updated":"2018-09-04T01:34:00.541Z","comments":true,"path":"20121123152533.html","link":"","permalink":"https://postgres.fun/20121123152533.html","excerpt":"","text":"MongoDB 安装后默认不启用认证，也就是说在本地可以通过 mongo 命令不输入用户名密码，直接登陆到数据库，下面介绍下启用 mongodb 用户认证，详细如下：启用 mongodb 认证只需要在启动 mongod 服务时配置 auth 参数成 ‘true’即可可 ，在配置参数前先添加超级用户。 一 启用认证1.1 增加管理用户1234567891011&gt; use admin; switched to db admin&gt; db.addUser('root','123456'); &#123; \"user\" : \"root\", \"readOnly\" : false, \"pwd\" : \"34e5772aa66b703a319641d42a47d696\", \"_id\" : ObjectId(\"50ad456a0b12589bdc45cf92\") &#125;&gt; db.system.users.find(); &#123; \"_id\" : ObjectId(\"50ad6ecda579c47efacf811b\"), \"user\" : \"root\", \"readOnly\" : false, \"pwd\" : \"34e5772aa66b703a319641d42a47d696\" &#125; 备注：在 admin 库中增加的用户为超级用户，权限最大，可以访问所有库。 1.2 增加普通用户12345678910111213&gt; use skytf; switched to db skytf&gt; db.addUser('skytf','skytf'); &#123; \"user\" : \"skytf\", \"readOnly\" : false, \"pwd\" : \"8c438fc9e2031577cea03806db0ee137\", \"_id\" : ObjectId(\"50ad45dd0b12589bdc45cf93\") &#125;&gt; db.system.users.find(); &#123; \"_id\" : ObjectId(\"50ad6ef3a579c47efacf811c\"), \"user\" : \"skytf\", \"readOnly\" : false, \"pwd\" : \"8c438fc9e2031577cea03806db0ee137\" &#125; 1.3 配置 auth 参数vim /database/mongodb/data/mongodb_27017.conf，增加 “ auth = true ”参数123456789fork = true bind_ip = 127.0.0.1 port = 27017 quiet = true dbpath = /database/mongodb/data/ logpath = /var/applog/mongo_log/mongo.log logappend = true journal = true auth = true 备注：增加 “auth = true” 配置。 1.4 重启 mongodb123456789101112mongo@redhatB data]$ ps -ef | grep mongo root 10887 10859 0 04:47 pts/0 00:00:00 su - mongo mongo 10889 10887 0 04:47 pts/0 00:00:00 -bash root 10984 10964 0 04:53 pts/1 00:00:00 su - mongo mongo 10986 10984 0 04:53 pts/1 00:00:00 -bash mongo 12749 1 0 07:54 ? 00:00:01 mongod -f /database/mongodb/data/mongodb_27017.conf mongo 13035 10986 13 08:21 pts/1 00:00:00 ps -ef mongo 13036 10986 0 08:21 pts/1 00:00:00 grep mongo[mongo@redhatB data]$ kill 12749[mongo@redhatB data]$ mongod -f /database/mongodb/data/mongodb_27017.conf forked process: 13042 all output going to: /var/applog/mongo_log/mongo.log 1.5 测试 skytf 帐号1234567891011121314151617181920212223242526272829303132333435[mongo@redhatB data]$ mongo 127.0.0.1/skytf -u skytf -p MongoDB shell version: 2.2.1 Enter password: connecting to: 127.0.0.1/skytf Error: &#123; errmsg: \"auth fails\", ok: 0.0 &#125; Thu Nov 22 08:23:11 uncaught exception: login failed exception: login failed[mongo@redhatB data]$ mongo 127.0.0.1/skytf -u skytf -p MongoDB shell version: 2.2.1 Enter password: connecting to: 127.0.0.1/skytf&gt; show collections; system.indexes system.users test_1 test_2 test_3 test_4 things things_1&gt; db.test_5.find(); &#123; \"_id\" : ObjectId(\"50ad7177d114dcf18a8bb220\"), \"id\" : 1 &#125;&gt; show dbs; Thu Nov 22 08:24:03 uncaught exception: listDatabases failed:&#123; \"errmsg\" : \"need to login\", \"ok\" : 0 &#125;&gt; use test; switched to db test&gt; show collections; Thu Nov 22 09:01:32 uncaught exception: error: &#123; \"$err\" : \"unauthorized db:test ns:test.system.namespaces lock type:0 client:127.0.0.1\", \"code\" : 10057 备注：从上看出， skytf 用户的认证已生效，并且能查看数据库 skytf 里的集合，但不能执行 “show dbs” 命令；并且能连接数据库 test ，但没有权限执行“show collections” 命令。 二 切换用户2.1 在普通库中切换成 root 用户12345&gt; use test; switched to db test&gt; db.auth('root','123456');db.auth('root','123456'); Error: &#123; errmsg: \"auth fails\", ok: 0.0 &#125; 0 备注：在普通库中切换成超级用户失败，超级用户需要在 admin 库中切换才能生效。 2.2 在 admin 库中切换成 root 用户12345&gt; use admin; switched to db admin &gt; db.auth('root','123456'); 1 备注：在 admin 库中切换成超级用户成功。 三 新增只读帐号3.1 增加只读帐号1234567891011&gt; db.addUser('skytf_select','skytf_select',true); &#123; \"user\" : \"skytf_select\", \"readOnly\" : true, \"pwd\" : \"e344f93a69f20ca9f3dfbc40da4a3082\", \"_id\" : ObjectId(\"50ad71c7d114dcf18a8bb221\") &#125;&gt; db.system.users.find();db.system.users.find(); &#123; \"_id\" : ObjectId(\"50ad6ef3a579c47efacf811c\"), \"user\" : \"skytf\", \"readOnly\" : false, \"pwd\" : \"8c438fc9e2031577cea03806db0ee137\" &#125; &#123; \"_id\" : ObjectId(\"50ad71c7d114dcf18a8bb221\"), \"user\" : \"skytf_select\", \"readOnly\" : true, \"pwd\" : \"e344f93a69f20ca9f3dfbc40da4a3082\" &#125; 备注：只需在 addUser命令中增加第三个参数，并指定为“true” ，即可创建只读帐号。 3.2 测试123456789101112131415161718192021[mongo@redhatB data]$ mongo 127.0.0.1/skytf -u skytf_select -p MongoDB shell version: 2.2.1 Enter password: connecting to: 127.0.0.1/skytf&gt; show collections; system.indexes system.users test_1 test_2 test_3 test_4 test_5 things things_1&gt; db.test_5.find(); &#123; \"_id\" : ObjectId(\"50ad7177d114dcf18a8bb220\"), \"id\" : 1 &#125; &#123; \"_id\" : ObjectId(\"50ad724ed114dcf18a8bb222\"), \"id\" : 2 &#125;&gt; db.test_5.save(&#123;id:3&#125;); unauthorized 备注：以只读帐号 skytf_select 登陆库 skytf，有权限执行查询操作，没有权限执行插入操作； 四 附 命令参考http://docs.mongodb.org/manual/tutorial/control-access-to-mongodb-with-authentication/http://docs.mongodb.org/manual/administration/security/http://blog.163.com/dazuiba_008/blog/static/36334981201110311534143/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"PostgreSQL: 查询其它模式表时不引用模式的方法","slug":"20121120144417","date":"2012-11-20T06:44:17.000Z","updated":"2018-09-04T01:34:00.479Z","comments":true,"path":"20121120144417.html","link":"","permalink":"https://postgres.fun/20121120144417.html","excerpt":"","text":"在数据库维护过程中，生产帐号需要妥善管理，平常开发人员有时需要查看线上数据，一般也只开通查询帐号，这时，如果用查询帐号去访问生产表（其它模式），需要引用模式名，当然，开发人员不愿意这么做，想直接引用表，在 PG 中，虽然有办法满足这个需求，但我个人不建议这么做，因为这不是一个好习惯，偶尔还会带来维护上的麻烦，下面具体演示下： 创建帐号并赋权1.1 环境信息PostgreSQL: 9.2数据库名： skytf生产帐号： skytf备注：假如 skytf 为生产库，现在开发人员需要申请 skytf 库的查询权限，给他创建 1.2 创建查询帐号一个查询帐号，如下：12postgres=# create role skytf_select login nocreatedb nocreaterole noinherit encrypted password 'skytf_select' CONNECTION limit 10; CREATE ROLE 1.3 赋权12345678postgres=# \\c skytf skytf You are now connected to database \"skytf\" as user \"skytf\".skytf=&gt; grant connect on database skytf to skytf_select; GRANTskytf=&gt; grant usage on schema skytf to skytf_select; GRANTskytf=&gt; grant select on all tables in schema skytf to skytf_select; GRANT 备注：给查询帐号 skytf_select 赋权，注意以上给的权限。 1.4 测试123456789101112skytf=&gt; \\c skytf skytf_select; You are now connected to database \"skytf\" as user \"skytf_select\".skytf=&gt; select * From products limit 1; ERROR: relation \"products\" does not exist LINE 1: select * From products limit 1; ^ skytf=&gt; select * From skytf.products limit 1; id | json ----+-------------------------------------------------------------- 1 | &#123;\"type\":\"television\", \"price\": 899.99, \"resolution\":\"1080p\"&#125; (1 row) 备注：用 skytf_select 帐号登陆 skytf 库，当引用表不加模式时，报表找不到的错误；在表名前加上模式名时可以找到，接着介绍下不加模式也能引用其它模式表的方法。 更改用户 Search_path2.1 查看当前 search_path123456skytf=&gt; \\c skytf skytf_select; You are now connected to database \"skytf\" as user \"skytf_select\".skytf=&gt; show search_path; search_path ---------------- \"$user\",public (1 row) 备注：当前的搜索模式有两个，首先是 $user，即和用户名同名的模式；第二个为 public 模式。 2.2 更改 skytf_select 用户 search_path1234skytf=&gt; \\c skytf postgres; You are now connected to database \"skytf\" as user \"postgres\".skytf=# alter role skytf_select set search_path to skytf,public; ALTER ROLE 备注：注意上面的命令，没设置好就达不到实验效果。 2.3 再次查看当前 search_path1234567skytf=# \\c skytf skytf_select; You are now connected to database \"skytf\" as user \"skytf_select\".skytf=&gt; show search_path; search_path --------------- skytf, public (1 row) 备注：search_path 已经包括 skytf 模式了，下面看看不加模式名是否可以找到表。 2.4 测试1234567891011skytf=&gt; select * from products limit 1; id | json ----+-------------------------------------------------------------- 1 | &#123;\"type\":\"television\", \"price\": 899.99, \"resolution\":\"1080p\"&#125; (1 row)skytf=&gt; select * from skytf.products limit 1; id | json ----+-------------------------------------------------------------- 1 | &#123;\"type\":\"television\", \"price\": 899.99, \"resolution\":\"1080p\"&#125; (1 row) 备注：引用表名时不加模式也可以正常查询了。 总结尽管以上实现了引用其它模式表时不加模式的功能，但依然不推荐这么做，因为假如一个库有两个模式 schema_1， schmea_2，并且两个模式下都有表 test_1，那么这时引用表名加模式是明智的，否则很容易造成混淆。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"MongoDB：执行计划","slug":"20121120143613","date":"2012-11-20T06:36:13.000Z","updated":"2018-09-04T01:34:00.416Z","comments":true,"path":"20121120143613.html","link":"","permalink":"https://postgres.fun/20121120143613.html","excerpt":"","text":"关于索引的创建在之前的 创建索引blog 中已有描述接下来看看 MongoDB 的执行计划，通过使用 explain() 方法可以很容易的查看执行计划。 1 查看索引1234567891011121314151617181920&gt; db.test_4.getIndexes(); [ &#123; \"v\" : 1, \"key\" : &#123; \"_id\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"_id_\" &#125;, &#123; \"v\" : 1, \"key\" : &#123; \"skyid\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"idx_test_4_skyid\", \"background\" : true &#125; ] 2 查看执行计划1234567891011121314151617181920212223&gt; db.test_4.find(&#123;skyid:1&#125;).explain();&#123; \"cursor\" : \"BtreeCursor idx_test_4_skyid\", \"isMultiKey\" : false, \"n\" : 1, \"nscannedObjects\" : 1, \"nscanned\" : 1, \"nscannedObjectsAllPlans\" : 1, \"nscannedAllPlans\" : 1, \"scanAndOrder\" : false, \"indexOnly\" : false, \"nYields\" : 0, \"nChunkSkips\" : 0, \"millis\" : 9, \"indexBounds\" : &#123; \"skyid\" : [ [ 1, 1 ] ] &#125;, \"server\" : \"redhatB.example.com:27017\" &#125; 备注： 上面为 mongodb 的执行计划，部分参数意思为以下： cursor： 值为 BasicCursor 或 BtreeCursor，后者表示此查询使用了索引 nscanned： 扫描的索引项 n: 返回的文档数，即返回的行 millis： 完成此查询所需的时间，单位为毫秒 indexBounds: 如果不为空，表示此查询使用的索引项 MongoDB 也能强制使用索引，通过使用 hint() ，感觉非常强大！ 3 删除老索引123456&gt; db.test_4.dropIndexes('idx_test_4_skyid'); &#123; \"nIndexesWas\" : 2, \"msg\" : \"non-_id indexes dropped for collection\", \"ok\" : 1 &#125; 4 新建索引1&gt; db.test_4.ensureIndex(&#123;skyid:1,name:1&#125;,&#123;name:\"idx_test_4_skyid_name\",background:true&#125;); 5 查看索引123456789101112131415161718192021&gt; db.test_4.getIndexes(); [ &#123; \"v\" : 1, \"key\" : &#123; \"_id\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"_id_\" &#125;, &#123; \"v\" : 1, \"key\" : &#123; \"skyid\" : 1, \"name\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"idx_test_4_skyid_name\", \"background\" : true &#125; ] 备注：索引 “idx_test_4_skyid_name” 已创建。 6 没使用索引的查询123456789101112131415161718&gt; db.test_4.find(&#123;name:'a'&#125;).explain(); &#123; \"cursor\" : \"BasicCursor\", \"isMultiKey\" : false, \"n\" : 100001, \"nscannedObjects\" : 100001, \"nscanned\" : 100001, \"nscannedObjectsAllPlans\" : 100001, \"nscannedAllPlans\" : 100001, \"scanAndOrder\" : false, \"indexOnly\" : false, \"nYields\" : 1, \"nChunkSkips\" : 0, \"millis\" : 156, \"indexBounds\" : &#123; &#125;, \"server\" : \"redhatB.example.com:27017\" &#125; 备注： indexBounds 为空表示没使用索引。 7 强制查询使用索引12345678910111213141516171819202122232425262728293031323334&gt; db.test_4.find(&#123;name:'a'&#125;).hint(&#123;skyid:1,name:1&#125;).explain(); &#123; \"cursor\" : \"BtreeCursor idx_test_4_skyid_name\", \"isMultiKey\" : false, \"n\" : 100001, \"nscannedObjects\" : 100001, \"nscanned\" : 100001, \"nscannedObjectsAllPlans\" : 100001, \"nscannedAllPlans\" : 100001, \"scanAndOrder\" : false, \"indexOnly\" : false, \"nYields\" : 0, \"nChunkSkips\" : 0, \"millis\" : 263, \"indexBounds\" : &#123; \"skyid\" : [ [ &#123; \"$minElement\" : 1 &#125;, &#123; \"$maxElement\" : 1 &#125; ] ], \"name\" : [ [ \"a\", \"a\" ] ] &#125;, \"server\" : \"redhatB.example.com:27017\" &#125; 备注：indexBounds 不为空表示已使用索引，并且 cursor 字段显示了使用的索引名称。 8 参考http://docs.mongodb.org/manual/reference/javascript/#cursor.explainhttp://docs.mongodb.org/manual/reference/javascript/#cursor.hint","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：创建索引","slug":"20121119215911","date":"2012-11-19T13:59:11.000Z","updated":"2018-09-04T01:34:00.354Z","comments":true,"path":"20121119215911.html","link":"","permalink":"https://postgres.fun/20121119215911.html","excerpt":"","text":"MongDB 支持丰富的查询，同时支持灵活的索引创建操作，例如支持升序/降序索引，组合索引，Multikey Index 等等，今天仅演示索引的相关操作，如创建，查看等。 MongDB 创建索引的命令为 ensureIndex，这个命令有两个参数，如下所示： keys: 包括索引的列以及索引的顺序（升序/降序）； optins: 创建索引的选项，可选参数，例如 background，unique，name 等。 以下是具体的演示过程： 1 创建测试表12345678910[mongo@redhatB ~]$ mongo MongoDB shell version: 2.2.1 connecting to: test&gt; for (var i=1; i&lt;=100001; i++ ) db.test_4.save(&#123;skyid:i,name:'a'&#125;);&gt; db.test_4.find().limit (5); &#123; \"_id\" : ObjectId(\"50aa31344e8ce0d3738f8afa\"), \"skyid\" : 1, \"name\" : \"a\" &#125; &#123; \"_id\" : ObjectId(\"50aa31344e8ce0d3738f8afb\"), \"skyid\" : 2, \"name\" : \"a\" &#125; &#123; \"_id\" : ObjectId(\"50aa31344e8ce0d3738f8afc\"), \"skyid\" : 3, \"name\" : \"a\" &#125; &#123; \"_id\" : ObjectId(\"50aa31344e8ce0d3738f8afd\"), \"skyid\" : 4, \"name\" : \"a\" &#125; &#123; \"_id\" : ObjectId(\"50aa31344e8ce0d3738f8afe\"), \"skyid\" : 5, \"name\" : \"a\" &#125; 备注：创建测试表并插入 10000 条数据。并插入 100001 条数据。 2 查看现有索引1234567891011&gt; db.test_4.getIndexes(); [ &#123; \"v\" : 1, \"key\" : &#123; \"_id\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"_id_\" &#125; ] 备注： 返回了一条索引项，包括四个元素，解释如下： v: 索引的版本号，这由 mongod 的版本决定，2.0 版本之前的 mongod 这个值为 0， 2.0 或之后的 mongod 的版本这个值为 1。 key: 索引项名称以及索引的排序规则（升序/降序） ns: 索引所在的集合的名称 name: 索引名称 3 创建索引1&gt; db.test_4.ensureIndex(&#123;skyid:1&#125;,&#123;name:\"idx_test_4_skyid\"&#125;); 备注：上面指定了 “name” 可选参数，即指定索引名称。 4 再次查看索引123456789101112131415161718&gt; db.test_4.getIndexes(); [ &#123; \"v\" : 1, \"key\" : &#123; \"_id\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"_id_\" &#125;, &#123; \"v\" : 1, \"key\" : &#123; \"skyid\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"idx_test_4_skyid\" &#125; ] 备注：返回的结果为两条索引项。 5 查询数据库里所有索引123&gt; db.system.indexes.find(&#123;\"ns\":\"skytf.test_4\"&#125;); &#123; \"v\" : 1, \"key\" : &#123; \"_id\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"_id_\" &#125; &#123; \"v\" : 1, \"key\" : &#123; \"skyid\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"idx_test_4_skyid\" &#125; 备注： system.indexes 存储了当前数据库所有的索引信息。 由于创建索引过程会阻塞其它数据库操作，建议以后台方式创建索引，即增加 “background” 可选参数。 6 删除索引12&gt; db.test_4.dropIndex('idx_test_4_skyid'); &#123; \"nIndexesWas\" : 2, \"ok\" : 1 &#125; 备注：删除表 test_4 的 idx_test_4_skyid 索引。 7 以后台方式创建索引1&gt; db.test_4.ensureIndex(&#123;skyid:1&#125;,&#123;name:\"idx_test_4_skyid\",background:true&#125;); 8 再次查看索引1234567891011121314151617181920&gt; db.test_4.getIndexes();db.test_4.getIndexes(); [ &#123; \"v\" : 1, \"key\" : &#123; \"_id\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"_id_\" &#125;, &#123; \"v\" : 1, \"key\" : &#123; \"skyid\" : 1 &#125;, \"ns\" : \"skytf.test_4\", \"name\" : \"idx_test_4_skyid\", \"background\" : true &#125; ] 9 总结 索引维护操作还有很多，以及索引创建过程中对其它操作的影响还没深入了解，今天先学习到这。 10 参考http://docs.mongodb.org/manual/reference/method/db.collection.getIndexes/#db.collection.getIndexeshttp://docs.mongodb.org/manual/reference/method/db.collection.ensureIndex/#db.collection.ensureIndexhttp://docs.mongodb.org/manual/administration/indexes/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：恢复数据( Mongorestore )","slug":"20121119151009","date":"2012-11-19T07:10:09.000Z","updated":"2018-09-04T01:34:00.291Z","comments":true,"path":"20121119151009.html","link":"","permalink":"https://postgres.fun/20121119151009.html","excerpt":"","text":"上篇学习了使用 mongodump 以二进制备份数据库，接着学习使用对应的 mongorestore 恢复数据库，以下是实验过程： 1 备份数据库12345678910111213141516171819[mongo@redhatB tf]$ mongodump -h 127.0.0.1 -d skytf -o skytf.dmp connected to: 127.0.0.1 Sat Nov 17 17:37:33 DATABASE: skytf to skytf.dmp/skytf Sat Nov 17 17:37:33 skytf.test_1 to skytf.dmp/skytf/test_1.bson Sat Nov 17 17:37:33 doing snapshot query Sat Nov 17 17:37:33 1 objects Sat Nov 17 17:37:33 Metadata for skytf.test_1 to skytf.dmp/skytf/test_1.metadata.json Sat Nov 17 17:37:33 skytf.things to skytf.dmp/skytf/things.bson Sat Nov 17 17:37:33 doing snapshot query Sat Nov 17 17:37:33 30 objects Sat Nov 17 17:37:33 Metadata for skytf.things to skytf.dmp/skytf/things.metadata.json Sat Nov 17 17:37:33 skytf.things_1 to skytf.dmp/skytf/things_1.bson Sat Nov 17 17:37:33 doing snapshot query Sat Nov 17 17:37:33 30 objects Sat Nov 17 17:37:33 Metadata for skytf.things_1 to skytf.dmp/skytf/things_1.metadata.json Sat Nov 17 17:37:33 skytf.test_2 to skytf.dmp/skytf/test_2.bson Sat Nov 17 17:37:33 doing snapshot query Sat Nov 17 17:37:33 4 objects Sat Nov 17 17:37:33 Metadata for skytf.test_2 to skytf.dmp/skytf/test_2.metadata.json 备注：上述命令备份数据库 skytf，产生的备份文件为目录 skytf.dmp/skytf。 2 删除数据库12345678910111213141516171819202122232425[mongo@redhatB ~]$ mongo MongoDB shell version: 2.2.1 connecting to: test &gt; show dbs; local (empty) skytf 0.0625GB test 0.0625GB&gt; use skytf; switched to db skytf&gt; show collections; system.indexes test_1 test_2 things things_1&gt; db.dropDatabase(); &#123; \"dropped\" : \"skytf\", \"ok\" : 1 &#125;&gt; show collections;&gt; show dbs; local (empty) skytf (empty) test 0.0625GB 备注：为了测试需要，先删除数据库 skytf。 3 还原数据库1234567891011121314151617[mongo@redhatB tf]$ mongorestore -h 127.0.0.1 -d skytf skytf.dmp/skytfconnected to: 127.0.0.1 Sat Nov 17 17:42:03 skytf.dmp/skytf/test_1.bson Sat Nov 17 17:42:03 going into namespace [skytf.test_1] 1 objects found Sat Nov 17 17:42:03 Creating index: &#123; key: &#123; _id: 1 &#125;, ns: \"skytf.test_1\", name: \"_id_\" &#125; Sat Nov 17 17:42:04 skytf.dmp/skytf/test_2.bson Sat Nov 17 17:42:04 going into namespace [skytf.test_2] 4 objects found Sat Nov 17 17:42:04 Creating index: &#123; key: &#123; _id: 1 &#125;, ns: \"skytf.test_2\", name: \"_id_\" &#125; Sat Nov 17 17:42:04 skytf.dmp/skytf/things_1.bson Sat Nov 17 17:42:04 going into namespace [skytf.things_1] 30 objects found Sat Nov 17 17:42:04 Creating index: &#123; key: &#123; _id: 1 &#125;, ns: \"skytf.things_1\", name: \"_id_\" &#125; Sat Nov 17 17:42:04 skytf.dmp/skytf/things.bson Sat Nov 17 17:42:04 going into namespace [skytf.things] 30 objects found Sat Nov 17 17:42:04 Creating index: &#123; key: &#123; _id: 1 &#125;, ns: \"skytf.things\", name: \"_id_\" &#125; 备注：使用 mongorestore 恢复数据库，从上面看到数据库 skytf 已恢复的细节。 4 验证123456789101112131415161718[mongo@redhatB ~]$ mongo MongoDB shell version: 2.2.1 connecting to: test&gt; show dbs; local (empty) skytf 0.0625GB test 0.0625GB&gt; use skytf; switched to db skytf&gt; show collections; system.indexes test_1 test_2 things things_1 备注：从上看出，数据库 skytf 已恢复；还原指定数据库，数据文件需要指定到备份文件下面的子目录，如果不指定子目录，会报以下错误。 5 ERROR1234[mongo@redhatB tf]$ mongorestore -h 127.0.0.1 -d skytf skytf.dmp/ connected to: 127.0.0.1 Sat Nov 17 17:41:15 ERROR: ERROR: root directory must be a dump of a single database Sat Nov 17 17:41:15 ERROR: when specifying a db name with --db 6 在恢复前先删除数据库中的对像12345678910111213141516171819202122[mongo@redhatB tf]$ mongorestore -h 127.0.0.1 -d skytf --drop skytf.dmp/skytf connected to: 127.0.0.1 Sat Nov 17 17:45:37 skytf.dmp/skytf/test_1.bson Sat Nov 17 17:45:37 going into namespace [skytf.test_1] Sat Nov 17 17:45:37 dropping 1 objects found Sat Nov 17 17:45:37 Creating index: &#123; key: &#123; _id: 1 &#125;, ns: \"skytf.test_1\", name: \"_id_\" &#125; Sat Nov 17 17:45:37 skytf.dmp/skytf/test_2.bson Sat Nov 17 17:45:37 going into namespace [skytf.test_2] Sat Nov 17 17:45:37 dropping 4 objects found Sat Nov 17 17:45:37 Creating index: &#123; key: &#123; _id: 1 &#125;, ns: \"skytf.test_2\", name: \"_id_\" &#125; Sat Nov 17 17:45:37 skytf.dmp/skytf/things_1.bson Sat Nov 17 17:45:37 going into namespace [skytf.things_1] Sat Nov 17 17:45:37 dropping 30 objects found Sat Nov 17 17:45:37 Creating index: &#123; key: &#123; _id: 1 &#125;, ns: \"skytf.things_1\", name: \"_id_\" &#125; Sat Nov 17 17:45:37 skytf.dmp/skytf/things.bson Sat Nov 17 17:45:37 going into namespace [skytf.things] Sat Nov 17 17:45:37 dropping 30 objects found Sat Nov 17 17:45:37 Creating index: &#123; key: &#123; _id: 1 &#125;, ns: \"skytf.things\", name: \"_id_\" &#125; 备注：使用 –drop 参数可以在恢复前先删除已存在的对像。 7 参考http://docs.mongodb.org/manual/reference/mongorestore/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：备份数据( Mongodump )","slug":"20121119143734","date":"2012-11-19T06:37:34.000Z","updated":"2018-09-04T01:34:00.229Z","comments":true,"path":"20121119143734.html","link":"","permalink":"https://postgres.fun/20121119143734.html","excerpt":"","text":"和之前介绍的 mongoexport 的数据导出工具不同， mongodump 是将数据以二进制形式导出，而 mongoexport 导出的数据格式为 csv 或 json 格式； mongodump 可以导出一个数据库，或者整个 MongoDB 服务上的所有数据库，因此 mongodump 是更大范围的备份工具。 一 备份一个数据库123456789101112131415161718[mongo@redhatB tf]$ mongodump -h 127.0.0.1 -d skytf -o skytf.dmpconnected to: 127.0.0.1 Sat Nov 17 17:11:19 DATABASE: skytf to skytf.dmp/skytf Sat Nov 17 17:11:19 skytf.test_1 to skytf.dmp/skytf/test_1.bson Sat Nov 17 17:11:19 doing snapshot query Sat Nov 17 17:11:19 1 objects Sat Nov 17 17:11:19 Metadata for skytf.test_1 to skytf.dmp/skytf/test_1.metadata.json Sat Nov 17 17:11:19 skytf.things to skytf.dmp/skytf/things.bson Sat Nov 17 17:11:19 doing snapshot query Sat Nov 17 17:11:19 30 objects Sat Nov 17 17:11:19 Metadata for skytf.things to skytf.dmp/skytf/things.metadata.json Sat Nov 17 17:11:19 skytf.things_1 to skytf.dmp/skytf/things_1.bson Sat Nov 17 17:11:19 doing snapshot query Sat Nov 17 17:11:19 30 objects Sat Nov 17 17:11:19 Metadata for skytf.things_1 to skytf.dmp/skytf/things_1.metadata.json Sat Nov 17 17:11:19 skytf.test_2 to skytf.dmp/skytf/test_2.bson Sat Nov 17 17:11:19 doing snapshot query Sat Nov 17 17:11:19 4 objects Sat Nov 17 17:11:19 Metadata for skytf.test_2 to skytf.dmp/skytf/test_2.metadata.json 备注：从日志看出，mongodump 的结果会生成一个目录，第一层目录为数据库名，再下一层为每个集合对应的备份文件。 二 备份一个集合1234567[mongo@redhatB tf]$ mongodump -h 127.0.0.1 -d skytf -c things -o things.dmp connected to: 127.0.0.1 Sat Nov 17 17:12:20 DATABASE: skytf to things.dmp/skytf Sat Nov 17 17:12:20 skytf.things to things.dmp/skytf/things.bson Sat Nov 17 17:12:20 doing snapshot query Sat Nov 17 17:12:20 30 objects Sat Nov 17 17:12:20 Metadata for skytf.things to things.dmp/skytf/things.metadata.json 备注： mongodump 加上 -c 参数则可备份指定集合。 三 备份整个实例1234567891011121314151617181920212223242526272829[mongo@redhatB tf]$ mongodump -h 127.0.0.1 -o all.dmp connected to: 127.0.0.1 Sat Nov 17 17:17:04 all dbs Sat Nov 17 17:17:04 DATABASE: skytf to all.dmp/skytf Sat Nov 17 17:17:04 skytf.test_1 to all.dmp/skytf/test_1.bson Sat Nov 17 17:17:04 doing snapshot query Sat Nov 17 17:17:04 1 objects Sat Nov 17 17:17:04 Metadata for skytf.test_1 to all.dmp/skytf/test_1.metadata.json Sat Nov 17 17:17:04 skytf.things to all.dmp/skytf/things.bson Sat Nov 17 17:17:04 doing snapshot query Sat Nov 17 17:17:04 30 objects Sat Nov 17 17:17:04 Metadata for skytf.things to all.dmp/skytf/things.metadata.json Sat Nov 17 17:17:04 skytf.things_1 to all.dmp/skytf/things_1.bson Sat Nov 17 17:17:04 doing snapshot query Sat Nov 17 17:17:04 30 objects Sat Nov 17 17:17:04 Metadata for skytf.things_1 to all.dmp/skytf/things_1.metadata.json Sat Nov 17 17:17:04 skytf.test_2 to all.dmp/skytf/test_2.bson Sat Nov 17 17:17:04 doing snapshot query Sat Nov 17 17:17:04 4 objects Sat Nov 17 17:17:04 Metadata for skytf.test_2 to all.dmp/skytf/test_2.metadata.json Sat Nov 17 17:17:04 DATABASE: test to all.dmp/test Sat Nov 17 17:17:04 test.test_2 to all.dmp/test/test_2.bson Sat Nov 17 17:17:04 doing snapshot query Sat Nov 17 17:17:04 4 objects Sat Nov 17 17:17:04 Metadata for test.test_2 to all.dmp/test/test_2.metadata.json Sat Nov 17 17:17:04 test.things to all.dmp/test/things.bson Sat Nov 17 17:17:04 doing snapshot query Sat Nov 17 17:17:04 30 objects Sat Nov 17 17:17:04 Metadata for test.things to all.dmp/test/things.metadata.json 备注：mongodump 如果不指定 -d 参数，刚会备份整个 MongoDB 实例。 3.1 查看目录 [mongo@redhatB tf]$ ll all.dmp/ total 8.0K drwxrwxr-x. 2 mongo mongo 4.0K Nov 17 17:17 skytf drwxrwxr-x. 2 mongo mongo 4.0K Nov 17 17:17 test 备注：在目录 all.dmp 下产生了两个目录，目录名分别对应 MongoDB 上实例上的数据库名。 四 参考http://docs.mongodb.org/manual/reference/mongodump/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：导入集合( Mongoimport )","slug":"20121117110112","date":"2012-11-17T03:01:12.000Z","updated":"2018-09-04T01:34:00.166Z","comments":true,"path":"20121117110112.html","link":"","permalink":"https://postgres.fun/20121117110112.html","excerpt":"","text":"上篇BLOG 介绍了 MongoDB 集合的导出，今天接着学习集合导入，下面演示了从一个库导出集合到另一个库的具体步骤，如下： 基础信息123456789101112131415161718[mongo@redhatB ~]$ mongo MongoDB shell version: 2.2.1 connecting to: test &gt; show dbs; local (empty) skytf 0.0625GB test 0.0625GB&gt; show collections; system.indexes test_2 things&gt; use skytf; switched to db skytf&gt; show collections; system.indexes test_1 备注：我们的目标是将 test 库的集合 things 导入到库 skytf 中。 以 JSON 格式导入导出1.1 从源库导出集合123456[mongo@redhatB tf]$ mongoexport -h 127.0.0.1 -d test -c things -v -o things.json Sat Nov 17 10:21:27 creating new connection to:127.0.0.1:27017 Sat Nov 17 10:21:27 BackgroundJob starting: ConnectBG Sat Nov 17 10:21:27 connected connection! connected to: 127.0.0.1 exported 30 records 备注：导出库 test_2 的集合 things，从上面看出导出了 30 条记录。 1.2 导入集合到目标库123[mongo@redhatB tf]$ mongoimport -h 127.0.0.1 -d skytf -c things --file things.json connected to: 127.0.0.1 Sat Nov 17 10:28:30 imported 30 objects 备注：从上看出导入了 30 个文档。 1.3 查看目标库12345678910&gt; use skytf; switched to db skytf&gt; show collections; system.indexes test_1 things&gt; db.things.count(); 30 备注：集合 things 已经导入到数据库 skytf 中，并且导入了 30 个文档。 另外，导入集合时需要指定 -c 参数，即目标的集合名，如果不指定会报以下 ERROR。 1.4 导入集合时不指定集合 -c 参数123[mongo@redhatB tf]$ mongoimport -h 127.0.0.1 -d skytf --file things.json connected to: 127.0.0.1 no collection specified! 备注：当然也可以在目标库中指定与源库集合不同名的集合。 例如下例中，源库skytf 库中的集合名为 things，可以指定目标库 skytf 的集合名为 things_1 1.5 指定不同名的集合名123[mongo@redhatB tf]$ mongoimport -h 127.0.0.1 -d skytf -c things_1 --file things.json connected to: 127.0.0.1 Sat Nov 17 10:30:11 imported 30 objects 1.6 再次查看目标库12345678&gt; show collections; system.indexes test_1 things things_1&gt; db.things_1.count(); 30 以 CSV 格式导入导出2.1 从源库以CSV格式导出集合123[mongo@redhatB tf]$ mongoexport -h 127.0.0.1 -d test -c test_2 -f id,name,address --csv -o test_2.csv connected to: 127.0.0.1 exported 4 records 2.2 导入CSV数据到目标库123[mongo@redhatB tf]$ mongoimport -h 127.0.0.1 -d skytf -c test_2 type csv -f id,name,address --headerline --file test_2.csv connected to: 127.0.0.1 Sat Nov 17 10:47:28 imported 4 objects 备注：这里指定参数 –type csv 为导入 csv 格式数据; –headerline 表示CSV格式的第一行为字段，如果不指定这个参数，则会将CSV格式第一行当数据导入到目标库。 2.3 查看目标库123456789101112131415&gt; use skytf; switched to db skytf &gt; show collections; system.indexes test_1 test_2 things things_1&gt; db.test_2.find(); &#123; \"_id\" : ObjectId(\"50a6fac0c32d362f080215fb\"), \"id\" : 1, \"name\" : \"francs\", \"address\" : \"\" &#125; &#123; \"_id\" : ObjectId(\"50a6fac0c32d362f080215fc\"), \"id\" : 2, \"name\" : \"fpZhou\", \"address\" : \"\" &#125; &#123; \"_id\" : ObjectId(\"50a6fac0c32d362f080215fd\"), \"id\" : 3, \"name\" : \"tutu\", \"address\" : \"\" &#125; &#123; \"_id\" : ObjectId(\"50a6fac0c32d362f080215fe\"), \"id\" : 4, \"name\" : \"am\", \"address\" : \"zhoushan\" &#125; 参考http://docs.mongodb.org/manual/reference/mongoimport/http://blog.chinaunix.net/uid-26785103-id-3282144.html","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：导出集合( Mongoexport )","slug":"20121116210033","date":"2012-11-16T13:00:33.000Z","updated":"2018-09-04T01:34:00.104Z","comments":true,"path":"20121116210033.html","link":"","permalink":"https://postgres.fun/20121116210033.html","excerpt":"","text":"数据导出是维护过程中比较常见的操作，今天学习了 MongoDB 自带的集合导出工具mongoexport，可以很容易的导出集合数据，下面演示下： 环境准备123456789[mongo@redhatB ~]$ mongo test MongoDB shell version: 2.2.1 connecting to: test&gt; db.test_2.find(); &#123; \"_id\" : ObjectId(\"50a245153b3b9e81c167fa98\"), \"id\" : 1, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50a245213b3b9e81c167fa99\"), \"id\" : 2, \"name\" : \"fpZhou\" &#125; &#123; \"_id\" : ObjectId(\"50a245213b3b9e81c167fa9a\"), \"id\" : 3, \"name\" : \"tutu\" &#125; &#123; \"_id\" : ObjectId(\"50a245213b3b9e81c167fa9b\"), \"id\" : 4, \"name\" : \"am\", \"address\" : \"zhoushan\" &#125; &gt; 备注：我们计划导出数据库 test 中的集合 test_2 中的文档。 以 CSV 格式导出1.1 以 csv 格式导出集合 test_2123[mongo@redhatB tf]$ mongoexport -h 127.0.0.1 -d test -c test_2 -f id,name,address --csv -o test_2.csv connected to: 127.0.0.1 exported 4 records 备注：以上导出数据库 test 的集合 test_2，并将数据以 csv 格式导出。 -h 表示主机IP或主机名; -d 表示数据库名; -c 表示集合名; -f 表示所选集合的字段; -o 表示导出的文件名。 1.2 查看 test_2.csv 内容 备注：将文件 test_2.csv 下载到本地 windows 机器上打开，如上所示。另外，如果以 csv 格式导出，需要指定导出集合的字段，否则会报以下ERROR。 1.3 不指定 -f 参数123[mongo@redhatB tf]$ mongoexport -h 127.0.0.1 -d test -c test_2 --csv -o test.csv connected to: 127.0.0.1 assertion: 9998 you need to specify fields 以 JSON 格式导出2.1 以 JSON 格式导出集合 test_2123[mongo@redhatB tf]$ mongoexport -h 127.0.0.1 -d test -c test_2 -o test_2.json connected to: 127.0.0.1 exported 4 records 备注：默认情况下使用 mongoexport 导出的数据文件格式为 JSON。 2.2 查看 test_2.json 数据12345[mongo@redhatB tf]$ cat test_2.json &#123; \"_id\" : &#123; \"$oid\" : \"50a245153b3b9e81c167fa98\" &#125;, \"id\" : 1, \"name\" : \"francs\" &#125; &#123; \"_id\" : &#123; \"$oid\" : \"50a245213b3b9e81c167fa99\" &#125;, \"id\" : 2, \"name\" : \"fpZhou\" &#125; &#123; \"_id\" : &#123; \"$oid\" : \"50a245213b3b9e81c167fa9a\" &#125;, \"id\" : 3, \"name\" : \"tutu\" &#125; &#123; \"_id\" : &#123; \"$oid\" : \"50a245213b3b9e81c167fa9b\" &#125;, \"id\" : 4, \"name\" : \"am\", \"address\" : \"zhoushan\" &#125; 参考http://docs.mongodb.org/manual/reference/mongoexport/http://docs.mongodb.org/manual/administration/import-export/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：关闭服务","slug":"20121114203945","date":"2012-11-14T12:39:45.000Z","updated":"2018-09-04T01:34:00.041Z","comments":true,"path":"20121114203945.html","link":"","permalink":"https://postgres.fun/20121114203945.html","excerpt":"","text":"MongoDB 提供几种关闭服务的命令，具体为以下： 一 使用 Crtl+C 关闭12[mongo@redhatB data]$ mongod --dbpath=/database/mongodb/data/ --logpath=/var/applog/mongo_log/mongo.log --logappend --port=27017 --journal &gt; start_mongo.log 2&gt;&amp;1光标：键入 Crtl+C 关闭 备注：如果以前台方式启动 MongoDB 服务，使用“Crtl+C” 服务会关闭，这种关闭方式会等待当前进行中的的操作完成，所以依然是干净的关闭方式。 二 使用数据库命令关闭2.1 开启服务1234[mongo@redhatB data]$ mongod -f /database/mongodb/data/mongodb_27017.confforked process: 18155all output going to: /var/applog/mongo_log/mongo.logchild process started successfully, parent exiting 2.2 登陆数据库123[mongo@redhatB data]$ mongoMongoDB shell version: 2.2.1connecting to: test 2.3 关闭 MongoDB 服务12345678 &gt; use admin;switched to db admin&gt; db.shutdownServer();Wed Nov 14 06:07:33 DBClientCursor::init call() failedWed Nov 14 06:07:33 query failed : admin.$cmd &#123; shutdown: 1.0 &#125; to: 127.0.0.1:27017server should be down...Wed Nov 14 06:07:33 trying reconnect to 127.0.0.1:27017Wed Nov 14 06:07:33 reconnect 127.0.0.1:27017 failed couldnt connect to server 127.0.0.1:27017 三 使用 Mongod 命令关闭12[mongo@redhatB data]$ mongod --shutdown --dbpath /database/mongodb/data/killing process with pid: 17747 备注：mongod 命令的 shutdown 选项能干净的关闭 MongoDB 服务。 四 使用 Kill 命令关闭4.1 查看 mongo 相关进程123456[mongo@redhatB data]$ ps -ef | grep mongoroot 17573 14213 0 05:10 pts/1 00:00:00 su - mongomongo 17574 17573 0 05:10 pts/1 00:00:00 -bashmongo 18288 1 0 06:12 ? 00:00:00 mongod -f /database/mongodb/data/mongodb_27017.confmongo 18300 17574 6 06:13 pts/1 00:00:00 ps -efmongo 18301 17574 0 06:13 pts/1 00:00:00 grep mongo 4.2 kill mongo 服务进程123[mongo@redhatB data]$ kill 18288[mongo@redhatB data]$ ps -ef | grep pmonmongo 18304 17574 0 06:13 pts/1 00:00:00 grep pmon 备注：可以使用操作系统的 kill 命令，给 mongod 进程发送 SIGINT 或 SIGTERM 信号，即 “kill -2 PID,” 或者 “kill -15 PID“。建议不要使用 ”kill -9 pid“，因为如果 MongoDB 运行在没开启日志（–journal）的情况，可能会造成数据损失。 五 参考http://www.mongodb.org/display/DOCS/Starting+and+Stopping+Mongohttp://docs.mongodb.org/manual/reference/mongod/http://www.cnblogs.com/taobataoma/archive/2007/08/30/875743.html","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：启动服务","slug":"20121114170720","date":"2012-11-14T09:07:48.000Z","updated":"2018-09-04T01:33:59.978Z","comments":true,"path":"20121114170720.html","link":"","permalink":"https://postgres.fun/20121114170720.html","excerpt":"","text":"MongoDB 启动方式有好几种，具体为以下： 一 命令行方式前台启动1[mongo@redhatB data]$ mongod --dbpath=/database/mongodb/data/ --logpath=/var/applog/mongo_log/mongo.log --logappend --port=27017 --journal &gt; start_mongo.log 2&gt;&amp;1 备注：这种是以前台方式启动，键入 Crtl+C 时或者退出时，数据库则会关闭；如果以这种方式启动，建议加上 nohup 命令将脚本放到后台执行。 二 命令行方式后台启动1[mongo@redhatB data]$ mongod --dbpath=/database/mongodb/data/ --logpath=/var/applog/mongo_log/mongo.log --logappend --port=27017 --journal --fork &gt; start_mongo.log 2&gt;&amp;1 备注：在 mongod 命令中设置 fork 参数，服务则以后台形式启动，相比前面的前台启动方式，这种方法是方便的。 三 使用配置文件启动3.1 编写配置文件 mongodb_27017.conf1234567fork = truebind_ip = 127.0.0.1port = 27017dbpath = /database/mongodb/data/logpath = /var/applog/mongo_log/mongo.loglogappend = truejournal = true 备注： fork： 表示是否以后台进程模式启动。 bind_ip： MongoDB 进程绑定的监听IP，默认为 localhost(127.0.0.1) port:： MongoDB 服务监听的 TCP 端口，默认为 27017 dbpath： 数据目录 logpath： 日志目录 logappend： 日志追加 journal: 是否开启预写日志以上为简单的配置参数，更多启动参数可参考以下手册。 3.2 启动 MongoDB 服务123[mongo@redhatB data]$ mongod -f /database/mongodb/data/mongodb_27017.confforked process: 17747all output going to: /var/applog/mongo_log/mongo.log 备注：个人觉得以配置文件形式启动更灵活，易配置各种启动参数，所以更倾向这种方式启动。 四 参考http://docs.mongodb.org/manual/administration/configuration/http://docs.mongodb.org/manual/reference/mongod/http://blog.chinaunix.net/uid-26785103-id-3227289.html","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：删除记录( Delete )","slug":"20121113211336","date":"2012-11-13T13:13:36.000Z","updated":"2018-09-04T01:33:59.916Z","comments":true,"path":"20121113211336.html","link":"","permalink":"https://postgres.fun/20121113211336.html","excerpt":"","text":"前面介绍了 MongoDB 的查询，插入，更新操作，接下来介绍 Delete 操作。 一. 根据查询条件删除文档1.1 查询 id=1 的所有文档1234&gt; db.test_2.find(&#123;id:1&#125;); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"name_2\" &#125; &#123; \"_id\" : ObjectId(\"50a12e34f73f2e4aa1ff71bc\"), \"id\" : 1, \"name\" : \"francs\" &#125; &gt; 1.2 删除 id=1 的所有文档1&gt; db.test_2.remove(&#123;id:1&#125;); 1.3 再次查询 id=1 的所有文档1&gt; db.test_2.find(&#123;id:1&#125;); 备注： id=1 的文档已清除。 二. 删除一个集合中的所有文档。2.1 查询 test_2 集合的所有文档123456789&gt; db.test_2.find(); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9c\"), \"id\" : 2, \"name\" : \"name_3\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9d\"), \"id\" : 3, \"name\" : \"tutu\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46deb825d827b0c3f9e\"), \"id\" : 4, \"name\" : \"am\", \"address\" : \"zhoushan\" &#125; &#123; \"_id\" : ObjectId(\"50a12e7cf73f2e4aa1ff71bd\"), \"id\" : 2, \"name\" : \"name_3\" &#125; &#123; \"_id\" : ObjectId(\"50a12f47f73f2e4aa1ff71be\"), \"id\" : 2, \"name\" : \"name_3\" &#125; &#123; \"_id\" : ObjectId(\"50a132177543857379c2bb37\"), \"id\" : 5, \"name\" : \"name_5\" &#125; &#123; \"_id\" : ObjectId(\"50a1328f7543857379c2bb38\"), \"id\" : 6, \"name\" : \"name_6\" &#125; &gt; 2.2 删除 test_2 集合的所有文档1&gt; db.test_2.remove(); 2.3 再次查询 test_2 集合1&gt; db.test_2.find(); 备注： test_2 集合文档已清空。 2.4 查看当前库所有集合12345&gt; show collections; system.indexes test_1 test_2 things 备注：虽然集合 test_2 文档已清空，但 remove() 命令并不会删除集合本身。 三. 删除集合3.1 查看当前库所有集合123456&gt; show collections; system.indexes test_1 test_2 things &gt; 3.2 查看集合 test_1 文档123&gt; db.test_1.find(); &#123; \"_id\" : ObjectId(\"50a2450e3b3b9e81c167fa96\"), \"id\" : 1 &#125; &#123; \"_id\" : ObjectId(\"50a245113b3b9e81c167fa97\"), \"id\" : 2 &#125; 3.3 删除集合 test_1123&gt; db.test_1.drop();true 3.4 再次查看当前库所有集合12345&gt; show collections; system.indexes test_2 things &gt; 备注：drop() 命令会删除集合本身。","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：更新记录( Update )","slug":"20121113175437","date":"2012-11-13T09:54:37.000Z","updated":"2018-09-04T01:33:59.853Z","comments":true,"path":"20121113175437.html","link":"","permalink":"https://postgres.fun/20121113175437.html","excerpt":"","text":"同上，紧接着学习 MongoDB 的数据更新语法。 一. 更新一条记录1.1 查询 id=1 的 documents123&gt; db.test_2.find(&#123;id:1&#125;); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50a12e34f73f2e4aa1ff71bc\"), \"id\" : 1, \"name\" : \"francs\" &#125; 1.2 更新 id=1 的 documents 的 name 字段为 ‘name_2’1&gt; db.test_2.update(&#123;id:1&#125;,&#123;$set:&#123;name:'name_2'&#125;&#125;); 1.3 再次查询 id=1 的 documents123&gt; db.test_2.find(&#123;id:1&#125;); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"name_2\" &#125; &#123; \"_id\" : ObjectId(\"50a12e34f73f2e4aa1ff71bc\"), \"id\" : 1, \"name\" : \"francs\" &#125; 备注：发现只更新一条匹配的记录。 二. 更新多条记录2.1 查询 id=2 的 documents1234&gt; db.test_2.find(&#123;id:2&#125;); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9c\"), \"id\" : 2, \"name\" : \"fpZhou\" &#125; &#123; \"_id\" : ObjectId(\"50a12e7cf73f2e4aa1ff71bd\"), \"id\" : 2, \"name\" : \"fpZhou\" &#125; &#123; \"_id\" : ObjectId(\"50a12f47f73f2e4aa1ff71be\"), \"id\" : 2, \"name\" : \"fpZhou\" &#125; 2.2 更新 id=2 的 documents 的 name 字段为 ‘name_3’1&gt; db.test_2.update(&#123;id:2&#125;,&#123;$set:&#123;name:'name_3'&#125;&#125;,&#123;multi:true&#125;); 2.3 再次查询 id=2 的 documents1234&gt; ;db.test_2.find(&#123;id:2&#125;); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9c\"), \"id\" : 2, \"name\" : \"name_3\" &#125; &#123; \"_id\" : ObjectId(\"50a12e7cf73f2e4aa1ff71bd\"), \"id\" : 2, \"name\" : \"name_3\" &#125; &#123; \"_id\" : ObjectId(\"50a12f47f73f2e4aa1ff71be\"), \"id\" : 2, \"name\" : \"name_3\" &#125; 备注：默认情况下 update() 仅更新第一条匹配的记录，如果想更新多条，需要设置multi 参数为 true。 三. 被更新的记录不存在的情况3.1 查询 id=6 的 documents1&gt; db.test_2.find(&#123;id:6&#125;); 备注：id=6 的记录不存在。 3.2 不加 upsert 参数的更新12&gt; db.test_2.update(&#123;id:6&#125;,&#123;$set:&#123;name:'name_6'&#125;&#125;); &gt; db.test_2.find(&#123;id:6&#125;); 备注：这时更新后，没有新增 id=6 的记录。 3.3 带 upsert 参数的更新1234&gt; db.test_2.update(&#123;id:6&#125;,&#123;$set:&#123;name:'name_6'&#125;&#125;,&#123;upsert:true&#125;); &gt; db.test_2.find(&#123;id:6&#125;); &#123; \"_id\" : ObjectId(\"50a1328f7543857379c2bb38\"), \"id\" : 6, \"name\" : \"name_6\" &#125; &gt; 备注：如果被更新的 document 不存在，可以通过指定 upsert 参数确定是否要插入一条新记录，如果为 truce 则插入，否则，不插入。 四. 附附一 .update() 语法1234567891011db.collection.update( criteria, objNew, upsert, multi )criteria - query which selects the record to update; objNew - updated object or $ operators (e.g., $inc) which manipulate the object upsert - if this should be an \"upsert\" operation; that is, if the record(s) do not exist, insert one. Upsert only inserts a single document. multi - indicates if all documents matching criteria should be updated rather than just one. Can be useful with the $ operators below. If you are coming from SQL, be aware that by default, update() only modifies the first matched object. If you want to modify all matched objects, you need to use the multi flag. 附二. $set 语法 Use the $set operator to set a particular value. The $set operator requires thefollowing syntax: db.collection.update( { field: value1 }, { $set: { field1: value2 } } ); This statement updates in the document in collection where field matches value1 byreplacing the value of the field field1 with value2. This operator will add the specified field or fields if they do not exist in this document or replace the existingvalueof the specified field(s) if they already exist. 五. 参考http://www.mongodb.org/display/DOCS/Updatinghttp://docs.mongodb.org/manual/reference/sql-comparison/http://docs.mongodb.org/manual/reference/operators/#_S_set","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：查询记录( Select ) ","slug":"20121113162146","date":"2012-11-13T08:21:46.000Z","updated":"2018-09-04T01:33:59.807Z","comments":true,"path":"20121113162146.html","link":"","permalink":"https://postgres.fun/20121113162146.html","excerpt":"","text":"上篇 blog 学习了 MongoDB 的插入，接下来学习数据查询相关的内容。MongoDB 查询有好几种方法，具体为以下： 基础表12345&gt; db.test_2.find(); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9c\"), \"id\" : 2, \"name\" : \"fpZhou\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9d\"), \"id\" : 3, \"name\" : \"tutu\" &#125; --find 游标查询 &#123; \"_id\" : ObjectId(\"50a0d46deb825d827b0c3f9e\"), \"id\" : 4, \"name\" : \"am\", \"address\" : \"zhoushan\" &#125; 1 使用 Find 游标查询12345678910111213141516171819var cursor=db.test_2.find(); while(cursor.hasNext()) printjson(cursor.next());&#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9c\"), \"id\" : 2, \"name\" : \"fpZhou\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9d\"), \"id\" : 3, \"name\" : \"tutu\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46deb825d827b0c3f9e\"), \"id\" : 4, \"name\" : \"am\", \"address\" : \"zhoushan\" &#125; 备注：通过 while 循环，使用 next() 方法依次读取所有 document，并用 printjson 方法显示结果。 2 使用 Find 的 forEach（）查询123456789101112131415161718&gt; db.test_2.find().forEach(printjson); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9c\"), \"id\" : 2, \"name\" : \"fpZhou\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9d\"), \"id\" : 3, \"name\" : \"tutu\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46deb825d827b0c3f9e\"), \"id\" : 4, \"name\" : \"am\", \"address\" : \"zhoushan\" &#125; 3 定制查询 MongoDB 的查询和关系型数据库的查询差异较大，但大多数据的关系型数据库的查询可以转换成MongoDB 的查询，以下为简单的几种情况。 3.1 select * From test_2;12345&gt; db.test_2.find(); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9c\"), \"id\" : 2, \"name\" : \"fpZhou\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9d\"), \"id\" : 3, \"name\" : \"tutu\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46deb825d827b0c3f9e\"), \"id\" : 4, \"name\" : \"am\", \"address\" : \"zhoushan\" &#125; 3.2 select id,name from test_2;12345&gt; db.test_2.find(&#123;&#125;,&#123;id:1,name:1&#125;); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9c\"), \"id\" : 2, \"name\" : \"fpZhou\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9d\"), \"id\" : 3, \"name\" : \"tutu\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46deb825d827b0c3f9e\"), \"id\" : 4, \"name\" : \"am\" &#125; 3.3 select id from test_2;12345&gt; db.test_2.find(&#123;&#125;,&#123;id:1&#125;); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1 &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9c\"), \"id\" : 2 &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9d\"), \"id\" : 3 &#125; &#123; \"_id\" : ObjectId(\"50a0d46deb825d827b0c3f9e\"), \"id\" : 4 &#125; 3.4 select * From test_2 where id=1;12&gt; db.test_2.find(&#123;id:1&#125;); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"francs\" &#125; 3.5 select id,address from test_2 where id=4;12&gt; db.test_2.find(&#123;address:'zhoushan'&#125;,&#123;id:1,name:1&#125;); &#123; \"_id\" : ObjectId(\"50a0d46deb825d827b0c3f9e\"), \"id\" : 4, \"name\" : \"am\" &#125; 3.6 select * from test_2 where id=1 and name=’francs’12&gt; db.test_2.find(&#123;id:1,name:'francs'&#125;); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"francs\" &#125; 备注：先学习这么多，其它更复杂的查询以后再补充。 4 Limit 用法123456789101112131415161718192021222324&gt; db.things.find();db.things.find(); &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3f9f\"), \"id\" : 1, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa0\"), \"id\" : 2, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa1\"), \"id\" : 3, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa2\"), \"id\" : 4, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa3\"), \"id\" : 5, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa4\"), \"id\" : 6, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa5\"), \"id\" : 7, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa6\"), \"id\" : 8, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa7\"), \"id\" : 9, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa8\"), \"id\" : 10, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa9\"), \"id\" : 11, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3faa\"), \"id\" : 12, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fab\"), \"id\" : 13, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fac\"), \"id\" : 14, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fad\"), \"id\" : 15, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fae\"), \"id\" : 16, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3faf\"), \"id\" : 17, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fb0\"), \"id\" : 18, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fb1\"), \"id\" : 19, \"name\" : \"aaa\" &#125; &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fb2\"), \"id\" : 20, \"name\" : \"aaa\" &#125; Type \"it\" for more&gt; db.things.find().limit(1); &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3f9f\"), \"id\" : 1, \"name\" : \"aaa\" &#125; 5 Skip 用法12&gt; db.things.find().skip(10).limit(1); &#123; \"_id\" : ObjectId(\"50a0d637eb825d827b0c3fa9\"), \"id\" : 11, \"name\" : \"aaa\" &#125; 6 Sorting 用法1db.things.find().sort(&#123;id:1&#125;); 7 附：db.collection.find The find() method selects documents in a collection and returns a cursor to the selected documents.The find() method takes the following parameters. Parameters: 1 query (document) – Optional. Specifies the selection criteria using query operators. Omit the query parameter or pass an empty document (e.g. {}) to return all documents in the collection. 2 projection (document) – Optional. Controls the fields to return, or the projection. The projection argument will resemble the following prototype:{ field1: boolean, field2: boolean … }The boolean can take the following include or exclude values:1 or true to include. The find() method always includes the _id field even if the field is not explicitly stated to return in the projection parameter.0 or false to exclude.The projection cannot contain both include and exclude specifications except for the exclusion of the _id field. Returns: A cursor to the documents that match the query criteria and contain the projection fields.","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：插入记录( Insert )","slug":"20121113104640","date":"2012-11-13T02:46:40.000Z","updated":"2018-09-04T01:33:59.244Z","comments":true,"path":"20121113104640.html","link":"","permalink":"https://postgres.fun/20121113104640.html","excerpt":"","text":"相比关系型数据库的 INSERT 操作， MongoDB 的插入操作虽然别扭，但依然简单，以下练习了 MongoDB 的插入操作。 一. 插入方式之一1.1 插入数据123456789&gt; x=&#123;id:1&#125;;x=&#123;id:1&#125;; &#123; \"id\" : 1 &#125; &gt; y=&#123;id:2&#125;;y=&#123;id:2&#125;; &#123; \"id\" : 2 &#125;&gt; db.test_1.save(x); &gt; db.test_1.save(y);&gt; db.test_1.find(); &#123; \"_id\" : ObjectId(\"50a0d35ceb825d827b0c3f99\"), \"id\" : 1 &#125; &#123; \"_id\" : ObjectId(\"50a0d375eb825d827b0c3f9a\"), \"id\" : 2 &#125; 备注：这里通过先创建对像 x，y ，然后将它们保存在 test_1 中。 1.2 显示 collections12345&gt; show collections; system.indexes test_1 things things_1 备注：test_1 已自动创建，这也说明 MongoDB 不需要预先定义 collections ，在第一次插入数据后，collections 会自动的创建。 二. 插入方式之二2.1 插入数据1234&gt; db.test_2.save(&#123;id:1,name:'francs'&#125; ); &gt; db.test_2.save(&#123;id:2,name:'fpZhou'&#125; ); &gt; db.test_2.save(&#123;id:3,name:'tutu'&#125; ); &gt; db.test_2.save(&#123;id:4,name:'am',address:'zhoushan'&#125; ); 2.2 查询数据12345&gt; db.test_2.find(); &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9b\"), \"id\" : 1, \"name\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9c\"), \"id\" : 2, \"name\" : \"fpZhou\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46ceb825d827b0c3f9d\"), \"id\" : 3, \"name\" : \"tutu\" &#125; &#123; \"_id\" : ObjectId(\"50a0d46deb825d827b0c3f9e\"), \"id\" : 4, \"name\" : \"am\", \"address\" : \"zhoushan\" &#125; 备注: 相比方法一，插入数据时没有预先通过创建对像x,y; 而且不同的 documents 可以有不同的字段(如第四条 documents 多了 “address” 字段)。 2.3 循环插入数据1&gt; for (var i = 1; i &lt;= 30; i++) db.things.save(&#123;id : i, name : 'aaa'&#125;); 2.4 查询 things 数据 &gt; db.things.find(); { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3f9f&quot;), &quot;id&quot; : 1, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fa0&quot;), &quot;id&quot; : 2, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fa1&quot;), &quot;id&quot; : 3, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fa2&quot;), &quot;id&quot; : 4, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fa3&quot;), &quot;id&quot; : 5, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fa4&quot;), &quot;id&quot; : 6, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fa5&quot;), &quot;id&quot; : 7, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fa6&quot;), &quot;id&quot; : 8, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fa7&quot;), &quot;id&quot; : 9, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fa8&quot;), &quot;id&quot; : 10, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fa9&quot;), &quot;id&quot; : 11, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3faa&quot;), &quot;id&quot; : 12, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fab&quot;), &quot;id&quot; : 13, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fac&quot;), &quot;id&quot; : 14, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fad&quot;), &quot;id&quot; : 15, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fae&quot;), &quot;id&quot; : 16, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3faf&quot;), &quot;id&quot; : 17, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fb0&quot;), &quot;id&quot; : 18, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fb1&quot;), &quot;id&quot; : 19, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fb2&quot;), &quot;id&quot; : 20, &quot;name&quot; : &quot;aaa&quot; } Type &quot;it&quot; for more &gt; it { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fb3&quot;), &quot;id&quot; : 21, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fb4&quot;), &quot;id&quot; : 22, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fb5&quot;), &quot;id&quot; : 23, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fb6&quot;), &quot;id&quot; : 24, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fb7&quot;), &quot;id&quot; : 25, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fb8&quot;), &quot;id&quot; : 26, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fb9&quot;), &quot;id&quot; : 27, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fba&quot;), &quot;id&quot; : 28, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fbb&quot;), &quot;id&quot; : 29, &quot;name&quot; : &quot;aaa&quot; } { &quot;_id&quot; : ObjectId(&quot;50a0d637eb825d827b0c3fbc&quot;), &quot;id&quot; : 30, &quot;name&quot; : &quot;aaa&quot; } 备注：默认只显示 20 条记录， 如需浏览更多记录，键入”it” 命令就可以了。 三. 总结通过以上练习，有以下几点需要总结： MongoDB 不需要预先定义 collections，在数据第一次插入时 collections 被自动创建。 在一个 collections 中不同的 documents 可以有不同的字段。 在 collections 中的每个 documents 在插入时会分配一个不同的 ObjectId，字段为 “_id”。 四. 参考http://www.mongodb.org/display/DOCS/Tutorial","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"MongoDB：安装简介","slug":"20121112153235","date":"2012-11-12T07:32:35.000Z","updated":"2018-09-04T01:33:59.181Z","comments":true,"path":"20121112153235.html","link":"","permalink":"https://postgres.fun/20121112153235.html","excerpt":"","text":"最近准备开始学习 MongoDB，了解一门 NoSQL 是非常必要的，今天学习了 MongoDB 的安装，步骤还是比较简单的。 1 环境信息笔记本虚机： Red Hat Enterprise Linux Server release 6.2MongoDB: 2.2.1 2 创建用户和目录2.1 创建用户12[root@redhatB database]# groupadd mongo [root@redhatB database]# useradd -g mongo mongo 2.2 创建数据目录12[root@redhatB database]# mkdir -p /database/mongodb/data/ [root@redhatB database]# chown -R mongo:mongo /database/mongodb/data/ 2.3 创建日志目录12[root@redhatB database]# mkdir -p /var/applog/mongo_log/ [root@redhatB database]# chown -R mongo:mongo /var/applog/mongo_log/ 3 下载 MongoDB 2.2.1http://fastdl.mongodb.org/linux/mongodb-linux-i686-2.2.1.tgz 4 解压123456789101112131415161718192021[root@redhatB soft_bak]# tar zxvf mongodb-linux-i686-2.2.1.tgz mongodb-linux-i686-2.2.1/GNU-AGPL-3.0 mongodb-linux-i686-2.2.1/README mongodb-linux-i686-2.2.1/THIRD-PARTY-NOTICES mongodb-linux-i686-2.2.1/bin/mongodump mongodb-linux-i686-2.2.1/bin/mongorestore mongodb-linux-i686-2.2.1/bin/mongoexport mongodb-linux-i686-2.2.1/bin/mongoimport mongodb-linux-i686-2.2.1/bin/mongostat mongodb-linux-i686-2.2.1/bin/mongotop mongodb-linux-i686-2.2.1/bin/mongooplog mongodb-linux-i686-2.2.1/bin/mongofiles mongodb-linux-i686-2.2.1/bin/bsondump mongodb-linux-i686-2.2.1/bin/mongoperf mongodb-linux-i686-2.2.1/bin/mongosniff mongodb-linux-i686-2.2.1/bin/mongod mongodb-linux-i686-2.2.1/bin/mongos mongodb-linux-i686-2.2.1/bin/mongo[root@redhatB soft_bak]# mv mongodb-linux-i686-2.2.1 /opt/mongodb2.2.1 [root@redhatB opt]# chown -R mongo:mongo mongodb2.2.1 5 修改环境变量修改 mongo 用户 .bash_profile 文件，增加以下。1234export MONGO_HOME=/opt/mongodb2.2.1 export MONGO_DATA=/database/mongodb/data/ export PATH=$MONGO_HOME/bin:$PATH[mongo@redhatB ~]$ source .bash_profile 6 启动 MongoDB1nohup mongod --dbpath=/database/mongodb/data/ --logpath=/var/applog/mongo_log/mongo.log --logappend --port=27017 &gt; /var/applog/mongo_log/start_mongodb.log 2&gt;&amp;1 &amp; 7 查看进程123456[mongo@redhatB ~]$ ps -ef | grep mongo root 5565 4969 0 14:58 pts/1 00:00:00 su - mongo mongo 5567 5565 0 14:58 pts/1 00:00:00 -bash mongo 5675 5567 2 15:05 pts/1 00:00:00 mongod --dbpath=/database/mongodb/data/ --logpath=/var/applog/mongo_log/mongo.log --logappend --port=27017 mongo 5684 5567 10 15:05 pts/1 00:00:00 ps -ef mongo 5685 5567 0 15:05 pts/1 00:00:00 grep mongo 8 第一次连接123456[mongo@redhatB ~]$ mongo MongoDB shell version: 2.2.1 connecting to: test &gt;&gt; show dbs; local (empty) 备注：默认连接的是本地的 test 库，到这步 MongoDB 已经安装完成了，但通过 “show dbs” 命令，查不到 test 库信息。 9 插入数据12db.test_1.save( &#123; 1:\"francs\" &#125; ) db.test_1.save( &#123; 2:\"fpzhou\" &#125; ) 10 查询数据1234db.test_1.find();&gt; db.test_1.find(); &#123; \"_id\" : ObjectId(\"50a0a07c7b54486ddb7f7593\"), \"1\" : \"francs\" &#125; &#123; \"_id\" : ObjectId(\"50a0a07f7b54486ddb7f7594\"), \"2\" : \"fpzhou\" &#125; 11 再次查看数据库列表123&gt; show dbs; local (empty) test 0.0625GB 备注：这时 test 库已创建并有数据了。 12 关闭 MongoDB1234567&gt; use admin;switched to db admin &gt; db.shutdownServer();Mon Nov 12 15:11:32 DBClientCursor::init call() failed Mon Nov 12 15:11:32 query failed : admin.$cmd &#123; shutdown: 1.0 &#125; to: 127.0.0.1:27017 server should be down... Mon Nov 12 15:11:32 trying reconnect to 127.0.0.1:27017 Mon Nov 12 15:11:32 reconnect 127.0.0.1:27017 failed couldn't connect to server 127.0.0.1:27017 13 参考http://docs.mongodb.org/manual/tutorial/install-mongodb-on-linux/http://blog.163.com/dazuiba_008/blog/static/363349812011928433493/","categories":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/categories/MongoDB/"}],"tags":[{"name":"MongoDB","slug":"MongoDB","permalink":"https://postgres.fun/tags/MongoDB/"}]},{"title":"PostgreSQL: 关于 Left Join 的基础知识","slug":"20121110112007","date":"2012-11-10T03:20:07.000Z","updated":"2018-09-04T01:33:59.103Z","comments":true,"path":"20121110112007.html","link":"","permalink":"https://postgres.fun/20121110112007.html","excerpt":"","text":"今天开发人员跑来咨询使用 left join 进行表关联的事情，这个查询 SQL 简单为以下: 查询SQL112345select a.column_1, a.column_2, b.column_3 from ( 结果集1 ) a left join b on a.id=b.id; 其中”结果集1”为部分表关联查询的中间结果，具体代码这里省略，开发人员的疑问是：已知结果集1的输出为 125 条， 而整个查询SQL1的结果却有 126 条，也就是说，left join 后比结果集1要多一条数据，而开发人员觉得使用 left join 后整个查询结果集应该与左边的结果集保持一致。平常对于 left/right join 关注得比较少，为了解释这个问题，先在测试环境下模拟一遍，看看是否真是如此？ 测试环境准备数据库版本12345skytf=&gt; select version(); version ----------------------------------------------------------------------------------------------------------- PostgreSQL 9.0.9 on i686-pc-linux-gnu, compiled by GCC gcc (GCC) 4.4.6 20110731 (Red Hat 4.4.6-3), 32-bit (1 row) 创建两张表并插入少量数据12345678910111213141516171819202122232425262728293031skytf=&gt; create table test_a (id serial,name varchar(32));NOTICE: CREATE TABLE will create implicit sequence \"test_a_id_seq\" for serial column \"test_a.id\" CREATE TABLEskytf=&gt; insert into test_a (name) values ('a'),('b'),('c'),('d'),('e'),('f'),('g'); INSERT 0 7skytf=&gt; select * From test_a; id | name ----+------ 1 | a 2 | b 3 | c 4 | d 5 | e 6 | f 7 | g (7 rows)skytf=&gt; create table test_b (id serial,name varchar(32)); NOTICE: CREATE TABLE will create implicit sequence \"test_b_id_seq\" for serial column \"test_b.id\" CREATE TABLE skytf=&gt; insert into test_b (name) values ('a'),('b'); INSERT 0 2skytf=&gt; select * from test_b; id | name ----+------ 1 | a 2 | b (2 rows) 备注：为了测试需要，创建了表 test_a 和 test_b 并插入少量数据。注意：test_a 表有 7 条数据，test_b 表有 2 条数据。 LEFT JOIN 测试1234567891011skytf=&gt; select a.id,a.name,b.id,b.name from test_a a left join test_b b on a.name=b.name; id | name | id | name ----+------+----+------ 1 | a | 1 | a 2 | b | 2 | b 3 | c | | 4 | d | | 5 | e | | 6 | f | | 7 | g | | (7 rows) 备注：这里结果集为 7 条数据，确实与表 test_a 结果集保持一致。 给表 test_b 插入一条记录，接着测试1234567891011121314151617181920212223skytf=&gt; insert into test_b (name) values ('b'); INSERT 0 1skytf=&gt; select * from test_b; id | name ----+------ 1 | a 2 | b 3 | b (3 rows)skytf=&gt; select a.id,a.name,b.id,b.name from test_a a left join test_b b on a.name=b.name; id | name | id | name ----+------+----+------ 1 | a | 1 | a 2 | b | 3 | b 2 | b | 2 | b 3 | c | | 4 | d | | 5 | e | | 6 | f | | 7 | g | | (8 rows) 备注：给表 test_b 插入记录 “3 b” 后，再次 left join，发现结果集为 8 条，确实多了一条，果然和开发人员遇到的问题一样，多的这一条其实为 test_b 的 “name” 字段的重复记录。 开始我也觉得奇怪，接着查下文档，看看手册上的说明： 手册上的解释 Cross join T1 CROSS JOIN T2INNER JOIN For each row R1 of T1, the joined table has a row for each row in T2 that satisfies the join condition with R1.LEFT OUTER JOIN First, an inner join is performed. Then, for each row in T1 that does not satisfy the join condition with any row in T2, a joined row is added with null values in columns of T2. Thus, the joined table always has at least one row for each row in T1. 备注：重点看”LEFT OUTER JOIN” 的解释：首先，会执行一个 inner join，然后，对于 T1 中不满足与 T2 关联条件的每一行会添加 null 值到 T2 的列，所以连接的表至少为 T1 中的每一行。 文档的解释已经很清楚了，如果还不是很清楚，看看下面的 INNER JOIN 输出就会更明白些。 INNER JOIN 测试1234567skytf=&gt; select a.id,a.name,b.id,b.name from test_a a inner join test_b b on a.name=b.name; id | name | id | name ----+------+----+------ 1 | a | 1 | a 2 | b | 3 | b 2 | b | 2 | b (3 rows) 备注：至此问题已经很明朗了。 参考 http://www.postgresql.org/docs/9.0/static/queries-table-expressions.html#QUERIES-FROM","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL流复制：备库主机宕机一例","slug":"20121030133709","date":"2012-10-30T05:37:09.000Z","updated":"2018-09-04T01:33:59.025Z","comments":true,"path":"20121030133709.html","link":"","permalink":"https://postgres.fun/20121030133709.html","excerpt":"","text":"今天一数据库流复制环境备库由于硬件原因导致主机宕机，PostgreSQL 版本为 9.2.1，持续时间为 5 小时左右，可能很多人会认为，备库挂了，可能要重做了，庆幸的是， pg 在 这方面非常省心，在这种情况下，理论上只要主库的保持的 WAL 足够多，备库就能赶上主库并恢复，简单记录下。 备库数据库日志备库系统启动后，数据库日志如下：123456789101112132012-10-29 20:07:50.193 CST,,,11520,,508e7196.2d00,1,,2012-10-29 20:07:50 CST,,0,LOG,00000,\"database system was interrupted while in recovery at log time 2012-10-29 15:12:23 CST\",,\"If this has occurred more than once some data might be corrupted and you might need to choose an earlier recovery target.\",,,,,,,\"\" 2012-10-29 20:07:50.226 CST,,,11520,,508e7196.2d00,2,,2012-10-29 20:07:50 CST,,0,LOG,00000,\"entering standby mode\",,,,,,,,,\"\" 2012-10-29 20:07:50.280 CST,,,11520,,508e7196.2d00,3,,2012-10-29 20:07:50 CST,1/0,0,LOG,00000,\"redo starts at DC/DAAC3460\",,,,,,,,,\"\" 2012-10-29 20:08:42.461 CST,,,11521,,508e7196.2d01,1,,2012-10-29 20:07:50 CST,,0,LOG,00000,\"restartpoint starting: xlog\",,,,,,,,,\"\" 2012-10-29 20:08:57.384 CST,,,11520,,508e7196.2d00,4,,2012-10-29 20:07:50 CST,1/0,0,LOG,00000,\"consistent recovery state reached at DD/CED97FE0\",,,,,,,,,\"\" 2012-10-29 20:08:57.385 CST,,,11518,,508e7195.2cfe,1,,2012-10-29 20:07:49 CST,,0,LOG,00000,\"database system is ready to accept read only connections\",,,,,,,,,\"\" 2012-10-29 20:08:57.403 CST,,,11520,,508e7196.2d00,5,,2012-10-29 20:07:50 CST,1/0,0,LOG,00000,\"unexpected pageaddr D8/FAEC8000 in log file 221, segment 206, offset 15499264\",,,,,,,,,\"\" 2012-10-29 20:08:57.442 CST,,,11607,,508e71d9.2d57,1,,2012-10-29 20:08:57 CST,,0,LOG,00000,\"streaming replication successfully connected to primary\",,,,,,,,,\"\" 2012-10-29 20:09:41.259 CST,,,11642,\"\",508e7205.2d7a,1,\"\",2012-10-29 20:09:41 CST,,0,LOG,00000,\"connection received: host=127.0.0.1 port=13025\",,,,,,,,,\"\" 2012-10-29 20:09:51.146 CST,,,11643,\"\",508e720f.2d7b,1,\"\",2012-10-29 20:09:51 CST,,0,LOG,00000,\"connection received: host=192.168.1.38 port=53984\",,,,,,,,,\"\" 2012-10-29 20:09:51.146 CST,,,11643,\"xxx.xxx.xxx.xxx:xxxxx\",508e720f.2d7b,2,\"\",2012-10-29 20:09:51 CST,,0,LOG,08P01,\"incomplete startup packet\",,,,,,,,,\"\" 2012-10-29 20:09:54.177 CST,,,11521,,508e7196.2d01,2,,2012-10-29 20:07:50 CST,,0,LOG,00000,\"restartpoint complete: wrote 81492 buffers (31.1%); 0 transaction log file(s) added, 120 removed, 257 recycled; write=56.140 s, sync=14.822 s, total=71.715 s; sync files=66, longest=5.080 s, average=0.224 s\",,,,,,,,,\"\" 2012-10-29 20:09:54.177 CST,,,11521,,508e7196.2d01,3,,2012-10-29 20:07:50 CST,,0,LOG,00000,\"recovery restart point at DD/5B013708\",\"last completed transaction was at log time 2012-10-29 15:26:04.08085+08\",,,,,,,,\"\" 备注：上面是备库系统宕机后，重新启动 PostgreSQL 时的日志，在备库恢复过程中还看不到 wal 接收进程，过会之后能看到如下进程。 检查备库 WAL 接收进程123[postgres@db](mailto:postgres@db)&gt; ps -ef | grep wal postgres 11607 11518 1 20:08 ? 00:00:43 postgres: wal receiver process streaming E1/1D11F308 postgres 16693 11871 0 20:59 pts/2 00:00:00 grep wal 主库 Wal_keep_segments 参数12345postgres=&gt; show wal_keep_segments ; wal_keep_segments ------------------- 1024 (1 row) 备注：主库设置的 wal_keep_segments 参数为 1024，一个较大的 wal_keep_segments 设置，允许备库在宕机较长的时间内依然能够重新追上主库，当然这与主库的繁忙程度有关，主库越忙，产生的 WAL 日志越多，之前的 WAL 日志越容易被覆盖。 总结此文仅简单记录了 PostgreSQL 流复制环境，standby 由于系统挂掉后恢复的情况，从以上看出，备库挂了之后，重新恢复是非常容易的，前提是在硬盘空间允许的范围内，主库尽量设置较大的 wal_keep_segments 参数，从而保证较多的 WAL 日志文件。","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"},{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/tags/PG高可用性/"}]},{"title":"PostgreSQL: 如何连接 Group By 结果集的行？","slug":"20121024133234","date":"2012-10-24T05:32:34.000Z","updated":"2018-09-04T01:33:58.962Z","comments":true,"path":"20121024133234.html","link":"","permalink":"https://postgres.fun/20121024133234.html","excerpt":"","text":"如何连接 “ group by” 结果集的行？ 有点像行列转换，但又不完全是，描述颇为费劲，举例如下： 假如一张表有以下数据： 中国 台北中国 香港中国 上海日本 东京日本 大阪 要求得到如下结果集： 中国 台北,香港,上海日本 东京，大阪 此文介绍几种实现以上要求的方法: PostgreSQL 版本为 9.0 创建初始表并插入数据12345678910111213141516171819202122skytf=&gt; create table city (country character varying(64),city character varying(64)); CREATE TABLEskytf=&gt; insert into city values ('中国','台北'); INSERT 0 1 skytf=&gt; insert into city values ('中国','香港'); INSERT 0 1 skytf=&gt; insert into city values ('中国','上海'); INSERT 0 1 skytf=&gt; insert into city values ('日本','东京'); INSERT 0 1 skytf=&gt; insert into city values ('日本','大阪'); INSERT 0 1skytf=&gt; select * From city; country | city ---------+------ 中国 | 台北 中国 | 香港 中国 | 上海 日本 | 东京 日本 | 大阪 (5 rows) 方法一: 使用聚集函数 array_agg1.1 函数说明 Function name array_aggArgument Type(s) anyReturn Type array of the argument typeDescription input values, including nulls, concatenated into an array 1.2 测试函数12345678910111213skytf=&gt; \\df array_agg List of functions Schema | Name | Result data type | Argument data types | Type ------------+-----------+------------------+---------------------+------ pg_catalog | array_agg | anyarray | anyelement | agg (1 row)skytf=&gt; select country,array_agg(city) from city group by country; country | array_agg ---------+------------------ 中国 | &#123;台北,香港,上海&#125; 日本 | &#123;东京,大阪&#125; (2 rows) 备注： 这正是我们需要的结果，返回的结果类型为数组。 方法二: 使用聚集函数 string_agg2.1函数说明 Function name string_agg(expression, delimiter)Argument Type(s) text, textReturn Type textDescription input values concatenated into a string, separated by delimiter 2.2 测试函数1234567891011skytf=&gt; select country,string_agg(city,',') from city group by country; country | string_agg ---------+---------------- 中国 | 台北,香港,上海 日本 | 东京,大阪 (2 rows)skytf=&gt; select country,string_agg(city,',') from city where country='中国' group by country; country | string_agg ---------+---------------- 中国 | 台北,香港,上海 (1 row) 备注；string_agg 函数返回结果为 text 类型。 方法三: 自定义聚集函数3.1 创建聚集函数123456skytf=&gt; CREATE AGGREGATE array_accum(anyelement) ( skytf(&gt; SFUNC = array_append, skytf(&gt; STYPE = anyarray, skytf(&gt; INITCOND = '&#123;&#125;' skytf(&gt; ); CREATE AGGREGATE 3.2 测试函数12345skytf=&gt; select country,array_accum(city) from city where country='中国' group by country; country | array_accum ---------+------------------ 中国 | &#123;台北,香港,上海&#125; (1 row) 备注：同样得到了预期的结果，返回的类型为数组。 参考 http://www.postgresql.org/docs/9.0/static/functions-aggregate.html http://www.postgresql.org/docs/9.0/static/sql-createaggregate.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Salesforce 准备用 PostgreSQL 替换 Oracle","slug":"20121018174027","date":"2012-10-18T09:40:27.000Z","updated":"2018-09-04T01:33:58.900Z","comments":true,"path":"20121018174027.html","link":"","permalink":"https://postgres.fun/20121018174027.html","excerpt":"","text":"Salesforce 准备用 PostgreSQL 替换 Oracle， 开源数据库 PostgreSQL 可能又将增加一大用户，详见： http://bits.blogs.nytimes.com/2012/10/15/salesforce-hires-to-go-open-source/http://tech.qq.com/a/20121018/000077.htm","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL : 如何升级小版本？","slug":"20120928162121","date":"2012-09-28T08:21:21.000Z","updated":"2018-09-04T01:33:58.837Z","comments":true,"path":"20120928162121.html","link":"","permalink":"https://postgres.fun/20120928162121.html","excerpt":"","text":"大版本升级，例如 PostgreSQL 9.1 升级到 9.2 可以使用 pg_dump/pg_restore ，或者 pg_upgrade 来完成，相比大版本升级，小版本升级的过程要简单得多。下面以 PostgreSQL 9.2.0 升级到 9.2.1 为例，以编译安装方式为例，简单介绍小版本升级的步骤（不同小版本升级要求不同，具体看Release Note）： 手册说明 As with other minor releases, to apply this update release you may simply shut down PostgreSQL, update its binaries and restart. Users upgrading between major versions will need to dump and reload their database or use pg_upgrade. Users of older versions who have skipped multiple update releases may need to perform additional post-update steps; see the Release Notes for each version for details. 备注：对于小版本的升级，手册中描述了需要关闭 PostgreSQL 服务，替换二进制文件，并重启。描述是简单的，下面通过实验操作一次。 Release NOTEE.1.1. Migration to Version 9.2.1A dump/restore is not required for those running 9.2.X.However, you may need to performREINDEXand/orVACUUMoperations to recover from the effects of the data corruption bug described in the first changelog item below. 环境信息1234567891011[pg92@redhatB ~]$ echo $PGHOME /opt/pgsql92[pg92@redhatB ~]$ /opt/pgsql92/bin/psql psql (9.2.0) Type \"help\" for help.postgres=# select version(); version ------------------------------------------------------------------------------------------------------- PostgreSQL 9.2.0 on i686-pc-linux-gnu, compiled by gcc (GCC) 4.4.6 20110731 (Red Hat 4.4.6-3), 32-bit (1 row) 备注：以上是 PostgreSQL 9.2.0 版本，目标升级到 9.2.1。 升级步骤 下载并安装 PostgreSQL 9.2.1 新版本程序到新目录，包括 configure，gmake，gmake install 步骤。 停原 PostgreSQL 服务。( pg_ctl stop -m fast -D $PGDATA ) 修改 $PHOME 环境变量，指向新版本的 PG，本例是 /opt/pgsql9.2.1 使用新版本的 PostgreSQL 程序启动服务（ $PGDATA 和原来一样）。 升级完版本后，最好是重建下库里的 btree 和 gin 索引。 备注：先新安装 PG 新版本程序，再升级，这种方法停库的时间比直接覆盖原 $PGHOME 安装方式的时间更短，省略了安装的时间。 升级后验证1234567891011[pg92@redhatB pg_log]$ echo $PGHOME /opt/pgsql9.2.1[pg92@redhatB pg_log]$ psql psql (9.2.1) Type \"help\" for help.postgres=# select version(); version ------------------------------------------------------------------------------------------------------- PostgreSQL 9.2.1 on i686-pc-linux-gnu, compiled by gcc (GCC) 4.4.6 20110731 (Red Hat 4.4.6-3), 32-bit (1 row) 备注：致此升级成功，特殊情况需要重建索引。 参考 http://www.postgresql.org/about/news/1416/ http://wiki.postgresql.org/wiki/20120924updaterelease","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：递归查询应用场景","slug":"20120919140842","date":"2012-09-19T06:08:42.000Z","updated":"2018-09-04T01:33:58.744Z","comments":true,"path":"20120919140842.html","link":"","permalink":"https://postgres.fun/20120919140842.html","excerpt":"","text":"今天在坛子里有人提出了一个问题，问题是这样的：在以下指定表中123456789id name fatherid 1 中国 0 2 辽宁 1 3 山东 1 4 沈阳 2 5 大连 2 6 济南 3 7 和平区 4 8 沈河区 4 现在给定一个id号，想得到它完整的名字。如： 当id=7时，名字是：中国辽宁沈阳和平区 当id=5时，名字是：中国辽宁大连 id是任意给定的，不确定在哪一层。递归往上找，直到 fatherid=0 为止。也就是最高层级时结束，求完整SQL语句。 看到这个问题，第一想到的是可以用 PG的递归查询实现，之前也写过类似的例子， https://postgres.fun/20101208135721.html ，只不过是向下递归，而这里的需求是向上递归，略有不同，于是忍不住演示下，这个问题的思路是分两步走，第一步：查询出指定节点的父节点；第二步：将查询出的所有父节点排列到一行。 创建测试表创建测试表，并插入测试数据1234567891011121314151617181920212223skytf=&gt; create table test_area(id int4,name varchar(32),fatherid int4); CREATE TABLEinsert into test_area values (1, '中国' ,0); insert into test_area values (2, '辽宁' ,1); insert into test_area values (3, '山东' ,1); insert into test_area values (4, '沈阳' ,2); insert into test_area values (5, '大连' ,2); insert into test_area values (6, '济南' ,3); insert into test_area values (7, '和平区' ,4); insert into test_area values (8, '沈河区' ,4);skytf=&gt; select * From test_area; id |name | fatherid ----+--------+---------- 1 | 中国 | 0 2 | 辽宁 | 1 3 | 山东 | 1 4 | 沈阳 | 2 5 | 大连 | 2 6 | 济南 | 3 7 | 和平区 | 4 8 | 沈河区 | 4 (8 rows) 查询指定节点以下的所有节点123456789101112WITH RECURSIVE r AS (SELECT * FROM test_area WHERE id = 4 union ALL SELECT test_area.* FROM test_area, r WHERE test_area.fatherid = r.id ) SELECT * FROM r ORDER BY id;id |name | fatherid ----+--------+---------- 4 | 沈阳 | 2 7 | 和平区 | 4 8 | 沈河区 | 4 (3 rows) 备注：通常的用法是查询指定节点以及指定节点以下的所有节点，那么本贴的需求刚好相反，需要查询指定节点以上的所有节点。 查询指定节点以上的所有节点123456789101112WITH RECURSIVE r AS ( SELECT * FROM test_area WHERE id = 4 union ALL SELECT test_area.* FROM test_area, r WHERE test_area.id = r.fatherid ) SELECT * FROM r ORDER BY id;id | name | fatherid ----+------+---------- 1 | 中国 | 0 2 | 辽宁 | 1 4 | 沈阳 | 2 (3 rows) 备注：这正是我们想要的结果，接下来需要将 name 字段结果集合并成一行，我这里想到的是创建个function，当然也有其它方法。 创建函数123456789101112131415161718192021CREATE or replace FUNCTION func_get_area(in in_id int4, out o_area text) AS$$ DECLARE v_rec_record RECORD; BEGINo_area = '';FOR v_rec_record IN (WITH RECURSIVE r AS (SELECT * FROM test_area WHERE id =in_id union ALL SELECT test_area.* FROM test_area, r WHERE test_area.id = r.fatherid)SELECT name FROM r ORDER BY id) LOOP o_area := o_area || v_rec_record.name; END LOOP; return; END; $$ LANGUAGE 'plpgsql'; 备注：函数的作用为拼接 name 字段。 执行函数1234567891011skytf=&gt; select func_get_area(7) ; func_get_area -------------------- 中国辽宁沈阳和平区 (1 row)skytf=&gt; select func_get_area(5) ; func_get_area --------------- 中国辽宁大连 (1 row) 备注：正好实现了需求，当表数据量较大时，考虑到性能，建议在表 test_area 字段 id，fatherid 上建立单独的索引。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: Hstore 数据类型使用介绍","slug":"20120918152029","date":"2012-09-18T07:20:29.000Z","updated":"2018-09-04T01:33:58.681Z","comments":true,"path":"20120918152029.html","link":"","permalink":"https://postgres.fun/20120918152029.html","excerpt":"","text":"hstore 数据类型作为 Extension 模块， 在之前版本已经有了，之前大概知道概念，一直没怎么研究，今天做了些测试。简单的说 hstore 数据类型用来存储具有多个属性值的数据，模板如 key =&gt; value ， key 代表存储的属性， values 为相应属性对应的值，一个简单的 hstore 例子如下： Hstore 示例12345678910skytf=&gt; select 'a=&gt;1,b=&gt;2'::hstore; hstore -------------------- \"a\"=&gt;\"1\", \"b\"=&gt;\"2\" (1 row)skytf=&gt; select hstore('a=&gt;1,b=&gt;2'); hstore -------------------- \"a\"=&gt;\"1\", \"b\"=&gt;\"2\" (1 row) Hstore 操作符演示2.1 查询属性(key) a 的值；12345skytf=&gt; select hstore('a=&gt;1,b=&gt;2') -&gt; 'a'; ?column? ---------- 1 (1 row) 2.2 判断是否包含指定属性(key)，如果包含，返回 t12345678910skytf=&gt; select hstore('a=&gt;1,b=&gt;2') ? 'a'; ?column? ---------- t (1 row)skytf=&gt; select hstore('a=&gt;1,b=&gt;2') ? 'c'; ?column? ---------- f (1 row) 2.3 判断左边的 hstore 是否包含右边的 hstore ，如果包含，返回 t。12345678910skytf=&gt; select hstore('a=&gt;1,b=&gt;2') @&gt; 'a=&gt;1'::hstore; ?column? ---------- t (1 row)skytf=&gt; select hstore('a=&gt;1,b=&gt;2') @&gt; 'a=&gt;2'::hstore; ?column? ---------- f (1 row) 备注：更多 hstore 函数和操作符，详见本文的附录部分。 Hstore 函数演示3.1 array 类型转换成 hstore12345skytf=&gt; select hstore(ARRAY['a','1','b','2']); hstore -------------------- \"a\"=&gt;\"1\", \"b\"=&gt;\"2\" (1 row) 3.2 将两个 array 类型数据转换成 hstore 类型，比较难表达，看例子理解。12345skytf=&gt; select hstore(ARRAY['a','b','c'], ARRAY['1','2','3']); hstore ------------------------------ \"a\"=&gt;\"1\", \"b\"=&gt;\"2\", \"c\"=&gt;\"3\" (1 row) 备注：看到了吧，非常强大。3.3 将 hstore 类型数据的 key 转换成 array12345skytf=&gt; select akeys('a=&gt;1,b=&gt;2'); akeys ------- &#123;a,b&#125; (1 row) 3.4 将 hstore 类型数据的 key 转换成 结果集123456skytf=&gt; select skeys('a=&gt;1,b=&gt;2'); skeys ------- a b (2 rows) 3.5 将 hstore 类型数据的 values 转换成 array12345skytf=&gt; select avals('a=&gt;1,b=&gt;2'); avals ------- &#123;1,2&#125; (1 row) 3.6 将 hstore 类型数据的 values 转换成 结果集123456skytf=&gt; select svals('a=&gt;1,b=&gt;2'); svals ------- 1 2 (2 rows) 3.7 删除一个属性12345skytf=&gt; select delete(hstore('a=&gt;1,b=&gt;2'),'b'); delete ---------- \"a\"=&gt;\"1\" (1 row) 3.8 增加一个属性12345skytf=&gt; select hstore('a=&gt;1,b=&gt;2') || 'c=&gt;3'; ?column? ------------------------------ \"a\"=&gt;\"1\", \"b\"=&gt;\"2\", \"c\"=&gt;\"3\" (1 row) 备注：先演示这么多吧，更多内容，参考本文的附。 Example1: Hstore 使用4.1 环境PG版本：PostgreSQL 9.2OS版本：Red Hat Enterprise Linux Server release 6.24.2 创建 hstore 外部模块123456789101112131415skytf=&gt; \\c skytf postgres; You are now connected to database \"skytf\" as user \"postgres\".skytf=# create extension hstore; CREATE EXTENSIONskytf=# \\c skytf skytf; You are now connected to database \"skytf\" as user \"skytf\". skytf=&gt; \\dT hstore; List of data types Schema | Name | Description --------+--------+------------- public | hstore | (1 row) 4.3 创建测试表，并生成测试数据1234567891011121314skytf=&gt; create table tbl_role(id serial primary key,role_name varchar(32), exp int8, wealth int8, status char(1)); NOTICE: CREATE TABLE will create implicit sequence \"tbl_role_id_seq\" for serial column \"tbl_role.id\" NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"tbl_role_pkey\" for table \"tbl_role\" CREATE TABLEskytf=&gt; insert into tbl_role(role_name,exp,wealth) select 'user_' || generate_series(1,100000) ,generate_series(1,100000),generate_series(100001,200000); INSERT 0 100000skytf=&gt; select * from tbl_role limit 3; id | role_name | exp | wealth | status ----+-----------+-----+--------+-------- 1 | user_1 | 1 | 100001 | 2 | user_2 | 2 | 100002 | 3 | user_3 | 3 | 100003 | (3 rows) 备注：上面创建一张角色信息表，属性值有 exp:经验值， wealth:财富值等。 4.4 增加 hstore 数据类型12345678910skytf=&gt; alter table tbl_role add column attr hstore; ALTER TABLEskytf=&gt; update tbl_role set attr=('exp=&gt;' || exp || ', wealth=&gt;' || wealth )::hstore;UPDATE 100000skytf=&gt; select * from tbl_role limit 3;; id | role_name | exp | wealth | status | attr ----+-----------+-----+--------+--------+--------------------------------- 22 | user_22 | 22 | 100022 | | \"exp\"=&gt;\"22\", \"wealth\"=&gt;\"100022\" 23 | user_23 | 23 | 100023 | | \"exp\"=&gt;\"23\", \"wealth\"=&gt;\"100023\" 24 | user_24 | 24 | 100024 | | \"exp\"=&gt;\"24\", \"wealth\"=&gt;\"100024\" (3 rows) 4.5 创建索引12skytf=&gt; create index concurrently idx_tbl_role_attr on tbl_role using GIST ( attr); CREATE INDEX 备注： hstore 类型的数据支持 GIN,GIST 索引扫描的操作符有 @&gt;, ?, ?&amp; 和 ?|4.6 测试：查询 hstore 子元素 exp 值为 22 的记录123456789skytf=&gt; select id,role_name,attr,attr -&gt; 'exp' From tbl_role where attr @&gt; 'exp=&gt;22'; id | role_name | attr | ?column? ----+-----------+---------------------------------+---------- 22 | user_22 | \"exp\"=&gt;\"22\", \"wealth\"=&gt;\"100022\" | 22 (1 row)skytf=&gt; select id,role_name,attr,attr -&gt; 'wealth' From tbl_role where attr @&gt; 'wealth=&gt;100001'; id | role_name | attr | ?column? ----+-----------+--------------------------------+---------- 1 | user_1 | \"exp\"=&gt;\"1\", \"wealth\"=&gt;\"100001\" | 100001 4.7 执行计划：12345678910skytf=&gt; explain analyze select id,role_name,attr,attr -&gt; 'exp' From tbl_role where attr @&gt; 'exp=&gt;22'; QUERY PLAN ------------------------------------------------------------------------------------------- Bitmap Heap Scan on tbl_role (cost=5.36..347.63 rows=100 width=54) (actual time=3.267..7.111 rows=1 loops=1) Recheck Cond: (attr @&gt; '\"exp\"=&gt;\"22\"'::hstore) Rows Removed by Index Recheck: 1545 -&gt; Bitmap Index Scan on idx_tbl_role_attr (cost=0.00..5.33 rows=100 width=0) (actual time=2.813..2.813 rows=1546 loops=1) Index Cond: (attr @&gt; '\"exp\"=&gt;\"22\"'::hstore) Total runtime: 7.185 ms (6 rows) 备注：消耗 7 ms 左右，这个查询速度并不很快。 Example2: 将结果集转换成 hstore 类型5.1 测试表123456skytf=&gt; select * From test_1; id | name ----+-------- 1 | francs 2 | fpzhou (2 rows) 5.2 将结果集转换成 hstore 类型输出123456789101112skytf=&gt; select hstore(test_1) From test_1; hstore ----------------------------- \"id\"=&gt;\"1\", \"name\"=&gt;\"francs\" \"id\"=&gt;\"2\", \"name\"=&gt;\"fpzhou\" (2 rows) skytf=&gt; select hstore(test_1) From test_1 where id=1; hstore ----------------------------- \"id\"=&gt;\"1\", \"name\"=&gt;\"francs\" (1 row) 附: Hstore 函数和操作符 https://www.postgresql.org/docs/9.2/static/hstore.html#id-1.11.7.26.5 总结hstore 适用于拥有多个属性值的对像，同时这些属性值查询并不非常频繁的场合，根据上面的测试，在 10 万数据中，根据元素属性值查询需要7ms 左右，这个速度已经算慢的了。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"hstore","slug":"hstore","permalink":"https://postgres.fun/tags/hstore/"}]},{"title":"PostgreSQL: 数据迁移之序列问题 ","slug":"20120913171925","date":"2012-09-13T09:19:25.000Z","updated":"2018-09-04T01:33:58.619Z","comments":true,"path":"20120913171925.html","link":"","permalink":"https://postgres.fun/20120913171925.html","excerpt":"","text":"在数据库管理工作中，数据迁移是 DBA 平常而又重要的工作，数据迁移的背景可以有很多，简单的说，需要将一个地方的数据迁移到另一个地方，这里需要说明的是，任何时候的数据迁移，如果涉及到序列，请注意序列问题，哪怕只有一个序列，也要注意。为什么序列对数据迁移如此重要，因为在数据迁移过程中，很容易碰到序列和表数据不匹配的情况，如表的 id ( 这里假设 id 字段是主键，且用序列填充) 最大值大于序列的 next 值，这种情况将造成数据插入不进的情况，从而影响了应用的正常运行。 当然，对于数据为冷备份迁移方式和停业务方式导出，导入的方式出现序列不一致的情况会稍微少些，但在完成数据迁移后，检查下重要核心表的序列是必要的；有一类情况更加需要检查序列，例如，某些情况，先将表定义导入到目标库，并且也建好了序列，之后再导数据，这种情况下，在业务开启之前，务必需要检查序列，说不准会有序列 next 值小于表的 id 最大值的情况，从而导入数据无法插入。 接下来简单介绍下序列的一些操作，虽然比较简单： 创建序列12skytf=&gt; create sequence seq_test_1 INCREMENT by 1 MINVALUE 1 NO MAXVALUE start with 1 ; CREATE SEQUENCE 查看序列属性1234567891011121314skytf=&gt; \\d seq_test_1 Sequence \"skytf.seq_test_1\" Column | Type | Value ---------------+---------+--------------------- sequence_name | name | seq_test_1 last_value | bigint | 1 start_value | bigint | 1 increment_by | bigint | 1 max_value | bigint | 9223372036854775807 min_value | bigint | 1 cache_value | bigint | 1 log_cnt | bigint | 0 is_cycled | boolean | f is_called | boolean | f 查看序列 next 值12345678910skytf=&gt; select nextval('seq_test_1'); nextval --------- 1 (1 row)skytf=&gt; select nextval('seq_test_1'); nextval --------- 2 (1 row) 查看序列最近使用值12345678910skytf=&gt; select currval('seq_test_1'); currval --------- 2 (1 row)skytf=&gt; select currval('seq_test_1'); currval --------- 2 (1 row) 接下来介绍下序列重置，这个操作比较常见，也比较重要，有两种方法。 序列重置方式一123456789101112131415skytf=&gt; select setval('seq_test_1',100); setval -------- 100 (1 row)skytf=&gt; select currval('seq_test_1'); currval --------- 100 (1 row)skytf=&gt; select nextval('seq_test_1'); nextval --------- 101 (1 row) 备注：将序列当前值设置为 100。 序列重置方式二skytf=&gt; alter sequence seq_test_1 restart with 200; ALTER SEQUENCE skytf=&gt; select nextval(&apos;seq_test_1&apos;); nextval --------- 200 (1 row) skytf=&gt; select nextval(&apos;seq_test_1&apos;); nextval --------- 201 (1 row) skytf=&gt; \\d seq_test_1 Sequence &quot;skytf.seq_test_1&quot; Column | Type | Value ---------------+---------+--------------------- sequence_name | name | seq_test_1 last_value | bigint | 201 start_value | bigint | 200 increment_by | bigint | 1 max_value | bigint | 9223372036854775807 min_value | bigint | 1 cache_value | bigint | 1 log_cnt | bigint | 31 is_cycled | boolean | f is_called | boolean | t 备注：将序列起始值设置成 200。 总结之所以写下这篇Blog，是因为自己曾经在序列问题上疏忽过，写下这篇提醒自己，也提醒大家。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"Sequence","slug":"Sequence","permalink":"https://postgres.fun/tags/Sequence/"}]},{"title":"PostgreSQL9.2：Range Data Type Using Case ","slug":"20120912002557","date":"2012-09-11T16:25:57.000Z","updated":"2018-09-04T01:33:58.556Z","comments":true,"path":"20120912002557.html","link":"","permalink":"https://postgres.fun/20120912002557.html","excerpt":"","text":"之前已经写了篇 Blog 简单介绍了 PostgreSQL9.2 新增的 RANGE 数据类型使用，链接如 https://postgres.fun/20120524163556.html , 今天 RANGE 类型的 使用情况源于一开发人员咨询 IP 地址库查询的 SQL 效率问题，后来转换成 inet 网络地址类型后，效率提升并不明显，之后经过德哥建议，使用 RANGE 类型，效率提升迅速，下面是具体过程：起初，开发人员咨询的 SQL 如下 优化前的 SQL1234567891011121314151617francs=&gt; select version(); version --------------------------------------------------------------------------------------------------------------- PostgreSQL 9.1.3 on x86_64-unknown-linux-gnu, compiled by gcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-51), 64-bit (1 row)francs=&gt; explain analyze SELECT PROVINCE,CITY francs-&gt; FROM ip_address_range francs-&gt; WHERE start_ip &lt;= 3708713472 francs-&gt; and end_ip&gt;=3708713472 LIMIT 1 ; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..0.32 rows=1 width=20) (actual time=3.138..3.138 rows=1 loops=1) -&gt; Index Scan using idx_start_end_ip on ip_address_range (cost=0.00..908.16 rows=2856 width=20) (actual time=3.136..3.136 rows=1 loops=1) Index Cond: ((start_ip &lt;= 3708713472::bigint) AND (end_ip &gt;= 3708713472::bigint)) Total runtime: 3.168 ms (4 rows) 备注：开发环境 PostgreSQL 9.1.3， 执行时间为 3 ms 左右，其实这个速度也不算非常慢，当然我们希望能够将它优化，当压力测试时力求达到更多的 tps。 表信息表结构123456789101112131415francs=&gt; \\d ip_address_range Table \"francs.ip_address_range\" Column | Type | Modifiers -----------+-------------------+----------- id | numeric | not null start_ip | bigint | end_ip | bigint | province | character varying | city | character varying | isp | character varying | start_ip1 | character varying | end_ip1 | character varying | Indexes: \"ip_address_range_pkey\" PRIMARY KEY, btree (id) \"idx_start_end_ip\" btree (start_ip, end_ip) 表数据1234567891011francs=&gt; select * from ip_address_range limit 1; id | start_ip | end_ip | province | city | isp | start_ip1 | end_ip1 ----+------------+------------+----------+--------+------+--------------+---------------- 1 | 3708713472 | 3708715007 | 河南省 | 信阳市 | 联通 | 221.14.122.0 | 221.14.127.255 (1 row)francs=&gt; select count(*) from ip_address_range; count ------- 32807 (1 row) 备注：最初的想法是将 start_ip,end_ip 字段转换成一个 inet 字段，然后通过操作符 &gt;&gt;= 进行判断，遗憾的是，进行一番折腾，操作符 &gt;&gt;= 不走索引，在德哥的提示下，建议使用 Range 类型。而 RANGE 是 PostgreSQL 9.2 的新特性，于是准备将这个库导到 9.2 版本环境下测试。数据库 9.2 版本测试增加 Range 字段12345678910111213141516francs=&gt; select version(); version ------------------------------------------------------------------------------------------------------------------ PostgreSQL 9.2beta4 on x86_64-unknown-linux-gnu, compiled by gcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-51), 64-bit (1 row)francs=&gt; alter table ip_address_range add column ip_range int8range;ALTER TABLEfrancs=&gt; update ip_address_range set ip_range= int8range(start_ip,end_ip,'(]'); UPDATE 32807francs=&gt; select * From ip_address_range limit 1; id | start_ip | end_ip | province | city | isp | start_ip1 | end_ip1 | ip_range ----+------------+------------+----------+--------+------+--------------+----------------+------------------------- 1 | 3708713472 | 3708715007 | 河南省 | 信阳市 | 联通 | 221.14.122.0 | 221.14.127.255 | [3708713473,3708715008) (1 row) 备注：在表 ip_address_range 上增加一个 int8range 字段。 创建GIST索引12francs=&gt; create index idx_ip_address_range_ip_range on ip_address_range using gist ( ip_range); CREATE INDEX 备注： GIST 索引可以用于操作符 =, &amp;&amp;, &lt;@, @&gt;, &lt;&lt;, &gt;&gt;, -|-, &amp;&lt;, and &amp;&gt;等，加速查询。 性能测试11234567891011francs=&gt; explain analyze SELECT PROVINCE,CITY francs-&gt; FROM ip_address_range francs-&gt; WHERE start_ip &lt;= 3708713472 francs-&gt; and end_ip&gt;=3708713472 LIMIT 1 ; QUERY PLAN ----------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..0.53 rows=1 width=19) (actual time=0.559..0.559 rows=1 loops=1) -&gt; Seq Scan on ip_address_range (cost=0.00..1481.11 rows=2779 width=19) (actual time=0.558..0.558 rows=1 loops=1) Filter: ((start_ip &lt;= 3708713472::bigint) AND (end_ip &gt;= 3708713472::bigint)) Total runtime: 0.591 ms (4 rows) 备注：迁移到 9.2 版本后，走的是 Seq Scan，但执行时间由9.1 版本的3ms 左右降低到了 0.59 ms 左右，这在一定程度上验证了 9.2 版本查询性能提升的说法( 9.2 版本 release note 说明中有描述)，当然这不是今天的重点。 性能测试212345678francs=&gt; explain analyze select * From ip_address_range where ip_range @&gt; 3708713472::int8; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------- Index Scan using idx_ip_address_range_ip_range on ip_address_range (cost=0.00..34.85 rows=33 width=106) (actual time=0.055..0.056 rows=1 loops=1) Index Cond: (ip_range @&gt; 3708713472::bigint) Total runtime: 0.085 ms (3 rows) 备注：性能测试1 根据的是起始IP查询，性能测试2 根据 range 字段查询，执行时间在 0.085 ms 左右，这个时间比性能测试1 提高 6 倍左右。 总结 本文中的整型字段 IP，是经过了一种的算法将普通 IP 转换成的，本文略。 RANGE 类型对 IP 地址定位的应用效率非常高，我们最终将IP 地址定位城市的 SQL 从原来的 3 ms 左右优化到 0.08 ms 左右。 参考http://www.postgresql.org/docs/9.2/static/rangetypes.htmlhttp://www.postgresql.org/docs/9.2/static/functions-range.html#RANGE-OPERATORS-TABLE","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"PostgreSQL 9.2 正式版发布","slug":"20120910215554","date":"2012-09-10T13:55:54.000Z","updated":"2018-12-04T00:26:45.555Z","comments":true,"path":"20120910215554.html","link":"","permalink":"https://postgres.fun/20120910215554.html","excerpt":"","text":"PostgreSQL 9.2 正式版于今天(2012-09-10)正式发布，详见官网 http://www.postgresql.org/about/news/1415/ PostgreSQL 9.2 在性能上提升是令人振奋的，release 中说明如下： Improved Performance and Scalability Improvements in vertical scalability increase PostgreSQL is ability to efficiently utilize hardware resources on larger servers. Advances in lock management, write efficiency, index-only access and other low-level operations allow the database engine to handle even larger-volume workloads.Numerically, this means: Up to 350,000 read queries per second (more than 4X faster) Index-only scans for data warehousing queries (2–20X faster) Up to 14,000 data writes per second (5X faster)Also, the addition of cascading replication enables users to run even larger stacks of horizontally scaled servers under PostgreSQL 9.2. 更多特性，详见 release note: http://www.postgresql.org/docs/9.2/static/release-9-2.html 之后慢慢研究吧。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: How to execute a sql script in a single transaction ? ","slug":"20120909111105","date":"2012-09-09T03:11:05.000Z","updated":"2018-09-04T01:33:58.415Z","comments":true,"path":"20120909111105.html","link":"","permalink":"https://postgres.fun/20120909111105.html","excerpt":"","text":"在数据库维护过程中，对生产数据库跑 SQL 脚本是再平常不过的操作了，比如业务升级，需要向某张表插入一批数据，或者更改好几张表的数据，为了保证数据的一致性，我们有必要使用事务，比如有一张表需要一次性插入5000条数据，我们希望这个操作要么全部执行成功，如有失败则全部回滚，在 Oracle 的 SQLPLUS 中可以使用 commit 命令达到这个需求，在 PostgreSQL 中也可以实现这个需求，尽管它的方式和 Orace 有一定区别。接下来会分两种场景进行演示： 场景一: 普通调用脚本方式1.1 创建测试表123456789101112mydb=&gt; create table test_single (id int4 primary key, name varchar(32)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_single_pkey\" for table \"test_single\" CREATE TABLEmydb=&gt; \\d test_single Table \"mydb.test_single\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null name | character varying(32) | Indexes: \"test_single_pkey\" PRIMARY KEY, btree (id) 1.2 编写脚本 insert_1.sql12345insert into test_single (id ,name ) values (1,'a'); insert into test_single (id ,name ) values (2,'b'); test_error1; insert into test_single (id ,name ) values (3,'b'); insert into test_single (id ,name ) values (4,'b'); 备注：脚本 insert_1.sql 有五条 sql，前两条向 test_single 插入两条记录，第三行 “test_error1;“ 为错误命令，为了测试。 1.3 执行脚本 insert_1.sql12345678[postgres@pgb tf]$ psql -d mydb -U mydb -f insert_1.sql INSERT 0 1 INSERT 0 1 psql:insert_1.sql:3: ERROR: syntax error at or near \"test_error1\" LINE 1: test_error1; ^ INSERT 0 1 INSERT 0 1 备注：命令在执行到第三行时报错，报语法错误，这正是我们预期的。 1.4 查询表 test_single 测试12345678mydb=&gt; select * From test_single; id | name ----+------ 1 | a 2 | b 3 | b 4 | b (4 rows) 备注：执行完 1.2 的调用脚本后，再次回到 psql 客户端查看到表 test_single 数据，发现四条数据都已进去了，说明在默认方式下，使用psql 的 “-f” 参数调用脚本时，当遇到 ERROR 时会继续往下执行，在很多情况下，这是我们所不希望看到的，我们希望发现错误并且修正它，并且所有操作都 rollback，接下来看第二个场景。 场景二: 事务方式调用脚本2.1重新测试把，先清空原表 test_single;1234567mydb=&gt; truncate table test_single; TRUNCATE TABLEmydb=&gt; select * From test_single; id | name ----+------ (0 rows) 2.2 以事务方式执行脚本12345678[postgres@pgb tf]$ psql -d mydb -U mydb single-transaction-f insert_1.sql INSERT 0 1 INSERT 0 1 psql:insert_1.sql:3: ERROR: syntax error at or near \"test_error1\" LINE 1: test_error1; ^ psql:insert_1.sql:4: ERROR: current transaction is aborted, commands ignored until end of transaction block psql:insert_1.sql:5: ERROR: current transaction is aborted, commands ignored until end of transaction block 备注：这个脚本加了参数 “–single-transaction” ，表示以事务方式调用脚本，其本质是在执行脚本时，会默认将命令“BEGIN/COMMIT ” 包裹脚本，脚本中遇到ERROR时，不会继续往下执行，并且所有操作都回滚，关于这个命令的详细信息参考本文末尾的附录部分。 2.3 再次查询测试1234mydb=&gt; select * From test_single; id | name ----+------ (0 rows) 备注：再查询表 test_single 时，无数据，说明以上的脚本执行的四条 INSERT 已经回滚了。在 PostgreSQL 中，默认是 autocommit ，即执行一条SQL 成功后，默认是 commit 的，这和 Oracle 完全不一样，在 oracle 中，在执行命令后，可以执行 commit/rollback 命令，提交或者回滚; 但在 pg 中，可以使用 begin/end 来 完成，例如： 123456789mydb=&gt; begin; mydb=&gt; select clock_timestamp(); clock_timestamp ------------------------------- 2012-09-09 10:48:20.480879+08 (1 row) mydb=&gt; sql ... mydb=&gt; sql .. mydb=&gt; end; 备注：但在 PostgreSQL 中，不可以中途 commit ，全部操作要么全执行成功，出现任务错误则全部回滚，而不像 Oracle 那样可以中途 commit，在这点上，习惯 Oracle 的朋友可能不习惯；另外也可以用这种方式以事务方式调用脚本，代码如下：12345678910111213141516mydb=&gt; begin; BEGIN mydb=&gt; \\i /home/postgres/script/tf/insert_1.sql INSERT 0 1 INSERT 0 1 psql:/home/postgres/script/tf/insert_1.sql:3: ERROR: syntax error at or near \"test_error1\" LINE 1: test_error1; ^ psql:/home/postgres/script/tf/insert_1.sql:4: ERROR: current transaction is aborted, commands ignored until end of transaction block psql:/home/postgres/script/tf/insert_1.sql:5: ERROR: current transaction is aborted, commands ignored until end of transaction block mydb=&gt; select * from test_single; ERROR: current transaction is aborted, commands ignored until end of transaction block mydb=&gt; end; ROLLBACK 备注：\\i 表示调用 SQL 脚本。 附: psql 选项说明 -1–single-transactionWhen psql executes a script with the -f option, adding this option wraps BEGIN/COMMIT around the script to execute it as a single transaction. This ensures that either all the commands complete successfully, or no changes are applied.If the script itself uses BEGIN, COMMIT, or ROLLBACK, this option will not have the desired effects. Also, if the script contains any command that cannot be executed inside a transaction block, specifying this option will cause that command (and hence the whole transaction) to fail.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"LINUX: Grep 命令去空行","slug":"20120904170039","date":"2012-09-04T09:00:39.000Z","updated":"2018-09-04T01:33:58.353Z","comments":true,"path":"20120904170039.html","link":"","permalink":"https://postgres.fun/20120904170039.html","excerpt":"","text":"今天学习了 grep 命令一个小技巧，即 “去空行”， grep 命令能够非常方便地查看配置文件哪些参数被修改了，今天测试了把，用来处理 PostgreSQL 的配置文件 postgresql.conf。 1 查看 postgresql.conf 文件中被修改的参数1234567891011121314151617181920212223242526272829303132333435363738394041[postgres@redhat6 pg_root]$ cat postgresql.conf | grep -v \"^$\" | grep -v \"^#\" # (change requires restart) # (change requires restart) # (change requires restart) # (change requires restart) listen_addresses = '*' # what IP address(es) to listen on; # comma-separated list of addresses; # defaults to 'localhost'; use '*' for all # (change requires restart) port = 1922 # (change requires restart) max_connections = 100 # (change requires restart) # (change requires restart) # (change requires restart) # (change requires restart) # (change requires restart) # 0 selects the system default # 0 selects the system default # 0 selects the system default shared_buffers = 32MB # min 128kB # (change requires restart) # (change requires restart) # in kB, or -1 for no limit # (change requires restart) wal_level = archive # minimal, archive, or hot_standby # (change requires restart) # off, local, remote_write, or on # supported by the operating system: # open_datasync # fdatasync (default on Linux) # fsync # fsync_writethrough # open_sync # (change requires restart) checkpoint_segments = 3 # in logfile segments, min 1, 16MB each checkpoint_timeout = 2min # range 30s-1h checkpoint_completion_target = 0.5 # checkpoint target duration, 0.0 - 1.0 archive_mode = on # allows archiving to be done # (change requires restart) archive_command = '/bin/date' # command to use to archive a logfile segment.....省略部分 备注：符号 ^ 表示匹配一行的开始； $ 表示匹配一行的结束; 所示 ^# 表示空行。 因此去掉 postgresql.conf 中的空行，并且排除掉以 # 打头的行，即为已更改的配置文件参数。 2 查看 postgreql.conf 中值为 “on” 的参数1234[postgres@redhat6 pg_root]$ cat postgresql.conf | grep \"= on$\" | grep -v \"^#\" log_checkpoints = on log_connections = on track_activities = on 3 查看 postgreql.conf 中值为 “off” 的参数12[postgres@redhat6 pg_root]$ cat postgresql.conf | grep \"= off$\" | grep -v \"^#\" log_disconnections = off 4 附件 postgresql.conf本来想上传 postgresql.conf ，网易 blog 居然不支持上传txt 格式文件，熟悉 postgresql 的朋友应该都知道这个配置文件，这里就不上传了。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"PostgreSQL : ALTER DEFAULT PRIVILEGES","slug":"20120817105800","date":"2012-08-17T02:58:00.000Z","updated":"2018-09-04T01:33:58.290Z","comments":true,"path":"20120817105800.html","link":"","permalink":"https://postgres.fun/20120817105800.html","excerpt":"","text":"关于权限的问题，曾经有个问题一直困扰着，由于关系不是很大一直没有深究，今天偶然的一次机会看手册时看到 “ALTER DEFAULT PRIVILEGES” 命令，立即解决了困扰已久的问题。 问题描述在生产数据库中通常不允许开发人员使用生产帐号连接数据库，那么有时候开发人员又需要查询数据库权限，用于日常问题排查，通常的做法是创建一个查询帐号，并把数据库中指定表的查询权限开放给给查询帐号，这也是目前我们生产系统维护过程中使用较多的方法，但是如果开发人员希望对整库所有表的查询权限均开放，而且以后生产库中新建的表也需要默认的开放此帐号的查询权限，这时如何处理？这时，今天学习到的命令即可解决这个问题，命令为 “ALTER DEFAULT PRIVILEGES”，下面简单测试下。 环境信息PostgreSQL 版本： 9.1.0数据库名: mydb数据库属主: mydb查询帐号 mydb_select 备注：假设数据库 mydb 库中用户仅创建 mydb schema。 创建只读帐号123456[postgres@pgb ~]$ psql psql (9.1.0) Type \"help\" for help. postgres=# create role mydb_select LOGIN NOSUPERUSER NOCREATEDB NOCREATEROLE encrypted password 'mydb_select'; CREATE ROLE 备注：上面创建帐号 mydb_select。 给帐号赋权123456789postgres=# \\c mydb mydb You are now connected to database \"mydb\" as user \"mydb\".mydb=&gt; grant connect on database mydb to mydb_select; GRANTmydb=&gt; grant usage on schema mydb to mydb_select; GRANTmydb=&gt; grant select on all tables in schema mydb to mydb_select; GRANT 备注：上面命令给帐号 mydb_select 开通了数据库 mydb 的只读权限，可以访问mydb 库中 mydb schema 下的所有表。 权限测试12345678mydb=&gt; \\c mydb mydb_select; You are now connected to database \"mydb\" as user \"mydb_select\".mydb=&gt; select * from mydb.test limit 1; id | name --------+------ 831681 | AAA (1 row) 备注： mydb_select 用户查询 mydb.test 表成功，没有问题。 在 mydb 库中创建一张新表1234567891011121314mydb=&gt; create table test_33 as select * From test limit 10 ; SELECT 10mydb=&gt; \\dp test_33 Access privileges Schema | Name | Type | Access privileges | Column access privileges --------+---------+-------+-------------------+-------------------------- mydb | test_33 | table | mydb=arwdDxt/mydb+|(1 row)mydb=&gt; \\c mydb mydb_select; You are now connected to database \"mydb\" as user \"mydb_select\".mydb=&gt; select * from mydb.test_33; ERROR: permission denied for relation test_33 备注：此时用户 mydb_select 没有权限查询，因为 mydb.test_33 是赋权后新建的表。 使用 “ALTER DEFAULT PRIVILEGES” 赋权1mydb=&gt; ALTER DEFAULT PRIVILEGES IN SCHEMA mydb grant select on tables to mydb_select;ALTER DEFAULT PRIVILEGES 备注：我们更改用户 mydb_select 的默认权限，使得在数据库 mydb中 mydb 模式下创建的新表，默认给用户 mydb_select 开通查询权限，这个命令仅对未来创建的数据库对像生效，对当前存在的数据库对像不起作用， 这个命令的详细信息，参考本文末尾的附一。 权限测试12345678910111213141516171819202122mydb=&gt; create table test_34 as select * From test limit 10 ; SELECT 10mydb=&gt; \\dp test_34 Access privileges Schema | Name | Type | Access privileges | Column access privileges --------+---------+-------+--------------------+-------------------------- mydb | test_34 | table | mydb=arwdDxt/mydb +| | | | mydb_select=r/mydb |(1 row)mydb=&gt; \\c mydb mydb_select You are now connected to database \"mydb\" as user \"mydb_select\".mydb=&gt; select * From mydb.test_34 limit 1; id | name --------+------ 831681 | AAA (1 row) mydb=&gt; select * From mydb.test_33 limit 1; ERROR: permission denied for relation test_33 备注：这时用户 mydb_select 果然拥有之后创建表(mydb.test_34) 的查询权限了，而对之前的表 mydb.test_33 依然没有查询权限，这也正好验证子这前关于这个命令的说明，即仅对未来创建的数据库对像生效。 附一 ALTER DEFAULT PRIVILEGE 命令123456789101112131415161718192021222324252627282930313233343536373839404142NameALTER DEFAULT PRIVILEGES -- define default access privileges SynopsisALTER DEFAULT PRIVILEGES [ FOR &#123; ROLE | USER &#125; target_role [, ...] ] [ IN SCHEMA schema_name [, ...] ] abbreviated_grant_or_revokewhere abbreviated_grant_or_revoke is one of:GRANT &#123; &#123; SELECT | INSERT | UPDATE | DELETE | TRUNCATE | REFERENCES | TRIGGER &#125; [,...] | ALL [ PRIVILEGES ] &#125; ON TABLES TO &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ WITH GRANT OPTION ]GRANT &#123; &#123; USAGE | SELECT | UPDATE &#125; [,...] | ALL [ PRIVILEGES ] &#125; ON SEQUENCES TO &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ WITH GRANT OPTION ]GRANT &#123; EXECUTE | ALL [ PRIVILEGES ] &#125; ON FUNCTIONS TO &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ WITH GRANT OPTION ]REVOKE [ GRANT OPTION FOR ] &#123; &#123; SELECT | INSERT | UPDATE | DELETE | TRUNCATE | REFERENCES | TRIGGER &#125; [,...] | ALL [ PRIVILEGES ] &#125; ON TABLES FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]REVOKE [ GRANT OPTION FOR ] &#123; &#123; USAGE | SELECT | UPDATE &#125; [,...] | ALL [ PRIVILEGES ] &#125; ON SEQUENCES FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]REVOKE [ GRANT OPTION FOR ] &#123; EXECUTE | ALL [ PRIVILEGES ] &#125; ON FUNCTIONS FROM &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ CASCADE | RESTRICT ]Description ALTER DEFAULT PRIVILEGES allows you to set the privileges that will be applied to objects created in the future. (It does not affect privileges assigned to already-existing objects.) Currently, only the privileges for tables (including views), sequences, and functions can be altered.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 禁止表上数据更新或删除的方法 ","slug":"20120810153010","date":"2012-08-10T07:30:10.000Z","updated":"2018-09-04T01:33:58.228Z","comments":true,"path":"20120810153010.html","link":"","permalink":"https://postgres.fun/20120810153010.html","excerpt":"","text":"在数据库维护过程中，有时候有这么一种需求，对指定表只允许查询和插入操作，而不允许修改和删除，一般来说，这种需求比较少见，但是在数据迁移，或者数据表维护时会有这样的需求；最近有个项目做数据迁移时就碰到了这种需求，此文主要介绍下在 PostgreSQL 中实现上述需求的方法，其实现方式有两种。 方法一: 创建规则在 PostgreSQL 中支持在 table 或者 view 上创建规则，实现命令的转换，简单的说当对表上执行操作时，可以转换成其它指定的命令; 解释起来破为费力，下面的例子将有助于理解。 1.1 创建测试表123456789101112131415161718192021francs=&gt; create table test_rule (id int4 primary key,name varchar(32)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_rule_pkey\" for table \"test_rule\" CREATE TABLEfrancs=&gt; insert into test_rule select generate_series(1,10),'a'; INSERT 0 10francs=&gt; select * from test_rule; id | name ----+------ 1 | a 2 | a 3 | a 4 | a 5 | a 6 | a 7 | a 8 | a 9 | a 10 | a (10 rows) 1.2 创建 update 规则1create or replace rule rul_test_rule_update as on update to test_rule do instead nothing; 备注：上面命令意思为在表 test_rule 上创建了一个规则，当在这张表上执行 update 操作时，啥也不干。 1.3 update 测试1234567891011121314francs=&gt; select * From test_rule where id=1; id | name ----+------ 1 | a (1 row)francs=&gt; update test_rule set name='aaa' where id=1; UPDATE 0francs=&gt; select * From test_rule where id=1; id | name ----+------ 1 | a (1 row) 备注：在表 test_rule 上创建了 update 规则后，那么之后在这张表上执行 update 操作时，实际上什么命令都没执行，但也没抛出 ERROR。 1.4 同理创建 delete 规则12345678910111213141516francs=&gt; create or replace rule rul_test_rule_delete as on delete to test_rule do instead nothing; CREATE RULEfrancs=&gt; \\d test_rule Table \"francs.test_rule\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null name | character varying(32) | Indexes: \"test_rule_pkey\" PRIMARY KEY, btree (id) Rules: rul_test_rule_delete AS ON DELETE TO test_rule DO INSTEAD NOTHING rul_test_rule_update AS ON UPDATE TO test_rule DO INSTEAD NOTHING 备注：同理，在表 test_rule 上创建 delete 规则，当在表 test_rule 上执行 delete 操作时，啥也不干。 1.5 delete 测试1234567891011121314francs=&gt; select * From test_rule where id=1; id | name ----+------ 1 | a (1 row)francs=&gt; delete from test_rule where id=1; DELETE 0francs=&gt; select * From test_rule where id=1; id | name ----+------ 1 | a (1 row) 备注：果然禁止了表 test_rule 上的 delete 操作。 上面是通过在表创建规则的方法，实现禁止表上更新和删除操作，关于创建规则的语法可参考本文末尾。接下来看下另外一种方法， 方法二: 创建触发器2.1 创建测试表123456789101112131415161718192021francs=&gt; create table test_trigger(id int4 primary key ,name varchar(32)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_trigger_pkey\" for table \"test_trigger\" CREATE TABLEfrancs=&gt; insert into test_trigger select generate_series(1,10),'b'; INSERT 0 10francs=&gt; select * from test_trigger; id | name ----+------ 1 | b 2 | b 3 | b 4 | b 5 | b 6 | b 7 | b 8 | b 9 | b 10 | b (10 rows) 2.2 创建触发器函数12345678CREATE OR REPLACE FUNCTION func_test_trigger() RETURNS trigger LANGUAGE plpgsql AS $function$ BEGIN RAISE EXCEPTION 'Attention: can not update or delete table test_trigger,Please contact francs !'; END; $function$; 2.3 创建 UPDATE 和 DELETE 触发器123456789101112131415create trigger trigger_test_trigger_update BEFORE UPDATE ON test_trigger FOR EACH ROW EXECUTE PROCEDURE func_test_trigger(); create trigger trigger_test_trigger_delete BEFORE DELETE ON test_trigger FOR EACH ROW EXECUTE PROCEDURE func_test_trigger();francs=&gt; \\d test_trigger Table \"francs.test_trigger\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null name | character varying(32) | Indexes: \"test_trigger_pkey\" PRIMARY KEY, btree (id) Triggers: trigger_test_trigger_delete BEFORE DELETE ON test_trigger FOR EACH ROW EXECUTE PROCEDURE func_test_trigger() trigger_test_trigger_update BEFORE UPDATE ON test_trigger FOR EACH ROW EXECUTE PROCEDURE func_test_trigger() 备注：在表 test_trigger 上创建 update 和 delete 两个触发器。 2.4 update 测试1234567891011121314francs=&gt; select * from test_trigger where id=1; id | name ----+------ 1 | b (1 row)francs=&gt; update test_trigger set name='bbb' where id=1; ERROR: Attention: can not update or delete table test_trigger,Please contact francs ! francs=&gt; select * from test_trigger where id=1; id | name ----+------ 1 | b (1 row) 2.5 delete 测试1234567891011121314francs=&gt; select * from test_trigger where id=1; id | name ----+------ 1 | b (1 row) francs=&gt; delete from test_trigger where id=1; ERROR: Attention: can not update or delete table test_trigger,Please contact francs !francs=&gt; select * from test_trigger where id=1; id | name ----+------ 1 | b (1 row) 备注：在表 test_trigger 上创建了 update/delete trigger 后，之后再去 update 或者 delete 表数据，将抛出 ERROR。 参考 http://www.postgresql.org/docs/9.2/static/sql-createrule.html http://www.postgresql.org/docs/9.2/static/sql-createtrigger.html 附一 CREATE RULE NameCREATE RULE – define a new rewrite ruleSynopsisCREATE [ OR REPLACE ] RULE name AS ON event TO table [ WHERE condition ] DO [ ALSO | INSTEAD ] { NOTHING | command | ( command ; command … ) } 附二 CREATE TRIGGER NameCREATE TRIGGER – define a new triggerSynopsisCREATE [ CONSTRAINT ] TRIGGER name { BEFORE | AFTER | INSTEAD OF } { event [ OR … ] } ON table [ FROM referenced_table_name ] { NOT DEFERRABLE | [ DEFERRABLE ] { INITIALLY IMMEDIATE | INITIALLY DEFERRED } } [ FOR [ EACH ] { ROW | STATEMENT } ] [ WHEN ( condition ) ] EXECUTE PROCEDURE function_name ( arguments )where event can be one of: INSERT UPDATE [ OF column_name [, … ] ] DELETE TRUNCATE","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"RHEL6学习： 自动映射网络文件系统 ( /etc/auto.master )","slug":"20120804213727","date":"2012-08-04T13:37:27.000Z","updated":"2018-09-04T01:33:58.165Z","comments":true,"path":"20120804213727.html","link":"","permalink":"https://postgres.fun/20120804213727.html","excerpt":"","text":"今天学习了NFS客户端挂载的方法，关于 NFS 服务端的搭建之前写了BLOG https://postgres.fun/20120306091612.html ，这里主要介绍 NFS 客户端映射网络NFS目录的方法，这种自动挂并不通过 /etc/fstab 进行。 环境信息NFS Server 192.168.1.36 RHEL6.2 ( 主机名：redhatB )NFS Client 192.168.1.35 RHEL6.2 ( 主机名：redhat6 ) NFS 服务端搭建之前的 BLOG 里详细介绍了了 NFS 服务搭建，为了实验需要，这里简单介绍下步骤。 2.1 修改 /etc/exports，添加以下1/nfs 192.168.0.0/16(ro,sync) 2.2 创建共享目录并创建文件123456mkdir -p /nfs [root@redhatB ~]# cd /nfs [root@redhatB nfs]# touch a.txt [root@redhatB nfs]# ll total 4 -rw-r--r--. 1 root root 5 Aug 4 21:00 a.txt 2.3 开启 nfs 服务123456789[root@redhatB nfs]# service nfs restart Shutting down NFS mountd: [ OK ] Shutting down NFS daemon: [ OK ] Shutting down NFS quotas: [ OK ] Shutting down NFS services: [ OK ] Starting NFS services: [ OK ] Starting NFS quotas: [ OK ] Starting NFS daemon: [ OK ] Starting NFS mountd: [ OK ] 2.4 设置 NFS 开机自启动1[root@redhatB nfs]# chkconfig nfs on NFS 客户端配置3.1 开启 autofs 服务123[root@redhatB nfs]# service autofs restart Stopping automount: [ OK ] Starting automount: [ OK ] 3.2 设置 autofs 开机自启动1[root@redhatB nfs]# chkconfig autofs on 备注：autofs 服务可以按需挂载 NFS 文件系统，不活动时间间隔为 5 分钟，这个配置在文件 /etc/sysconfig/autofs 中的参数 TIMEOUT 参数设置。 3.3 查看 NFS 服务端共享的目录列表123[root@redhat6 nfs]# showmount -e 192.168.1.36 Export list for 192.168.1.36: /nfs 192.168.0.0/16 备注：上面只是准备工作，真正的映射步骤如下。 方法一：使用 /net 目录访问 NFS 文件系统当 NFS Client 端开启 autofs 服务时，有个特殊映射目录 /net，平常这个目录为空，当访问 /nfs/nfsserver.domain 时，会自动创建目录并 mount，并显示 NFS 服务共享目录的文件，如下所示： 4.1 进入 /net 目录12345678910[root@redhat6 net]# cd /net [root@redhat6 net]# ls[root@redhat6 net]# cd 192.168.1.36 [root@redhat6 192.168.1.36]# ll total 0 drwxr-xr-x. 2 root root 0 Aug 4 20:43 nfs[root@redhat6 192.168.1.36]# cd nfs [root@redhat6 nfs]# ll total 4 -rw-r--r--. 1 root root 5 Aug 4 21:00 a.txt 备注：开始进入 /net 目录显示为空，当进入相应 NFS 服务器目录时，自动 mount NFS 文件系统。 方法二：自定义目录访问 NFS 文件系统除了使用 /net 目录映射 NFS 文件系统，还可以自定义目录映射 NFS 文件系统，这些配置需要在配置文件 /etc/auto.master 进行配置。 5.1 修改 /etc/auto.master 配置文件 ( NFS 客户端上操作)修改配置文件，增加最后一行，如下：1234567# Sample auto.master file # This is an automounter map and it has the following format # key [ -mount-options-separated-by-comma ] location # For details of the format look at autofs(5). # /misc /etc/auto.misc /francs /etc/auto.francs 备注：配置两项，第一项为挂载的目录，第二项为挂载的配置文件，即 NFS 映射匹配表。 5.2 复制配置文件 /etc/auto.misc1[root@redhat6 nfs]# cp /etc/auto.misc /etc/auto.francs 5.3 修改配置文件 /etc/auto.francs 增加以下内容1test_nfs -ro,soft,intr 192.168.1.36:/nfs 备注：在配置文件 /etc/auto.francs 中定义明细的 nfs 文件系统挂载点。 5.4 创建新的挂载目录1[root@redhat6 nfs]# mkdir -p /francs 5.5 重启 autofs1234567[root@redhat6 francs]# service autofs stop Stopping automount: [ OK ] [root@redhat6 francs]# cd /net [root@redhat6 net]# ls [root@redhat6 net]# cd / [root@redhat6 /]# service autofs start Starting automount: [ OK ] 5.6 测试12345[root@redhat6 /]# cd /francs [root@redhat6 francs]# ls [root@redhat6 francs]# cd test_nfs [root@redhat6 test_nfs]# ls a.txt 备注：在新的目录 /francs 下可以访问 nfs 文件系统了。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"RHEL6 学习：使用 Cryptsetup 给分区加密","slug":"20120729204607","date":"2012-07-29T12:46:07.000Z","updated":"2018-09-04T01:33:58.087Z","comments":true,"path":"20120729204607.html","link":"","permalink":"https://postgres.fun/20120729204607.html","excerpt":"","text":"今天学习了 RHEL 对硬盘分区加密的知识，在 RHEL 系统里可以通过使用 cryptsetup 工具对硬盘分区进行加密，加密后的分区需要输入密码才能打开，可以把比较敏感的文件放在指定分区中，并启用加密，从而增强了文件的安全性，下面演示下。 Cryptsetup 给分区加密1.1 增加分区1234567891011121314151617181920212223242526272829303132333435363738394041[root@redhatB ~]# fdisk -cu /dev/sdcCommand (m for help): pDisk /dev/sdc: 10.7 GB, 10737418240 bytes 255 heads, 63 sectors/track, 1305 cylinders, total 20971520 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0xb097ae92 Device Boot Start End Blocks Id System /dev/sdc1 63 4209029 2104483+ 8e Linux LVM /dev/sdc2 4209030 8418059 2104515 8e Linux LVM /dev/sdc3 8418060 12627089 2104515 8e Linux LVM /dev/sdc4 12627090 20964824 4168867+ 5 Extended /dev/sdc5 12627153 14747669 1060258+ 8e Linux LVM /dev/sdc6 14747733 16868249 1060258+ 8e Linux LVMCommand (m for help): n First sector (16870298-20964824, default 16870298): Using default value 16870298 Last sector, +sectors or +size&#123;K,M,G&#125; (16870298-20964824, default 20964824): +1GCommand (m for help): pDisk /dev/sdc: 10.7 GB, 10737418240 bytes 255 heads, 63 sectors/track, 1305 cylinders, total 20971520 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0xb097ae92 Device Boot Start End Blocks Id System /dev/sdc1 63 4209029 2104483+ 8e Linux LVM /dev/sdc2 4209030 8418059 2104515 8e Linux LVM /dev/sdc3 8418060 12627089 2104515 8e Linux LVM /dev/sdc4 12627090 20964824 4168867+ 5 Extended /dev/sdc5 12627153 14747669 1060258+ 8e Linux LVM /dev/sdc6 14747733 16868249 1060258+ 8e Linux LVM /dev/sdc7 16870298 18967449 1048576 83 LinuxCommand (m for help): w The partition table has been altered!Calling ioctl() to re-read partition table.WARNING: Re-reading the partition table failed with error 16: Device or resource busy. The kernel still uses the old table. The new table will be used at the next reboot or after you run partprobe(8) or kpartx(8) Syncing disks. 备注：上例增加了分区 /dev/sdc7，大小为 1 GB。 1.2 刷新kernel12345678910111213141516171819202122[root@redhatB ~]# partx -a /dev/sdc BLKPG: Device or resource busy error adding partition 1 BLKPG: Device or resource busy error adding partition 2 BLKPG: Device or resource busy error adding partition 3 BLKPG: Device or resource busy error adding partition 4 BLKPG: Device or resource busy error adding partition 5 BLKPG: Device or resource busy error adding partition 6[root@redhatB ~]# ll /dev/sdc* brw-rw----. 1 root disk 8, 32 Jul 29 20:00 /dev/sdc brw-rw----. 1 root disk 8, 33 Jul 22 20:51 /dev/sdc1 brw-rw----. 1 root disk 8, 34 Jul 22 20:51 /dev/sdc2 brw-rw----. 1 root disk 8, 35 Jul 22 20:51 /dev/sdc3 brw-rw----. 1 root disk 8, 36 Jul 22 20:51 /dev/sdc4 brw-rw----. 1 root disk 8, 37 Jul 22 20:51 /dev/sdc5 brw-rw----. 1 root disk 8, 38 Jul 22 20:51 /dev/sdc6 brw-rw----. 1 root disk 8, 39 Jul 29 20:01 /dev/sdc7 备注：使用命令 partx 刷新 kernel，使系统能读到新增分区 /dev/sdc7。 1.3 对分区进行加密，并设置密码1234567[root@redhatB ~]# cryptsetup luksFormat /dev/sdc7WARNING! ======== This will overwrite data on /dev/sdc7 irrevocably.Are you sure? (Type uppercase yes): YES Enter LUKS passphrase: Verify passphrase: 备注：关于 cryptsetup 命令的用法，可以 man 下，这里关键选项”luksFormat”，注意大小写。 1.4 输入密码，打开分区1234567[root@redhatB ~]# cryptsetup luksOpen /dev/sdc7 secret Enter passphrase for /dev/sdc7: No key available with this passphrase. Enter passphrase for /dev/sdc7: You have new mail in /var/spool/mail/root[root@redhatB ~]# ll /dev/mapper/secret lrwxrwxrwx. 1 root root 7 Jul 29 20:06 /dev/mapper/secret -&gt; ../dm-3 备注：成功打开分区后，将分区映射成 /dev/mapper/secret，这里关键选项”luksOpen”，注意大小写。 1.5 格式化分区123456789101112131415161718192021[root@redhatB ~]# mke2fs -t ext4 /dev/mapper/secret mke2fs 1.41.12 (17-May-2010) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 65408 inodes, 261632 blocks 13081 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=268435456 8 block groups 32768 blocks per group, 32768 fragments per group 8176 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376Writing inode tables: done Creating journal (4096 blocks): done Writing superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 31 mounts or 180 days, whichever comes first. Use tune2fs -c or -i to override. 1.6 挂载123456789101112[root@redhatB ~]# mkdir -p /mnt/secret [root@redhatB ~]# mount -t ext4 /dev/mapper/secret /mnt/secret/ [root@redhatB ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/vg_redhatb-lv_root 9.9G 3.6G 5.9G 38% / tmpfs 250M 264K 250M 1% /dev/shm /dev/sda1 485M 31M 429M 7% /boot /dev/sdb 9.9G 330M 9.1G 4% /pgdata_xc /dev/mapper/vg1-pgdata1 1008M 34M 924M 4% /database/pgdata1 /dev/mapper/secret 1006M 18M 938M 2% /mnt/secret 备注：/mnt/secret 目录挂载成功。 1.7 写入文件测试1234[root@redhatB ~]# cd /mnt/secret [root@redhatB secret]# history &gt; history.txt [root@redhatB secret]# ls history.txt lost+found 1.8 查看加密分区映射对应的分区。123456789[root@redhatB mnt]# cryptsetup status secret /dev/mapper/secret is active and is in use. type: LUKS1 cipher: aes-cbc-essiv:sha256 keysize: 256 bits device: /dev/sdc7 offset: 4096 sectors size: 2093056 sectors mode: read/write 使用 Cryptsetup 关闭分区2.1 umount123[root@redhatB ~]# umount /mnt/secret [root@redhatB ~]# ll /dev/mapper/secret lrwxrwxrwx. 1 root root 7 Jul 29 20:06 /dev/mapper/secret -&gt; ../dm-3 2.2 关闭分区123[root@redhatB ~]# cryptsetup luksClose /dev/mapper/secret[root@redhatB ~]# ll /dev/mapper/secret[root@redhatB ~]# ll /dev/mapper/secret ls: cannot access /dev/mapper/secret: No such file or directory 备注：这步可以理解成删除之前映射的分区 /dev/mapper/secret，这里选项关键字”luksClose”，注意大小写；关闭分区后，之前映射在文件 /dev/mapper/secret 已经不存在了。 总结本文演示了RHEL 对硬盘分区进行加密，mount，并且 umount 的过程，其中还可以设定密码文件实现开机自动 mount， 这里不演示了。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"PostgreSQL: 如何判断字符串中是否包含指定字符","slug":"20120726140528","date":"2012-07-26T06:05:28.000Z","updated":"2018-09-04T01:33:58.024Z","comments":true,"path":"20120726140528.html","link":"","permalink":"https://postgres.fun/20120726140528.html","excerpt":"","text":"今天有开发人员问到： PostgreSQL 中是否有函数可以判断一个字符串中是否包含指定字符，如果包含则返回 ture ，否则返回 false，例如，如果字符串 ‘abcde’ 中包含 ‘ab’ 则返回 true，于是想了想，共总结以下三种方法，暂且不考虑性能。 创建测试表12345678910111213141516171819202122232425262728293031323334francs=&gt; create table test_compare(id serial primary key, name varchar(32)); NOTICE: CREATE TABLE will create implicit sequence \"test_compare_id_seq\" for serial column \"test_compare.id\" NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_compare_pkey\" for table \"test_compare\" CREATE TABLEfrancs=&gt; insert into test_compare(name) values ('a'); INSERT 0 1 francs=&gt; insert into test_compare(name) values ('ab'); INSERT 0 1 francs=&gt; insert into test_compare(name) values ('abc'); INSERT 0 1 francs=&gt; insert into test_compare(name) values ('bcd'); INSERT 0 1 francs=&gt; insert into test_compare(name) values ('bcde'); INSERT 0 1 francs=&gt; insert into test_compare(name) values ('bcde123'); INSERT 0 1 francs=&gt; insert into test_compare(name) values ('bcde12'); INSERT 0 1 francs=&gt; insert into test_compare(name) values ('cdef'); INSERT 0 1francs=&gt; select * from test_compare; id | name ----+--------- 1 | a 2 | ab 3 | abc 4 | bcd 5 | bcde 6 | bcde123 7 | bcde12 8 | cdef (8 rows) 备注：上面创建了测试表并插入测试数据，目标找出 name 字段中包含字符串”bc” 的记录。 方法一: 使用 position 函数1.1 使用 position 函数123456789francs=&gt; select * from test_compare where position('bc' in name) &gt;0; id | name ----+--------- 3 | abc 4 | bcd 5 | bcde 6 | bcde123 7 | bcde12 (5 rows) 1.2 position 函数简价 Function: position(substring in string)Return Type: intDescription: Location of specified substring 1.3 example12345francs=&gt; select position('bc' in 'bcd'); position ---------- 1 (1 row) 备注： position 函数是找出一个字符串在另一个字符串中出现的位置，如果找不到则返回整型 0。 方法二: 使用正责表达式2.1 使用正责表达式123456789francs=&gt; select * from test_compare where name ~ 'bc'; id | name ----+--------- 3 | abc 4 | bcd 5 | bcde 6 | bcde123 7 | bcde12 (5 rows) 备注：上面使用的是正责表达式，关于正责表达式的更多用法，请参考手册 http://www.postgresql.org/docs/9.2/static/functions-matching.html 也可以参考之前的 blog https://postgres.fun/20110322134848.html 方法三: 使用矩阵比较函数 @&gt;3.1 使用矩阵比较函数 @&gt;123456789francs=&gt; select * from test_compare where regexp_split_to_array(name,'') @&gt; array['b','c']; id | name ----+--------- 3 | abc 4 | bcd 5 | bcde 6 | bcde123 7 | bcde12 (5 rows) 备注：方法三使用了字符串函数 regexp_split_to_array 和矩阵操作符 @&gt; 3.2 regexp_split_to_array 函数简介 regexp_split_to_array 函数用于将字符串转换成矩阵，用户如下 Function: regexp_split_to_array(string text, pattern text [, flags text ])Return Type: text[]Description: Split string using a POSIX regular expression as the delimiter. See Section 9.7.3 for more information. 3.3 example1234567891011francs=&gt; select regexp_split_to_array('hello world',' '); regexp_split_to_array ----------------------- &#123;hello,world&#125; (1 row)francs=&gt; select regexp_split_to_array('hello',''); regexp_split_to_array ----------------------- &#123;h,e,l,l,o&#125; (1 row) 3.4 矩阵 @&gt; 操作符简介 Operator: @&gt;Description: containsExample: ARRAY[1,4,3] @&gt; ARRAY[3,1]Result: t 3.5 example1234567891011francs=&gt; select ARRAY[1,4,3] @&gt; ARRAY[3,1]; ?column? ---------- t (1 row)francs=&gt; select ARRAY[1,4,3] @&gt; ARRAY[3,1,5]; ?column? ---------- f (1 row) 备注：@&gt; 操作符是用来比较矩阵数据是否包含，关于矩阵操作符和函数，参考手册http://www.postgresql.org/docs/9.2/static/functions-array.html。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 使用 Pgfincore 预加载数据优化一列 ","slug":"20120724133839","date":"2012-07-24T05:38:39.000Z","updated":"2018-09-04T01:33:57.962Z","comments":true,"path":"20120724133839.html","link":"","permalink":"https://postgres.fun/20120724133839.html","excerpt":"","text":"最近有个新项目业务上升较快，数据库的压力也是直线上升，日最高负载上升到 100 左右，日平均负载上升到 25 左右，这个负载已经相当的高了，出现这种情况通常是由于少数 SQL 语句性能较差，需要从 SQL 调优着手，目前这个应用需要优化的地方很多，比如有些走索引的 count 语句并没有做到前台应用缓存，以及部分 SQL 需要从业务角度进行逻辑优化，但下一个应用版本业务方没有这么快能够上线，所以针对这种情况，只能从数据库端进行优化，寻找解决问题的方法。 前面已经说过这个业务近期数据量增长比较快，数据库内存已经不够缓存业务数据了，数据库内存 24 GB，核心业心业务表数据（ 24 GB） 左右，核心数据索引 30 GB 左右，数据库内存已经没有能力完全缓存这部分数据和索引，但是目压力最大的一个 SQL 为新注册用户发起，新用户注册上线即会触发这个 SQL，而新用户查找的数据从前台缓存（ memcache ） 无法命中，所以这部分查询落到数据库中实时查询，故数据库压力会很大，频繁出现数据库负载的超过 100 的尴尬局面。 经过分析，根据 iostat 命令输出，发现其中一块盘，读非常频繁，远远大于写，而数据库的数据大部分落在这个盘，此时也说明内存已经不够了，这时应该给数据库内存扩容了。 最后，给数据库内存扩容到 64 GB，之后并且用 pgfincore 缓存技术将核心业务表和常用索引预先加载到数据库缓存，这样可以大大提高新用户注册后查询数据库时 cache 的命中率(由于新用户注册时发出的 SQL ，并不能命中 memcache 前台缓存)，关于 pgfincore 的使用可以参考之前的 blog :https://postgres.fun/20100805161611.html ， 数据库内存扩容后，日最高负载大辐下降，日平均负载大大下降，如下图： 扩容前后日最高负载图备注：扩容日期 7-19，数据库扩容后，日最高负载由 100 降到 20 左右。 扩容前后日平均负载图备注：扩容日期 7-19， 数据库扩容后，日平均负载由 25 降到 2 左右。 扩容前的数据库负载备注：扩容前07-16 数据库的负载，上图的负载够猛的！ 扩容后的数据库负载 备注：上图为扩容后 7-20 号的数据库负载。 后续优化工作通过增加内存增加数据库性带来的效果确实不错，但这并不是解决问题的最优方法，目前这个项目需要优化的地方还很多，例如走索引的 count 语句做到前台memcache 中，以及应用逻辑的优化等，但通过增加内存并且利用 pgfincore 技术将业务数据提前缓存到 OS cache 中的做法确实能对特定场合产生极大的效应，例如当数据库的压力大部分由应用系统新用户注册发起时。","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"pgfincore","slug":"pgfincore","permalink":"https://postgres.fun/tags/pgfincore/"}]},{"title":"RHEL6: VNC 服务配置","slug":"20120722203804","date":"2012-07-22T12:38:40.000Z","updated":"2018-09-04T01:33:57.915Z","comments":true,"path":"20120722203804.html","link":"","permalink":"https://postgres.fun/20120722203804.html","excerpt":"","text":"RHEL6 VNC 服务配置较 5 版本的简单，5 版本默认不开启 VNC 的图形化界面，需要手工更改配置才可以，而RHEL6 版本则不需要，以下为配置过程。 环境信息VNC RHEL6 服务端： 192.168.1.35 主机名 redhat6VNC RHEL6 客户端： 192.168.1.36 主机名 redhatBVNC Windows 客户端： 192.168.1.55 主机名 xxxxxxx VNC 服务安装VNC 服务所需的包为 tigervnc.i686，LINUX 客户端需要的包为 tigervnc.i686。 1.1 tigervnc 包安装123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@redhat6 ~]# yum search tigervncLoaded plugins: product-id, refresh-packagekit, security, subscription-managerUpdating certificate-based repositories.====================================================== N/S Matched: tigervnc =======================================================tigervnc.i686 : A TigerVNC remote display systemtigervnc-server.i686 : A TigerVNC server Name and summary matches only, use \"search all\" for everything.[root@redhat6 ~]# yum install tigervnc.i686 tigervnc-server.i686Loaded plugins: product-id, refresh-packagekit, security, subscription-managerUpdating certificate-based repositories.Setting up Install ProcessResolving Dependencies--&gt; Running transaction check---&gt; Package tigervnc.i686 0:1.0.90-0.17.20110314svn4359.el6 will be installed---&gt; Package tigervnc-server.i686 0:1.0.90-0.17.20110314svn4359.el6 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved==================================================================================================================================== Package Arch Version Repository Size====================================================================================================================================Installing: tigervnc i686 1.0.90-0.17.20110314svn4359.el6 my_repo 268 k tigervnc-server i686 1.0.90-0.17.20110314svn4359.el6 my_repo 1.1 MTransaction Summary====================================================================================================================================Install 2 Package(s)Total download size: 1.4 MInstalled size: 3.6 MIs this ok [y/N]: yDownloading Packages:------------------------------------------------------------------------------------------------------------------------------------Total 12 MB/s | 1.4 MB 00:00Running rpm_check_debugRunning Transaction TestTransaction Test SucceededRunning TransactionWarning: RPMDB altered outside of yum. Installing : tigervnc-1.0.90-0.17.20110314svn4359.el6.i686 1/2 Installing : tigervnc-server-1.0.90-0.17.20110314svn4359.el6.i686 2/2Installed products updated.Installed: tigervnc.i686 0:1.0.90-0.17.20110314svn4359.el6 tigervnc-server.i686 0:1.0.90-0.17.20110314svn4359.el6Complete!You have new mail in /var/spool/mail/root VNC 服务配置2.1 修改配置文件 /etc/sysconfig/vncservers在 vncserver 配置文件 /etc/sysconfig/vncservers 中添加以下内容123VNCSERVERS=\"1:postgres 2:usera\"VNCSERVERARGS[1]=\"-geometry 800x600\"VNCSERVERARGS[2]=\"-geometry 800x600\" 2.2 postgres 用户设置 vnc 密码1234[root@redhat6 ~]# su - postgres[postgres@redhat6 ~]$ vncpasswdPassword:Verify: 2.3 usera 用户设置 vnc 密码1234[root@redhat6 ~]# su - usera[usera@redhat6 ~]$ vncpasswdPassword:Verify: 2.4 开启 vncserver 服务123456789101112[root@redhat6 ~]# service vncserver startStarting VNC server: 1:postgres xauth: creating new authority file /home/postgres/.XauthorityNew 'redhat6:1 (postgres)' desktop is redhat6:1Creating default startup script /home/postgres/.vnc/xstartupStarting applications specified in /home/postgres/.vnc/xstartupLog file is /home/postgres/.vnc/redhat6:1.log2:usera xauth: creating new authority file /home/usera/.XauthorityNew 'redhat6:2 (usera)' desktop is redhat6:2Creating default startup script /home/usera/.vnc/xstartupStarting applications specified in /home/usera/.vnc/xstartupLog file is /home/usera/.vnc/redhat6:2.log[ OK ] 也可以如下开启1234[root@rhedhat6 soft_bak]# vncserverNew 'rhedhat6:1 (root)' desktop is rhedhat6:1Starting applications specified in /root/.vnc/xstartupLog file is /root/.vnc/rhedhat6:1.log 2.4 查看 vncserver 进程123456[root@redhat6 ~]# ps -ef | grep vncpostgres 10903 1 1 19:46 ? 00:00:00 /usr/bin/Xvnc :1 -desktop redhat6:1 (postgres) -auth /home/postgres/.Xauthority -geometry 800x600 -rfbwait 30000 -rfbauth /home/postgres/.vnc/passwd -rfbport 5901 -fp catalogue:/etc/X11/fontpath.d -pnpostgres 10909 10908 0 19:46 ? 00:00:00 vncconfig -iconicusera 10938 1 1 19:46 ? 00:00:00 /usr/bin/Xvnc :2 -desktop redhat6:2 (usera) -auth /home/usera/.Xauthority -geometry 800x600 -rfbwait 30000 -rfbauth /home/usera/.vnc/passwd -rfbport 5902 -fp catalogue:/etc/X11/fontpath.d -pnusera 11020 11019 0 19:46 ? 00:00:00 vncconfig -iconicroot 11190 9281 1 19:46 pts/1 00:00:00 grep vnc 备注：为了测试方便，暂时把防火墙关了，如果启用了防火墙，则需要打开 5901 和 5902 端口。 通过 Windows 客户端连接测试3.1 打开VNC 客户端 备注：注意IP地址后面的数字，由于计划使用 postgres 用户登陆，所以输入“192.168.1.35：1”，启用 1 号实例。 3.2 输入密码 备注：postgres 用户 vnc 登陆成功，usea 也登陆成功，不详细演示。 通过 RHEL6 客户端主机 RedhatB 测试4.1 在另一台 RHEL6 机器上测试在另一台 RHEL6 虚拟机图形化界面测试，执行以下命令，vncviewer 命令需要安装包tigervnc.i686中，如果找不到这条命令，说明此包没安装。1[root@redhatB ~]# vncviewer 192.168.1.35:1 备注：上面命令输入密码后，可以开启图形化界面，如果想同时让多个 client 端以图形化界面连接，则需要加入-Shared 参数；如果只想让客户端只读，而不能操作，则需要加上 -ViewOnly 参数。 4.2 以加密的方式开启VNC 连接由于 VNC 是明文的网络协议，通信容易被窃听； vncviewer 命令提供选项 -via user@host， 该选项先以 user 身份 ssh 到 host 上的 SSH 服务，然后再尝试从 host 主机连接到 VNC 服务器，这样 VNC 将通过 SSH 隧道的方式传递信息，要实现这样的目标，需要修改配置 ，只允许 localhost 连接 VNC。修改配置 /etc/sysconfig/vncservers12VNCSERVERS=\"1:postgres 2:usera\"VNCSERVERARGS[1]=\"-geometry 800x600 -localhost\" 备注：配置修改后，重启 VNC 服务。 从主机 redhatB 机器尝试连接备注：此时直接通过 VNC 连接 192.168.1.35:1 行不通。 通过 ssh 加密方式连接 VNC备注：通过以上 ssh 加密的 vncviewer 则可以开启连接。 总结VNC 服务能够非常方便开启图形化连接，一般在生产维护过程中很少用到图形化界面，但在某些情况则很会用到，例如 Oracle 数据库的安装等，因此在特定的时候通过 VNC 服务开启图形化连接是非常必要而且非常方便的。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"VNC","slug":"VNC","permalink":"https://postgres.fun/tags/VNC/"}]},{"title":"RHEL6: 使用 SSH 密钥 ","slug":"20120722180026","date":"2012-07-22T10:00:26.000Z","updated":"2018-09-04T01:33:57.853Z","comments":true,"path":"20120722180026.html","link":"","permalink":"https://postgres.fun/20120722180026.html","excerpt":"","text":"今天学习了 RHEL6 SSH 密钥的使用，大体上说和 RHEL5 生成 SSH 密钥 方法相同，稍微还是有点区别，演示如下。 1 环境信息IP: 192.168.1.36 主机名 redhat6IP: 192.168.1.35 主机名 redhatB备注：笔记本上跑了两台 RHEL6 虚拟机，目标在主机 redhat6 上能够 ssh postgres@192.168.1.36 并且不输入密码。 2 密钥生成以 postgres 用户登陆主机 redhat6，使用 ssh-keygen 生成公钥和私钥。123456789101112131415161718192021222324252627282930313233[postgres@redhat6 .ssh]$ cd ~/.ssh [postgres@redhat6 .ssh]$ pwd /home/postgres/.ssh [postgres@redhat6 .ssh]$ lltotal 4.0K -rw-r--r--. 1 postgres postgres 394 Jul 22 17:41 known_hosts[postgres@redhat6 .ssh]$ ssh-keygenGenerating public/private rsa key pair. Enter file in which to save the key (/home/postgres/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/postgres/.ssh/id_rsa. Your public key has been saved in /home/postgres/.ssh/id_rsa.pub. The key fingerprint is: a5:ed:54:bd:dd:2b:a8:b2:7e:bd:09:d7:5b:9d:40:29 [postgres@redhat6](mailto:postgres@redhat6) The keys randomart image is: +--[ RSA 2048]----+ | | | .. | | .E.o. | | + .o o.| | S o .. o| | o o . +| | .oo o +.| | . .+.. + | | .o+. o.. | +-----------------+ [postgres@redhat6 .ssh]$ lltotal 12K-rw-------. 1 postgres postgres 1.7K Jul 22 17:44 id_rsa-rw-r--r--. 1 postgres postgres 398 Jul 22 17:44 id_rsa.pub-rw-r--r--. 1 postgres postgres 394 Jul 22 17:41 known_hosts 备注：执行完 ssh-keygen 后，在目录 ~/.ssh/下产生 id_rsa 和 id_rsa.pub 密钥其中 id_rsa 为私钥，id_rs.pub 为公钥，可以将公钥分发到任何一台主机，从而 ssh 登陆时不用输入密码。 3 将公钥发送到主机 redhatB123456[postgres@redhat6 .ssh]$ ssh-copy-id -i id_rsa.pub postgres@192.168.1.36postgres@192.168.1.36's password: Now try logging into the machine, with \"ssh 'postgres@192.168.1.36'\", and check in: .ssh/authorized_keysto make sure we haven't added extra keys that you weren't expecting. 备注：这里使用命令 ssh-copy-id 将公钥 id_rsa.pub 发送到目标主机，默认使用的是 ~/.id_rsa.pub 公钥，也可以使用 -i 参数指定密钥。 4 测试在主机 redhat6 上以 postgres 用户连接到 redhatB 上测试。123[postgres@redhat6 .ssh]$ ssh postgres@192.168.1.36[postgres@redhatB ~]$ hostname redhatB 备注：在主机 redhat6 上以 postgres 用户连接到 redhatB 上果然不需要输入密码。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"RHEL6 学习：文件特殊权限位学习( SUID、SGID、STICKY )","slug":"20120721213859","date":"2012-07-21T13:38:59.000Z","updated":"2018-09-04T01:33:57.790Z","comments":true,"path":"20120721213859.html","link":"","permalink":"https://postgres.fun/20120721213859.html","excerpt":"","text":"关于Linux 系统上文件的特殊权限( SUID、SGID、STICKY ) 之前也知道些，可是过段时间又忘了，今天再次学习了这方面的知识，好记心不如烂笔头，还是写下来比较好。 SUID 简介当一个可执行文件被设置了 SUID 属性时，其它用户执行此文件时，将以此文件的属主身份运行，设置方式为 “ chmod u+s file_name “，一个典型的例子就是 passwd 命令修改密码。我们知道 Linux 的用户可以执行 passwd 命令修改自己的密码，而用户的密码信息保存在文件 “etc/shadow”文件里，此文件的权限为 000，那为什么普通用户执行 passwd 命令修改密码时，也有权限写这个文件呢？ 1.1 查看 /etc/shadow 文件权限12[root@redhat6 test]# ll /etc/shadow ----------. 1 root root 1621 Jul 21 18:10 /etc/shadow 1.2 查看 passwd 脚本权限1234[root@redhat6 test]# which passwd /usr/bin/passwd [root@redhat6 test]# ll /usr/bin/passwd -rwsr-xr-x. 1 root root 26980 Jan 29 2010 /usr/bin/passwd 备注：原来脚本 /usr/bin/passwd 的权限已设置 SUID 属性 ，所以当普通用户执行 /usr/bin/passwd 命令修改密码时，实际上是以 root 的身份执行这个脚本。 1.3 SUID 和 SHELL 脚本 这里做个简单的测试，来体验一下给可执行文件标记GUID的效果，先用 root 用户创建一个脚本 1.sh，这个脚本的任务是查看属于 root 的一个日志文件，然后给 1.sh 加上 GUID， 目标给普通用户执行这个脚本1.sh 并查看本身没有读权限的文件。 1.3.1 查看 /test 目录权限123[root@redhat6 test]# cd / [root@redhat6 /]# ll -d test drwxrwxrwx. 4 root root 4096 Jul 22 22:05 test 备注： /test 目录权限为 777。 1.3.2 编写测试脚本 1.sh123[root@redhat6 test]# cat 1.sh #!/bin/bashcat /test/log.txt 1.3.3 测试脚本 1.sh 权限和 log.txt 权限12345[root@redhat6 test]# chmod 4755 1.sh [root@redhat6 test]# ll 1.sh -rwsr-xr-x. 1 root root 31 Jul 22 22:05 1.sh[root@redhat6 test]# ll log.txt -rwx------. 1 root root 53 Jul 22 21:56 log.txt 备注：脚本 1.sh 的属主为 root，脚本内容是查看 log.txt 的内容， log.txt 的权限为 700，只有 root 用户才能查看。 1.3.4 以 usera 用户登陆测试123456[root@redhat6 ~]# su - usera [usera@redhat6 ~]$ cd /test [usera@redhat6 test]$ ll 1.sh -rwsr-xr-x. 1 root root 31 Jul 22 22:05 1.sh [usera@redhat6 test]$ ./1.sh cat: /test/log.txt: Permission denied 备注：虽然脚本 1.sh 已经标记 GUID，但当脚本执行到 cat 命令时，仍然报权限问题。 1.3.5 查看 cat 命令权限1234[root@redhat6 test]# which cat /bin/cat [root@redhat6 test]# ll /bin/cat -rwxr-xr-x. 1 root root 47976 Oct 5 2011 /bin/cat 1.3.6 给 /bin/cat 脚本加上 SUID123[root@redhat6 test]# chmod u+s /bin/cat [root@redhat6 test]# ll /bin/cat -rwsr-xr-x. 1 root root 47976 Oct 5 2011 /bin/cat 1.3.7 切换到 usera 再次测试123456[usera@redhat6 test]$ ./1.sh 2012-07-22 21:33:22 2012-07-22 21:33:28 test 000 000 备注：给 /bin/cat 脚本加上 GUID 后，普通用户能够调用脚本 1.sh 查看 log.txt(700)的内容了。 SGID 简介SGID 一般在目录上设置，在设置 SGID 属性的目录里创建的文件的 GID 会被默认设置成上组目录的GID，设置方式为 “chmod g+s directory”，下面演示下。 2.1 创建用户12[root@redhat6 /]# useradd usera [root@redhat6 /]# useradd userb 2.2 创建目录123456789[root@redhat6 /]# mkdir -p /test/share_dir [root@redhat6 test]# ll -d share_dir drwxr-xr-x. 2 root root 4096 Jul 21 21:12 share_dir[root@redhat6 test]# chmod g+s share_dir[root@redhat6 test]# ll -d share_dir drwxr-sr-x. 2 root root 4096 Jul 21 21:12 share_dir[root@redhat6 test]# chmod 2777 share_dir [[root@redhat6 test]# ll -d share_dir drwxrwsrwx. 2 root root 4096 Jul 21 21:16 share_dir 备注：也可以一步到位，使用 “chmod 2777 share_dir” 命令修改目录权限。 2.3 usera 登陆并创建文件123456[root@redhat6 test]# su - usera [usera@redhat6 ~]$ cd /test/share_dir [usera@redhat6 share_dir]$ touch a.txt [usera@redhat6 share_dir]$ ll total 0 -rw-rw-r--. 1 usera root 0 Jul 21 21:15 a.txt 备注：usera 创建的文件的 GID 默认被改为了 root。 2.4 userb 登陆并创建文件1234567[root@redhat6 test]# su - userb [userb@redhat6 ~]$ cd /test/share_dir [userb@redhat6 share_dir]$ touch b.txt [userb@redhat6 share_dir]$ ll total 0 -rw-rw-r--. 1 usera root 0 Jul 21 21:15 a.txt -rw-rw-r--. 1 userb root 0 Jul 21 21:16 b.txt 备注：userb 创建的文件的 GID 默认被改为了 root。 STICKY 位STICKY 位主要是用来控制在共享目录下文件的删除权限的，假如在一个权限为 777 的共享目录下， usera 创建的文件可能被其它用户删除，显示这是不合理的，那么可以在这个目录下设置 STICKY 位，从而避免这种情况发生，典型的例子就是 /tmp 目录。 3.1 /tmp 目录权限123[userb@redhat6 share_dir]$ cd / [userb@redhat6 /]$ ll -d /tmp drwxrwxrwt. 14 root root 4096 Jul 21 20:10 /tmp 备注：在 /tmp 目录下，用户只能删除自己的文件，而不能删除其它用户的文件，接下来再做个测试。 3.2 创建目录12345678[root@redhat6 test]# cd /test [root@redhat6 test]# mkdir work_dir [root@redhat6 test]# ll -d work_dir drwxr-xr-x. 2 root root 4096 Jul 21 21:20 work_dir[root@redhat6 test]# chmod o+t work_dir/[root@redhat6 test]# chmod 1777 work_dir [root@redhat6 test]# ll -d work_dir drwxrwxrwt. 2 root root 4096 Jul 21 21:20 work_dir 备注：也可以使用命令 “chmod 1777 work_dir” 一步完成权限设置。 3.3 usera 创建文件12345678[root@redhat6 ~]# su - usera [usera@redhat6 ~]$ pwd /home/usera [usera@redhat6 ~]$ cd /test/work_dir [usera@redhat6 work_dir]$ touch a.txt [usera@redhat6 work_dir]$ ll total 0 -rw-rw-r--. 1 usera usera 0 Jul 21 21:23 a.txt 3.4 userb 创建文件1234567[root@redhat6 ~]# su - userb [userb@redhat6 ~]$ cd /test/work_dir [userb@redhat6 work_dir]$ touch b.txt [userb@redhat6 work_dir]$ ll total 0 -rw-rw-r--. 1 usera usera 0 Jul 21 21:23 a.txt -rw-rw-r--. 1 userb userb 0 Jul 21 21:23 b.txt 3.5 userb 尝试删除 usera 的文件123[userb@redhat6 work_dir]$ rm a.txt rm: remove write-protected regular empty file `a.txt'? y rm: cannot remove `a.txt': Operation not permitted 备注：当 userb 尝试删除目录 /test/work_dir 下 usera 的文件 a.txt 时，报权限问题。 参考 http://molinux.blog.51cto.com/2536040/469523 http://blog.chinaunix.net/space.php?uid=23370286&amp;do=blog&amp;id=2427160","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"PostgreSQL: Window Functions 初步使用 ","slug":"20120720211147","date":"2012-07-20T13:11:47.000Z","updated":"2018-09-04T01:33:57.712Z","comments":true,"path":"20120720211147.html","link":"","permalink":"https://postgres.fun/20120720211147.html","excerpt":"","text":"PostgreSQL 支持 Window Functions，可以对查询出的结果集进行分组处理，非常方便，接下来举个简单的例子演示下。 创建测试表创建一张成绩表，取各科目成绩最高的同学。123456789101112francs=&gt; create table score ( id serial primary key,subject varchar(32),stu_name varchar(64),score numeric(3,0) ); NOTICE: CREATE TABLE will create implicit sequence \"score_id_seq\" for serial column \"score.id\" NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"score_pkey\" for table \"score\" CREATE TABLEfrancs=&gt; \\d score Table \"francs.score\" Column | Type | Modifiers ----------+-----------------------+----------- subject | character varying(32) | stu_name | character varying(64) | score | numeric(3,0) | 插入测试数据1234567891011121314151617181920212223242526272829303132francs=&gt; insert into score ( subject,stu_name,score ) values ('Chinese','francs',70); INSERT 0 1 francs=&gt; insert into score ( subject,stu_name,score ) values ('English','francs',90); INSERT 0 1 francs=&gt; insert into score ( subject,stu_name,score ) values ('Math','francs',80); INSERT 0 1 francs=&gt; insert into score ( subject,stu_name,score ) values ('Chinese','fpzhou',70); INSERT 0 1 francs=&gt; insert into score ( subject,stu_name,score ) values ('English','fpzhou',75); INSERT 0 1 francs=&gt; insert into score ( subject,stu_name,score ) values ('Math','fpzhou',99); INSERT 0 1 francs=&gt; insert into score ( subject,stu_name,score ) values ('Chinese','tutu',80); INSERT 0 1 francs=&gt; insert into score ( subject,stu_name,score ) values ('English','tutu',60); INSERT 0 1 francs=&gt; insert into score ( subject,stu_name,score ) values ('Math','tutu',65); INSERT 0 1francs=&gt; select * from score; subject | stu_name | score ---------+----------+------- Chinese | francs | 70 English | francs | 90 Math | francs | 80 Chinese | fpzhou | 70 English | fpzhou | 75 Math | fpzhou | 99 Chinese | tutu | 80 English | tutu | 60 Math | tutu | 65 (9 rows) 方法一: 使用窗口函数12345678910111213select * from (select subject, stu_name, score, row_number() over(partition by subject order by score desc ) as score_desc from score) as order_score where score_desc &lt; 2 order by subject ;subject | stu_name | score | score_desc ---------+----------+-------+------------ Chinese | tutu | 80 | 1 English | francs | 90 | 1 Math | fpzhou | 99 | 1 (3 rows) 备注：这里使用了 windows function ，其中 “over(partition by subject order by score desc )”是 windows function 核心，”partition by ..”表示将结果集根据指定字段进行分组，上例中是将结果集根据 subject 进行分组； “order by ..” 是指将每组的结果集根据指定字段排序。 方法二: 使用嵌套查询12345678910111213select a.* from score a, (select subject, max(score) as score from score group by subject) b where a.subject = b.subject and a.score = b.score order by a.subject; id | subject | stu_name | score ----+---------+----------+------- 7 | Chinese | tutu | 80 2 | English | francs | 90 6 | Math | fpzhou | 99 (3 rows) 备注：暂且不考虑语句效率，仅实现功能。 在结果集中增加各科目平均成绩信息12345678910111213francs=&gt; select subject,stu_name,score,avg(score) over (partition by subject) from score; subject | stu_name | score | avg ---------+----------+-------+--------------------- Chinese | fpzhou | 70 | 73.3333333333333333 Chinese | francs | 70 | 73.3333333333333333 Chinese | tutu | 80 | 73.3333333333333333 English | fpzhou | 75 | 75.0000000000000000 English | francs | 90 | 75.0000000000000000 English | tutu | 60 | 75.0000000000000000 Math | francs | 80 | 81.3333333333333333 Math | tutu | 65 | 81.3333333333333333 Math | fpzhou | 99 | 81.3333333333333333 (9 rows) Row_number() 窗口函数12345678910111213francs=&gt; select subject,stu_name,score,row_number() over (partition by subject) from score; subject | stu_name | score | row_number ---------+----------+-------+------------ Chinese | fpzhou | 70 | 1 Chinese | francs | 70 | 2 Chinese | tutu | 80 | 3 English | fpzhou | 75 | 1 English | francs | 90 | 2 English | tutu | 60 | 3 Math | francs | 80 | 1 Math | tutu | 65 | 2 Math | fpzhou | 99 | 3 (9 rows) 备注：使用 row_number() over … 函数可以对结果集按某字段分组后的记录进行标记，因此使用 row_number() 窗口函数很容易实现取分组后指定记录的功能。 Rank() over 窗口函数12345678910111213francs=&gt; select subject,stu_name,score,rank() over (partition by subject order by score desc) from score; subject | stu_name | score | rank ---------+----------+-------+------ Chinese | tutu | 80 | 1 Chinese | francs | 70 | 2 Chinese | fpzhou | 70 | 2 English | francs | 90 | 1 English | fpzhou | 75 | 2 English | tutu | 60 | 3 Math | fpzhou | 99 | 1 Math | francs | 80 | 2 Math | tutu | 65 | 3 (9 rows) 备注: rank() 窗口函数和 ow_number() 窗口函数类似，但 rank() 窗口函数会将结果集分组后相同值的记录的标记相等，例如上例中红色的记录。 取行号取结果集行号，相当于Oracle 里的 rownum12345678910111213francs=&gt; select row_number() OVER (ORDER BY id) AS rownum ,* from score; rownum | id | subject | stu_name | score --------+----+---------+----------+------- 1 | 1 | Chinese | francs | 70 2 | 2 | English | francs | 90 3 | 3 | Math | francs | 80 4 | 4 | Chinese | fpzhou | 70 5 | 5 | English | fpzhou | 75 6 | 6 | Math | fpzhou | 99 7 | 7 | Chinese | tutu | 80 8 | 8 | English | tutu | 60 9 | 9 | Math | tutu | 65 (9 rows) 参考 http://www.postgresql.org/docs/9.2/static/tutorial-window.html http://www.postgresql.org/docs/9.2/static/functions-window.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Window Functions","slug":"Window-Functions","permalink":"https://postgres.fun/tags/Window-Functions/"}]},{"title":"PostgreSQL: Array 数组类型添加元素","slug":"20120719154745","date":"2012-07-19T07:47:45.000Z","updated":"2018-09-04T01:33:57.649Z","comments":true,"path":"20120719154745.html","link":"","permalink":"https://postgres.fun/20120719154745.html","excerpt":"","text":"今天在德哥 blog 中学习到一个不错的数组函数，可以批量对数组元素进行删除，原文链接 http://blog.163.com/digoal@126/blog/static/163877040201261273149437/，在这篇 blog 中 德哥新增了函数 multi_text_array_remove (i_src text[],i_remove text[]) 用来应对数组中多个元素删除的情况，例如数组 ARRAY[1,2,3,4,5] 如果要去掉一个元素，可以用 array_remove 函数 ( 这个函数在9.3 版本中才会有 )，但这个函数只能删除一个元素，如果要去除多个元素，则可调用函数 multi_text_array_remove Multi_text_array_remove 函数演示12345postgres=# select multi_text_array_remove(ARRAY['abc','a','c','d'], ARRAY['a','c','d']); multi_text_array_remove ------------------------- &#123;abc&#125; (1 row) 那么添加数组元素情况如何呢？在 PostgreSQL 中已经有函数 array_append 函数，但是这个函数只能一次添加一个元素，如果想添加多个，需要多次调用。 Array_append 函数演示123456789101112francs=&gt; \\df array_append List of functions Schema | Name | Result data type | Argument data types | Type ------------+--------------+------------------+----------------------+-------- pg_catalog | array_append | anyarray | anyarray, anyelement | normal (1 row)francs=&gt; select array_append(array[1,2,3],4); array_append -------------- &#123;1,2,3,4&#125; (1 row) 根据德哥的函数，依葫芦画瓢，这里写一个 int4[] 类型数组元素批量增加的函数 创建 multi_array_append_int4 函数1234567891011121314151617181920create or replace function multi_array_append_int4(i_src int4[],i_append int4[]) returns text[] AS $$ DECLARE v_text int4; v_result int4[]; BEGIN v_result := i_src; if i_append is null then return v_result; end if; foreach v_text in ARRAY i_append loop select array_append(v_result,v_text) into v_result; end loop; return v_result; END; $$ LANGUAGE 'plpgsql'; 备注： 其中 “foreach v_text in ARRAY i_append loop “ 代码是用来遍历数组中的每个元素，具体语法可参考本文末尾的附一。 multi_array_append_int4 函数测试 11234567891011121314151617francs=&gt; select multi_array_append_int4(array[1,2,3],array[4]); multi_array_append_int4 ------------------------- &#123;1,2,3,4&#125; (1 row)francs=&gt; select multi_array_append_int4(array[1,2,3],array[4,5]); multi_array_append_int4 ------------------------- &#123;1,2,3,4,5&#125; (1 row)francs=&gt; select multi_array_append_int4(array[1,2,3],null); multi_array_append_int4 ------------------------- &#123;1,2,3&#125; (1 row) 备注：向数组array[1,2,3] 末尾追加元素。 multi_array_append_int4 函数测试 21234567francs=&gt; set a 4 francs=&gt; set b 5francs=&gt; select multi_array_append_int4(array[1,2,3],array[:a,:b]); multi_array_append_int4 ------------------------- &#123;1,2,3,4,5&#125; 上面函数只是针对 integer 类型的，如果是字符类型就不行，同理可以写个函数。 创建 multi_array_append_text 函数1234567891011121314151617181920create or replace function multi_array_append_text(i_src text[],i_append text[]) returns text[] AS $$ DECLARE v_text text; v_result text[]; BEGIN v_result := i_src; if i_append is null then return v_result; end if; foreach v_text in ARRAY i_append loop select array_append(v_result,v_text) into v_result; end loop; return v_result; END; $$ LANGUAGE 'plpgsql'; 测试1234567891011121314151617francs=&gt; select multi_array_append_text(array['a','b','c'],null); multi_array_append_text ------------------------- &#123;a,b,c&#125; (1 row) francs=&gt; select multi_array_append_text(array['a','b','c'],array['d']); multi_array_append_text ------------------------- &#123;a,b,c,d&#125; (1 row)francs=&gt; select multi_array_append_text(array['a','b','c'],array['d','e']); multi_array_append_text ------------------------- &#123;a,b,c,d,e&#125; (1 row) 附一 : Looping Through Arrays The FOREACH loop is much like a FOR loop, but instead of iterating through the rows returned by a SQL query, it iterates through the elements of an array value. (In general, FOREACH is meant for looping through components of a composite-valued expression; variants for looping through composites besides arrays may be added in future.) The FOREACH statement to loop over an array is:[ &lt;&gt; ]FOREACH target [ SLICE number ] IN ARRAY expression LOOP statementsEND LOOP [ label ]; 参考 http://blog.163.com/digoal@126/blog/static/163877040201261273149437/ http://www.postgresql.org/docs/9.1/static/plpgsql-control-structures.html http://www.depesz.com/2012/07/12/waiting-for-9-3-add-array_remove-and-array_replace-functions/comment-page-1/#comment-35948","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: What's the location of pg_default tablespace ? ","slug":"20120710150237","date":"2012-07-10T07:02:37.000Z","updated":"2018-09-04T01:33:57.587Z","comments":true,"path":"20120710150237.html","link":"","permalink":"https://postgres.fun/20120710150237.html","excerpt":"","text":"pg_default 为 PostgreSQL 的默认表空间，也可以理解成系统表空间，也许有人疑问这个表空间物理的位置在哪里？在不同操作系统上 PostgreSQL 的安装路径可能不同，但在 Linux 环境下，pg_default 表空间对应的物理位置为 $PGDATA/base 目录下。 查看 PostgreSQL 的表空间1234567891011[postgres@redhat6 ~]$ psql psql (9.2beta1) Type \"help\" for help.postgres=# \\db List of tablespaces Name | Owner | Location ------------+----------+------------------------------------------ pg_default | postgres | pg_global | postgres | tbs_francs | postgres | /database/1922/pgdata1/pg_tbs/tbs_francs 备注：上面显示了三个表空间，其中 tbs_francs 为后来创建的表空间，另外两个为系统表空间。 查看表空间的 oid1234567postgres=# select oid,spcname from pg_tablespace; oid | spcname -------+------------ 1663 | pg_default 1664 | pg_global 16385 | tbs_francs (3 rows) 查看表空间目录查看 tbs_francs 表空间目录，如下：12345postgres=# select pg_tablespace_location(16385) ; pg_tablespace_location ------------------------------------------ /database/1922/pgdata1/pg_tbs/tbs_francs (1 row) 查看表空间 pg__default 的目录12345postgres=# select pg_tablespace_location(1663) ; pg_tablespace_location ------------------------ (1 row) 备注：系统默认表空间 pg_default 查出来的物理位置为空。 查看数据库的 oid12345678postgres=# select oid,datname from pg_database; oid | datname -------+----------- 1 | template1 12865 | template0 12870 | postgres 16386 | francs (4 rows) 操作系统对应的目录12345678910[postgres@redhat6 base]$ cd $PGDATA/base [postgres@redhat6 base]$ pwd /opt/pgdata9.2/pg_root/base [postgres@redhat6 base]$ ll total 20K drwx------. 2 postgres postgres 12K May 31 03:07 1 drwx------. 2 postgres postgres 4.0K May 17 14:37 12865 drwx------. 2 postgres postgres 4.0K Jul 7 04:08 12870 备注：在 $PGDATA/base 目录下有三个子目录，分别为数据库 template1，template0，postgres 的数据库对像存储目录。 创建测试表在 francs 库中创建测试表12345678910111213141516171819202122232425262728[postgres@redhat6 ~]$ psql psql (9.2beta1) Type \"help\" for help.postgres=# grant all on tablespace pg_default to francs; GRANTpostgres=# \\c francs francs You are now connected to database \"francs\" as user \"francs\". francs=&gt; show default_tablespace; default_tablespace -------------------- (1 row)francs=&gt; set default_tablespace='pg_default'; SETfrancs=&gt; show default_tablespace; default_tablespace -------------------- pg_default (1 row)francs=&gt; create table test_default (id integer); CREATE TABLEfrancs=&gt; insert into test_default values (1),(2),(3); INSERT 0 3 再次查看系统目录12345678910[postgres@redhat6 base]$ cd $PGDATA/base [postgres@redhat6 base]$ pwd /opt/pgdata9.2/pg_root/base [postgres@redhat6 base]$ ll total 24K drwx------. 2 postgres postgres 12K May 31 03:07 1 drwx------. 2 postgres postgres 4.0K May 17 14:37 12865 drwx------. 2 postgres postgres 4.0K Jul 7 04:08 12870 drwx------. 2 postgres postgres 4.0K Jul 7 05:03 16386 备注： $PGDATA/base 下正好多了目录 16386。 接着往下看12345678francs=&gt; select oid,relname from pg_class where relname='test_default'; oid | relname -------+-------------- 24706 | test_default (1 row)[postgres@redhat6 base]$ ll 16386 total 8.0K -rw-------. 1 postgres postgres 8.0K Jul 7 05:03 24706 备注：已经很明白了，不多解释。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 是否可以更改表中已有字段的顺序？","slug":"20120708150926","date":"2012-07-08T07:09:26.000Z","updated":"2018-09-04T01:33:57.524Z","comments":true,"path":"20120708150926.html","link":"","permalink":"https://postgres.fun/20120708150926.html","excerpt":"","text":"今天有朋友问 “ 在 PostgreSQL 的表增加字段后，能否更改字段的顺序？ ”，后来查了下资料，并做了些测试，并没有找到能够直接在原表的基础上实现已有字段顺序更改的方法，当然可以通过间接方法来实现。 暂且不论这个需求的原因，个人觉得调整表字段顺序的需求比较奇怪，既然有人问题，不妨测试下。 关于 pg_attribute 系统表首先看下系统表 pg_catalog.pg_attribute，这个系统表记录的是数据库所有表的字段信息。其中 pg_attribute.attnum 存储的是表字段的顺序，那么是否可以通过更改这个值来实现目的呢？接着往下看； 创建测试表12345678910111213francs=&gt; create table test_6(col1 int4,col2 int4,col3 int4); CREATE TABLEfrancs=&gt; insert into test_6 values (1,2,3); INSERT 0 1francs=&gt; insert into test_6 values (4,5,6); INSERT 0 1francs=&gt; select * from test_6; col1 | col2 | col3 ----+------+------ 1 | 2 | 3 4 | 5 | 6 (2 rows) 查看字段顺序12345678910111213francs=&gt; select attrelid,attname,attnum From pg_attribute where attrelid='francs.test_6'::regclass; attrelid | attname | attnum ----------+----------+-------- 24700 | tableoid | -7 24700 | cmax | -6 24700 | xmax | -5 24700 | cmin | -4 24700 | xmin | -3 24700 | ctid | -1 24700 | col1 | 1 24700 | col2 | 2 24700 | col3 | 3 (9 rows) 备注：col1 的 attnum值为 1，col2 的 attnum值为 2，col3 的 attnum值为 3,，正好是字段的顺序值。其中 attnum 值为负数的行表示表的隐含字段，例如 ctid，xmin,cmin 等。 修改字段的 attnum12345678910111213141516francs=&gt; \\c francs postgres You are now connected to database \"francs\" as user \"postgres\".francs=# update pg_attribute set attnum=4 where attrelid='francs.test_6'::regclass and attname='col3'; UPDATE 1francs=# update pg_attribute set attnum=3 where attrelid='francs.test_6'::regclass and attname='col2'; UPDATE 1francs=# update pg_attribute set attnum=2 where attrelid='francs.test_6'::regclass and attname='col3'; UPDATE 1francs=# select * from francs.test_6; col1 | col3 | col2 ----+------+------ 1 | 2 | 3 4 | 5 | 6 (2 rows) 备注：修改系统表 pg_attribute ，更改表 francs.test_6 字段的 attnum 值，结果发现只是字段名称换了下顺序，而字段中的值却没有更改，显然通过修改系统表 pg_attribute.attnum 值是行不通的，更进一步，假设这步成功了，如果表字段上有索引，或者约束，那么还得修改相应的系统表，显然修改数据库系统表的做法是危险的，容易给数据库带来灾难，万不得已，不建议这么做。 既然直接修改系统表字段顺序的方法行不通，那么可以通过其它间接的方法，这里想到了两种，第一种是重建表，即新建表结构再把老表数据导进去。第二种是新建一个符合规则的视图，以后应用程序不直接查原表，而是查视图。 重建表的方法重建表的方法很多，例如在创建好目标表之后可以使用“insert into ” 方式或者 copy 方式导入数据。这里不再详述 重建视图的方法创建目标视图，使字段顺序为目标的字段顺序，从而隐含了视图引用表的字段实际物理顺序，创建视图 SQL 略。 参考 http://archives.postgresql.org/pgsql-admin/2007-06/msg00037.php http://www.postgresql.org/docs/9.2/static/catalog-pg-attribute.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Postgres-XC: Waiting For Online Data Redistribution ","slug":"20120705160717","date":"2012-07-05T08:07:17.000Z","updated":"2018-09-04T01:33:57.446Z","comments":true,"path":"20120705160717.html","link":"","permalink":"https://postgres.fun/20120705160717.html","excerpt":"","text":"Postgres-XC 开发组正在计划开发 “online data redistribution” 功能，即可以实现 Postgres-XC 数据节点的数据重分布，也可以更改表的 DISTRIBUTE 类型，例如，replicated 模式改成 hash 模式。这是个非常有效的功能，例如，在增加了数据节点后，为了提高性能，可能需要将表的数据重新分布到所有数据节点，也可能由于某种原因，需要更改表的 DISTRIBUTE 类型，这个模块还在设计阶段。 Postgres-XC 数据重分布设想 fetch all the data of the table to be redistributed on Coordinator Truncate the table Update the catalogs to the new distribution type Redistribute the data cached on Coordinator Postgres-XC 数据重分布预计优化 Save materialization if it is not necessary (new distribution set to round robin, replication) Truncate the table on a portion of nodes if a replicated table has its subset of nodes reduced COPY only necessary data for a replicated table to new nodes if its subset of nodes is increased And a couple of other things备注：在步骤 1 的初步设想成功实现后， Postgres-XC 项目组计划优化上面列举出来的事项。 数据重分布命令12345ALTER TABLE DISTRIBUTE BY &#123; REPLICATION | ROUND ROBIN | &#123; [HASH | MODULO ] ( column_name ) &#125; &#125; TO &#123; GROUP groupname | NODE ( nodename [, ... ] ) &#125; ADD NODE ( nodename [, ... ] ) DELETE NODE ( nodename [, ... ] ) 备注：只要执行一条 “ALTER TABLE 。。。” SQL ，数据节点的数据就会自动的实现重分布，可以实现将某张表现有数据重新布到新的数据节点；也可以删除某个数据节点的数据，还可以更改表的 DISTRIBUTE 类型，并根据新的 DISTRIBUTE 类型重新分布表数据，功能比较强大。 参考 http://michael.otacoo.com/postgresql-2/postgres-xc-online-data-redistribution/ 备注：对 PostgreSQL -XC 有兴趣的朋友，可以参考 Michael Paquier 上面的 BLOG。","categories":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/categories/Postgres-XC/"}],"tags":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/tags/Postgres-XC/"}]},{"title":"PostgreSQL: Oracle_fdw 数据迁移之高效 ","slug":"20120705141814","date":"2012-07-05T06:18:14.000Z","updated":"2018-09-04T01:33:57.399Z","comments":true,"path":"20120705141814.html","link":"","permalink":"https://postgres.fun/20120705141814.html","excerpt":"","text":"最近在做一个 Oracle 迁移 PostgreSQL 项目 ，数据迁移初步计划用 PostgreSQL 的外部表 oracle_fdw 来做，oracle，pg 测试环境搭好后，今天做了下测试，发现 Oracle_fdw 迁移数据效率挺高，22 GB 数据，迁移花了 55 分钟左右，下面是迁移的步骤： 环境信息服务器配置信息DELL R6108 核 24 GB硬盘：两块 SCSI 300 GB ( RAID 1) 版本信息123OS： Red Hat Enterprise Linux Server release 5.6 PG: PostgreSQL 9.2beta2 Oracle: 10.2.0.4.0 PG 部分参数配置12345shared_buffers = 1024MB synchronous_commit = off autovacuum = off checkpoint_segments = 128 checkpoint_timeout = 60 min 备注：为了提高写入速度，数据导入过程中关闭 autovacuum。 Oracle 参数配置12345sga_target=5G pga_aggregate_target=1797M db_file_multiblock_read_count=16 TEMP 表空间： 10GB UNDO 表空间： 8GB 关于 oracle_fdw 安装pg 上需要安装 oracle_fdw 模块，关于 oracle_fdw 的安装，本文略，具体可以参考之前写的 blog: https://postgres.fun/20120303174557.html Oracle 库表信息Oracle 需要抽取的表的表结构1234567891011121314151617181920212223242526272829303132create table francs.TBL_FDW_TEST ( USER_ID NUMBER(10) not null, STATE NUMBER(10) not null, USER_NAME VARCHAR2(64), BIND_MOBILE VARCHAR2(20), BIND_EMAIL VARCHAR2(64), PASSWD VARCHAR2(64) not null, LOGIN_TIME DATE, IMSI VARCHAR2(20), IMEI VARCHAR2(20), LOGIN_COUNT NUMBER(10) not null, LAST_STIME DATE, UPTAG NUMBER(10) not null );alter table francs.TBL_FDW_TEST add primary key (USER_ID);alter table francs.TBL_FDW_TEST add constraint UK_AUTH_USERNAME unique (USER_NAME) using index tablespace TBS_francs_IDX pctfree 10 initrans 2 maxtrans 255 storage ( initial 64K minextents 1 maxextents unlimited );create index francs.IDX_TBL_FDW_TEST_LOGIN_TIME on francs.TBL_FDW_TEST (LOGIN_TIME); PostgreSQL 表结构123456789101112131415161718create table TBL_FDW_TEST ( USER_ID NUMERIC(10) not null, STATE NUMERIC(10) not null, USER_NAME VARCHAR(64) null, BIND_MOBILE VARCHAR(20) null, BIND_EMAIL VARCHAR(64) null, PASSWD VARCHAR(64) not null, LOGIN_TIME timestamp(0) without time zone, IMSI VARCHAR(20) null, IMEI VARCHAR(20) null, LOGIN_COUNT NUMERIC(10) not null, LAST_STIME timestamp(0) without time zone, UPTAG NUMERIC(10) not null, constraint PK_AUTH_SKYID primary key (USER_ID), constraint UK_AUTH_USERNAME unique (USER_NAME) );CREATE INDEX IDX_TBL_FDW_TEST_LOGIN_TIME ON TBL_FDW_TEST USING btree (LOGIN_TIME); 外部表信息12345678910111213141516171819202122232425francs=&gt; \\det ft_TBL_FDW_TEST List of foreign tables Schema | Table | Server --------+------------------+------------ francs | ft_TBL_FDW_TEST | oracle_srv (1 row)francs-&gt; \\d ft_TBL_FDW_TEST Foreign table \"francs.ft_TBL_FDW_TEST\" Column | Type | Modifiers | FDW Options -------------+--------------------------------+-----------+------------- USER_ID | numeric(10,0) | not null | state | numeric(10,0) | not null | user_name | character varying(64) | | bind_mobile | character varying(20) | | bind_email | character varying(64) | | passwd | character varying(64) | not null | login_time | timestamp(0) without time zone | | imsi | character varying(20) | | imei | character varying(20) | | login_count | numeric(10,0) | not null | last_stime | timestamp(0) without time zone | | uptag | numeric(10,0) | not null | Server: oracle_srv FDW Options: (schema 'francs', \"table\" 'TBL_FDW_TEST') 备注：PG中建立相应的外部表 ft_TBL_FDW_TEST ，表结构和pg目标表 TBL_FDW_TEST 结构一致，当然外部表有些限制，比如不能有 default 值等。 迁移脚本ORACLE 库中取最大 user_id1234SQL&gt; select max(sky_id) from francs.TBL_USER_auth;MAX(SKY_ID) ----------- 295799801 insert_auth_1.sql 脚本12\\timing insert into TBL_USER_AUTH select * from ft_TBL_USER_AUTH where USER_ID &lt; 100000000; insert_auth_2.sql 脚本12\\timing insert into TBL_USER_AUTH select * from ft_TBL_USER_AUTH whereUSER_ID &gt;= 100000000 and SKY_ID &lt; 150000000; insert_auth_3.sql 脚本12\\timing insert into TBL_FDW_TEST select * from ft_TBL_FDW_TEST where USER_ID &gt;= 150000000 and USER_ID &lt; 200000000; insert_auth_4.sql 脚本12\\timing insert into TBL_FDW_TEST select * from ft_TBL_FDW_TEST where USER_ID &gt;= 200000000 and USER_ID &lt; 250000000; insert_auth_5.sql 脚本 \\timing insert into TBL_FDW_TEST select * from ft_TBL_FDW_TEST where USER_ID &gt;= 250000000 and USER_ID &lt; 300000000; 备注：由于在 PostgreSQL 中单个查询只能利用到一个 CPU 核，而不能像 Oracle 库中的 启用并行查询。所以这里逻辑上写了多个 sql，根据主键 USER_ID 进行划分，上面分成了五个脚本。 同时后台执行上面五个脚本，最后执行完的脚本的执行时间为 55 分钟左右。55 分钟只是数据迁移时间，索引的创建时间并没有包括，索引的创建时间较长，尤其是主键和 unique index 的添加耗时更长。 总结22 GB 的数据从 Oracle 库中迁移到 PG 库只花费了 55 分钟左右，这个速度已经比较快了，oracle_fdw 模块是 Oracle 转 PG的一大利器，使用得好可以大大提高 O 转 P 的效率。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"oracle_fdw","slug":"oracle-fdw","permalink":"https://postgres.fun/tags/oracle-fdw/"}]},{"title":"PostgreSQL: Using pgsql_fdw in a funnction Result ,ERROR: cache lookup failed for type 0","slug":"20120630142533","date":"2012-06-30T06:25:33.000Z","updated":"2018-09-04T01:33:57.337Z","comments":true,"path":"20120630142533.html","link":"","permalink":"https://postgres.fun/20120630142533.html","excerpt":"","text":"今天读德哥 blog (原文 http://blog.163.com/digoal@126/blog/static/16387704020125218171919/ )，里面描述的问题引起了我的好奇，问题是这样的，在 function 中 使用 pgsql_fdw 调用pg远程外部表时，会报错，而在 session 中不会，具体演示下。 环境信息源库:IP 192.168.1.26/1923database mydbtable test1.2 目标库IP 192.168.1.26/1923database skytftable test_tf备注： 关于 pgsql_fdw 安装略，可以参考之前写的 BLOG：https://postgres.fun/20120607112420.html 目标库表信息1234567891011121314mydb=&gt; \\d test Table \"mydb.test\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Indexes: \"idx_test_1\" btree (id)mydb=&gt; select count(*) from test; count --------- 1990000 (1 row) 版本信息pgsql_fdw 模块版本：pgsql_fdw-1.0postgresql 版本： PostgreSQL 9.1.0 外部表测试2.1 create foreign server123456789101112131415161718192021skytf=&gt; \\c skytf postgres You are now connected to database \"skytf\" as user \"postgres\".skytf=# grant usage on foreign data wrapper pgsql_fdw to skytf; GRANTskytf=# \\c skytf skytf You are now connected to database \"skytf\" as user \"skytf\". skytf=&gt; CREATE SERVER pgsql_srv FOREIGN DATA WRAPPER pgsql_fdw skytf-&gt; OPTIONS (host '127.0.0.1', port '1923', dbname 'mydb'); CREATE SERVERskytf=&gt; \\des List of foreign servers Name | Owner | Foreign-data wrapper --------------+-------+---------------------- file_srv | skytf | file_fdw mysql_svr_25 | skytf | mysql_fdw pgsql_srv | skytf | pgsql_fdw (3 rows) create mapping user123skytf=&gt; CREATE USER MAPPING FOR public SERVER pgsql_srv skytf-&gt; OPTIONS (user 'mydb', password 'mydb'); CREATE USER MAPPING create foreign table123456skytf=&gt; CREATE FOREIGN TABLE ft_test ( skytf(&gt; id integer , skytf(&gt; name character varying(32) ) skytf-&gt; SERVER pgsql_srv skytf-&gt; OPTIONS (nspname 'mydb', relname 'test'); CREATE FOREIGN TABLE 查询外部表测试123456789101112131415skytf=&gt; \\d ft_test; Foreign table \"skytf.ft_test\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Server: pgsql_srvskytf=&gt; select * From ft_test limit 3; id | name -------+------ 54241 | AAA 54242 | AAA 54243 | AAA (3 rows) 创建中间表12skytf=&gt; create table test_tf (id int4,name varchar(32)); CREATE TABLE 创建函数12345678CREATE or replace FUNCTION func_sync_bill() RETURNS INTEGER AS $$ BEGIN begin insert into test_tf (id,name) select id,name from ft_test; return 0; end; END; $$ LANGUAGE 'plpgsql'; 执行函数12345678910skytf=&gt; select func_sync_bill() ; ERROR: cache lookup failed for type 0 CONTEXT: SQL statement \"insert into test_tf (id,name) select id,name from ft_test\" PL/pgSQL function \"func_sync_bill\" line 4 at SQL statementskytf=&gt; select count(*) from test_tf; count ------- 0 (1 row) 备注：错误出现在语句 “insert into test_tf (id,name) select id,name from ft_test” 而这个语句单独在 session 中执行是正确的。 2.8 在会话中执行插入语句12345678skytf=&gt; select count(*) from test_tf; count ------- 0 (1 row)skytf=&gt; insert into test_tf (id,name) select id,name from ft_test; INSERT 0 1990000 备注：在 seesion 中调用 pgsql_fdw 表是正常的，而在 plpgsql function 中却不行， 之后网上查了很多资料也没有相关 bug 确认信息，后来 dba.stackexchange.com 问了下，有人建议使用 “EXCUTE” 语句，原贴如下 http://dba.stackexchange.com/questions/19919/using 使用 EXCUTE 测试 pgsql_fdw创建 function123456789skytf=&gt; CREATE or replace FUNCTION func_sync_bill() RETURNS INTEGER AS $$ skytf$&gt; BEGIN skytf$&gt; begin skytf$&gt; EXECUTE 'insert into test_tf (id,name) select id,name from ft_test'; skytf$&gt; return 0; skytf$&gt; end; skytf$&gt; END; skytf$&gt; $$ LANGUAGE 'plpgsql'; CREATE FUNCTION 再次测试123456789101112skytf=&gt; truncate table test_tf; TRUNCATE TABLEskytf=&gt; select func_sync_bill(); func_sync_bill ---------------- 0 (1 row)skytf=&gt; select count(*) from test_tf; count --------- 1990000 (1 row) 备注：在 function 中采用 EXECUTE 语句执行后报错消失，估计 pgsql_fdw 模块目前还不很成熟，如果需要查询远程库数据的需求，也可以用比较原始的方法，即 dblink 来代替 pgsql_fdw。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"pgsql_fdw","slug":"pgsql-fdw","permalink":"https://postgres.fun/tags/pgsql-fdw/"}]},{"title":"PostgreSQL: Unique constraint allow mutiple nulls ","slug":"20120627205420","date":"2012-06-27T12:54:20.000Z","updated":"2018-09-04T01:33:57.290Z","comments":true,"path":"20120627205420.html","link":"","permalink":"https://postgres.fun/20120627205420.html","excerpt":"","text":"今天在和一个项目沟通过程中，开发人员说在 PostgreSQL 中的 unique 约束中可以存储多个null 值的记录，一开始觉得比较奇怪，难道 null 值不等于 null 值，带着疑问，赶紧测试下。 PostgreSQL 中测试 null1.1 创建表并创建 unique 索引1234567891011121314francs=&gt; create table test_unique (id int4,name varchar(32)); CREATE TABLEfrancs=&gt; create unique index idx_test_unique_id on test_unique using btree (id); CREATE INDEXfrancs=&gt; \\d test_unique; Table \"francs.test_unique\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Indexes: \"idx_test_unique_id\" UNIQUE, btree (id) 1.2 重复插入数据 112345francs=&gt; insert into test_unique (id ,name )values (1,'a'); INSERT 0 1francs=&gt; insert into test_unique (id ,name )values (1,'a'); ERROR: duplicate key value violates unique constraint \"idx_test_unique_id\" DETAIL: Key (id)=(1) already exists. 1.3 重复插入 null 值1234567891011francs=&gt; insert into test_unique (id ,name )values (null,'b'); INSERT 0 1 francs=&gt; insert into test_unique (id ,name )values (null,'b'); INSERT 0 1francs=&gt; select * from test_unique; id | name ----+------ 1 | a | b | b (3 rows) 备注：果然在唯一约束字段上可以存储多个 null 的记录，觉得很奇怪，猜测是因为 null 是一个不确定的值，没有确定的类型，所以 null != null，接下来看下在 Oracle 库的情况。 Oracle 库中测试 null2.1 创建测试表123456789101112131420:45:03 [SYS@skytf](mailto:SYS@skytf)&gt; select *From v$version;BANNER ---------------------------------------------------------------- Oracle Database 10g Enterprise Edition Release 10.2.0.1.0 - Prod PL/SQL Release 10.2.0.1.0 - Production CORE 10.2.0.1.0 Production TNS for 32-bit Windows: Version 10.2.0.1.0 - Production NLSRTL Version 10.2.0.1.0 - Production20:00:38 [SKYTF@skytf](mailto:SKYTF@skytf)&gt; create table test_unique (id integer,name varchar(32)); 表已创建。20:00:52 [SKYTF@skytf](mailto:SKYTF@skytf)&gt; create unique index idx_test_unique_id on test_unique (id); 索引已创建。 2.2 插入数据测试123456789101112131420:45:03 [SYS@skytf](mailto:SYS@skytf)&gt; select *From v$version;BANNER ---------------------------------------------------------------- Oracle Database 10g Enterprise Edition Release 10.2.0.1.0 - Prod PL/SQL Release 10.2.0.1.0 - Production CORE 10.2.0.1.0 Production TNS for 32-bit Windows: Version 10.2.0.1.0 - Production NLSRTL Version 10.2.0.1.0 - Production20:00:38 [SKYTF@skytf](mailto:SKYTF@skytf)&gt; create table test_unique (id integer,name varchar(32)); 表已创建。20:00:52 [SKYTF@skytf](mailto:SKYTF@skytf)&gt; create unique index idx_test_unique_id on test_unique (id); 索引已创建。 备注：可见 Oracle 中，唯一约束的字段上也允许存储多个 null，后来网上查了下资料，大致是说 NULL 是数据库中特殊的数据类型，这个值是不确定的，所以对于 NULL的判断是这样的：NULL 即不是NULL，也不是 NOT NULL，因为 NULL 具有无数的可能性，关于 NULL 的解释 itput 上有篇贴子解释得很好，有兴趣的朋友可以看看：http://www.itpub.net/thread-932786-1-1.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.2: Remove the spclocation field from pg_tablespace ","slug":"20120626202307","date":"2012-06-26T12:23:07.000Z","updated":"2018-09-04T01:33:57.227Z","comments":true,"path":"20120626202307.html","link":"","permalink":"https://postgres.fun/20120626202307.html","excerpt":"","text":"今天在 PostgreSQL9.2 测试版上做实验时发现了一奇怪现象，以前可以通过 pg_tablespace 系统表定位表空间对应的物理文件，而在 PostgreSQL9.2 版本不可以，后来查询了下 PostgreSQL9.2 Realease 才知道这个信息有变化。 PostgreSQL9.1 查询表空间123456789101112[postgres@pgb ~]$ psql psql (9.1.0) Type \"help\" for help.postgres=# select * from pg_tablespace; spcname | spcowner | spclocation | spcacl | spcoptions ---------------+----------+-----------------------------------------------+--------+------------ pg_default | 10 | | | pg_global | 10 | | | tbs_mydb | 16384 | /database/pgdata1923_9.1/pg_tbs/tbs_mydb | | tbs_skytf | 41818 | /database/pgdata1923_9.1/pg_tbs/tbs_skytf | | tbs_skytf_idx | 41818 | /database/pgdata1923_9.1/pg_tbs/tbs_skytf_idx | | (5 rows) 备注：pg_tablespace.spclocation 显示了表空间对应系统上的数据目录。 PostgreSQL9.2 beta1 查询表空间12345678910[postgres@redhat6 ~]$ psql psql (9.2beta1) Type \"help\" for help.postgres=# select oid,* from pg_tablespace; oid | spcname | spcowner | spcacl | spcoptions -------+------------+----------+-----------------------------------------+------------ 1663 | pg_default | 10 | | 1664 | pg_global | 10 | | 16385 | tbs_francs | 10 | &#123;postgres=C/postgres,francs=C/postgres&#125; | (3 rows) 备注： PostgreSQL9.2 版的 pg_tablespace 上没有 spclocation 字段，当然查不到表空间对应的数据目录。 PostgreSQL9.2 beta1 的发行说明 Remove the spclocation field from pg_tablespace (Magnus Hagander)This field was duplicative of the symbolic links already present in the data directory. This allows tablespace directories to be moved while the server is down. Also add pg_tablespace_location() to allow querying of the symbolic links. 备注：原来在 PostgreSQL9.2 版本中，spclocation 字段信息已经被去除了，同时在 9.2 版本中，提供函数 pg_tablespace_location() 查询表空间对应的数据目录。 PostgreSQL9.2 查询表空间目录postgres=# \\df pg_tablespace_location List of functions Schema | Name | Result data type | Argument data types | Type ------------+------------------------+------------------+---------------------+-------- pg_catalog | pg_tablespace_location | text | oid | normal (1 row) postgres=# select pg_tablespace_location(16385); pg_tablespace_location ------------------------------------------ /database/1922/pgdata1/pg_tbs/tbs_francs (1 row) 也可以通过元子命令查询 postgres=# \\db List of tablespaces Name | Owner | Location ------------+----------+------------------------------------------ pg_default | postgres | pg_global | postgres | tbs_francs | postgres | /database/1922/pgdata1/pg_tbs/tbs_francs 参考 http://www.postgresql.org/docs/9.2/static/release-9-2.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 如何查询表的创建时间？ ","slug":"20120626194944","date":"2012-06-26T11:49:44.000Z","updated":"2018-09-04T01:33:57.180Z","comments":true,"path":"20120626194944.html","link":"","permalink":"https://postgres.fun/20120626194944.html","excerpt":"","text":"经常有人问这么个问题，” 在 PostgreSQL 中如何查找表的创建时间？” ，因为在其它关系型数据库中这个信息很容易得到，例如在 oracel中，可以通过查询 dba_objects 数据字典得到表的创建时间，如下所示。 Oracle 查询表创建时间1234SQL&gt; select owner,object_name,CREATED from dba_objects where object_name='TBL_1';OWNER OBJECT_NAME CREATED ---------- -------------------- ------------------- SKYAAA TBL_1 2010-10-18 08:24:40 那么在 PostgreSQL 中如何查找表的创建时间呢？GOOGLE 了不少网页，大致的说法是在 PostgreSQL 中没有系统表存储表的创建时间信息，因此不能直接在 PostgreSQL 库中找到这方面的信息，然而可以通过其它方法实现，我这里总结了两种方法。 方法一: 通过查找表数据文件方式这种方法通过查找表的数据文件的方式从而确定表的创建时间，但是这种方法并不能准备查询表的创建时间，而且有时候，这种方法得到的信息还有可能是错误的，下面大致演示下。 1.1 创建表并插入数据12345678910111213141516171819202122francs=&gt; create table test_ctime (id int4 primary key ,name varchar(32)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_ctime_pkey\" for table \"test_ctime\" CREATE TABLEfrancs=&gt; insert into test_ctime select generate_series(1,10000),'create_time test'; INSERT 0 10000 francs=&gt; \\d test_ctime; Table \"francs.test_ctime\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null name | character varying(32) | Indexes: \"test_ctime_pkey\" PRIMARY KEY, btree (id)francs=&gt; \\dt+ test_ctime; List of relations Schema | Name | Type | Owner | Size | Description --------+------------+-------+--------+--------+------------- francs | test_ctime | table | francs | 536 kB | (1 row) 备注：表创建好了，接下来演示如何定位表的物理文件。 1.2 定位表所在的表空间12345francs=&gt; select relname,relfilenode,reltablespace from pg_class where relname='test_ctime'; relname | relfilenode | reltablespace ------------+-------------+--------------- test_ctime | 24650 | 0 (1 row) 备注：在 PostgreSQL 的逻辑结构体系中，表位于数据库中，同时表位于表空间上，面表空间对应系统上一个文件目录，每个表由一个或者多个文件组成； 根据上面的结果，表 test_ctime 的 reltablespace 值为 0,表示位于所属数据库的默认表空间，注意 relfilenode 值为 24650。 1.3 查询数据库 francs 的默认表空间1234francs=&gt; select oid,datname,dattablespace from pg_database where datname='francs'; oid | datname | dattablespace -------+---------+--------------- 16386 | francs | 16385 备注：上面查出数据库 francs 的默认表空间的 oid 为 16385。 1.4 查找 oid 为 16385 的表空间12345francs=&gt; select oid,* from pg_tablespace where oid=16385; oid | spcname | spcowner | spcacl | spcoptions -------+------------+----------+-----------------------------------------+------------ 16385 | tbs_francs | 10 | &#123;postgres=C/postgres,francs=C/postgres&#125; | (1 row) 备注：查了半天才查到表 test_ctime 的默认表空间为 tbs_francs，这里之所以饶这么大圈，是为了展示 postgresql 中的一些逻辑结构关系，如果自己对环境比较熟悉，可以直接定位到哪个表空间。 1.5 查询表空间 tbs_francs 对应的物理目录 12345678 francs=&gt; \\db List of tablespaces Name | Owner | Location ------------+----------+------------------------------------------ pg_default | postgres | pg_global | postgres | tbs_francs | postgres | /database/1922/pgdata1/pg_tbs/tbs_francs (3 rows) 备注：表空间 tbs_francs 的数据目录为 /database/1922/pgdata1/pg_tbs/tbs_francs。 1.6 进入数据目录123456789[postgres@redhat6 16386]$ cd /database/1922/pgdata1/pg_tbs/tbs_francs [postgres@redhat6 tbs_francs]$ ll total 4.0K drwx------. 4 postgres postgres 4.0K May 22 10:35 PG_9.2_201204301[postgres@redhat6 tbs_francs]$ cd PG_9.2_201204301/ [postgres@redhat6 PG_9.2_201204301]$ ll total 16K drwx------. 2 postgres postgres 12K Jun 26 19:03 16386 drwx------. 2 postgres postgres 4.0K May 22 10:37 pgsql_tmp 备注：根据前面的步骤1.3查询的信息知道 16386 为数据库 francs 的 oid。 再根据步骤1.2 的信息知道表 test_ctime 的 relfilenode 值为 24650 1.7 查找表 test_ctime 的数据文件12[postgres@redhat6 16386]$ ll 24650 -rw-------. 1 postgres postgres 512K Jun 26 18:57 24650 备注：根据数据文件 24650 知道表的创建时间为 2012-06-26 18:57。但这种方法并不准确，因为表上的操作可能导致表重新生成文件，接着演示。 1.8 cluster 表123456789101112francs=&gt; cluster verbose test_ctime using test_ctime_pkey; INFO: clustering \"francs.test_ctime\" using index scan on \"test_ctime_pkey\" INFO: \"test_ctime\": found 0 removable, 10000 nonremovable row versions in 64 pages DETAIL: 0 dead row versions cannot be removed yet. CPU 0.00s/0.03u sec elapsed 0.08 sec. CLUSTERfrancs=&gt; select relname,relfilenode,reltablespace from pg_class where relname='test_ctime'; relname | relfilenode | reltablespace ------------+-------------+--------------- test_ctime | 24655 | 0 (1 row) 备注：表 test_ctime 经过 cluster 操作后，重新生成了数据文件，文件号由原来的 24650 变成了 24655 1.9 系统上再次查询表数据文件1234[postgres@redhat6 16386]$ ll 24650 -rw-------. 1 postgres postgres 0 Jun 26 19:19 24650[postgres@redhat6 16386]$ ll 24655 -rw-------. 1 postgres postgres 512K Jun 26 19:19 24655 备注：显然新文件的时间 24655 并不是表 test_ctime 的初始创建时间。 1.10 vacuum full 表123456francs=&gt; vacuum full test_ctime;VACUUM francs=&gt; select relname,relfilenode,reltablespace from pg_class where relname='test_ctime'; relname | relfilenode | reltablespace ------------+-------------+--------------- test_ctime | 24659 | 0 (1 row) 备注： vacuum full 操作后，同样产生了新文件，新文件号为 24659 1.11 系统上再次查询表数据文件12[postgres@redhat6 16386]$ ll 24659 -rw-------. 1 postgres postgres 512K Jun 26 19:22 24659 方法二: 通过查询数据库日志方式这里配置 postgresql.conf 配置文件，通过记录表的 DDL 信息，从而确定表的创建时间。 2.1 配置 postgresql.conf123456log_destination = 'csvlog' logging_collector = on log_directory = '/var/applog/pg_log/1922/pg_log' log_rotation_age = 1d log_rotation_size = 10MB log_statement = 'ddl' # none, ddl, mod, all 备注：最重要的参数配置 log_statement 值为 DDL，表示记录表上的所有 DDL 操作，其它的参数为日志格式的控制参数。 2.2 创建测试表测试1234567francs=&gt; select now(); now ------------------------------- 2012-06-26 19:31:05.900842+08 (1 row)francs=&gt; create table test_ctime_bak as select * From test_ctime; SELECT 9000 2.3 查看 csv 数据库日志12012-06-26 19:31:10.657 CST,\"francs\",\"francs\",13753,\"[local]\",4fe99d61.35b9,2,\"idle\",2012-06-26 19:30:41 CST,3/580,0,LOG,00000,\"statement: create table test_ctime_bak as select * From test_ctime;\",,,,,,,,,\"psql\" 备注：上面的信息有两个时间，第一个为 log_time ，表示当前动作的执行时间，第二个时间为 session_start_time 表示会话开始时间，当然也可以将整个 csv 日志导到对应表中。如果数据库很多，为了管理方便，通常需要建立监控服务器，通过编写日志搜集脚本将所有数据库的日志导到监控数据库中，以便监控。 2.4 附: csv 日志表结构123456789101112131415161718192021222324252627CREATE TABLE postgres_log ( log_time timestamp(3) with time zone, user_name text, database_name text, process_id integer, connection_from text, session_id text, session_line_num bigint, command_tag text, session_start_time timestamp with time zone, virtual_transaction_id text, transaction_id bigint, error_severity text, sql_state_code text, message text, detail text, hint text, internal_query text, internal_query_pos integer, context text, query text, query_pos integer, location text, application_name text, PRIMARY KEY (session_id, session_line_num) ); 总结根据上面两种方法演示情况来看，方法一由于表上的操作可能产生新的数据文件，所以不太靠谱，而方法二通过数据库日志的方法比较准确。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"德哥 PostgreSQL DBA 培训视频在线观看","slug":"20120624122133","date":"2012-06-24T04:21:33.000Z","updated":"2018-09-04T01:33:57.118Z","comments":true,"path":"20120624122133.html","link":"","permalink":"https://postgres.fun/20120624122133.html","excerpt":"","text":"群里很多人问德哥视频地址，这里记录下， http://i.youku.com/u/UNDQxMjUwNjk2/videos这是今年五月份德哥举办的 PostgreSQL DBA 2000 免费培训杭州站录的视频，视频为现场录的，稍微有点歪，但对于学习 PostgreSQL 还是不影响的，对于想了解，想学习 PostgreSQL 的朋友可以看看。 另外由于 PostgreSQL 在国内应用目前还不太多，中文资料比较少，我想，这次的中文培训视频对于很多想了解 PostgreSQL 的朋友还是很有帮助的。","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL : Frequent Function Call Error ","slug":"20120623142336","date":"2012-06-23T06:23:36.000Z","updated":"2018-09-04T01:33:57.071Z","comments":true,"path":"20120623142336.html","link":"","permalink":"https://postgres.fun/20120623142336.html","excerpt":"","text":"学习 PostgreSQL 函数语法时，在调用函数时经常出现以下 ERROR，今天总结下。 调用函数常见错误错误一1ERROR: a column definition list is required for functions returning \"record\" 错误二1ERROR: a column definition list is only allowed for functions returning \"record\" 上面具体错误原因暂时不分析，接下来针对上面 ERROR，通过实验来演示：. 错误一演示创建测试表并插入数据12345678910111213141516171819skytf=&gt; create table test_result3 (id integer,name varchar(32),addr varchar(32),result varchar(32)); CREATE TABLEskytf=&gt; insert into test_result3 select generate_series(1,10),'test_'|| generate_series(1,10),'no','no'; INSERT 0 10skytf=&gt; select * From test_result3; id | name | addr | result ----+---------+------+-------- 1 | test_1 | no | no 2 | test_2 | no | no 3 | test_3 | no | no 4 | test_4 | no | no 5 | test_5 | no | no 6 | test_6 | no | no 7 | test_7 | no | no 8 | test_8 | no | no 9 | test_9 | no | no 10 | test_10 | no | no (10 rows) 创建函数123456789101112CREATE OR REPLACE FUNCTION skytf.func_test_result3_query ( in_id integer) RETURNS SETOF RECORD as $$ DECLARE v_rec RECORD; BEGIN return query select * from test_result3 where id = in_id; return; END; $$ LANGUAGE PLPGSQL; 执行函数(不指定目标列)123skytf=&gt; select * from func_test_result3_query(1); ERROR: a column definition list is required for functions returning \"record\" LINE 1: select * from func_test_result3_query(1); 备注：执行函数报错，提示需要指定 definition column。 执行函数(指定目标列)1234skytf=&gt; select * from func_test_result3_query(1) t(id integer,name varchar,addr varchar,result varchar); id | name | addr | result ----+--------+------+-------- 1 | test_1 | no | no 备注：在返回类型为 RECORD 函数，由于 RECORD 类型为不确定数据类型，在调用函数时需要指定返回类型目标列， 错误二演示创建用户表并插入数据123456789101112131415161718192021mydb=&gt; create table test_user (id int4 primary key ,username varchar(32),pwd varchar,nickname varchar(32)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_user_pkey\" for table \"test_user\" CREATE TABLE mydb=&gt; insert into test_user select generate_series(1,10),'user'|| generate_series(1,10),md5(generate_series(1,10)::varchar),'no'; INSERT 0 10 mydb=&gt; select * from test_user; id | username | pwd | nickname ----+----------+----------------------------------+---------- 1 | user1 | c4ca4238a0b923820dcc509a6f75849b | no 2 | user2 | c81e728d9d4c2f636f067f89cc14862c | no 3 | user3 | eccbc87e4b5ce2fe28308fd9f2a7baf3 | no 4 | user4 | a87ff679a2f3e71d9181a67b7542122c | no 5 | user5 | e4da3b7fbbce2345d7772b0674a318d5 | no 6 | user6 | 1679091c5a880faf6fb5e6087eb1b2dc | no 7 | user7 | 8f14e45fceea167a5a36dedd4bea2543 | no 8 | user8 | c9f0f895fb98ab9159f51fd0297e236d | no 9 | user9 | 45c48cce2e2d7fbdea1afc51c7c6ad26 | no 10 | user10 | d3d9446802a44259755d38e6d163e820 | no (10 rows) 创建函数12345678910111213141516CREATE OR REPLACE FUNCTION func_user_login ( in in_id integer, out o_username varchar,out o_pwd varchar ) RETURNS SETOF RECORD as $$ DECLARE rec record; BEGIN for rec in (select id,username,pwd from test_user where id=in_id) loop o_username :=rec.username; o_pwd :=rec.pwd; RETURN NEXT ; end loop; return; END; $$ LANGUAGE PLPGSQL; 调用函数（不指定目标列）12345mydb=&gt; select * From func_user_login(1); o_username | o_pwd ------------+---------------------------------- user1 | 3efe667c41640f41cb87ad24eec2ed40 (1 row) 备注：这里没有指定目标列但没有报错，这是因为在定义函数 func_user_login 时，已指明了输出参数 o_username 和 o_pwd，所以在调用函数时不需要指定目标列，下面测试下如果指定目标列会是啥子情况。 调用函数（指定目标列） mydb=&gt; select * From func_user_login(1) t ( username varchar, pwd varchar); ERROR: a column definition list is only allowed for functions returning &quot;record&quot; LINE 1: select * From func_user_login(1) t ( username varchar, pwd v... ^ 备注：指定目标列后，函数 func_user_login 执行报错。 总结在PostgreSQL中调用函数时有两种情况：即需要指定目标列和不需要指定目标列，这种情况取决于函数是否指定明确输出参数，简单总结如下 如果函数指定输出参数为 “ RETURNS SETOF RECORD” ，则调用函数时需要指定目标列。 如果函数指定输出参数为 “ RETURNS SETOF datatype” ,datatype 为返回的类型，调用函数时不需要指定目标列。 如果函数中有指定的输出参数，例如 (out o_name varchar)，则调用函数时不需要指定目标列。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL: Function 返回结果集单列和多列的例子 ","slug":"20120623131021","date":"2012-06-23T05:10:21.000Z","updated":"2018-09-04T01:33:57.024Z","comments":true,"path":"20120623131021.html","link":"","permalink":"https://postgres.fun/20120623131021.html","excerpt":"","text":"今天有人问到在 PostgreSQL 函数中如何返回结果集的单列，返回结果集（多列）的方法很多，那么如何返回结果集的单列呢，做了下测试，具体步骤如下: 测试一: 返回多条记录（单列）1.1 创建测试表并插入记录1234567891011121314151617181920212223242526272829303132333435363738skytf=&gt; create table test_result1 (id integer,name varchar(32)); CREATE TABLEskytf=&gt; create table test_result2 (id integer,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_result1 select generate_series(1,10),'a'; INSERT 0 10skytf=&gt; insert into test_result2 select generate_series(1,10),'b'; INSERT 0 10skytf=&gt; select * From test_result1; id | name ----+------ 1 | a 2 | a 3 | a 4 | a 5 | a 6 | a 7 | a 8 | a 9 | a 10 | a (10 rows)skytf=&gt; select * From test_result2; id | name ----+------ 1 | b 2 | b 3 | b 4 | b 5 | b 6 | b 7 | b 8 | b 9 | b 10 | b (10 rows) 1.2 方法一：返回多条记录（ 单列）1234567891011121314CREATE OR REPLACE FUNCTION skytf.func_test_result_single ( in_id integer) RETURNS SETOF varchar as $$ DECLARE v_name varchar; BEGIN for v_name in ( (select name from test_result1 where id = in_id) union (select name from test_result2 where id = in_id) )loop RETURN NEXT v_name; end loop; return; END; $$ LANGUAGE PLPGSQL; 执行函数123456skytf=&gt; SELECT * FROM func_test_result_single(1) ; func_test_result_single ------------------------- b a (2 rows) 1.3 方法二：使用 reutrn query 返回多条记录（ 单列）123456789101112CREATE OR REPLACE FUNCTION skytf.func_test_result_query_single ( in_id integer) RETURNS SETOF varchar as $$ DECLARE v_rec RECORD; BEGIN return query ( (select name from test_result1 where id = in_id) union (select name from test_result2 where id = in_id) ); return; END; $$ LANGUAGE PLPGSQL; 执行函数123456skytf=&gt; select func_test_result_query_single (1); func_test_result_query_single ------------------------------- b a (2 rows) 备注： 在返回指定 SETOF varchar 返回 varchar 类型单个字段，接下来介绍下返回多条记录多列的场景。 测试二: 返回多条记录（多列）2.1 使用游标和”RETURNS SETOF RECORD” 返回多条记录（ 多列）1234567891011121314CREATE OR REPLACE FUNCTION skytf.func_test_result_muti ( in_id integer) RETURNS SETOF RECORD as $$ DECLARE v_rec RECORD; BEGIN for v_rec in ( (select id , name from test_result1 where id = in_id) union (select id , name from test_result2 where id = in_id) )loop RETURN NEXT v_rec; end loop; return; END; $$ LANGUAGE PLPGSQL; 执行函数123456skytf=&gt; SELECT * FROM func_test_result_muti(1) t(id integer,name varchar); id | name ----+------ 1 | a 1 | b (2 rows) 2.2 使用 reutrn query 返回多条记录（ 多列）123456789101112CREATE OR REPLACE FUNCTION skytf.func_test_result_query ( in_id integer) RETURNS SETOF RECORD as $$ DECLARE v_rec RECORD; BEGIN return query ( (select id , name from test_result1 where id = in_id) union (select id , name from test_result2 where id = in_id) ); return; END; $$ LANGUAGE PLPGSQL; 执行函数123456skytf=&gt; SELECT * FROM func_test_result_query(1) t(id integer,name varchar); id | name ----+------ 1 | a 1 | b (2 rows) 2.3 使用 out 输出参数 返回多条记录（ 多列）12345678910111213141516CREATE OR REPLACE FUNCTION skytf.func_test_result_out ( in_id integer,out o_id integer,out o_name varchar) RETURNS SETOF RECORD as $$ DECLARE v_rec RECORD; BEGIN for v_rec in ( (select id , name from test_result1 where id = in_id) union (select id , name from test_result2 where id = in_id) )loop o_id := v_rec.id; o_name := v_rec.name; RETURN NEXT ; end loop; return; END; $$ LANGUAGE PLPGSQL; 执行函数123456skytf=&gt; select skytf.func_test_result_out(1); func_test_result_out ---------------------- (1,a) (1,b) (2 rows) 总结以上只是为了演示 PostgreSQL 函数的语法给出简单的例子，生产过程中的 function 会复杂很多。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 优化一例: 使用函数索引和联合索引 ","slug":"20120621210555","date":"2012-06-21T13:05:55.000Z","updated":"2018-09-04T01:33:56.962Z","comments":true,"path":"20120621210555.html","link":"","permalink":"https://postgres.fun/20120621210555.html","excerpt":"","text":"今天有个生产库上的语句比较慢，花费近 500 毫秒左右，为了便于测试，后来将这几个表导到测试环境下测试，并且优化后，时间下降到仅需要 0.104 ms，下面是优化过程。 表信息表结构123456789101112131415161718192021222324252627francs=&gt; \\d test_count Table \"francs.test_count\" Column | Type | Modifiers -------------+-------------------+--------------- answerid | character varying | skyid | bigint | questid | character varying | username | character varying | id | integer | not null create_time | character varying | default now() flag | integer | default 0 Indexes: \"v_a_c_pk_id\" PRIMARY KEY, btree (id) \"idx_test_count_ctime\" btree (create_time)francs=&gt; \\d test Table \"francs.test\" Column | Type | Modifiers -------------+-----------------------------+--------------- answer | character varying | create_time | timestamp without time zone | default now() questid | character varying(20) | id | integer | not null answercount | bigint | default 0 Indexes: \"pk_ans_id\" PRIMARY KEY, btree (id) \"idx_test_ctime\" btree (create_time) 表大小和数据12345678910111213141516171819202122232425francs=&gt; \\dt+ test_count List of relations Schema | Name | Type | Owner | Size | Description --------+-------------------+-------+--------+-------+------------- francs | test_count | table | francs | 56 MB | (1 row)francs=&gt; \\dt+ test List of relations Schema | Name | Type | Owner | Size | Description --------+-------------+-------+--------+---------+------------- francs | test | table | francs | 8920 kB | (1 row)francs=&gt; select count(*) from test_count; count -------- 500455 (1 row)francs=&gt; select count(*) from test; count -------- 101118 (1 row) 备注：两个表都不大。 优化前的SQL和执行计划123456789101112131415161718francs=&gt; explain analyze SELECT vac.skyid, vac.username, va.answer FROM test_count vac, test va WHERE vac.questid = '20110224123544' and vac.answerid = va.id ::character varying and vac.flag = 0 ORDER BY vac.create_time DESC LIMIT 3; QUERY PLAN -------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..494.32 rows=3 width=69) (actual time=81.274..420.706 rows=3 loops=1) -&gt; Nested Loop (cost=0.00..85352.00 rows=518 width=69) (actual time=81.273..420.702 rows=3 loops=1) Join Filter: ((vac.answerid)::text = ((va.id)::character varying)::text) -&gt; Index Scan Backward using idx_test_count_ctime on test_count vac (cost=0.00..20208.38 rows=24 width=56) (actual time=80.522..263.236 rows=3 loops=1) Filter: (((questid)::text = '20110224123544'::text) AND (flag = 0)) -&gt; Materialize (cost=0.00..3220.77 rows=101118 width=22) (actual time=0.026..31.220 rows=67673 loops=3) -&gt; Seq Scan on test va (cost=0.00..2122.18 rows=101118 width=22) (actual time=0.009..28.288 rows=101118 loops=1) Total runtime: 537.650 ms 备注：这是原始的SQL，花费了537.650 ms，生产上的表导到测试库后，执行时间也这么多； 多表关联一般在关联字段上创建索引，而表 vac.answerid 字段上没有索引，计划在这个字段上创建索引。 查看 Answerid 字段的选择性1234567891011francs=&gt; select count(distinct answerid),count(*) from test_count; count | count -------+-------- 32847 | 500455 (1 row) francs=&gt; create index idx_test_count_answeid on test_count using btree ( answerid ); CREATE INDEXfrancs=&gt; analyze test_count; ANALYZE 备注：answerid 字段 选择性不错，可以创建索引。 查看PLAN1234567891011121314151617francs=&gt; explain analyze SELECT vac.skyid, vac.username, va.answer FROM test_count vac, test va WHERE vac.questid = '20110224123544' and vac.answerid = va.id ::character varying and vac.flag = 0 ORDER BY vac.create_time DESC LIMIT 3; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..489.59 rows=3 width=69) (actual time=81.897..422.056 rows=3 loops=1) -&gt; Nested Loop (cost=0.00..85352.00 rows=523 width=69) (actual time=81.895..422.052 rows=3 loops=1) Join Filter: ((vac.answerid)::text = ((va.id)::character varying)::text) -&gt; Index Scan Backward using idx_test_count_ctime on test_count vac (cost=0.00..20208.38 rows=24 width=56) (actual time=81.131..264.123 rows=3 loops=1) Filter: (((questid)::text = '20110224123544'::text) AND (flag = 0)) -&gt; Materialize (cost=0.00..3220.77 rows=101118 width=22) (actual time=0.027..31.352 rows=67673 loops=3) -&gt; Seq Scan on test va (cost=0.00..2122.18 rows=101118 width=22) (actual time=0.009..28.355 rows=101118 loops=1) Total runtime: 529.655 ms 备注：还是走了表 test_count 上的时间索引 idx_test_count_ctime，并且 test 走了全表扫描由于表关联字段类型不一致，va.id 为 integer ，vac.answerid 为 character varying，这时是用不到表 test 的主键的，这里需要在表 test 上创建函数索引。 创建函数索引123456789101112131415francs=&gt; create index idx_test_id on test using btree (( id::character varying)); CREATE INDEXfrancs=&gt; \\d test Table \"francs.test\" Column | Type | Modifiers -------------+-----------------------------+--------------- answer | character varying | create_time | timestamp without time zone | default now() questid | character varying(20) | id | integer | not null answercount | bigint | default 0 Indexes: \"pk_ans_id\" PRIMARY KEY, btree (id) \"idx_test_ctime\" btree (create_time) \"idx_test_id\" btree ((id::character varying)) 创建函数索引后再次查看PALN1234567891011121314151617francs=&gt; explain analyze SELECT vac.skyid, vac.username, va.answer FROM test_count vac, test va WHERE vac.questid = '20110224123544' and vac.answerid = va.id ::character varying and vac.flag = 0 ORDER BY vac.create_time DESC LIMIT 3; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..125.03 rows=3 width=69) (actual time=87.517..269.151 rows=3 loops=1) -&gt; Nested Loop (cost=0.00..21795.61 rows=523 width=69) (actual time=87.514..269.146 rows=3 loops=1) -&gt; Index Scan Backward using idx_test_count_ctime on test_count vac (cost=0.00..20208.38 rows=24 width=56) (actual time=87.478..269.049 rows=3 loops=1) Filter: (((questid)::text = '20110224123544'::text) AND (flag = 0)) -&gt; Index Scan using idx_test_id on test va (cost=0.00..57.28 rows=506 width=22) (actual time=0.019..0.019 rows=1 loops=3) Index Cond: (((id)::character varying)::text = (vac.answerid)::text) Total runtime: 269.228 ms (7 rows) 备注：这时表 test 已经走索引了，此时语句已优化到了只需要 200多 毫秒，看看是否还有优化空间。根据上面 PLAN ，时间主要花在 idx_test_count_ctime 扫描上，花费了182 ( 269 -87 ) 毫秒 。 尝试删除索引12345678francs=&gt; drop index idx_test_count_ctime; DROP INDEXfrancs=&gt; create index idx_test_count_questid_ctime on test_count using btree (questid,create_time desc); CREATE INDEXfrancs=&gt; analyze test_count; ANALYZE 备注：尝试删除 create_time 索引，并在字段 (questid,create_time ) 上创建联合索引。 再次查看 PALN1234567891011121314151617181920francs=&gt; explain analyze SELECT vac.skyid, vac.username, va.answer FROM test_count vac, test va WHERE vac.questid = '20110224123544' and vac.answerid = va.id ::character varying and vac.flag = 0 ORDER BY vac.create_time DESC LIMIT 3; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..9.36 rows=3 width=69) (actual time=0.038..0.057 rows=3 loops=1) -&gt; Nested Loop (cost=0.00..1609.55 rows=516 width=69) (actual time=0.037..0.056 rows=3 loops=1) -&gt; Index Scan using idx_test_count_questid_ctime on test_count vac (cost=0.00..22.32 rows=24 width=56) (actual time=0.028..0.033 rows=3 loops=1) Index Cond: ((questid)::text = '20110224123544'::text) Filter: (flag = 0) -&gt; Index Scan using idx_test_id on test va (cost=0.00..57.28 rows=506 width=22) (actual time=0.006..0.006 rows=1 loops=3) Index Cond: (((id)::character varying)::text = (vac.answerid)::text) Total runtime: 0.104 ms (8 rows) 备注，现在仅花费了 0.104 ms，走了新建的索引 idx_test_count_questid_ctime，比最初的 500 多毫秒，优化了近 5000 倍！ 总结 上面仅是在测试库上做的测试，优化后的 SQL从原来的 537.650 ms 优化到 0.104 ms ，优化了近 5000倍，但上生产前还需要和开发人员沟通，看看索引是否会影响到其它 SQL。 这里由于关联字段类型不一致，所以创建了一个函数索引，一般在应用中，关联字段的类型是一致的。 近期项目中低性能 SQL 语句较多（ 500 毫秒以上），应该引起足够重视，争取每天都能做下优化。 在order by语句中，排序字段建议创建索引，但具体建法要视情况而定( 单列索引或者组合索引)。","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"PGCon：2012 PostgreSQL 全国大会圆满举行 ( 第二届 北京 )","slug":"20120618124216","date":"2012-06-18T04:42:16.000Z","updated":"2018-09-04T01:33:56.915Z","comments":true,"path":"20120618124216.html","link":"","permalink":"https://postgres.fun/20120618124216.html","excerpt":"","text":"这几天参加了北京 PostgreSQL 2012 全国大会，这是第二届PostgreSQL 全国大会，大会邀请了来自英国，瑞典，日本，以及俄罗斯的专家和国内的资深人士做主题演讲，在这次大会中了解了PostgreSQL国外社区的的情况以及国外PostgreSQL的应用，比如日本电信 NTT 公司的应用，他们的PostgreSQL TB 级数据量的数据库已有很多，能现场参加是幸运的，受益匪浅，下面是大会两天的 topic。 基础培训专场（Free DBA Training Course ）PostgreSQL数据库基础培训时间：6月15日(周五) 9:00-13:00讲师：萧少聪地点：中国人民大学信息楼4层学术报告厅(15号),内容：PostgresQL 架构及安装维护 Postgres-XC专场 (Postgres-XC Summit)地点：中国人民大学信息楼4层学术报告厅(15号) 6.15日(周五）14:00-16:00 PG-XC培训 PG-XC首席架构师:铃木幸一 Suzuki Koichi 6.15日(周五) 16:00-17:00 PG-XC讨论 PG-XC首席架构师:铃木幸一 Suzuki Koichi，主持人：许佳捷 主会场 (PgconfChina2012 Conference)6月16日(周六)上午9:30-12:15 地点：人民大学逸夫会议中心第一报告厅(16-17号) （ Keynotes，9:30-12:15 Saturday, June 16） 9:00 - 9:05 大会介绍 李元佳(Galy) 9:05 - 9:15 大会致辞 李元佳(Galy) 9:15 - 10:15(演讲45分钟，提问15分钟) 9.2的新功能及路线图 Magnus Hagander 10:15 - 11:15(演讲45分钟，提问15分钟) Postgres-XC,读写可扩展的PostgreSQL集群 Koichi Suzuki 11:15 - 11:45 PostgreSQL在电信市场的应用 Koichi Suzuki 12:00 - 12:15 集体合影 6月16日(周六)下午13:30-17:30 地点：人民大学逸夫会议中心第一报告厅(16-17号) （ Discussion/QA/Award，13:30-17:30 Sat, June 16） 13:30 - 14:30(45分钟演讲，15分钟提问) 企业级的PostgreSQL及未来的发展方向 Simon Riggs 14:30 - 15:30(45分钟演讲，15分钟提问) Involving in PostgreSQL Development Magnus Hagander 15:30 - 16:30(45分钟演讲，15分钟提问) PostgreSQL流复制及未来的发展方向 Simon Riggs 16:30 - 17:00 我和PostgreSQL那些事 何伟平 He Weiping(Laser) 17:00 - 17:30 Dispatching a multi-petabyte media storage in PostgreSQL Andrew Pantyukhin 6月17日(周日)上午09:00-12:30 地点：人民大学逸夫会议中心第一报告厅(16-17号) （ DBA/Kernel/Development，09:30-12:30 Sunday, June 17） 9:00 - 9:45 DataBase As Application 罗翼(去哪儿) 09:50 - 10:35 PostgreSQL vs Oracle 谭峰(斯凯网络) 10:40 - 11:10 PostgreSQL经验谈 德哥(斯凯网络) 11:10 - 11:40 Greenplum数据库经验谈 唐诚 11:40 - 12:10 数据库压力和成本向应用层的分摊 肖涛 6月17日(周日)上午13:30-17:00 地点：人民大学逸夫会议中心第一报告厅(16-17号) （ DBA/Kernel/Development，13:30-17:00 Sunday, June 17） 13:30 - 14:15 PostgreSQL 内核简介 彭煜玮(武汉大学) 14:15 - 14:35 高校数据库教学中PostgreSQL的学习、使用与定制简介 张孝(人民大学) 14:35 - 15:05 Multi-Instance Aware Query Processing Cao Yu(EMC) 15:05 - 15:35 PostgreSQL的数据分析工具MADlib Gavin Yang(VMWARE) 15:35 - 16:05 基于PostgreSQL的物联网数据管理 高需(中科院) 16:05 - 16:35 Slony-II复制 黄坚(广州正虹) 16:35 - 15:05 龙芯3A上PostgreSQL初探 何雄 上面仅列出大会的 topic， 关于嘉宾介绍和大会的详细信息参考：http://wiki.postgresql.org/wiki/Pgconfchina2012 大会的合影 大会的精彩瞬间大会的精彩瞬间参考德哥的 bloghttp://blog.163.com/digoal@126/blog/static/163877040201251710524904/ 大会的PPT下载大会的PPT 资料下载:http://bbs.pgsqldb.com/client/post_show.php?zt_auto_bh=55858 我发言的 PPThttp://wenku.it168.com/d_000372377.shtml","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"PostgreSQL: Network Address Types ","slug":"20120608161225","date":"2012-06-08T08:12:25.000Z","updated":"2018-09-04T01:33:56.852Z","comments":true,"path":"20120608161225.html","link":"","permalink":"https://postgres.fun/20120608161225.html","excerpt":"","text":"PostgreSQL 提供专门的网络数据类型(inet,cidr , macaddr )用来存储 IPv4, IPv6, 和 MAC 地址，官方建议使用 PostgreSQL中提供的专有网络数据类型存储网络地址，而不是用 text 类型存储，因为网络类型数据提供数据检验和各种网络操作符，函数，带来很大方便。 网络地址数据类型Network Address Types Inet 和 Cidr The essential difference between inet and cidr data types is that inet accepts values with nonzero bits to the right of the netmask, whereas cidr does not. 备注：上面是手册上说的，没整明白，网上看了半天也没找到 inet 和 cidr 的区别，但根据实验来看, cidr 默认存储子网的信息而 inet 可以不存储，实验如下。 测试 Inet 与 Cidr以下测试 Inet 与 Cidr 网络地址类型的差异。 测试 cidr 类型1234567891011121314151617181920212223skytf=&gt; create table test_cidr (id serial,ip cidr); NOTICE: CREATE TABLE will create implicit sequence \"test_cidr_id_seq\" for serial column \"test_cidr.id\" CREATE TABLEskytf=&gt; insert into test_cidr(ip) values ('192.168.1.10/32'); INSERT 0 1skytf=&gt; insert into test_cidr(ip) values ('192.168.1.11/32'); INSERT 0 1skytf=&gt; insert into test_cidr(ip) values ('192.168.1.1'); INSERT 0 1skytf=&gt; insert into test_cidr(ip) values ('192.168.1.0'); INSERT 0 1skytf=&gt; insert into test_cidr(ip) values ('192.168.1.0/24'); INSERT 0 1skytf=&gt; select * from test_cidr; id | ip ----+----------------- 1 | 192.168.1.10/32 2 | 192.168.1.11/32 3 | 192.168.1.1/32 4 | 192.168.1.0/32 5 | 192.168.1.0/24 (5 rows) 备注：从上面看出,cidr 类型网络数据都是以 address/y 类型存储，address 为网络IP，y 为子网掩码， 如果 y 没有指定，默认为填充 32(ipv4) 或者 128(ipv6)。 测试 inet 类型1234567891011121314151617181920skytf=&gt; create table test_inet(id serial,ip inet); NOTICE: CREATE TABLE will create implicit sequence \"test_inet_id_seq\" for serial column \"test_inet.id\" CREATE TABLEskytf=&gt; insert into test_inet (ip) values (inet'192.168.1.1'); INSERT 0 1 skytf=&gt; insert into test_inet (ip) values (inet'192.168.1.1/32'); INSERT 0 1 skytf=&gt; insert into test_inet (ip) values (inet'192.168.1.0/23'); INSERT 0 1 skytf=&gt; insert into test_inet (ip) values (inet'192.168.1.0/24'); INSERT 0 1skytf=&gt; select * From test_inet; id | ip ----+---------------- 1 | 192.168.1.1 2 | 192.168.1.1 3 | 192.168.1.0/23 4 | 192.168.1.0/24 (4 rows) 备注： inet 类型网络数据，可以不存储子网掩码。 测试索引插入测试数据并创建索引12345678910111213141516171819202122232425262728293031skytf=&gt; insert into test_inet select generate_series(5,10000),inet('192.168.1.2/32'); INSERT 0 9996 skytf=&gt; create index concurrently idx_test_inet_ip on test_inet using btree (ip); CREATE INDEXskytf=&gt; \\d test_inet Table \"skytf.test_inet\" Column | Type | Modifiers --------+---------+-------------------------------------------------------- id | integer | not null default nextval('test_inet_id_seq'::regclass) ip | inet | Indexes: \"idx_test_inet_ip\" btree (ip) skytf=&gt; analyze test_inet; ANALYZEskytf=&gt; select * from test_inet limit 10; id | ip ----+---------------- 1 | 192.168.1.1 2 | 192.168.1.1 3 | 192.168.1.0/23 4 | 192.168.1.0/24 5 | 192.168.1.2 6 | 192.168.1.2 7 | 192.168.1.2 8 | 192.168.1.2 9 | 192.168.1.2 10 | 192.168.1.2 (10 rows) 查看执行计划123456789101112131415161718192021222324skytf=&gt; explain analyze select * from test_inet where ip='192.168.1.1'; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------- Index Scan using idx_test_inet_ip on test_inet (cost=0.00..8.27 rows=1 width=11) (actual time=0.127..0.150 rows=2 loops=1) Index Cond: (ip = '192.168.1.1'::inet) Total runtime: 0.260 ms (3 rows) skytf=&gt; select * from test_inet where ip &lt; '192.168.1.2'; id | ip ----+---------------- 3 | 192.168.1.0/23 4 | 192.168.1.0/24 1 | 192.168.1.1 2 | 192.168.1.1 (4 rows)skytf=&gt; explain analyze select * from test_inet where ip &lt; '192.168.1.2'; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------- Index Scan using idx_test_inet_ip on test_inet (cost=0.00..8.32 rows=4 width=11) (actual time=0.028..0.075 rows=4 loops=1) Index Cond: (ip &lt; '192.168.1.2'::inet) Total runtime: 0.201 ms (3 rows) 备注：在 inet 类型数据上创建普通的 btree 索引，在查询时可以用到索引。 Inet 和 Cidr 相关函数host 函数取 IP1234567891011skytf=&gt; select cidr '192.168.1.1/32'; cidr ---------------- 192.168.1.1/32 (1 row)skytf=&gt; select host (cidr '192.168.1.1/32'); host ------------- 192.168.1.1 (1 row) text 函数: 取出 IP 地址和子网掩码，输出为 text 类型12345skytf=&gt; select text (cidr '192.168.1.1/32'); text ---------------- 192.168.1.1/32 (1 row) 取 netmask1234567891011skytf=&gt; select netmask (cidr '192.168.1.1/32'); netmask ----------------- 255.255.255.255 (1 row)skytf=&gt; select netmask (cidr '192.168.0.0/16'); netmask ------------- 255.255.0.0 (1 row) 参考 http://www.postgresql.org/docs/9.2/static/datatype-net-types.html#DATATYPE-INET","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 浅谈 PostgreSQL 的 timestamp 类型 ","slug":"20120607143619","date":"2012-06-07T06:36:19.000Z","updated":"2018-09-04T01:33:56.805Z","comments":true,"path":"20120607143619.html","link":"","permalink":"https://postgres.fun/20120607143619.html","excerpt":"","text":"今天再次有同事问我 PostgreSQL 中时间类型的格式显示问题，例如 “ 2012-06-07 14:00:02.412827+08” 如何显示成 “ 2012-06-07 14:00:02” ，既然问这个问题的人还不少，于是决定记下来，尽管内容比较简单。回答这个问题之前，先来看看 PG 的手册，补充下基础知识。 PostgreSQL 日期/时间类型备注：这里不准备详细介绍各种类型，请注意上面表格中的[ (p) ] ，这个是什么呢？这个是指时间的精度，time, timestamp, 和 interval 类型都可以指定精度，精度的取值范围是 0 到 6，下面通过具体实验来体验下精度。 Current_timestamp 实验2.1 查询 current_timestamp12345skytf=&gt; select current_timestamp; now ------------------------------- 2012-06-07 14:00:02.412827+08 (1 row) 备注：current_timestamp 函数返回时间类型为 timestamp with time zone，故返回结果后面包括时区 +08 ，以及精度 412827，那么如何去掉精度和时区呢？ 2.2 去掉精度12345skytf=&gt; select current_timestamp(0); timestamptz ------------------------ 2012-06-07 14:07:17+08 (1 row) 2.3 去掉时区12345skytf=&gt; select current_timestamp(0)::timestamp without time zone; timestamp --------------------- 2012-06-07 14:07:49 (1 row) 2.4 也可以用 cast 函数类型转换12345skytf=&gt; select cast (current_timestamp(0) as timestamp without time zone); timestamp --------------------- 2012-06-07 14:14:55 (1 row) 2.5 了解 [p] 的含义1234567891011skytf=&gt; select current_timestamp(2)::timestamp without time zone; timestamp ------------------------ 2012-06-07 14:15:42.64 (1 row)skytf=&gt; select current_timestamp(6)::timestamp without time zone; timestamp ---------------------------- 2012-06-07 14:15:46.281422 (1 row) 备注：可见 [p] 是指时间类型小数点后面的精度，如果 p 指定 2，则精度为2，如果 p 指定 6 则精度为 6； 所以在定义表的时候就应该事先定义 timestamp 时间类型的精度。 定义时间类型精度为0创建测试表，如下： 123456789101112131415161718192021222324252627skytf=&gt; create table test_p (id int4 primary key, create_time timestamp(0) without time zone); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_p_pkey\" for table \"test_p\" CREATE TABLEskytf=&gt; \\d test_p Table \"skytf.test_p\" Column | Type | Modifiers -------------+--------------------------------+----------- id | integer | not null create_time | timestamp(0) without time zone | Indexes: \"test_p_pkey\" PRIMARY KEY, btree (id)skytf=&gt; select current_timestamp; now ------------------------------- 2012-06-07 14:18:31.683105+08 (1 row) skytf=&gt; insert into test_p values (1,current_timestamp); INSERT 0 1skytf=&gt; select * from test_p; id | create_time ----+--------------------- 1 | 2012-06-07 14:19:02 (1 row)","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: Using pgsql_fdw connect remote PostgteSQL DB","slug":"20120607112420","date":"2012-06-07T03:24:20.000Z","updated":"2018-09-04T01:33:56.743Z","comments":true,"path":"20120607112420.html","link":"","permalink":"https://postgres.fun/20120607112420.html","excerpt":"","text":"今天在从 德哥 blog 中发现 PostgreSQL9.1 外部表支持另一个模块，即 pgsql_fdw ，使用这个模块可以方便地在一个 PG 库中访问远程 PG 库中的表，当然是以创建外部表的形式， 这种方法可以近似地理解成 dblink ，关于PostgreSQL 中的 dblink 可以参考以下文章： https://postgres.fun/20110621134831.html https://postgres.fun/20100904121139.html 接下来演示下 pgsql_fdw 的实验过程。 pgsql_fdw 安装1.1 RequirementPostgreSQL 9.1 or later 1.2 下载https://build.opensuse.org/package/files?package=pgsql_fdw&amp;project=home%3Adeadpoint 1.3 解压1[postgres@pgb extension]$ tar zxvf pgsql_fdw-1.0.tar.gz 1.4 编译1234567[postgres@pgb pgsql_fdw-1.0]$ make USE_PGXS=1 gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I/opt/pgsql9.1/include -I. -I. -I/opt/pgsql9.1/include/server -I/opt/pgsql9.1/include/internal -D_GNU_SOURCE -I/usr/include/libxml2 -c -o pgsql_fdw.o pgsql_fdw.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I/opt/pgsql9.1/include -I. -I. -I/opt/pgsql9.1/include/server -I/opt/pgsql9.1/include/internal -D_GNU_SOURCE -I/usr/include/libxml2 -c -o option.o option.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I/opt/pgsql9.1/include -I. -I. -I/opt/pgsql9.1/include/server -I/opt/pgsql9.1/include/internal -D_GNU_SOURCE -I/usr/include/libxml2 -c -o deparse.o deparse.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I/opt/pgsql9.1/include -I. -I. -I/opt/pgsql9.1/include/server -I/opt/pgsql9.1/include/internal -D_GNU_SOURCE -I/usr/include/libxml2 -c -o connection.o connection.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I/opt/pgsql9.1/include -I. -I. -I/opt/pgsql9.1/include/server -I/opt/pgsql9.1/include/internal -D_GNU_SOURCE -I/usr/include/libxml2 -c -o ruleutils.o ruleutils.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wformat-security -fno-strict-aliasing -fwrapv -fpic -shared -o pgsql_fdw.so pgsql_fdw.o option.o deparse.o connection.o ruleutils.o -L/opt/pgsql9.1/lib -L/usr/lib -Wl,-rpath,'/opt/pgsql9.1/lib',--enable-new-dtags -L/opt/pgsql9.1/lib -lpq 1.5 安装12345[postgres@pgb pgsql_fdw-1.0]$ make USE_PGXS=1 install/bin/mkdir -p '/opt/pgsql9.1/lib' /bin/mkdir -p '/opt/pgsql9.1/share/extension' /bin/sh /opt/pgsql9.1/lib/pgxs/src/makefiles/../../config/install-sh -c -m 755 pgsql_fdw.so '/opt/pgsql9.1/lib/pgsql_fdw.so' /bin/sh /opt/pgsql9.1/lib/pgxs/src/makefiles/../../config/install-sh -c -m 644 ./pgsql_fdw.control '/opt/pgsql9.1/share/extension/' /bin/sh /opt/pgsql9.1/lib/pgxs/src/makefiles/../../config/install-sh -c -m 644 ./pgsql_fdw--1.0.sql '/opt/pgsql9.1/share/extension/' pgsql_fdw 演示准备访问同一台 PostgreSQLServer中的另外一个库的表。 源库: 127.0.0.1/1921 mydb目标库 127.0.0.1/1921 skytf 2.1 创建 extension123456789101112131415[postgres@pgb pgsql_fdw-1.0]$ psql skytf postgres psql (9.1.0) Type \"help\" for help.skytf=# create extension pgsql_fdw; CREATE EXTENSIONskytf=# \\dx List of installed extensions Name | Version | Schema | Description -----------+---------+------------+---------------------------------------------------- file_fdw | 1.0 | public | foreign-data wrapper for flat file access mysql_fdw | 1.0 | public | Foreign data wrapper for querying a MySQL server pgsql_fdw | 1.0 | public | foreign-data wrapper for remote PostgreSQL servers plpgsql | 1.0 | pg_catalog | PL/pgSQL procedural language (4 rows) 备注： pgsql_fdw 已创建。 2.2 创建 server1234567891011121314151617skytf=# grant usage on foreign data wrapper pgsql_fdw to skytf; GRANTskytf=# \\c skytf skytf You are now connected to database \"skytf\" as user \"skytf\".CREATE SERVER pgsql_srv FOREIGN DATA WRAPPER pgsql_fdw OPTIONS (host '127.0.0.1', port '1923', dbname 'mydb');skytf=&gt; \\des List of foreign servers Name | Owner | Foreign-data wrapper --------------+-------+---------------------- file_srv | skytf | file_fdw mysql_svr_25 | skytf | mysql_fdw pgsql_srv | skytf | pgsql_fdw (3 rows) 备注： pgsql_srv 已创建。 2.3 create mapping user123CREATE USER MAPPING FOR public SERVER pgsql_srv OPTIONS (user 'mydb', password 'mydb'); CREATE USER MAPPING 2.4 create foreign table123456skytf=&gt; CREATE FOREIGN TABLE ft_test ( skytf(&gt; id integer , skytf(&gt; name character varying(32) ) skytf-&gt; SERVER pgsql_srv skytf-&gt; OPTIONS (nspname 'mydb', relname 'test'); CREATE FOREIGN TABLE 2.5 查询测试12345skytf=&gt; select * from ft_test limit 1; id | name ------+------ 5024 | AAAA (1 row) 2.6 执行计划12345678skytf=&gt; explain analyze select * from ft_test where id=1; QUERY PLAN ------------------------------------------------------------------------------------------------------------ Foreign Scan on ft_test (cost=100.00..111.46 rows=2 width=8) (actual time=2.190..2.190 rows=0 loops=1) Filter: (id = 1) Remote SQL: DECLARE pgsql_fdw_cursor_10 SCROLL CURSOR FOR SELECT id, name FROM mydb.test WHERE (id = 1) Total runtime: 3.092 ms (4 rows) 备注：根据where 条件查询， pgsql_fdw 会将查询语句发送到目标远程 PG 库，并且根据文档描述，pgsql_fdw 优化了以下： 1. 减少了远程数据传输； 2. 将 where 条件子表发送到目标服务器。 统计外部表记录总数，执行计划如下： 12345678skytf=&gt; explain analyze select count(*) from ft_test ; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------------------- Aggregate (cost=33446.70..33446.71 rows=1 width=0) (actual time=27718.201..27718.206 rows=1 loops=1) -&gt; Foreign Scan on ft_test (cost=100.00..28553.56 rows=1957256 width=0) (actual time=40.259..15705.231 rows=1990000 loops=1) Remote SQL: DECLARE pgsql_fdw_cursor_8 SCROLL CURSOR FOR SELECT NULL, NULL FROM mydb.test Total runtime: 27718.583 ms (4 rows) 2.7 Connection management12345skytf=&gt; select * From pgsql_fdw_connections; srvid | srvname | usesysid | usename -------+-----------+----------+--------- 74984 | pgsql_srv | 41818 | skytf (1 row) 2.8 查看系统进程123[postgres@pgb pg_root]$ ps -ef | grep mydb postgres 18454 31854 0 10:37 ? 00:00:00 postgres: mydb mydb 127.0.0.1(25949) idle in transaction postgres 18514 31854 1 10:55 ? 00:00:00 postgres: mydb mydb 127.0.0.1(46007) idle in transaction 备注：在一个 session 中查看外部表是以开启事务方式进行，直到退出当前 session 或者切换用户，事务才结束。 常见 ERROR3.1 查询外部表时报错12345678[postgres@pgb ~]$ psql skytf skytf psql (9.1.0) Type \"help\" for help.skytf=&gt; select * from ft_test limit 1; ERROR: password is required DETAIL: Non-superuser cannot connect if the server does not request a password. HINT: Target server is authentication method must be changed. 备注：上面报错是因为 pg_hba.conf 文件认证问题，本机配置的是 local 连接所有库都是 trust 方式，即不需要密码，而 pgsql_fdw 连接时需要提供用户名和密码。 解决方法修改 pg_hba.conf，增加红色字体行，并 reloadpg_hba.conf 文件123456# IPv4 local connections: host mydb mydb 127.0.0.1/32 md5 host all all 127.0.0.1/32 trustreload 配置文件[postgres@pgb pg_root]$ pg_ctl reload -D $PGDATA server signaled pgsql_fdw 外部表限制 外部表只读，其它修改操作不允许，例如 update，delete，INSERT 等，并且外部表也不会被 vacuum。 当开启 session 查询外部表时， pgsql_fdw 以事务方式开启，即便当前 session 空闲，事务也没有提交，直到退出当前 session 或者切换到其它用户后，事务。 参考 http://interdbconnect.sourceforge.net/pgsql_fdw/pgsql_fdw-en.html http://blog.163.com/digoal@126/blog/static/163877040201231514057303/","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"pgsql_fdw","slug":"pgsql-fdw","permalink":"https://postgres.fun/tags/pgsql-fdw/"}]},{"title":"Postgres-XC : Shutting Down and Starting Up ","slug":"20120606154005","date":"2012-06-06T07:40:05.000Z","updated":"2018-09-04T01:33:56.696Z","comments":true,"path":"20120606154005.html","link":"","permalink":"https://postgres.fun/20120606154005.html","excerpt":"","text":"今天总结下 Postgres-XC 的关闭与开启，由于 Postgres-XC 体系比较复杂，组件较多，在关闭的时候需要保持所有节点事务的一致性，所以需要小心，关于 Postgres-XC 的关闭与开启总结如下。 Postgres-XC 关闭步骤 shutdown all datanode shutdown all coordinate shutdown all gtm_proxy shutdown all gtm_stnadby if exists shutdown all gtm Postgres-XC 开启步骤 startup gtm startup gtm_standby if exists startup gtm_proxy if exists startup all datanode startup all coordinator备注：以上是 Postgres-XC 关闭与开启的步骤，下面通过实验具体演示下。 Postgres-XC 关闭测试123456789101112131 stop datanodepg_ctl stop -Z datanode -m fast -D /pgdata_xc/db_1/pg_root pg_ctl stop -Z datanode -m fast -D /pgdata_xc/db_2/pg_root 2 stop coordinatorpg_ctl stop -Z coordinator -m fast -D /database/1922/pgdata1/pgdata_xc/coord1 pg_ctl stop -Z coordinator -m fast -D /database/1922/pgdata1/pgdata_xc/coord2 3 stop gtm_stnadbygtm_ctl stop -m fast -S gtm -D /pgdata_xc/gtm_standby 4 stop gtmgtm_ctl stop -m fast -S gtm -D /database/1922/pgdata1/pgdata_xc/gtm Postgres-XC 开启测试123456789101112131 start gtmgtm -D /database/1922/pgdata1/pgdata_xc/gtm &amp; 2 start gtm_standbygtm -D /pgdata_xc/gtm_standby &amp; 3 start datanodepostgres -X -D /pgdata_xc/db_1/pg_root -p 15431 -i &amp; postgres -X -D /pgdata_xc/db_2/pg_root -p 15432 -i &amp; 4 start coordinatorpostgres -C -D /database/1922/pgdata1/pgdata_xc/coord1 -p 1921 -i &amp; postgres -C -D /database/1922/pgdata1/pgdata_xc/coord2 -p 1925 -i &amp; 附：GTM 配置今天在测试 Postgres-XC 关闭和启动时，新增加了 gtm_standby 组件，关于 Postgres-XC 的安装可参考之前的BLOG：https://postgres.fun/20120603152309.html ， 这里贴下 gtm 和 gtm_standby 配置。 3.1 附 gtm 配置参数1234567nodename = 'gtm' # Specifies the node name. # (changes requires restart) listen_addresses = '*' # Listen addresses of this GTM. # (changes requires restart) port = 6666 # Port number of this GTM. # (changes requires restart) startup = ACT # Start mode. ACT/STANDBY. 3.2 附 gtm_standby 配置参数1234567891011nodename = 'gtm_stnadby' # Specifies the node name. # (changes requires restart) listen_addresses = '*' # Listen addresses of this GTM. # (changes requires restart) port = 6666 # Port number of this GTM. # (changes requires restart) startup = STANDBY # Start mode. ACT/STANDBY. active_host = '192.168.1.35' # Listen address of active GTM. # (changes requires restart) active_port =6666 # Port number of active GTM. # (changes requires restart) 参考 http://postgres-xc.sourceforge.net/docs/1_0/runtime.html","categories":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/categories/Postgres-XC/"}],"tags":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/tags/Postgres-XC/"}]},{"title":"PostgreSQL-XC : Data Replication Or Distribution ? ","slug":"20120604173740","date":"2012-06-04T09:37:40.000Z","updated":"2018-09-04T01:33:56.649Z","comments":true,"path":"20120604173740.html","link":"","permalink":"https://postgres.fun/20120604173740.html","excerpt":"","text":"在 PostgreSQL-XC 体系中，数据分布有两种形式，即 Replication 或者 Distribution，这里简单描述下 PostgreSQL-XC 这两种数据分布，下面是实验过程。 Replication Or Distribution1.1 Replication表的每一行存在所有数据节点( datanode )中，即每个数据节点都有完整的表数据。 1.2 Distribution表的每一行仅存在一个数据节点( datanode )中，即每个数据节点仅保留表的部分数据。 Replication 表测试2.1创建 replication 表并插入数据12345678910francs=&gt; create table test_replication (id int4 primary key,name varchar(32)) distribute by replication; NOTICE:CREATE TABLE / PRIMARY KEY will create implicit index \"test_replication_pkey\" for table \"test_replication\" CREATE TABLE francs=&gt; insert into test_replication select generate_series(1,10000),'replication'; INSERT 0 10000 francs=&gt; select count(*) from test_replication ; count ------- 10000 (1 row) 2.2 到数据节点一验证数据1234567891011[pgxc@redhatB gtm_standby]$ psql -p 15431 francs francs psql (PGXC 1.0beta2, based on PG 9.1.3) Type \"help\" for help. francs=&gt; select count(*) from test_replication ; WARNING: Do not have a GTM snapshot available WARNING: Do not have a GTM snapshot available count ------- 10000 (1 row) 2.3到数据节点二验证数据1234567891011[pgxc@redhatB pg_root]$ psql -p 15432 francs francs psql (PGXC 1.0beta2, based on PG 9.1.3) Type \"help\" for help. francs=&gt; select count(*) from test_replication ; WARNING: Do not have a GTM snapshot available WARNING: Do not have a GTM snapshot available count ------- 10000 (1 row) 备注：可见 replication 表数据在每个数据节点都有完整数据( 如果在创建表时仅指定数据节点的情况除外)。 Distribute 表测试 Distribute 表数据分片方式有多种，包括 ROUND ROBIN， HASH ，MODULO，接下来以 hash ,rounnd robin 分片方式举例。 3.1 创建 hash 分区表并插入数据123456789francs=&gt; create table test_hash (id int4 primary key ,name varchar(32)) distribute by hash(id);NOTICE:CREATE TABLE / PRIMARY KEY will create implicit index \"test_hash_pkey\" for table \"test_hash\" CREATE TABLE francs=&gt; insert into test_hash select generate_series(1,10000),'hash'; INSERT 0 10000 francs=&gt; select count(*) from test_hash; count ------- 10000 (1 row) 3.2 到数据节点一验证数据1234567891011[pgxc@redhatB gtm_standby]$ psql -p 15431 francs francs psql (PGXC 1.0beta2, based on PG 9.1.3) Type \"help\" for help. francs=&gt; select count(*) from test_hash; WARNING: Do not have a GTM snapshot available WARNING: Do not have a GTM snapshot available count ------- 5039 (1 row) 3.2 到数据节点二验证数据1234567891011[pgxc@redhatB pg_root]$ psql -p 15432 francs francs psql (PGXC 1.0beta2, based on PG 9.1.3) Type \"help\" for help. francs=&gt; select count(*) from test_hash; WARNING: Do not have a GTM snapshot available WARNING: Do not have a GTM snapshot available count ------- 4961 (1 row) 备注：从上面看出 distributed 表数据节点表只存部分数据，当然创建表的时候也可以指定数据节点。 执行计划比较比较以下场景下复制表和分片表的执行计划。 查询单条记录场景12345678910111213141516171819202122francs=&gt; explain verbose select * from test_hash where id=1; QUERY PLAN ---------------------------------------------------------------------------- Data Node Scan on \"__REMOTE_FQS_QUERY__\" (cost=0.00..0.00 rows=0 width=0) Output: test_hash.id, test_hash.name Node/s: db_1 Remote query: SELECT id, name FROM test_hash WHERE (id = 1) (4 rows) francs=&gt; explain verbose select * from test_replication where id=1; QUERY PLAN ---------------------------------------------------------------------------- Data Node Scan on \"__REMOTE_FQS_QUERY__\" (cost=0.00..0.00 rows=0 width=0) Output: test_replication.id, test_replication.name Node/s: db_1 Remote query: SELECT id, name FROM test_replication WHERE (id = 1) (4 rows) francs=&gt; explain select * from test_hash where name='A'; QUERY PLAN ---------------------------------------------------------------------------- Data Node Scan on \"__REMOTE_FQS_QUERY__\" (cost=0.00..0.00 rows=0 width=0) Node/s: db_1, db_2(2 rows) 备注：只查询单条记录， replication 只扫描一个数据节点。而 distribute 表如果根据 分区键查询， 扫描一个节点，如果根据非分区键查询，则需要扫描多个节点。 count(*) 场景12345678francs=&gt; explain select count(*) from test_hash; QUERY PLAN --------------------------------------------------------------------------------------------- Aggregate (cost=2.50..2.51 rows=1 width=0) -&gt; Materialize (cost=0.00..0.00 rows=0 width=0) -&gt; Data Node Scan on \"__REMOTE_GROUP_QUERY__\" (cost=0.00..0.00 rows=1000 width=0) Node/s: db_1, db_2 (4 rows) 备注：distributd 表 count 语句扫描所有数据节点。123456francs=&gt; explain select count(*) from test_replication; QUERY PLAN ---------------------------------------------------------------------------- Data Node Scan on \"__REMOTE_FQS_QUERY__\" (cost=0.00..0.00 rows=0 width=0) Node/s: db_1 (2 rows) 备注：replication 表 count 语句只扫描一个数据节点； round robin分片方式123456789101112francs=&gt; create table test_round (id int4,name varchar(32)) distribute by round robin;CREATE TABLE francs=&gt; insert into test_round select generate_series(1,10000),'a'; INSERT 0 10000 francs=&gt; explain select * from test_round where id=1; QUERY PLAN ---------------------------------------------------------------------------- Data Node Scan on \"__REMOTE_FQS_QUERY__\" (cost=0.00..0.00 rows=0 width=0) Node/s: db_1, db_2(2 rows) 备注： Round Robin 方式会将表数据分散到各个数据节点，但查询数据时由于不知道分片规则，故需要遍历所有数据节点。 总结5.1 replication表 replication 表查询时只需要读任一个数据节点； replication 表更改数据时，需要同时对所有数据节点进行，代价较大； replication 适用于读比较繁忙的静态数据表。 5.2 distribute 表 单独查询或者写一条记录时，如果根据分区键查询，只需要扫描一个数据节点 ( Round Robin 分片方式除外)； 单独查询或者写一条记录时，如果根据非分区键，需要扫描所有数据节点。 如果查询需要扫描多个数据节点，性能会有所降低； 手册解释 REPLICATION Each row of the table will be replicated into all the datanode of the Postgres-XC database cluster. HASH ( column_name ) Each row of the table will be placed based on the hash value of the specified column. Following typeis allowed as distribution column: INT8, INT2, OID, INT4, BOOL, INT2VECTOR, OIDVECTOR, CHAR, NAME, TEXT,BPCHAR, BYTEA, VARCHAR, FLOAT4, FLOAT8, NUMERIC, CASH, ABSTIME, RELTIME, DATE, TIME, TIMESTAMP, TIMESTAMPTZ,INTERVAL, and TIMETZ. 参考 http://postgres-xc.sourceforge.net/docs/1_0/sql-createtable.html","categories":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/categories/Postgres-XC/"}],"tags":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/tags/Postgres-XC/"}]},{"title":"PostgreSQL-XC : Installation","slug":"20120603152309","date":"2012-06-03T07:23:09.000Z","updated":"2018-09-04T01:33:56.586Z","comments":true,"path":"20120603152309.html","link":"","permalink":"https://postgres.fun/20120603152309.html","excerpt":"","text":"上篇博客 介绍了 PostgreSQL-XC 基本原理，对 PostgreSQL-XC 体系结构和组件有了初步认识，今天计划在虚拟机上安装 PostgreSQL-XC, 以做进一步学习分析，下面是 PostgreSQL-XC 安装的整个过程。 安装准备1.1 requiremetns硬件需求：官方建议使用 64 位的系统，内存至少 4GB系统需求：64bit CentOS 5.4 或者其它其它 LINUX 系统软件需求：GNU make version: 3.80 or newer Flex: 2.5.31 or later Perl: 5.8 or later GCC: Recent versions of GCC are recommendable 软件需求，具体可参考 http://postgres-xc.sourceforge.net/docs/1_0/install-requirements.html 1.2 实际安装环境两台笔记本虚拟机 RHEL 6.2 ，内存 512 MIP 192.168.1.35 ；192.168.1.36 1.3 安装规划一个GTM节点，两个协调(Coordinator)节点，二个数据节点；这是第一次安装，为了操作简便，没有安装GTM-Standby,节点和 GTM-Proxy，这两个节点可以后期测试。192.168.1.35GTM节点: gtm 6666协调节点一: coord1 1921协调节点二: coord1 1925 192.168.1.36数据节点一: db_1 15431数据节点二: db_2 15432 1.4 安装规划图 系统配置两台主机上都操作。2.1 创建 pgxc 系统用户并赋权123456789[root@redhatB bin]# groupadd pgxc [root@redhatB bin]# useradd pgxc -g pgxc [root@redhatB bin]# passwd pgxc Changing password for user pgxc. New password: BAD PASSWORD: it is based on a dictionary word Retype new password: passwd: all authentication tokens updated successfully.[root@redhatB pgxc]# chown -R pgxc:pgxc /pgdata_xc 2.2系统层面其它配置根据需要配置其它系统层参数，由于是在虚拟机上运行，其它系统参数暂不修改。 安装 Postgres-XC两台主机上都安装 Postgres-XC 软件 3.1 pgxc_v1.0beta2 下载https://sourceforge.net/projects/postgres-xc/files/下载 pgxc_v1.0beta2.tar.gz 3.2 将安装包解压 ( 两台 )12cd /opt/soft_bak tar zxvf pgxc_v1.0beta2.tar.gz 备注：解压后，产生目录 pgxc 3.3 编译配置1./configure --prefix=/opt/pgsql_xc --with-segsize=8 --with-wal-segsize=64 --with-wal-blocksize=64 --with-perl --with-python --with-openssl --with-pam --with-ldap --with-libxml --with-libxslt --enable-thread-safety 备注：conifugre 过程中如有提示缺少相关包，yum 安装即可。 3.4 编译gmake备注：编译后，如果出现”All of PostgreSQL successfully made. Ready to install.”，说明编译成功。 3.5 安装 PostgreSQL XC1gmake install 备注：安装后如果提示 “PostgreSQL installation complete.” 说明安装成功。 配置数据节点192.168.1.36 配置以下：4.1 修改环境变量( .bash_profile )12345678910export PGPORT=15431 export PGDATA=/pgdata_xc/db_1/pg_root export LANG=en_US.utf8export PGHOME=/opt/pgsql_xc export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib export DATE=`date +\"%Y%m%d%H%M\"` export PATH=$PGHOME/bin:$PATH:. export MANPATH=$PGHOME/share/man:$MANPATH alias rm='rm -i' alias ll='ls -lh' 4.2 创建数据目录12pgxc@redhatB pgdata_xc]$ mkdir -p /pgdata_xc/db_1/pg_root [pgxc@redhatB pgdata_xc]$ mkdir -p /pgdata_xc/db_2/pg_root 4.3 初始化数据节点11initdb -D /pgdata_xc/db_1/pg_root --nodename db_1 -E UTF8 --locale=C -U postgres -W 4.4 修改数据节点一配置文件 postgresql.conf 参数123456789101112131415161718# - Connection Settings - port = 15431# - Where to Log - log_destination = 'csvlog' logging_collector = on log_directory = 'pg_log' log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' log_truncate_on_rotation = on log_rotation_age = 1d log_rotation_size = 10MB gtm_host = 'localhost'# GTM CONNECTION gtm_host = '192.168.1.35' gtm_port = 6666 pgxc_node_name = 'db_1'# DATA NODES AND CONNECTION POOLING pooler_port = 6667 max_pool_size = 100 4.5修改数据节点一 pg_hba.conf ，增加以下12host all all 192.168.1.35/32 trust host all all 0.0.0.0/0 md5 4.6 初始化数据节点21initdb -D /pgdata_xc/db_2/pg_root --nodename db_2 -E UTF8 --locale=C -U postgres -W 4.7 修改数据节点二配置文件 postgresql.conf 参数123456789101112131415161718# - Connection Settings - port = 15432# - Where to Log - log_destination = 'csvlog' logging_collector = on log_directory = 'pg_log' log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' log_truncate_on_rotation = on log_rotation_age = 1d log_rotation_size = 10MB gtm_host = 'localhost'# GTM CONNECTION gtm_host = '192.168.1.35' gtm_port = 6666 pgxc_node_name = 'db_2'# DATA NODES AND CONNECTION POOLING pooler_port = 6667 max_pool_size = 100 4.8 修改数据节点一 pg_hba.conf ，增加以下12host all all 192.168.1.35/32 trust host all all 0.0.0.0/0 md5 配置 GTM 节点192.168.1.35 配置以下5.1 修改 .bash_profile12345678910export PGPORT=1921 export PGDATA=/database/1922/pgdata1/pgdata_xc/coord1 export LANG=en_US.utf8export PGHOME=/opt/pgsql_xcexport LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib export DATE=`date +\"%Y%m%d%H%M\"` export PATH=$PGHOME/bin:$PATH:. export MANPATH=$PGHOME/share/man:$MANPATH alias rm='rm -i' alias ll='ls -lh' 5.2 创建 gtm 数据目录1[pgxc@redhat6 pgdata_xc]$ mkdir -p /database/1922/pgdata1/pgdata_xc/gtm 5.3 install gtm123456789[pgxc@redhat6 bin]$ initgtm -Z gtm -D /database/1922/pgdata1/pgdata_xc/gtm The files belonging to this GTM system will be owned by user \"pgxc\". This user must also own the server process.fixing permissions on existing directory /database/1922/pgdata1/pgdata_xc/gtm ... ok creating configuration files ... okSuccess. You can now start the GTM server using: gtm -D /database/1922/pgdata1/pgdata_xc/gtm or gtm_ctl -Z gtm -D /database/1922/pgdata1/pgdata_xc/gtm -l logfile start 配置 Coordinator 节点192.168.1.35 配置以下 6.1 创建 coordinator 数据目录12[pgxc@redhat6 pgdata_xc]$ mkdir -p /database/1922/pgdata1/pgdata_xc/coord1 [pgxc@redhat6 pgdata_xc]$ mkdir -p /database/1922/pgdata1/pgdata_xc/coord2 6.2 install coord11initdb -D /database/1922/pgdata1/pgdata_xc/coord1 --nodename coord1 -E UTF8 --locale=C -U postgres -W 6.3 修改 coord1 配置文件 postgresql.conf12345678910111213141516171819# - Connection Settings - listen_addresses = '*' port = 1921# - Where to Log - log_destination = 'csvlog' logging_collector = on log_directory = 'pg_log' log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' log_truncate_on_rotation = on log_rotation_age = 1d log_rotation_size = 10MB gtm_host = 'localhost'# GTM CONNECTION gtm_host = 'localhost' gtm_port = 6666 pgxc_node_name = 'coord1'# DATA NODES AND CONNECTION POOLING pooler_port = 6667 max_pool_size = 100 6.4 配置 coord1 pg_hba.conf12host all all 192.168.1.36/32 trust host all all 0.0.0.0/0 md5 6.5 install coord21initdb -D /database/1922/pgdata1/pgdata_xc/coord2 --nodename coord2 -E UTF8 --locale=C -U postgres -W 6.6 配置 coord2 配置文件 postgresql.conf 12345678910111213141516171819# - Connection Settings - listen_addresses = '*' port = 1925# - Where to Log - log_destination = 'csvlog' logging_collector = on log_directory = 'pg_log' log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' log_truncate_on_rotation = on log_rotation_age = 1d log_rotation_size = 10MB gtm_host = 'localhost'# GTM CONNECTION gtm_host = 'localhost' gtm_port = 6666 pgxc_node_name = 'coord2'# DATA NODES AND CONNECTION POOLING pooler_port = 6668 max_pool_size = 100 6.7 配置 coord1 pg_hba.conf12host all all 192.168.1.36/32 trust host all all 0.0.0.0/0 md5 6.8 修改防火墙，打开相应端口 （ 两台）为了测试简便，先暂时关闭防火墙和 seliniux。 启动 PostgreSQL-XC7.1 start gtm12[pgxc@redhat6 bin]$ gtm -D /database/1922/pgdata1/pgdata_xc/gtm &amp; [1] 6185 查看 gtm 是否启动1234567[pgxc@redhat6 bin]$ gtm_ctl status -S gtm -D /database/1922/pgdata1/pgdata_xc/gtm pid: 6185 data: /database/1922/pgdata1/pgdata_xc/gtm active: 1[pgxc@redhat6 bin]$ ps -ef | grep gtm pgxc 6185 6135 0 20:50 pts/4 00:00:00 gtm -D /database/1922/pgdata1/pgdata_xc/gtm 备注： gtm 进程已经正常启动。 7.2 启动数据节点（ 192.168.1.36 ）12postgres -X -D /pgdata_xc/db_1/pg_root -p 15431 -i &amp; postgres -X -D /pgdata_xc/db_2/pg_root -p 15432 -i &amp; 备注：-X 表示datanode 节点。查看数据节点是否启动1234567891011121314151617[pgxc@redhatB pg_log]$ ps -ef | grep pgxc root 371 351 0 09:16 pts/0 00:00:00 su - pgxc pgxc 372 371 0 09:16 pts/0 00:00:00 -bash root 420 398 0 09:16 pts/1 00:00:00 su - pgxc pgxc 421 420 0 09:16 pts/1 00:00:00 -bash pgxc 588 421 0 09:28 pts/1 00:00:00 postgres -X -D /pgdata_xc/db_1/pg_root -p 15431 -i pgxc 589 588 0 09:28 ? 00:00:00 postgres: logger process pgxc 591 588 0 09:28 ? 00:00:00 postgres: writer process pgxc 592 588 0 09:28 ? 00:00:00 postgres: wal writer process pgxc 593 588 0 09:28 ? 00:00:00 postgres: autovacuum launcher process pgxc 594 588 0 09:28 ? 00:00:00 postgres: stats collector process pgxc 595 421 0 09:28 pts/1 00:00:00 postgres -X -D /pgdata_xc/db_2/pg_root -p 15432 -i pgxc 596 595 0 09:28 ? 00:00:00 postgres: logger process pgxc 598 595 0 09:28 ? 00:00:00 postgres: writer process pgxc 599 595 0 09:28 ? 00:00:00 postgres: wal writer process pgxc 600 595 0 09:28 ? 00:00:00 postgres: autovacuum launcher process pgxc 601 595 0 09:28 ? 00:00:00 postgres: stats collector process 7.3 启动 coordinator 节点12postgres -C -D /database/1922/pgdata1/pgdata_xc/coord1 -p 1921 -i &amp; postgres -C -D /database/1922/pgdata1/pgdata_xc/coord2 -p 1925 -i &amp; 备注：-C 表示 coordinator 节点。 查看 coordinator 节点是否启来1234567891011121314151617pgxc 11633 8961 0 10:25 pts/3 00:00:00 postgres -C -D /database/1922/pgdata1/pgdata_xc/coord1 -p 1921 -i pgxc 11634 11633 0 10:25 ? 00:00:00 postgres: logger process pgxc 11637 11633 0 10:25 ? 00:00:00 postgres: pooler process pgxc 11638 11633 0 10:25 ? 00:00:00 postgres: writer process pgxc 11639 11633 0 10:25 ? 00:00:00 postgres: wal writer process pgxc 11640 11633 0 10:25 ? 00:00:00 postgres: autovacuum launcher process pgxc 11641 11633 0 10:25 ? 00:00:00 postgres: stats collector process pgxc 11643 8961 0 10:25 pts/3 00:00:00 postgres -C -D /database/1922/pgdata1/pgdata_xc/coord2 -p 1925 -i pgxc 11644 11643 0 10:26 ? 00:00:00 postgres: logger process pgxc 11647 11643 0 10:26 ? 00:00:00 postgres: pooler process pgxc 11648 11643 0 10:26 ? 00:00:00 postgres: writer process pgxc 11649 11643 0 10:26 ? 00:00:00 postgres: wal writer process pgxc 11650 11643 0 10:26 ? 00:00:00 postgres: autovacuum launcher process pgxc 11651 11643 0 10:26 ? 00:00:00 postgres: stats collector process pgxc 12203 11633 0 10:53 ? 00:00:00 postgres: postgres postgres ::1(13531) idle pgxc 12372 11643 0 11:01 ? 00:00:00 postgres: postgres postgres ::1(14789) idle pgxc 12661 11643 0 11:16 ? 00:00:00 postgres: francs francs ::1(14825) idle 备注：coord1,coord2 节点分别多了个pooler process 进程，pooler process 用来与数据节点进行通信的。 7.4 查看GTM，POOL连接123456789[root@redhat6 ~]# netstat -anp | grep gtm tcp 0 0 0.0.0.0:6666 0.0.0.0:* LISTEN 11620/gtm tcp 0 0 :::6666 :::* LISTEN 11620/gtm tcp 0 0 ::1:6666 ::1:52228 ESTABLISHED 11620/gtm [root@redhat6 ~]# ps -ef | grep pool pgxc 11637 11633 0 10:25 ? 00:00:00 postgres: pooler process pgxc 11647 11643 0 10:26 ? 00:00:00 postgres: pooler process root 13375 11658 0 11:53 pts/2 00:00:00 grep pool 备注：如果到了这步，没有看到 pooler process ，或者没有 gtm 相关连接，说明配置有问题。 注册节点8.4 在 coord1,cord2 上注册数据节点123456789101112131415161718192021222324252627282930313233CREATE NODE db_1 WITH (TYPE='datanode',HOST = '192.168.1.36', PORT=15431); CREATE NODE db_2 WITH (TYPE='datanode',HOST = '192.168.1.36', PORT=15432); CREATE NODE coord2 WITH (TYPE='coordinator',HOST = 'localhost', PORT=1925); CREATE NODE coord1 WITH (TYPE='coordinator',HOST = 'localhost', PORT=1921);[pgxc@redhat6 gtm]$ psql -p 1921 -U postgres psql (PGXC 1.0beta2, based on PG 9.1.3) Type \"help\" for help.postgres=# select * from pgxc_node; node_name | node_type | node_port | node_host | nodeis_primary | nodeis_preferred | node_id -----------+-----------+-----------+-----------+----------------+------------------+------------ coord1 | C | 5432 | localhost | f | f | 1885696643 (1 row)postgres=# CREATE NODE db_1 WITH (TYPE='datanode',HOST = '192.168.1.36', PORT=15431,PRIMARY, PREFERRED); CREATE NODE postgres=# CREATE NODE db_2 WITH (TYPE='datanode',HOST = '192.168.1.36', PORT=15432); CREATE NODE postgres=# CREATE NODE coord2 WITH (TYPE='coordinator',HOST = '192.168.1.35', PORT=1925); CREATE NODE postgres=# select * from pgxc_node; node_name | node_type | node_port | node_host | nodeis_primary | nodeis_preferred | node_id -----------+-----------+-----------+--------------+----------------+------------------+------------- coord1 | C | 5432 | localhost | f | f | 1885696643 db_1 | D | 15431 | 192.168.1.36 | f | f | 1356996994 db_2 | D | 15432 | 192.168.1.36 | f | f | -822936791 coord2 | C | 1925 | localhost | f | f | -1197102633postgres=# select pgxc_pool_reload(); pgxc_pool_reload ------------------ t (1 row) 备注：到了这里， PostgreSQL-XC 安装完成，接下来验证下。 测试9.1 coord1 创建测试库和表1234567891011121314151617181920212223242526272829303132333435363738[pgxc@redhat6 coord2]$ psql -p 1921 -U postgres psql (PGXC 1.0beta2, based on PG 9.1.3) Type \"help\" for help.postgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+---------+-------+------------------------ francs | postgres | UTF8 | C | C | =Tc/postgres + | | | | | postgres=CTc/postgres + | | | | | francs=C*T*c*/postgres postgres | postgres | UTF8 | C | C | template0 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres (4 rows)postgres=# create database test_xc; CREATE DATABASEpostgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+---------+-------+------------------------ postgres | postgres | UTF8 | C | C | template0 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres test_xc | postgres | UTF8 | C | C | (5 rows)postgres=# \\c test_xc You are now connected to database \"test_xc\" as user \"postgres\". test_xc=# create table test_1 (id integer,name varchar(32)); CREATE TABLE test_xc=# insert into test_1 select generate_series(1,100),'test_xc'; INSERT 0 100 备注：在 coord1 节点上创建了测试库 test_xc，并在里面创建了一张表，接下来看看 coord2, db_1,db_2 节点情况。 9.2 coord2 上验证12345678910111213141516171819202122232425262728[pgxc@redhat6 pg_log]$ psql -p 1925 -U postgres psql (PGXC 1.0beta2, based on PG 9.1.3) Type \"help\" for help.postgres=# \\l List of databases Name | Owner | Encoding | Collate | Ctype | Access privileges -----------+----------+----------+---------+-------+------------------------ postgres | postgres | UTF8 | C | C | template0 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres test_xc | postgres | UTF8 | C | C | (5 rows)postgres=# \\c test_xc You are now connected to database \"test_xc\" as user \"postgres\". test_xc=# \\d List of relations Schema | Name | Type | Owner --------+--------+-------+---------- public | test_1 | table | postgres (1 row)test_xc=# select count(*) from test_1; count ------- 100 (1 row) 9.3 数据节点 db_1 上验证123456789101112131415161718[pgxc@redhatB pg_log]$ psql -p 15431 -U postgres -d test_xc psql (PGXC 1.0beta2, based on PG 9.1.3) Type \"help\" for help.test_xc=# \\d WARNING: Do not have a GTM snapshot available WARNING: Do not have a GTM snapshot available List of relations Schema | Name | Type | Owner --------+--------+-------+---------- public | test_1 | table | postgres (1 row)test_xc=# select count(*) from test_1; WARNING: Do not have a GTM snapshot available WARNING: Do not have a GTM snapshot available count ------- 42 (1 row) 9.4 数据节点 db_2 上验证12345678910[pgxc@redhatB pg_log]$ psql -p 15432 -U postgres -d test_xc psql (PGXC 1.0beta2, based on PG 9.1.3) Type \"help\" for help.test_xc=# select count(*) from test_1; WARNING: Do not have a GTM snapshot available WARNING: Do not have a GTM snapshot available count ------- 58 (1 row) 备注：表共有100条记录，数据节点一分布了 42 条，数据节点二分布了 58 条，可见数据已经分片到数据节点上。 当然在创建表时，也可以设置 replication 模式，这样数据就会完全复制到每个节点。 常见错误123[pgxc@redhat6 coord1]$ psql -p 1921 -U postgres postgres=# create database test; ERROR: Failed to get pooled connections 备注：如果在 coord1 节点上操作时，报 “ERROR: Failed to get pooled connections”，可能的原因很多，可能是节点没有注册好，这时要查看 pgxc_node 视图；也有可能是 pg_hba.conf，防火墙，selinux 等问题，总之逐一排查。 总结 本次测试只是简单将 PostgreSQL-XC 搭建起来了，其中 gmt_standby，GTM-Proxy 没有配置，这个以后可以测试下； 由于在 PostgreSQL-XC 体系中，coordinator 并不存储数据，数据被分片在数据节点中，这种机制与Greenplum有点类似；而 coordinator 节点新增了 pooler process 进程，个人觉得 coordinator 很像一个连接池。 PostgreSQL-XC 所谓的多主节点( muti-master) 同时对外服务，实际上对应用服务的是 coordinator 节点，而不是数据节点本身；原来俺的理解是数据节点同时读写(多份数据，mater 同时读写)，而在 PostgreSQL-XC 中并不是这样。 由于每个数据节点存储一部分数据，那么如果数据节点 down 掉，整个 PostgreSQL-XC 将不可用，不知是否有更好的方案，例如给每个数据节点配置个 standby 节点？ 由于在 PostgreSQL-XC 体系中数据分片在多个数据节点中，IO 性能会提升，同时对网络压力会提升；具体性能目前还没有测试。6.尽管疑问重多，但还是要感谢来自日本的 PostgreSQL-XC 开发团队，他们为 PostgreSQL 带来了 muti-master的集群方案。 参考http://postgres-xc.sourceforge.net/http://postgres-xc.sourceforge.net/docs/1_0/index.htmlhttp://blog.163.com/digoal@126/blog/static/16387704020121952051174/http://michael.otacoo.com/tag/postgres-xc/","categories":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/categories/Postgres-XC/"}],"tags":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/tags/Postgres-XC/"}]},{"title":"PostgreSQL-XC: Introduction","slug":"20120603135702","date":"2012-06-03T05:57:02.000Z","updated":"2018-09-04T01:33:56.524Z","comments":true,"path":"20120603135702.html","link":"","permalink":"https://postgres.fun/20120603135702.html","excerpt":"","text":"这几天在学习 PostgreSQL-XC，虽然目前 Postgres-XC 的文档还不完整，但学习一下还是有帮助的，至少PostgreSQL-XC 是一种可以实现 multi-master 方案，这里简单介绍下 PostgreSQL-XC。 What Is Postgres-XC?PostgreSQL-XC 是一种提供写可靠性，多主节点数据同步，数据传输的开源集群方案，它包括很多组件，稍后会详细介绍这些组件，这些 PostgreSQL-XC组件可以分别安装在多台物理机器或者虚拟机上。 写可靠性 (Write-scalable )是指可以部署多个数据库主节点，并且向这些主节点发出 update 语句，这种特性单个 PostgreSQL 库是无法提供的； 多主节点 (Multi-master )：是指有多个数据库可以提供统一完整的数据库视图；主节点数据同步（Synchronous):是指在一台数据库上更新操作会立刻呈现在另一台数据库中； 数据传输（Transparent）是指；数据位于不同的数据库节点中，当查询数据时，不必关心数据位于具体的节点。你可以将 PostgreSQL-XC 配置一台或者多台主机上， Postgresql-XC 数据以分布式存储，有两种方式，partitioned 或者 replicated ，当向 PostgreSQL-XC 发送查询 SQL时， PostgreSQL-XC 会自动向数据节点发出查询语句并获取数据。 Postgres-XC 的目标PostgreSQL-XC 最大的特性就是提供多主(multi-master ) 数据同步以及读写( read/write ) 的可靠性。具体地说， PostgreSQL-XC 提供以下特性： PostgreSQL-XC 可以提供多个主节点同时处理来自应用端发出的 SQL 语句，这些节点称为 master 节点，但在 PostgreSQL-XC 中称为 “coordinator”。 PostgreSQL-XC 可以提供多个 masters 节点。 任何一个 master 节点都有全局数据库视图，也就是说当任一台 master 接收 update 语句时，在另外的 master 节点可以迅速地看到。 表可以以 replicated 或者 distributed 方式分布式存储，并且这对应用来说是透明的； PostgreSQL-XC 可以提供统一全局的数据库视图。 Postgres-XC 重要组件 这里介绍下 PostgreSQL-XC 的组件，PostgreSQL-XC 包含三个主要组件，分别是 GTM (Global Transaction Manager), Coordinator and Datanode。 GTM (Global Transaction Manager) GTM 是 PostgreSQL-XC 的核心组件，用于全局事务控制以及 tuple 的可见性控制。 PostgreSQL 的事务控制是基于 MVCC 机制的， 在 PostgreSQL-XC 体系中将这种技术单独划分出来，称之为 GTM。 Coordinator协调呆节点 (Coordinator) 是数据节点 (Datanode) 与应用之间的接口，由于表数据会以分片或者复制的方式分布式存储，所以 Coordinator 节点并不物理上存储表数据，表数据位于数据节点上，数据节点接下来会介绍，当应用发起SQL时，会先到达 Coordinator 节点，然后 Coordinator 节点将 sql 分发到各个数据节点，汇总数据，这一系统过程是通过 GXID 和 Global Snapshot 来控制的。 DatanodeDatanode 即数据节点，物理存储表的数据，表数据存储方式包括分片 ( distributed ) 和 完全复制 (replicated) 的方式，数据节点只存储本地的数据。 PostgreSQL-XC 体系结构图备注：从上图看出，Coordinator 和 datanode 节点可以配置多个，并且可以分别位于不同主上，官方建议 Coordinator 节点和 datanode 节点数相同。另外，只有 Coordinator 节点直接对应用服务，数据节点对应用透明。 Postgres-XC 和 PostgreSQLPostgres-XC is an extension to PostgreSQL and inherits most of its features.It is an open-source descendant of PostgreSQL and its original Berkeley code. It supports a large part of the SQL standard and offers many modern features: complex queries foreign keys [3] triggers [4] views transactional integrity multiversion concurrency control Also, similar to PostgreSQL, Postgres-XC can be extended by the user in many ways, for example by adding new： data types functions operators aggregate functions index methods procedural languages And because of the liberal license same as PostgreSQL, Postgres-XC can be used, modified, and distributed by anyone free of charge for any purpose, be it private, commercial, or academic. 参考 http://postgres-xc.sourceforge.net/docs/1_0/intro-whatis.html http://postgres-xc.sourceforge.net/","categories":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/categories/Postgres-XC/"}],"tags":[{"name":"Postgres-XC","slug":"Postgres-XC","permalink":"https://postgres.fun/tags/Postgres-XC/"}]},{"title":"PostgreSQL: epoch 新纪元时间的使用","slug":"20120530154449","date":"2012-05-30T07:44:49.000Z","updated":"2018-09-04T01:33:56.477Z","comments":true,"path":"20120530154449.html","link":"","permalink":"https://postgres.fun/20120530154449.html","excerpt":"","text":"新纪元时间 Epoch 是以 1970-01-01 00:00:00 UTC 为标准的时间，将目标时间与 1970-01-01 00:00:00 时间的差值以秒来计算 ，单位是秒，可以是负值; 有些应用会将时间存储成epoch 时间形式，以提高读取效率， 下面演示下 pg 中 epoch 时间的使用换算方法。 将 timestamp 时间转换成 epoch1234567891011121314151617181920212223francs=&gt; select extract(epoch from timestamp without time zone '1970-01-01 01:00:00'); date_part ----------- 3600 (1 row)francs=&gt; select extract(epoch from timestamp without time zone '1970-01-01 02:00:00'); date_part ----------- 7200 (1 row)francs=&gt; select extract(epoch from interval '+1 hours'); date_part ----------- 3600 (1 row) francs=&gt; select extract(epoch from interval '-1 hours'); date_part ----------- -3600 (1 row) 将 epoch 时间转换成 timestamp1234567891011francs=&gt; select timestamp without time zone 'epoch' + 3600 * interval '1 second'; ?column? --------------------- 1970-01-01 01:00:00 (1 row)francs=&gt; select timestamp without time zone 'epoch' + 7200 * interval '1 second'; ?column? --------------------- 1970-01-01 02:00:00 (1 row) 手册上关于 epoch 的解释 For date and timestamp values, the number of seconds since 1970-01-01 00:00:00 UTC (can be negative);for interval values, the total number of seconds in the interval","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Pgstatspack_version_2.3.1 报告内容分析","slug":"20120530134012","date":"2012-05-30T05:40:12.000Z","updated":"2018-09-04T01:33:56.430Z","comments":true,"path":"20120530134012.html","link":"","permalink":"https://postgres.fun/20120530134012.html","excerpt":"","text":"pgstatspack_version_2.3.1 去年就已经出来了，一直没怎么去用，今天测试了下新版的 pgstatspack，新版的 statspack 报告的内容比以前详细很多，越来越像 Oracle 的 awr 报告了。关闭 pgstatspack 的安装 ，报告生成可以参考以前写的 blog: https://postgres.fun/20100810142325.html 今天主要演示新版 pgstatspack 报告内容。 Pgstatspack 报告内容以下是在很空闲的测试库上生成的 pgstatspack 报告，带”备注”是分析。 123456789101112131415########################################################################################################### PGStatspack version 2.3 by [uwe.bartels@gmail.com](mailto:uwe.bartels@gmail.com) ###########################################################################################################Snapshot information Begin snapshot : snapid | ts | description --------+---------------------------+---------------------- 1 | 2012-05-30 09:34:10.05015 | my first pgstatspack (1 row)End snapshot : snapid | ts | description --------+----------------------------+----------------------- 2 | 2012-05-30 09:35:17.261398 | my second pgstatspack (1 row)Seconds in snapshot: 67.211248 备注：报告中需要根据 “Begin snapshot” 和 “ End snapshot “ 才能生成，这和 oracle 的 statpack 一样。 数据库版本和统计信息123456789101112131415161718Database version version ---------------------------------------------------------------------------------------------------------- PostgreSQL 9.2beta1 on i686-pc-linux-gnu, compiled by gcc (GCC) 4.4.6 20110731 (Red Hat 4.4.6-3), 32-bit (1 row)current_database | dbsize ------------------+-------- francs | 312 MB (1 row) Database statistics database | tps | hitrate | lio_ps | pio_ps | rollbk_ps -----------+------+---------+--------+--------+----------- francs | 0.18 | 99.00 | 99.48 | 0.71 | 0.00 postgres | 0.06 | 99.00 | 3.84 | 0.00 | 0.00 template1 | 0.03 | 98.00 | 0.82 | 0.00 | 0.00 template0 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 (4 rows) 备注：这里统计了快照之间数据库的信息. tps: 快照之间提交的事务数。 (pg_stat_database.xact_commit) hitrate： 快照之间缓存命中率 pg_stat_database.blks_hit - cache reads pg_stat_database.blks_read - physical reads lio_ps: 快照之间每秒逻辑读次数 (pg_stat_database.blks_hit + pg_stat_database.blks_read) pio_ps: 快照之间每秒物理读次数 (pg_stat_database.blks_read ) rollbk_ps: 快照之间事务回滚的次数 (pg_stat_database.xact_rollback) 表数据变化 top 20123456789101112131415161718192021222324Top 20 tables ordered by table size changes table | table_growth | index_growth ------------------------------------+--------------+-------------- pg_catalog.pg_shseclabel | | 0 pg_toast.pg_toast_2396 | | 0 pg_catalog.pg_ts_config | | 0 public.pgstatspack_tables | | 8192 pg_catalog.pg_ts_dict | | 0 pg_catalog.pg_foreign_server | | 0 francs.test_float4 | | pg_catalog.pg_language | | 0 pg_toast.pg_toast_3596 | | 0 francs.test_check | | 0 pg_catalog.pg_cast | | 0 public.pgstatspack_indexes | | 8192 francs.test_dropindex | | 0 pg_catalog.pg_depend | | 0 public.pgstatspack_sequences | | 8192 public.pgstatspack_database | | 0 pg_catalog.pg_largeobject_metadata | | 0 pg_catalog.pg_authid | | 0 pg_catalog.pg_namespace | | 0 pg_catalog.pg_extension | | 0 (20 rows) Top 20 tables ordered by high table to index read ratio1234567891011121314151617181920212223table | system_read_pct | table_read_pct | index_read_pct --------------------------+-----------------+----------------+---------------- francs.test_1 | 38 | 100 | 0 pg_catalog.pg_class | 22 | 90 | 9 pg_catalog.pg_proc | 19 | 99 | 0 public.pgstatspack_names | 8 | 88 | 11 pg_catalog.pg_index | 4 | 91 | 8 pg_catalog.pg_attribute | 2 | 0 | 100 pg_catalog.pg_opclass | 1 | 0 | 100 pg_catalog.pg_am | 0 | 100 | 0 pg_catalog.pg_database | 0 | 54 | 45 pg_catalog.pg_namespace | 0 | 25 | 74 pg_catalog.pg_amproc | 0 | 0 | 100 pg_catalog.pg_type | 0 | 0 | 100 pg_catalog.pg_amop | 0 | 0 | 100 pg_catalog.pg_constraint | 0 | 0 | 100 pg_catalog.pg_cast | 0 | 0 | 100 pg_catalog.pg_authid | 0 | 0 | 100 pg_catalog.pg_rewrite | 0 | 0 | 100 pg_catalog.pg_statistic | 0 | 0 | 100 pg_catalog.pg_operator | 0 | 0 | 100 pg_catalog.pg_aggregate | 0 | 0 | 100 (20 rows) 表数据插入 top 20123456789101112131415161718192021222324Top 20 tables ordered by inserts table | table_inserts ------------------------------------+--------------- public.pgstatspack_names | 325 public.pgstatspack_indexes | 135 public.pgstatspack_tables | 103 public.pgstatspack_settings | 45 pg_catalog.pg_statistic | 29 public.pgstatspack_database | 4 public.pgstatspack_sequences | 3 public.pgstatspack_bgwriter | 1 public.pgstatspack_snap | 1 pg_catalog.pg_foreign_server | 0 pg_catalog.pg_description | 0 pg_catalog.pg_cast | 0 francs.test_dropindex | 0 pg_catalog.pg_depend | 0 pg_catalog.pg_ts_dict | 0 francs.test_check | 0 pg_catalog.pg_rewrite | 0 pg_catalog.pg_namespace | 0 pg_catalog.pg_authid | 0 pg_catalog.pg_largeobject_metadata | 0 (20 rows) 表数据修改 top 20123456789101112131415161718192021222324Top 20 tables ordered by updates table | table_updates ------------------------------------+--------------- pg_toast.pg_toast_3596 | 0 pg_catalog.pg_ts_dict | 0 pg_catalog.pg_language | 0 public.pgstatspack_names | 0 pg_catalog.pg_cast | 0 pg_catalog.pg_foreign_server | 0 public.pgstatspack_sequences | 0 pg_catalog.pg_db_role_setting | 0 pg_catalog.pg_namespace | 0 francs.test_check | 0 pg_catalog.pg_description | 0 public.pgstatspack_indexes | 0 francs.test_dropindex | 0 pg_catalog.pg_depend | 0 information_schema.sql_parts | 0 public.pgstatspack_database | 0 pg_catalog.pg_largeobject_metadata | 0 pg_catalog.pg_authid | 0 pg_catalog.pg_rewrite | 0 francs.test_float4 | 0 (20 rows) 表数据删除 top 20123456789101112131415161718192021222324Top 20 tables ordered by deletes table | table_deletes ------------------------------------+--------------- pg_toast.pg_toast_3596 | 0 pg_catalog.pg_ts_dict | 0 pg_catalog.pg_language | 0 public.pgstatspack_names | 0 pg_catalog.pg_cast | 0 pg_catalog.pg_foreign_server | 0 public.pgstatspack_sequences | 0 pg_catalog.pg_db_role_setting | 0 pg_catalog.pg_namespace | 0 francs.test_check | 0 pg_catalog.pg_description | 0 public.pgstatspack_indexes | 0 francs.test_dropindex | 0 pg_catalog.pg_depend | 0 information_schema.sql_parts | 0 public.pgstatspack_database | 0 pg_catalog.pg_largeobject_metadata | 0 pg_catalog.pg_authid | 0 pg_catalog.pg_rewrite | 0 francs.test_float4 | 0 (20 rows) 表记录数据读取 top 20123456789101112131415161718192021222324252627Tables ordered by percentage of tuples scanned table | rows_read_pct | tab_hitrate | idx_hitrate | tab_read | tab_hit | idx_read | idx_hit --------------------------+---------------+-------------+-------------+----------+---------+----------+--------- francs.test_1 | 38 | 97 | 0 | 0 | 45 | 0 | 0 pg_catalog.pg_class | 22 | 99 | 99 | 0 | 732 | 0 | 623 pg_catalog.pg_proc | 19 | 99 | 88 | 0 | 125 | 0 | 8 public.pgstatspack_names | 8 | 99 | 99 | 5 | 626 | 7 | 1351 pg_catalog.pg_index | 4 | 99 | 99 | 0 | 124 | 0 | 105 pg_catalog.pg_attribute | 2 | 99 | 99 | 0 | 309 | 1 | 611 pg_catalog.pg_opclass | 1 | 99 | 97 | 0 | 116 | 0 | 36 pg_catalog.pg_aggregate | 0 | 66 | 80 | 0 | 2 | 0 | 4 pg_catalog.pg_attrdef | 0 | 75 | 83 | 0 | 3 | 0 | 5 pg_toast.pg_toast_2618 | 0 | 41 | 88 | 6 | 5 | 0 | 8 pg_catalog.pg_tablespace | 0 | 75 | 83 | 0 | 3 | 0 | 5 pg_catalog.pg_amproc | 0 | 95 | 97 | 0 | 20 | 0 | 45 pg_catalog.pg_am | 0 | 80 | 0 | 0 | 4 | 0 | 0 pg_catalog.pg_type | 0 | 94 | 96 | 0 | 16 | 0 | 32 pg_catalog.pg_amop | 0 | 97 | 98 | 0 | 34 | 0 | 65 pg_catalog.pg_database | 0 | 96 | 96 | 0 | 26 | 0 | 32 pg_catalog.pg_constraint | 0 | 50 | 66 | 0 | 1 | 0 | 2 pg_catalog.pg_cast | 0 | 96 | 99 | 0 | 25 | 0 | 109 pg_catalog.pg_namespace | 0 | 99 | 99 | 0 | 149 | 0 | 145 pg_catalog.pg_authid | 0 | 90 | 94 | 0 | 9 | 0 | 16 pg_catalog.pg_rewrite | 0 | 58 | 92 | 4 | 7 | 0 | 12 pg_catalog.pg_statistic | 0 | 97 | 99 | 0 | 39 | 0 | 132 pg_catalog.pg_operator | 0 | 94 | 97 | 0 | 17 | 0 | 41 (23 rows) 索引扫描次数排序123456789Indexes ordered by scans index | table | scans | tup_read | tup_fetch | idx_blks_read | idx_blks_hit ----------------------------------------------------+-------------------------------+-------+----------+-----------+---------------+-------------- pg_catalog.pg_class_oid_index | pg_catalog.pg_class | 547 | 547 | 547 | 0 | 550 pg_catalog.pg_attribute_relid_attnum_index | pg_catalog.pg_attribute | 304 | 736 | 736 | 1 | 611 public.idx_pgstatspack_names_name | public.pgstatspack_names | 284 | 277 | 277 | 5 | 1025 pg_catalog.pg_namespace_oid_index | pg_catalog.pg_namespace | 138 | 138 | 138 | 0 | 140 ....略 序列读取信息用于查找竞争激烈的序列123456Sequences ordered by blks_read sequence | blks_read | blks_hit --------------------------+-----------+---------- public.pgstatspackid | 0 | 1 public.pgstatspacknameid | 0 | 325 (2 rows) 执行时间 top 20 的 sql1234Top 20 SQL statements ordered by total_time calls | total_time | total_time_percent | rows | user | query -------+------------+--------------------+------+------+------- (0 rows) 备注：这个模块是新版的亮点，记得老版本没有统计 SQL 信息，查找 SQL 信息还得安装 pg_stat_statements 模块。 执行时间 top 20 的 function1234Top 20 user functions ordered by total_time funcid | function_name | calls | total_time | self_time --------+---------------+-------+------------+----------- (0 rows) Backgroud Writer 进程信息1234567891011background writer stats checkpoints_timed | checkpoints_req | buffers_checkpoint | buffers_clean | maxwritten_clean | buffers_backend | buffers_alloc -------------------+-----------------+--------------------+---------------+------------------+-----------------+--------------- 0 | 0 | 0 | 0 | 0 | 0 | 47 (1 row) background writer relative stats checkpoints_timed | minutes_between_checkpoint | buffers_checkpoint | buffers_clean | buffers_backend | total_writes | avg_checkpoint_write -------------------+----------------------------+--------------------+---------------+-----------------+--------------+---------------------- | | | | | 0.000 MB/s | (1 row) 当前参数配置1234567891011Parameters name | start_setting | stop_setting | source ----------------------------+----------------------------------------+----------------------------------------+---------------------- max_stack_depth | 2048 | 2048 | environment variable hba_file | /opt/pgdata9.2/pg_root/pg_hba.conf | /opt/pgdata9.2/pg_root/pg_hba.conf | override lc_time | C | C | configuration file archive_command | /bin/date | /bin/date | configuration file log_destination | csvlog | csvlog | configuration file autovacuum | on | on | configuration file .....略 关于快照的删除如果在生产库上部署了 pgstatpack ，则建议定期删除快照，否则上述的快照相关表会很大，快照删除脚本可写在 crontab 里，参考如下12 3 * * * /somepath/pgstatspack/bin/delete_snapshot.sh 1&gt; /some_path/log_file 2&gt;&amp;1 快照删除脚本 delete_snapshot.sh 会调用函数 pgstatspack_delete_snap()，这个函数中可以设置删除多久以前的快照，默认删除 30 天 前的快照，如需修改，修改函数pgstatspack_delete_snap()以下代码：1SELECT current_timestamp - interval '30 days' INTO old_snap_time;","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.2Beta: Add a deadlock counter to the pg_stat_database system view ","slug":"20120529211611","date":"2012-05-29T13:16:11.000Z","updated":"2018-09-04T01:33:56.368Z","comments":true,"path":"20120529211611.html","link":"","permalink":"https://postgres.fun/20120529211611.html","excerpt":"","text":"pg_stat_database 系统视图记录每个数据库的运行信息，包括 blks_read,blks_hit，tup_fetched，tup_inserted 等信息，在 9.2 版本增加了 deadlocks 信息，用于统计数据库的死锁次数。 环境准备1.1 一张测试表123456789101112131415161718192021222324francs=&gt; select * from test_1 limit 10; id | name ----+------ 2 | a 3 | a 4 | a 5 | a 6 | a 7 | a 8 | a 9 | a 10 | a 11 | a (10 rows)francs=&gt; \\d test_1 Table \"francs.test_1\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Indexes: \"idx_test_1_id\" UNIQUE, btree (id) \"idx_test_1_name\" btree (name) 1.2 实验前查询 pg_stat_database 视图123456789101112131415161718192021francs=# select * from pg_stat_database where datname='francs'; -[ RECORD 1 ]--+----------------------------- datid | 16386 datname | francs numbackends | 1 xact_commit | 4871 xact_rollback | 118 blks_read | 280156 blks_hit | 9684659 tup_returned | 30715579 tup_fetched | 59701 tup_inserted | 27185774 tup_updated | 168 tup_deleted | 12800 conflicts | 0 temp_files | 2 temp_bytes | 72048640 deadlocks | 0 blk_read_time | 0 blk_write_time | 0 stats_reset | 2012-05-17 14:42:46.51038+08 备注: 此时的 pg_stat_database.deadlocks 值为 0。 死锁实验2.1 session A123456789francs=&gt; begin; BEGIN francs=&gt; select * from test_1 where id=2; id | name ----+------ 2 | a (1 row)francs=&gt; update test_1 set name='aaa' where id=2; UPDATE 1 2.2 session B12345678910111213141516francs=&gt; begin; BEGIN francs=&gt; select * from test_1 where id=2; id | name ----+------ 2 | a (1 row)francs=&gt; select * from test_1 where id=3; id | name ----+------ 3 | a (1 row)francs=&gt; update test_1 set name='aaa3' where id=3; UPDATE 1francs=&gt; update test_1 set name='aaaa' where id=2; &lt; hang &gt; 2.3 再次回到 session A12345francs=&gt; update test_1 set name='aaa3333' where id=3; ERROR: deadlock detected DETAIL: Process 25454 waits for ShareLock on transaction 1995; blocked by process 25508. Process 25508 waits for ShareLock on transaction 1994; blocked by process 25454. HINT: See server log for query details. 备注：死锁发生。 2.4 再次查询 pg_stat_database 视图123456789101112131415161718192021francs=# select * from pg_stat_database where datname='francs'; -[ RECORD 1 ]--+----------------------------- datid | 16386 datname | francs numbackends | 4 xact_commit | 4907 xact_rollback | 119 blks_read | 280159 blks_hit | 9687460 tup_returned | 30725377 tup_fetched | 61093 tup_inserted | 27185774 tup_updated | 168 tup_deleted | 12800 conflicts | 0 temp_files | 2 temp_bytes | 72048640 deadlocks | 1 blk_read_time | 0 blk_write_time | 0 stats_reset | 2012-05-17 14:42:46.51038+08 备注：发现 deadlocks 值为1。 附：pg_stat_database 说明Table 27-4. pg_stat_database view 参考 http://www.postgresql.org/docs/9.2/static/release-9-2.html http://www.postgresql.org/docs/9.2/static/monitoring-stats.html#MONITORING-STATS-VIEWS http://www.depesz.com/2012/02/09/waiting-for-9-2-deadlock-counter/","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 分区表应用之二(取模分区) ","slug":"20120529163722","date":"2012-05-29T08:37:22.000Z","updated":"2018-09-04T01:33:56.321Z","comments":true,"path":"20120529163722.html","link":"","permalink":"https://postgres.fun/20120529163722.html","excerpt":"","text":"在 PostgreSQL 中，分区表的使用并不像 oracle 那么智能， PostgreSQL 中是靠继承和触发器来实现分区表的，由于trigger 的使用，当业务繁忙时会大大降低数据库性能，所以 trigger 并不建议使用；因此 PostgreSQL 中的分区表对应用不再透明，例如，如果不使用 trigger，那么插入时程序需要指定子表等。今天测试了下取模分区的场景：以下为详细步骤 创建父表创建父表并插入测试数据，如下:1234567891011121314151617181920212223242526272829303132333435francs=&gt; create table tbl_name (id int4 primary key,name varchar(32),remark varchar(64)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"tbl_name_pkey\" for table \"tbl_name\" CREATE TABLEfrancs=&gt; insert into tbl_name select generate_series(1,20000000),generate_series(1,20000000) || '_a','0'; INSERT 0 20000000francs=&gt; select * from tbl_name limit 10; id | name | remark ----+------+-------- 1 | 1_a | 0 2 | 2_a | 0 3 | 3_a | 0 4 | 4_a | 0 5 | 5_a | 0 6 | 6_a | 0 7 | 7_a | 0 8 | 8_a | 0 9 | 9_a | 0 10 | 10_a | 0 (10 rows)francs=&gt; create unique index concurrently idx_tbl_name_name on tbl_name using btree (name); CREATE INDEXfrancs=&gt; \\d tbl_name Table \"francs.tbl_name\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null name | character varying(32) | remark | character varying(64) | Indexes: \"tbl_name_pkey\" PRIMARY KEY, btree (id) \"idx_tbl_name_name\" UNIQUE, btree (name) 父表备份123456789francs=&gt; create table tbl_name_old as select * from tbl_name; SELECT 20000000francs=&gt; alter table tbl_name_old add primary key(id); NOTICE: ALTER TABLE / ADD PRIMARY KEY will create implicit index \"tbl_name_old_pkey\" for table \"tbl_name_old\" ALTER TABLEfrancs=&gt; create unique index concurrently idx_tbl_name_old_name on tbl_name_old using btree (name); CREATE INDEX 备注：这里备份父表，用于之后做性能比较，因为分表后 tbl_name 数据要清空。 创建子表创建子表， 分区字段 id，如下：1234567891011121314151617181920212223242526272829303132create table tbl_name_0 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_1 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_2 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_3 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_4 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_5 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_6 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_7 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_8 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_9 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_10 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_11 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_12 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_13 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_14 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_15 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_16 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_17 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_18 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_19 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_20 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_21 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_22 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_23 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_24 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_25 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_26 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_27 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_28 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_29 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_30 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); create table tbl_name_31 ( like tbl_name including constraints including defaults INCLUDING COMMENTS ) inherits ( tbl_name ); 备注：创建子表时这里没有继承 index，是因为先创建索引再导数据速度会比较慢； 将数据分发到子表1234567891011121314151617181920212223242526272829303132insert into tbl_name_0 select * from only tbl_name where mod(id,32) =0; insert into tbl_name_1 select * from only tbl_name where mod(id,32) =1; insert into tbl_name_2 select * from only tbl_name where mod(id,32) =2; insert into tbl_name_3 select * from only tbl_name where mod(id,32) =3; insert into tbl_name_4 select * from only tbl_name where mod(id,32) =4; insert into tbl_name_5 select * from only tbl_name where mod(id,32) =5; insert into tbl_name_6 select * from only tbl_name where mod(id,32) =6; insert into tbl_name_7 select * from only tbl_name where mod(id,32) =7; insert into tbl_name_8 select * from only tbl_name where mod(id,32) =8; insert into tbl_name_9 select * from only tbl_name where mod(id,32) =9; insert into tbl_name_10 select * from only tbl_name where mod(id,32) =10; insert into tbl_name_11 select * from only tbl_name where mod(id,32) =11; insert into tbl_name_12 select * from only tbl_name where mod(id,32) =12; insert into tbl_name_13 select * from only tbl_name where mod(id,32) =13; insert into tbl_name_14 select * from only tbl_name where mod(id,32) =14; insert into tbl_name_15 select * from only tbl_name where mod(id,32) =15; insert into tbl_name_16 select * from only tbl_name where mod(id,32) =16; insert into tbl_name_17 select * from only tbl_name where mod(id,32) =17; insert into tbl_name_18 select * from only tbl_name where mod(id,32) =18; insert into tbl_name_19 select * from only tbl_name where mod(id,32) =19; insert into tbl_name_20 select * from only tbl_name where mod(id,32) =20; insert into tbl_name_21 select * from only tbl_name where mod(id,32) =21; insert into tbl_name_22 select * from only tbl_name where mod(id,32) =22; insert into tbl_name_23 select * from only tbl_name where mod(id,32) =23; insert into tbl_name_24 select * from only tbl_name where mod(id,32) =24; insert into tbl_name_25 select * from only tbl_name where mod(id,32) =25; insert into tbl_name_26 select * from only tbl_name where mod(id,32) =26; insert into tbl_name_27 select * from only tbl_name where mod(id,32) =27; insert into tbl_name_28 select * from only tbl_name where mod(id,32) =28; insert into tbl_name_29 select * from only tbl_name where mod(id,32) =29; insert into tbl_name_30 select * from only tbl_name where mod(id,32) =30; insert into tbl_name_31 select * from only tbl_name where mod(id,32) =31; 创建子表约束1234567891011121314151617181920212223242526272829303132alter table tbl_name_0 add CONSTRAINT con_tbl_name_0 check ( mod(id,32) =0); alter table tbl_name_1 add CONSTRAINT con_tbl_name_1 check ( mod(id,32) =1); alter table tbl_name_2 add CONSTRAINT con_tbl_name_2 check ( mod(id,32) =2); alter table tbl_name_3 add CONSTRAINT con_tbl_name_3 check ( mod(id,32) =3); alter table tbl_name_4 add CONSTRAINT con_tbl_name_4 check ( mod(id,32) =4); alter table tbl_name_5 add CONSTRAINT con_tbl_name_5 check ( mod(id,32) =5); alter table tbl_name_6 add CONSTRAINT con_tbl_name_6 check ( mod(id,32) =6); alter table tbl_name_7 add CONSTRAINT con_tbl_name_7 check ( mod(id,32) =7); alter table tbl_name_8 add CONSTRAINT con_tbl_name_8 check ( mod(id,32) =8); alter table tbl_name_9 add CONSTRAINT con_tbl_name_9 check ( mod(id,32) =9); alter table tbl_name_10 add CONSTRAINT con_tbl_name_10 check ( mod(id,32) =10); alter table tbl_name_11 add CONSTRAINT con_tbl_name_11 check ( mod(id,32) =11); alter table tbl_name_12 add CONSTRAINT con_tbl_name_12 check ( mod(id,32) =12); alter table tbl_name_13 add CONSTRAINT con_tbl_name_13 check ( mod(id,32) =13); alter table tbl_name_14 add CONSTRAINT con_tbl_name_14 check ( mod(id,32) =14); alter table tbl_name_15 add CONSTRAINT con_tbl_name_15 check ( mod(id,32) =15);alter table tbl_name_16 add CONSTRAINT con_tbl_name_16 check ( mod(id,32) =16); alter table tbl_name_17 add CONSTRAINT con_tbl_name_17 check ( mod(id,32) =17); alter table tbl_name_18 add CONSTRAINT con_tbl_name_18 check ( mod(id,32) =18); alter table tbl_name_19 add CONSTRAINT con_tbl_name_19 check ( mod(id,32) =19); alter table tbl_name_20 add CONSTRAINT con_tbl_name_20 check ( mod(id,32) =20); alter table tbl_name_21 add CONSTRAINT con_tbl_name_21 check ( mod(id,32) =21); alter table tbl_name_22 add CONSTRAINT con_tbl_name_22 check ( mod(id,32) =22); alter table tbl_name_23 add CONSTRAINT con_tbl_name_23 check ( mod(id,32) =23); alter table tbl_name_24 add CONSTRAINT con_tbl_name_24 check ( mod(id,32) =24); alter table tbl_name_25 add CONSTRAINT con_tbl_name_25 check ( mod(id,32) =25); alter table tbl_name_26 add CONSTRAINT con_tbl_name_26 check ( mod(id,32) =26); alter table tbl_name_27 add CONSTRAINT con_tbl_name_27 check ( mod(id,32) =27); alter table tbl_name_28 add CONSTRAINT con_tbl_name_28 check ( mod(id,32) =28); alter table tbl_name_29 add CONSTRAINT con_tbl_name_29 check ( mod(id,32) =29); alter table tbl_name_30 add CONSTRAINT con_tbl_name_30 check ( mod(id,32) =30); alter table tbl_name_31 add CONSTRAINT con_tbl_name_31 check ( mod(id,32) =31); 创建索引增加主键，如下1234567891011121314151617181920212223242526272829303132ALTER TABLE tbl_name_0 ADD CONSTRAINT pk_tbl_name_id_0 PRIMARY KEY (id); ALTER TABLE tbl_name_1 ADD CONSTRAINT pk_tbl_name_id_1 PRIMARY KEY (id); ALTER TABLE tbl_name_2 ADD CONSTRAINT pk_tbl_name_id_2 PRIMARY KEY (id); ALTER TABLE tbl_name_3 ADD CONSTRAINT pk_tbl_name_id_3 PRIMARY KEY (id); ALTER TABLE tbl_name_4 ADD CONSTRAINT pk_tbl_name_id_4 PRIMARY KEY (id); ALTER TABLE tbl_name_5 ADD CONSTRAINT pk_tbl_name_id_5 PRIMARY KEY (id); ALTER TABLE tbl_name_6 ADD CONSTRAINT pk_tbl_name_id_6 PRIMARY KEY (id); ALTER TABLE tbl_name_7 ADD CONSTRAINT pk_tbl_name_id_7 PRIMARY KEY (id); ALTER TABLE tbl_name_8 ADD CONSTRAINT pk_tbl_name_id_8 PRIMARY KEY (id); ALTER TABLE tbl_name_9 ADD CONSTRAINT pk_tbl_name_id_9 PRIMARY KEY (id); ALTER TABLE tbl_name_10 ADD CONSTRAINT pk_tbl_name_id_10 PRIMARY KEY (id); ALTER TABLE tbl_name_11 ADD CONSTRAINT pk_tbl_name_id_11 PRIMARY KEY (id); ALTER TABLE tbl_name_12 ADD CONSTRAINT pk_tbl_name_id_12 PRIMARY KEY (id); ALTER TABLE tbl_name_13 ADD CONSTRAINT pk_tbl_name_id_13 PRIMARY KEY (id); ALTER TABLE tbl_name_14 ADD CONSTRAINT pk_tbl_name_id_14 PRIMARY KEY (id); ALTER TABLE tbl_name_15 ADD CONSTRAINT pk_tbl_name_id_15 PRIMARY KEY (id); ALTER TABLE tbl_name_16 ADD CONSTRAINT pk_tbl_name_id_16 PRIMARY KEY (id); ALTER TABLE tbl_name_17 ADD CONSTRAINT pk_tbl_name_id_17 PRIMARY KEY (id); ALTER TABLE tbl_name_18 ADD CONSTRAINT pk_tbl_name_id_18 PRIMARY KEY (id); ALTER TABLE tbl_name_19 ADD CONSTRAINT pk_tbl_name_id_19 PRIMARY KEY (id); ALTER TABLE tbl_name_20 ADD CONSTRAINT pk_tbl_name_id_20 PRIMARY KEY (id); ALTER TABLE tbl_name_21 ADD CONSTRAINT pk_tbl_name_id_21 PRIMARY KEY (id); ALTER TABLE tbl_name_22 ADD CONSTRAINT pk_tbl_name_id_22 PRIMARY KEY (id); ALTER TABLE tbl_name_23 ADD CONSTRAINT pk_tbl_name_id_23 PRIMARY KEY (id); ALTER TABLE tbl_name_24 ADD CONSTRAINT pk_tbl_name_id_24 PRIMARY KEY (id); ALTER TABLE tbl_name_25 ADD CONSTRAINT pk_tbl_name_id_25 PRIMARY KEY (id); ALTER TABLE tbl_name_26 ADD CONSTRAINT pk_tbl_name_id_26 PRIMARY KEY (id); ALTER TABLE tbl_name_27 ADD CONSTRAINT pk_tbl_name_id_27 PRIMARY KEY (id); ALTER TABLE tbl_name_28 ADD CONSTRAINT pk_tbl_name_id_28 PRIMARY KEY (id); ALTER TABLE tbl_name_29 ADD CONSTRAINT pk_tbl_name_id_29 PRIMARY KEY (id); ALTER TABLE tbl_name_30 ADD CONSTRAINT pk_tbl_name_id_30 PRIMARY KEY (id); ALTER TABLE tbl_name_31 ADD CONSTRAINT pk_tbl_name_id_31 PRIMARY KEY (id); 创建 unique 索引1234567891011121314151617181920212223242526272829303132CREATE UNIQUE INDEX idx_tbl_name_name_0 ON tbl_name_0 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_1 ON tbl_name_1 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_2 ON tbl_name_2 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_3 ON tbl_name_3 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_4 ON tbl_name_4 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_5 ON tbl_name_5 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_6 ON tbl_name_6 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_7 ON tbl_name_7 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_8 ON tbl_name_8 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_9 ON tbl_name_9 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_10 ON tbl_name_10 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_11 ON tbl_name_11 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_12 ON tbl_name_12 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_13 ON tbl_name_13 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_14 ON tbl_name_14 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_15 ON tbl_name_15 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_16 ON tbl_name_16 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_17 ON tbl_name_17 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_18 ON tbl_name_18 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_19 ON tbl_name_19 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_20 ON tbl_name_20 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_21 ON tbl_name_21 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_22 ON tbl_name_22 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_23 ON tbl_name_23 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_24 ON tbl_name_24 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_25 ON tbl_name_25 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_26 ON tbl_name_26 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_27 ON tbl_name_27 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_28 ON tbl_name_28 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_29 ON tbl_name_29 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_30 ON tbl_name_30 USING btree (name); CREATE UNIQUE INDEX idx_tbl_name_name_31 ON tbl_name_31 USING btree (name); 清空父表数据1truncate table only tbl_name; 执行计划查看参数 constraint_exclusion12345francs=&gt; show constraint_exclusion; constraint_exclusion ---------------------- partition (1 row) 显示查询父表的PLAN123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172francs=&gt; explain select id,name,remark from tbl_name where id =32 ; QUERY PLAN ----------------------------------------------------------------------------------------------------------- Result (cost=0.00..73.11 rows=33 width=16) -&gt; Append (cost=0.00..73.11 rows=33 width=16) -&gt; Seq Scan on tbl_name (cost=0.00..0.00 rows=1 width=16) Filter: (id = 32) -&gt; Index Scan using pk_tbl_name_id_0 on tbl_name_0 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_1 on tbl_name_1 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_2 on tbl_name_2 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_3 on tbl_name_3 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_4 on tbl_name_4 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_5 on tbl_name_5 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_6 on tbl_name_6 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_7 on tbl_name_7 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_8 on tbl_name_8 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_9 on tbl_name_9 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_10 on tbl_name_10 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_11 on tbl_name_11 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_12 on tbl_name_12 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_13 on tbl_name_13 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_14 on tbl_name_14 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_15 on tbl_name_15 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_16 on tbl_name_16 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_17 on tbl_name_17 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_18 on tbl_name_18 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_19 on tbl_name_19 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_20 on tbl_name_20 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_21 on tbl_name_21 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_22 on tbl_name_22 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_23 on tbl_name_23 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_24 on tbl_name_24 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_25 on tbl_name_25 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_26 on tbl_name_26 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_27 on tbl_name_27 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_28 on tbl_name_28 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_29 on tbl_name_29 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_30 on tbl_name_30 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) -&gt; Index Scan using pk_tbl_name_id_31 on tbl_name_31 tbl_name (cost=0.00..2.28 rows=1 width=16) Index Cond: (id = 32) (68 rows) 备注：查询父表时，扫描了所有分区。 修改SQL，再次显示PLAN1234567891011francs=&gt; explain select id,name,remark from tbl_name where mod(id,32)=0 and id =32 ; QUERY PLAN --------------------------------------------------------------------------------------------------------- Result (cost=0.00..2.29 rows=2 width=16) -&gt; Append (cost=0.00..2.29 rows=2 width=16) -&gt; Seq Scan on tbl_name (cost=0.00..0.00 rows=1 width=16) Filter: ((id = 32) AND (mod(id, 32) = 0)) -&gt; Index Scan using pk_tbl_name_id_0 on tbl_name_0 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) (7 rows) 备注：查询父表时，如果加上 “where mod(id,32)= ? “时，不再扫描所有分区，而只扫描一个分区。 关闭 constraint_exclusion 参数，再次查看 PALN123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106francs=&gt; set constraint_exclusion=off; SETfrancs=&gt; explain select id,name,remark from tbl_name where mod(id,32)=0 and id =32 ; QUERY PLAN ----------------------------------------------------------------------------------------------------------- Result (cost=0.00..73.27 rows=33 width=16) -&gt; Append (cost=0.00..73.27 rows=33 width=16) -&gt; Seq Scan on tbl_name (cost=0.00..0.00 rows=1 width=16) Filter: ((id = 32) AND (mod(id, 32) = 0)) -&gt; Index Scan using pk_tbl_name_id_0 on tbl_name_0 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_1 on tbl_name_1 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_2 on tbl_name_2 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_3 on tbl_name_3 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_4 on tbl_name_4 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_5 on tbl_name_5 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_6 on tbl_name_6 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_7 on tbl_name_7 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_8 on tbl_name_8 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_9 on tbl_name_9 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_10 on tbl_name_10 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_11 on tbl_name_11 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_12 on tbl_name_12 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_13 on tbl_name_13 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_14 on tbl_name_14 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_15 on tbl_name_15 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_16 on tbl_name_16 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_17 on tbl_name_17 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_18 on tbl_name_18 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_19 on tbl_name_19 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_20 on tbl_name_20 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_21 on tbl_name_21 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_22 on tbl_name_22 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_23 on tbl_name_23 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_24 on tbl_name_24 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_25 on tbl_name_25 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_26 on tbl_name_26 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_27 on tbl_name_27 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_28 on tbl_name_28 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_29 on tbl_name_29 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_30 on tbl_name_30 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) -&gt; Index Scan using pk_tbl_name_id_31 on tbl_name_31 tbl_name (cost=0.00..2.29 rows=1 width=16) Index Cond: (id = 32) Filter: (mod(id, 32) = 0) (100 rows) 备注： 关闭 constraint_exclusion 参数后，即扫描所有分区，关于这个参数的作用可以参考本文末尾的附录。 性能测试下面测试两种场景： 场景一: 根据分区字段 id 查询测试 场景二: 根据非分区字段 name 查询测试 根据分区字段 id 查询测试分表前查询1234567891011121314151617181920francs=&gt; select id,name,remark from tbl_name_old where id =128 ; id | name | remark -----+-------+-------- 128 | 128_a | 0 (1 row)Time: 0.305 ms francs=&gt; select id,name,remark from tbl_name_old where id =128 ; id | name | remark -----+-------+-------- 128 | 128_a | 0 (1 row)Time: 0.277 ms francs=&gt; select id,name,remark from tbl_name_old where id =128 ; id | name | remark -----+-------+-------- 128 | 128_a | 0 (1 row)Time: 0.351 ms 分表后查询1234567891011121314151617181920francs=&gt; select id,name,remark from tbl_name_0 where id =64 ; id | name | remark ----+------+-------- 64 | 64_a | 0 (1 row)Time: 0.331 ms francs=&gt; select id,name,remark from tbl_name_0 where id =64 ; id | name | remark ----+------+-------- 64 | 64_a | 0 (1 row)Time: 0.322 ms francs=&gt; select id,name,remark from tbl_name_0 where id =64 ; id | name | remark ----+------+-------- 64 | 64_a | 0 (1 row)Time: 0.301 ms skyid 测试 测试一 测试二 测试三 分表前 0.305 ms 0.277 ms 0.351 ms 分表后 0.331 ms 0.322 ms 0.301 ms 备注：分表后，这里建议应用程序根据 id 取模，然后直接定位子表查询，这里数据来看，性能无明显变化，数据量大的时候分表后的这种场景性能有小辐上升。 根据非分区字段 name 查询测试分表前123456789101112131415161718francs=&gt; select id,name,remark from tbl_name_old where name='32_a' ; id | name | remark ----+------+-------- 32 | 32_a | 0 (1 row)Time: 0.466 ms francs=&gt; select id,name,remark from tbl_name_old where name='32_a' ; id | name | remark ----+------+-------- 32 | 32_a | 0 (1 row)Time: 0.348 ms francs=&gt; select id,name,remark from tbl_name_old where name='32_a' ; id | name | remark ----+------+-------- 32 | 32_a | 0 (1 row)Time: 0.326 ms 分表后1234567891011121314151617181920francs=&gt; select id,name,remark from tbl_name where name='32_a' ; id | name | remark ----+------+-------- 32 | 32_a | 0 (1 row)Time: 1.587 ms francs=&gt; select id,name,remark from tbl_name where name='32_a' ; id | name | remark ----+------+-------- 32 | 32_a | 0 (1 row)Time: 1.478 ms francs=&gt; select id,name,remark from tbl_name where name='32_a' ; id | name | remark ----+------+-------- 32 | 32_a | 0 (1 row)Time: 2.227 ms name 测试 测试一 测试二 测试三 分表前 0.466 ms 0.348 ms 0.326 ms 分表后 1.587 ms 1.478 ms 2.227 ms 备注：根据非分区字段查询，分表后由于需要扫描所有分区，性能有下降辐度较大。 总结 pg 中的表分区后，如果根据 “ where 分区键= ？” 查询，如果扫描父表，性能降低! pg 中的表分区后，如果根据 “ where 分区键= ？” 查询，如果程序定位到子表，性能小辐度上升! pg 中的表分区后，如果根据 “ where 非分区键= ？” 查询，且非分区键有索引，性能大辐下降。 pg 中的表分区后，非分区字段的唯一性无法保证。 PG 中的表分区后，为后期的分库奠定了基础。 备注：签于以上特点，故业务表分区需要权衡各方面的利弊，例如如果应用 90% 以上都是根据分区键查询，则建议分区。 附 constraint_exclusion (enum) Controls the query planner is use of table constraints to optimize queries. The allowed values of constraint_exclusion are on (examine constraints for all tables), off (never examine constraints), and partition (examine constraints only for inheritance child tables and UNION ALL subqueries). partition is the default setting. It is often used with inheritance and partitioned tables to improve performance.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"PostgreSQL9.2Beta: Add CONCURRENTLY option to DROP INDEX CONCURRENTLY ","slug":"20120525145255","date":"2012-05-25T06:52:55.000Z","updated":"2018-09-04T01:33:56.258Z","comments":true,"path":"20120525145255.html","link":"","permalink":"https://postgres.fun/20120525145255.html","excerpt":"","text":"PostgreSQL9.2Beta 对 drop index 命令增加 CONCURRENTLY 选项，当删除索引时，如果使用 CONCURRENTL 参数，那么在索引删除过程中不会阻塞其它 session。 Release 的说明 Add CONCURRENTLY option to DROP INDEX CONCURRENTLY (Simon Riggs)This allows index removal without blocking other sessions. Concurrently 参数说明 CONCURRENTLY When this option is used, PostgreSQL will drop the index without taking any locks that prevent concurrent selects, inserts, updates, or deletes on the table; whereas a standard index drop waits for a lock that locks out everything on the table until it is done. Concurrent drop index is a two stage process. First, we mark the index both invalid and not ready then commit the change. Next we wait until there are no users locking the table who can see the index. 备注：意思是删除索引时如果指定 CONCURRENTLY 选项，那么删除索引的过程中不会阻塞当前发生在这张表上的的 Select, inserts, updates, 和 deletes 会话；而普通的删除索引操作，会获取排它锁，从而当删除索引过程中，会阻塞这张表上的任何操作，下面演示下； Drop Index 举例创建测试表123456789101112131415francs=&gt; create table test_dropindex (id integer ,name varchar(32)); CREATE TABLEfrancs=&gt; insert into test_dropindex select generate_series(1,10000),'a'; INSERT 0 10000francs=&gt; create index idx_test_dropindex_id on test_dropindex using btree (id); CREATE INDEXfrancs=&gt; \\d test_dropindex Table \"francs.test_dropindex\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Indexes: \"idx_test_dropindex_id\" btree (id) 开启删除索引事务12345678910francs=&gt; select pg_backend_pid(); pg_backend_pid ---------------- 19301 (1 row)francs=&gt; begin; BEGIN francs=&gt; drop index idx_test_dropindex_id; DROP INDEX 备注：注意事务并没提交； 开另一会话12345francs=&gt; select locktype,database,relation,pid,mode,granted from pg_locks where relation=16547; locktype | database | relation | pid | mode | granted ----------+----------+----------+-------+---------------------+--------- relation | 16386 | 16547 | 19301 | AccessExclusiveLock | t (1 row) 备注：说明会话 19301 获取的是 “AccessExclusiveLock” 排它锁，这种类型索在获取的过程中会阻塞任何类型索的获取，换句话说在这个命令执行过程中，表上的所有操作将被阻塞。 注意事项Drop Index Concurrently 使用注意以下： drop index 命令可以在事务中执行，而 drop index concurrently 命令不可以；并且 CASCADE 选项也不支持； 那么 drop index concurrently获取的是什么类型的锁呢？由于 drop index concurrently 不支持在事务中执行，那么将不能模拟上述实验观察它获取锁的类型。 总结drop index concurrently 特性适合业务繁忙的库，尽管删除索引的过程会很快，如果是比较大的表，并且是业务特别繁忙的库，那么删除索引过程时间可能较长，使用这个选项无疑是把利器，感谢 Simon Riggs 的贡献！ 参考 http://www.postgresql.org/docs/9.2/static/sql-dropindex.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: Auto abort user's statement that takes over the specified time ","slug":"20120524212459","date":"2012-05-24T13:24:59.000Z","updated":"2018-09-04T01:33:56.196Z","comments":true,"path":"20120524212459.html","link":"","permalink":"https://postgres.fun/20120524212459.html","excerpt":"","text":"有的时候生产库上的一个长时间 SQL 很容易消耗整个服务器的资源，那么数据库层面是否有方法应对这种问题呢？今天听德哥说 PostgreSQL 可以设置用户级别的 SQL 超时参数，如果某个用户的发出的 SQL 超过设定时间，则会被 cancel ，后来查了下手册，并做了下测试，果然可以实现。 关于 statement_timeout首先看一个参数，手册上的解释 statement_timeout (integer) Abort any statement that takes over the specified number of milliseconds, starting from the time the command arrives at the server from the client. If log_min_error_statement is set to ERROR or lower, the statement that timed out will also be logged. A value of zero (the default) turns this off.Setting statement_timeout in postgresql.conf is not recommended because it affects all sessions. 备注：这个参数是全局的，如果设置，则会对整个 PostgreSQL Cluster 生效，如果设置这个参数，那么超过设定时间的 SQL 都会被 cancel ，显然是不可取的，手册上也说并不推荐，通常将这个参数设为0，即关闭这个参数。 幸运的是 PostgreSQL 可以在 Role 级别设置配置参数，即可以在 Role 级别设置语句的超时参数 用户级别设置 statement_timeout1.1 设置前信息123456789[postgres@redhat6 tf]$ psql francs francs psql (9.2beta1) Type \"help\" for help.francs=&gt; show statement_timeout; statement_timeout ------------------- 0 (1 row) 1.2 修改 francs 的 statement_timeout 参数 并验证12postgres=# alter role francs set statement_timeout=5000; ALTER ROLE 备注：这里设置了用户 francs 的 statement_timeout 值为 5 秒，如果加上 IN DATABASE 选项，则只对指定用户登陆指定数据库有效，例如” alter role francs in database francs set statement_timeout=5000 ;” 这个命令仅当 francs 用户连接 francs 库时生效。 1.3 以 francs 用户登陆 postgres 库验证12345678postgres=# \\c postgres francs; You are now connected to database \"postgres\" as user \"francs\".postgres=&gt; show statement_timeout; statement_timeout ------------------- 5s (1 row) 1.4 以 francs 用户登陆 francs 库验证12345678postgres=&gt; \\c francs francs; You are now connected to database \"francs\" as user \"francs\".francs=&gt; show statement_timeout; statement_timeout ------------------- 5s (1 row) 备注：可见用户级的 statement_timeout 已成功设置。 1.5 sql 测试12345678910111213francs=&gt; \\c francs francs; You are now connected to database \"francs\" as user \"francs\". francs=&gt; select pg_sleep(4); pg_sleep ---------- (1 row) Time: 4005.496 msfrancs=&gt; select pg_sleep(5); ERROR: canceling statement due to statement timeout Time: 5002.358 ms 备注：可见当语句执行时间达到设定值 5 秒时，则被 cancel 了；注意这里 cancel 的只是 SQL 语句本身，session 并不会被 kill 。 1.6 放在事务中测试1.6.1 session A123456[postgres@redhat6 tf]$ psql francs francs psql (9.2beta1) Type \"help\" for help.francs=&gt; francs=&gt; begin; BEGIN 1.6.2查看系统上 francs 进程123[postgres@redhat6 pg_root]$ ps -ef | grep francs postgres 18536 18072 0 21:06 pts/1 00:00:00 psql francs francs postgres 18537 12033 0 21:06 ? 00:00:00 postgres: francs francs [local] idle in transactionpostgres 18543 18136 0 21:06 pts/3 00:00:00 grep francs 1.6.3 session A 继续往下执行12345678[postgres@redhat6 tf]$ psql francs francs psql (9.2beta1) Type \"help\" for help.francs=&gt; francs=&gt; begin; BEGIN francs=&gt; select pg_sleep(5); ERROR: canceling statement due to statement timeout 1.6.4 再次查看系统上 francs 进程123[postgres@redhat6 pg_root]$ ps -ef | grep francs postgres 18536 18072 0 21:06 pts/1 00:00:00 psql francs francs postgres 18537 12033 0 21:06 ? 00:00:00 postgres: francs francs [local] idle in transaction (aborted)postgres 18555 18136 0 21:07 pts/3 00:00:00 grep francs 备注：说明语句被 cancel ，事务被 abort。 总结这种方法在生产上用得很少，如果要用时得谨慎。 参考 http://www.postgresql.org/docs/9.2/static/sql-alterrole.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.2Beta: Add support for Range Data Types ","slug":"20120524163556","date":"2012-05-24T08:35:56.000Z","updated":"2018-09-04T01:33:56.133Z","comments":true,"path":"20120524163556.html","link":"","permalink":"https://postgres.fun/20120524163556.html","excerpt":"","text":"PostgreSQL9.2 新增数据类型 Range Type，即范围类型，Range Type 类型的数据可以展现一个范围内的数据，Range Type 可以是多种类型，包括 integer,timestamp 等，Range type 适用于数值，时间需要展现范围的场景，接下来演示 Range Type 的初步应用。 Range Types 数据类型 INT4RANGE ― Range of INTEGER INT8RANGE ― Range of BIGINT NUMRANGE ― Range of NUMERIC TSRANGE ― Range of TIMESTAMP WITHOUT TIME ZONE TSTZRANGE ― Range of TIMESTAMP WITH TIME ZONE DATERANGE ― Range of DATE Range Types Input/Output (lower-bound,upper-bound) (lower-bound,upper-bound] [lower-bound,upper-bound)[lower-bound,upper-bound] empty 举个例子(0,100) 不包含0和100，并且包括0到100之间的所有整数(0,100] 不包含0，包含100,，并且包括0到100之间的所有整数[0,100) 不包含100，包含0，并且包括0到100之间的所有整数[0,100] 包括0和100， 并且包括0到100之间的所有整数 Range 类型相关函数12345francs=&gt; select '[1,4]'::int4range; int4range ----------- [1,5) (1 row) 查询 range 下界12345francs=&gt; select lower('[1,4]'::int4range); lower ------- 1 (1 row) 查询 range 上界12345francs=&gt; select upper('[1,4]'::int4range); upper ------- 5 (1 row) 查询 ramnge 数值是否为空12345francs=&gt; select isempty('[1,4]'::int4range); isempty --------- f (1 row) Range 类型应用场景接下来演示 int4range 类型，比如创建一张学生成绩等级表，分为四部分， 0-60 不及格，60-80 一般，80-90 良，90-100 优秀，下面定义这张表。 创建测试表123francs=&gt; create table test_int4range(id integer primary key,grade int4range,remark varchar(32)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_int4range_pkey\" for table \"test_int4range\" CREATE TABLE 增加 grade 字段约束1234567891011121314francs=&gt; alter table test_int4range add exclude using gist (grade with &amp;&amp;); NOTICE: ALTER TABLE / ADD EXCLUDE will create implicit index \"test_int4range_grade_excl\" for table \"test_int4range\" ALTER TABLEfrancs=&gt; \\d test_int4range; Table \"francs.test_int4range\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null grade | int4range | remark | character varying(32) | Indexes: \"test_int4range_pkey\" PRIMARY KEY, btree (id) \"test_int4range_grade_excl\" EXCLUDE USING gist (grade WITH &amp;&amp;) 插入数据1234567francs=&gt; insert into test_int4range values (1,'[0,60)','bad'); INSERT 0 1francs=&gt; insert into test_int4range values (2,'[60,80)','just so so'); INSERT 0 1francs=&gt; insert into test_int4range values (3,'[70,80)','just so so'); ERROR: conflicting key value violates exclusion constraint \"test_int4range_grade_excl\" DETAIL: Key (grade)=([70,80)) conflicts with existing key (grade)=([60,80)). 备注：插入数据有重复，引发 conflicting。123456789101112francs=&gt; insert into test_int4range values (3,'[80,90)','good'); INSERT 0 1francs=&gt; insert into test_int4range values (4,'[90,100)','Excellent'); INSERT 0 1francs=&gt; select * From test_int4range; id | grade | remark ----+----------+------------ 1 | [0,60) | bad 2 | [60,80) | just so so 3 | [80,90) | good 4 | [90,100) | Excellent (4 rows) 4.4 查询包括指定元素的记录1234567891011francs=&gt; select * From test_int4range where grade @&gt; 1::int4; id | grade | remark ----+--------+-------- 1 | [0,60) | bad (1 row) francs=&gt; select * From test_int4range where grade @&gt; 75::int4; id | grade | remark ----+---------+------------ 2 | [60,80) | just so so (1 row) 附: Range Types 操作符 参考 http://www.postgresql.org/docs/9.2/static/rangetypes.html http://www.postgresql.org/docs/9.2/static/functions-range.html#RANGE-OPERATORS-TABLE","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.2Beta: 字段类型转换依然需要 rewrite 表 ","slug":"20120524095212","date":"2012-05-24T01:52:12.000Z","updated":"2018-09-04T01:33:56.071Z","comments":true,"path":"20120524095212.html","link":"","permalink":"https://postgres.fun/20120524095212.html","excerpt":"","text":"PostgreSQL9.2 对于”ALTER TABLE “ 字段扩长已有很多场景不需要重写表了，具体可以对照之前的 BLOG: https://postgres.fun/20120523180504.html PostgreSQL9.2Beta 类型转换时是否需要 rewrite 表。 测试场景一: float –&gt; integer创建测试表并导入数据12345678910111213141516171819202122232425262728293031francs=&gt; create table test_4 (id float8,name varchar(32)); CREATE TABLEfrancs=&gt; insert into test_4 select generate_series(1,3000000),'a'; INSERT 0 3000000francs=&gt; select * from test_4 order by id desc limit 5; id | name ---------+------ 3000000 | a 2999999 | a 2999998 | a 2999997 | a 2999996 | a (5 rows)francs=&gt; \\dt+ test_4 List of relations Schema | Name | Type | Owner | Size | Description --------+--------+-------+--------+--------+------------- francs | test_4 | table | francs | 115 MB | (1 row)francs=&gt; timing Timing is on.francs=&gt; \\d test_4 Table \"francs.test_4\" Column | Type | Modifiers --------+-----------------------+----------- id | double precision | name | character varying(32) | 修改字段类型12345678910francs=&gt; alter table test_4 alter column id type integer ; ALTER TABLE Time: 23805.774 msfrancs=&gt; \\d test_4 Table \"francs.test_4\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | 备注： 由 float8 转换成 integer 类型花费了 23 秒，显然重写表了。 测试场景二: integer –&gt; text1234567891011121314151617francs=&gt; \\d test_4 Table \"francs.test_4\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) |francs=&gt; alter table test_4 alter column id type text; ALTER TABLE Time: 31735.712 msfrancs=&gt; \\d test_4 Table \"francs.test_4\" Column | Type | Modifiers --------+-----------------------+----------- id | text | name | character varying(32) | 备注： integer 类型转换成 text 类型花费了 31 秒左右，显然重写表了。 测试场景三: text –&gt; integer1234567891011121314151617francs=&gt; \\d test_4; Table \"francs.test_4\" Column | Type | Modifiers --------+-----------------------+----------- id | text | name | character varying(32) |francs=&gt; alter table test_4 alter column id type integer using (id::integer); ALTER TABLE Time: 34431.975 ms francs=&gt; \\d test_4 Table \"francs.test_4\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | 备注：text 转换成 integer 时，重写了表。 总结对于字段类型转换例如( float –&gt; integer，integer –&gt; text， text –&gt; integer ) 会发生重写表，更进一步发现表的 pg_class.relfilenode 发生了变化，推测字段类型转换(不是字段扩长)都要 rewrite 表，因为涉及到字段类型转换时需要检测每条记录是否合法，这里只是推测，需要进一步验证。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.2Beta: Reduce need to rebuild tables and indexes for various ALTER TABLE ","slug":"20120523180504","date":"2012-05-23T10:05:04.000Z","updated":"2018-09-04T01:33:56.024Z","comments":true,"path":"20120523180504.html","link":"","permalink":"https://postgres.fun/20120523180504.html","excerpt":"","text":"今天在读 PostgreSQL9.2Beta Release Note 过程中，发现了令人振奋的消息，这里用振奋来形容，是因为这个特性让解决了生产维护过程中 PostgreSQL 的一个软肋， 一个曾经让人非常蛋疼的特性。Release Note是这么说的 “Reduce need to rebuild tables and indexes for various ALTER TABLE operations (Noah Misch) DUPLICATE” ，意思是 9.2 版本中对于 Alter table DDL 语句，已经有部分不需要重写表和索引了，注意这里只是部分，而不是全部。而之前的版本 PostgreSQL 对于扩字段长度，例如 varchar，numeric 类型扩字段长度需要重写表，对于生产库的大表而言，这需要花费大量的时间，而且在DDL 语句过程中，全表锁，这无疑是不可接受的，关于之前版本需要重写表的内容，可以参考之前写的BLOG https://postgres.fun/20110215200654.html https://postgres.fun/20110923105316.html 今天测试 PostgreSQL9.2Beta 版一些 “Alter table” 语句，看看哪些不需要 rewrite 表了。 测试场景一 ( varchar 字段扩长: 字段上无索引 )PostgreSQL9.2 Beta1 测试，如下：123456789101112131415161718192021[postgres@redhat6 pg_root]$ psql francs francs psql (9.2beta1) Type \"help\" for help.francs=&gt; create table test_alter (id integer,name varchar(32),flag numeric(1,0)); CREATE TABLEfrancs=&gt; insert into test_alter select generate_series(1,2000000),'francs',0; INSERT 0 2000000 francs=&gt; \\d test_alter Table \"francs.test_alter\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | flag | numeric(1,0) francs=&gt; select * from test_alter limit 1; id | name | flag ----+--------+------ 1 | francs | 0 (1 row) varchar 字段扩长1234567891011francs=&gt; alter table test_alter alter name type character varying(64); ALTER TABLE Time: 1161.613 ms francs=&gt; \\d test_alter Table \"francs.test_alter\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(64) | flag | numeric(1,0) | 备注： PostgreSQL9.2 版本， 这里 varchar(32) 扩长到 varchar(64) 花费了 1 秒左右。 PostgreSQL 9.1.2 测试，如下：123456789101112131415161718192021222324252627[postgres@redhat6 tf]$ psql skytf skytf psql (9.1.2) Type \"help\" for help.skytf=&gt; create table test_alter (id integer,name varchar(32),flag numeric(1,0)); CREATE TABLEskytf=&gt; insert into test_alter select generate_series(1,2000000),'francs',0; INSERT 0 2000000skytf =&gt; \\d test_alter Table \"francs.test_alter\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | flag | numeric(1,0) skytf=&gt; alter table test_alter alter name type character varying(64); ALTER TABLE Time: 7844.453 msskytf=&gt; \\d test_alter Table \"skytf.test_alter\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(64) | flag | numeric(1,0) | 备注：PostgreSQL 9.1 版本中 ，varchar(32) 字段扩长到 varchar(64) 花费了 7 秒左右，显然重写了表，这个表数据只有 200 万，如果表更大，这个时间会更长。 测试场景二 ( varchar 字段扩长: 字段上有索引 )PostgreSQL9.2Beta12345678910111213141516francs=&gt; create index idx_test_alter_name on test_alter using btree (name); CREATE INDEXfrancs=&gt; \\d test_alter Table \"francs.test_alter\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null name | character varying(64) | flag | numeric(2,0) | Indexes: \"idx_test_alter_name\" btree (name)francs=&gt; alter table test_alter alter column name type character varying(128); ALTER TABLE Time: 142.443 ms PostgreSQL9.1，创建索引：1234567skytf=&gt; create index idx_test_alter_name on test_alter using btree (name); CREATE INDEX Time: 12472.807 msskytf=&gt; alter table test_alter alter column name type character varying(128); ALTER TABLE Time: 17125.181 ms 备注：varchar 字段上有索引时， PostgreSQL9.2 在扩字段长度时依然不需要重写表，而 PostgreSQL 9.1 版本需要重写表。 测试场景三 ( numeric 字段扩长 )PostgreSQL9.2Beta numeric 字段扩字12345678910111213141516171819francs=&gt;\\d test_alter Table \"francs.test_alter\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(64) | flag | numeric(1,0) | francs=&gt; alter table test_alter alter column flag type numeric (2,0); ALTER TABLE Time: 79.782 msfrancs=&gt; \\d test_alter Table \"francs.test_alter\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null name | character varying(64) | flag | numeric(2,0) | PostgreSQL9.112345678910skytf=&gt; \\d test_alter Table \"skytf.test_alter\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(64) | flag | numeric(1,0) |skytf=&gt; alter table test_alter alter column flag type numeric (2,0); ALTER TABLE Time: 13236.818 ms 备注：numerica 字段扩长时， PostgreSQL9.2 版本花费 79 ms,而 PostgreSQL 9.1 版本花费 13236 毫秒，显然重写表了。 测试场景四 ( varchar(64) –&gt; text )PostgreSQL9.2Beta1234567891011121314151617181920212223francs=&gt; \\d test_alter Table \"francs.test_alter\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null name | character varying(64) | flag | numeric(2,0) | Indexes: \"idx_test_alter_name\" btree (name) francs=&gt; alter table test_alter alter column name type text; ALTER TABLE Time: 1405.116 ms francs=&gt; \\d test_alter Table \"francs.test_alter\" Column | Type | Modifiers --------+--------------+----------- id | integer | not null name | text | flag | numeric(2,0) | Indexes: \"idx_test_alter_name\" btree (name) PostgreSQL9.112345678910111213skytf=&gt; alter table test_alter alter column name type text; ALTER TABLE Time: 13487.664 ms skytf=&gt; \\d test_alter Table \"skytf.test_alter\" Column | Type | Modifiers --------+--------------+----------- id | integer | name | text | flag | numeric(2,0) | Indexes: \"idx_test_alter_name\" btree (name) 备注：当 varchar 字段扩长到 text 字段时， PostgreSQL9.2 版本没重写表，PostgreSQL9.1 版本需要重写。 不需要重写表的场景在 PostgreSQL9.2 中不需要重写表的“ALTER TABLE “ 场景： varchar(x) to varchar(y) when y&gt;=x. It works too if going from varchar(x) to varchar or text (no size limitation) numeric(x,z) to numeric(y,z) when y&gt;=x, or to numeric without specifier varbit(x) to varbit(y) when y&gt;=x, or to varbit without specifier timestamp(x) to timestamp(y) when y&gt;=x or timestamp without specifier timestamptz(x) to timestamptz(y) when y&gt;=x or timestamptz without specifier interval(x) to interval(y) when y&gt;=x or interval without specifier 总结大体上说 PostgreSQL9.2 版本大多数字段扩长DDL语句不需要写表了，这解决了之前版非常致命的软肋，非常感谢 Noah Misch 的贡献。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.2 Beta: pg_stat_activity view changes a lot ","slug":"20120522141351","date":"2012-05-22T06:13:51.000Z","updated":"2018-09-04T01:33:55.961Z","comments":true,"path":"20120522141351.html","link":"","permalink":"https://postgres.fun/20120522141351.html","excerpt":"","text":"根据 PostgreSQL9.2beta1 版的 release note，监控视图 pg_stat_activity 变化蛮大，包括 Rename pg_stat_activity.procpid to pid, to match other system tables (Magnus Hagander)重命名字段 pg_stat_activity.procpid 为 pid。 Create a separate pg_stat_activity column to report state information, e.g. idle (Scott Mead, Magnus Hagander, Greg Smith)新增 state 字段，显示语句状态。 Rename pg_stat_activity.current_query to query because it is not cleared when the query completes (Magnus Hagander)重命名字段 pg_stat_activity.current_query 为 query。 从上面看出，pg_stat_activity 变化挺大，重命名了两个字段，并且新增了 state 字段。 关于 pg_stat_activity 视图字段，详见手册，根据手册说明，query 字段含义有变化：当 state 状态为 active 的会话时，显示当前运行的语句，当 state 为其它状态时，query 显示最近执行的语句；而之前的版本 current_query 只显示当会运行的会话，如果是空闲会话，current_query 值为 。 新增的 state 字段有以下值: active: The backend is executing a query. ( 显示当前活跃会话的SQL ) idle: The backend is waiting for a new client command. ( 当前会话空闲) idle in transaction: The backend is in a transaction, but is not currently executing a query. 事务没有提交或回滚时. idle in transaction (aborted): This state is similar to idle in transaction, except one ofthe statements in the transaction caused an error. ( 当事务中的语句出错时.) fastpath function call: The backend is executing a fast-path function.这个还不太理解，以后查到相关资料再来补充。 disabled: This state is reported if track_activities is disabled in this backend. ( 当 track_activities 设置为 off 时 ). 构造 state 值为 idle 场景1.1 session A1234[postgres@redhat6 tf]$ psql francs francs psql (9.2beta1) Type \"help\" for help.francs=&gt; 1.2 session B12345postgres=# select pid, datname,usename,query ,state from pg_stat_activity where pid &lt;&gt; pg_backend_pid(); pid | datname | usename | query | state ------+---------+---------+-------+------- 8151 | francs | francs | | idle (1 row) 构造 state 值为 active 场景1234postgres=# select pid, datname,usename,query ,state from pg_stat_activity; pid | datname | usename | query | state ------+----------+----------+-----------------------------------------------------------------+-------- 8134 | postgres | postgres | select pid, datname,usename,query ,state from pg_stat_activity; | active 备注：state 值为 active 时，表示会话正在执行； 直接查询当前会话时，state 值为 active, 当然可以构造慢查询，然后重新开启窗口验证，这里就不做验证了，如果一个库中 active 状态的语句比较多说明语句需要优化了。 构造 state 值为 idle in transaction 场景3.1 session A12345678910[postgres@redhat6 tf]$ psql francs francs psql (9.2beta1) Type \"help\" for help.francs=&gt; begin; BEGIN francs=&gt; select count(*) from test_1; count ------- 10000 (1 row) 3.2 sesion B12345postgres=# select pid, datname,usename,query ,state from pg_stat_activity where pid &lt;&gt; pg_backend_pid(); pid | datname | usename | query | state ------+---------+---------+------------------------------+--------------------- 8165 | francs | francs | select count(*) from test_1; | idle in transaction (1 row) 备注：当事务没有结束时，state 字段显示为 idle in transaction， 同时 query 显示的是最近执行的语句。注意这并不是当前执行的语句，而是历史最近执行的语句。 构造 state 值为 idle in transaction (aborted) 场景4.1 session A123456789[postgres@redhat6 tf]$ psql francs francs psql (9.2beta1) Type \"help\" for help.francs=&gt; begin; BEGIN francs=&gt; select count(*) from test_22; ERROR: relation \"test_22\" does not exist LINE 1: select count(*) from test_22; ^ 4.2 session B1234postgres=# select pid, datname,usename,query ,state from pg_stat_activity where pid &lt;&gt; pg_backend_pid(); pid | datname | usename | query | state ------+---------+---------+-------------------------------+------------------------------- 8181 | francs | francs | select count(*) from test_22; | idle in transaction (aborted)(1 row) 构造 state 值为 disabled 场景5.1 修改 $PGDATA/postgresql.conf 修改参数配置文件 $PGDATA/postgresql.conf ，设置 track_activities = off，默认为 on。 5.2 修改后 pg_ctl reload -D $PGDATA 重新加载配置文件12[postgres@redhat6 pg_root]$ pg_ctl reload -D $PGDATA server signaled 5.3session A1234[postgres@redhat6 pg_root]$ psql francs francs psql (9.2beta1) Type \"help\" for help.francs=&gt; 5.4 session B1234567891011postgres=# show track_activities; track_activities ------------------ off (1 row)postgres=# select pid, datname,usename,query ,state from pg_stat_activity where pid &lt;&gt; pg_backend_pid(); pid | datname | usename | query | state ------+---------+---------+-------+---------- 8201 | francs | francs | | disabled (1 row) 备注：当参数 track_activities 设置为 off 时，pg_stat_activity.state 值始终为 disabled， 表示不记录 session 信息。 构造有 session 被阻塞的情况session A 删除一条记录1234567891011121314151617[postgres@redhat6 pg_root]$ psql francs francs psql (9.2beta1) Type \"help\" for help.francs=&gt; \\d test_1 Table \"francs.test_1\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Indexes: \"idx_test_1_id\" UNIQUE, btree (id)francs=&gt; begin; BEGIN francs=&gt; delete from test_1 where id=1; DELETE 1 备注： session 准备删除表的一条记录，注意此时事务未提交。 session B 创建索引1234[postgres@redhat6 pg_root]$ psql francs francs psql (9.2beta1) Type \"help\" for help.francs=&gt; create index idx_test_1_name on test_1 using btree (name); 备注： session B 准备在表 test_1 上创建一个索引，注意此时会话被 BLOCK，光标下不去。 session C 监控会话12345postgres=# select pid, datname,usename,query ,state ,waiting from pg_stat_activity where pid &lt;&gt; pg_backend_pid(); pid | datname | usename | query | state | waiting ------+---------+---------+------------------------------------------------------------+---------------------+--------- 8641 | francs | francs | delete from test_1 where id=1; | idle in transaction | f 8713 | francs | francs | create index idx_test_1_name on test_1 using btree (name); | active | t(2 rows) 备注：session B 的进程号为 8713 ，虽然它的 state 状态为 active ，但是它处理于等侍状态， waiting 为 t；也就是说 waiting 字段和 state 字段是独立的，如果 state 字段为 active ， 那么会话的 wating 字段可能为 true 也可能为 false。 附 track_activities (boolean) Enables the collection of information on the currently executing command of each session, alongwith the time when that command began execution. This parameter is on by default. Note that even whenenabled, this information is not visible to all users, only to superusers and the user owning the sessionbeing reported on, so it should not represent a security risk. Only superusers can change this setting. 参考 http://www.postgresql.org/docs/9.2/static/release-9-2.html http://www.postgresql.org/docs/9.2/static/monitoring-stats.html#MONITORING-STATS-VIEWS-TABLE","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.2 Beta: Test Index-only scans","slug":"20120517171040","date":"2012-05-17T09:10:40.000Z","updated":"2018-09-04T01:33:55.914Z","comments":true,"path":"20120517171040.html","link":"","permalink":"https://postgres.fun/20120517171040.html","excerpt":"","text":"PostgreSQL 9.2 Beta 已经支持 Index-only scans ，今天测试了下，发现性能没明显区别 ，这里比较的是 PostgreSQL9.0。 Index-only scans Index-only scans, allowing users to avoid inefficient scans of base tables备注： Index-only scans 支持当查询索引项数据时，可以不用返回查询基表了，直接返回索引的索引项数据，当然这是有前提的，即索引指向的Heap Block 上的 tuples 都可见。 测试: PostgreSQL 9.01.1查询版本12345skytf=&gt; select version(); version ----------------------------------------------------------------------------------------------------------- PostgreSQL 9.0.2 on i686-pc-linux-gnu, compiled by GCC gcc (GCC) 4.4.6 20110731 (Red Hat 4.4.6-3), 32-bit (1 row) 1.2 创建测试表并插入测试数据12345skytf=&gt; create table test_indexonly (id integer primary key,name varchar(32)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_indexonly_pkey\" for table \"test_indexonly\" CREATE TABLEskytf=&gt; insert into test_indexonly select generate_series(1,10000),'index only'; INSERT 0 10000 1.3 测试索引单条查询1234567891011121314151617181920212223skytf=&gt; explain analyze select id from test_indexonly where id=1; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ Index Scan using test_indexonly_pkey on test_indexonly (cost=0.00..8.27 rows=1 width=4) (actual time=0.089..0.091 rows=1 loops=1) Index Cond: (id = 1) Total runtime: 0.173 ms (3 rows) skytf=&gt; explain analyze select id from test_indexonly where id=1; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ Index Scan using test_indexonly_pkey on test_indexonly (cost=0.00..8.27 rows=1 width=4) (actual time=0.022..0.026 rows=1 loops=1) Index Cond: (id = 1) Total runtime: 0.068 ms (3 rows) skytf=&gt; explain analyze select id from test_indexonly where id=1; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ Index Scan using test_indexonly_pkey on test_indexonly (cost=0.00..8.27 rows=1 width=4) (actual time=0.057..0.060 rows=1 loops=1) Index Cond: (id = 1) Total runtime: 0.204 ms (3 rows) 1.4 测试扫描索引范围的情况12345678910111213141516171819skytf=&gt; explain analyze select count(*) from test_indexonly where id&gt;300 and id &lt;401; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ Aggregate (cost=10.53..10.54 rows=1 width=0) (actual time=0.366..0.367 rows=1 loops=1) -&gt; Index Scan using test_indexonly_pkey on test_indexonly (cost=0.00..10.27 rows=101 width=0) (actual time=0.041..0.220 rows=100 loops=1) Index Cond: ((id &gt; 300) AND (id &lt; 401)) Total runtime: 0.462 ms (4 rows)skytf=&gt; explain analyze select count(*) from test_indexonly where id&gt;300 and id &lt;401; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------- Aggregate (cost=10.53..10.54 rows=1 width=0) (actual time=0.357..0.358 rows=1 loops=1) -&gt; Index Scan using test_indexonly_pkey on test_indexonly (cost=0.00..10.27 rows=101 width=0) (actual time=0.035..0.201 rows=10 0 loops=1) Index Cond: ((id &gt; 300) AND (id &lt; 401)) Total runtime: 0.421 ms (4 rows) 测试: PostgreSQL 9.2 Beta12.1 查询版本12345francs=&gt; select version(); version ---------------------------------------------------------------------------------------------------------- PostgreSQL 9.2beta1 on i686-pc-linux-gnu, compiled by gcc (GCC) 4.4.6 20110731 (Red Hat 4.4.6-3), 32-bit (1 row) 2.2 创建测试表并插入测试数据12345francs=&gt; create table test_indexonly (id integer primary key,name varchar(32)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_indexonly_pkey\" for table \"test_indexonly\" CREATE TABLEfrancs=&gt; insert into test_indexonly select generate_series(1,10000),'index only'; INSERT 0 10000 2.3 测试索引单条查询12345678910111213141516171819202122232425262728francs=&gt; explain analyze select id from test_indexonly where id=1; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ Index Only Scan using test_indexonly_pkey on test_indexonly (cost=0.00..8.28 rows=1 width=4) (actual time=0.068..0.071 rows=1 loops=1) Index Cond: (id = 1) Heap Fetches: 1 Total runtime: 0.142 ms(4 rows)francs=&gt; explain analyze select id from test_indexonly where id=1; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------- Index Only Scan using test_indexonly_pkey on test_indexonly (cost=0.00..8.28 rows=1 width=4) (actual time=0.016..0.019 rows=1 loops=1) Index Cond: (id = 1) Heap Fetches: 1 Total runtime: 0.049 ms (4 rows)francs=&gt; explain analyze select id from test_indexonly where id=1; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ Index Only Scan using test_indexonly_pkey on test_indexonly (cost=0.00..8.28 rows=1 width=4) (actual time=0.027..0.030 rows=1 loops=1) Index Cond: (id = 1) Heap Fetches: 1 Total runtime: 0.085 ms (4 rows) 2.4 测试扫描索引范围的情况1234567891011121314151617181920francs=&gt; explain analyze select count(*) from test_indexonly where id&gt;300 and id &lt;401; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------- Aggregate (cost=10.53..10.54 rows=1 width=0) (actual time=0.451..0.452 rows=1 loops=1) -&gt; Index Only Scan using test_indexonly_pkey on test_indexonly (cost=0.00..10.28 rows=101 width=0) (actual time=0.086..0.275 rows=100 loops=1) Index Cond: ((id &gt; 300) AND (id &lt; 401)) Heap Fetches: 100 Total runtime: 0.579 ms (5 rows) francs=&gt; explain analyze select count(*) from test_indexonly where id&gt;300 and id &lt;401; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ Aggregate (cost=10.53..10.54 rows=1 width=0) (actual time=0.518..0.520 rows=1 loops=1) -&gt; Index Only Scan using test_indexonly_pkey on test_indexonly (cost=0.00..10.28 rows=101 width=0) (actual time=0.167..0.353 rows=100 loops=1) Index Cond: ((id &gt; 300) AND (id &lt; 401)) Heap Fetches: 100 Total runtime: 0.583 ms(5 rows) 总结3.1 测试场景一：单条记录索引扫描 单条记录索引扫描 PostgreSQL 9.0 PostgreSQL 9.2 第一次查询 0.173 ms 0.142 ms 第二次查询 0.068 ms 0.049 ms 第三次查询 0.204 ms 0.085 ms 3.2 测试场景二：多条记录索引范围扫描 多条记录索引范围扫描 PostgreSQL 9.0 PostgreSQL 9.2 第一次查询 0.462 ms 0.579 ms 第二次查询 0.421 ms 0.583 ms “Index Only Scan “ 和 “Index Scan using “ 的性能在单条检索，和范围检索上性能没有明显区别。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.2 安装报错 Psql: connection pointer is NULL ","slug":"20120517151404","date":"2012-05-17T07:14:04.000Z","updated":"2018-09-04T01:33:55.867Z","comments":true,"path":"20120517151404.html","link":"","permalink":"https://postgres.fun/20120517151404.html","excerpt":"","text":"PostgreSQL9.2beta1 已经 Release，今天打算安装测试下， PostgreSQL 编译安装成功，但连接 PostgreSQL 时报”psql: connection pointer is NULL” ，数据库安装步骤略。 1 使用 9.2 客户端登陆报错12[postgres@redhat6 ~]$ /opt/pgsql9.2beta/bin/psql -p 1922 psql: connection pointer is NULL 2 用 9.0 客户端正常12345[postgres@redhat6 ~]$ psql -h 127.0.0.1 -p 1922 psql (9.0.2, server 9.2beta1) WARNING: psql version 9.0, server version 9.2. Some psql features might not work. Type \"help\" for help. 备注：后来 GOOGLE 下，没查到有用信息，后来猜想可能是环境变量没有正确设置，因为在这台主机上已经安装了 postgresql9.0.2，环境变量没有做相应修改。 3 系统postgres 用户环境变量123456789101112131415161718# .bash_profile# Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi# User specific environment and startup programsPATH=$PATH:$HOME/binexport PATHexport PGPORT=1921 export PGDATA=/opt/pgdata/pg_root export LANG=en_US.utf8export PGHOME=/opt/pgsql9.0 export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib export DATE=`date +\"%Y%m%d%H%M\"` export PATH=$PGHOME/bin:$PATH:. export MANPATH=$PGHOME/share/man:$MANPATH alias rm='rm -i' alias ll='ls -lh' 备注：以上是 postgresql 用户环境变量 4 在 session 端修改环境变量12345678910export PGPORT=1922 export PGDATA=/database/pgdata9.2/pg_root export LANG=en_US.utf8export PGHOME=/opt/pgsql9.2beta export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib export DATE=`date +\"%Y%m%d%H%M\"` export PATH=$PGHOME/bin:$PATH:. export MANPATH=$PGHOME/share/man:$MANPATH alias rm='rm -i' alias ll='ls -lh' 5 再次测试123[postgres@redhat6 ~]$ /opt/pgsql9.2beta/bin/psql -p 1922 psql (9.2beta1) Type \"help\" for help. 备注：登陆成功，经过测试，导入正确的 LD_LIBRARY_PATH 环境变量后，问题解决！ 6 End 为了方便可以将上面环境变量写到一个文件里，需要用时在 session 端 source 一下即可。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL 9.2 Beta 1 Available for Testing","slug":"20120515140112","date":"2012-05-15T06:01:12.000Z","updated":"2018-09-04T01:33:55.805Z","comments":true,"path":"20120515140112.html","link":"","permalink":"https://postgres.fun/20120515140112.html","excerpt":"","text":"PostgreSQL 9.2 Beta 版已经出来了，主要特性包括 Index-only scans，read-only 场景强大的支撑 (300,000 queries per second)，写性能的改善，CPU资源消耗降低，以及 Cascading replication 等，有空可以测试下，下面是来自 PostgreSQL 官网 http://www.postgresql.org/about/news/1395/ The PostgreSQL Global Development Group announces the beta release of PostgreSQL 9.2,which will include major increases in performance and both vertical and horizontal scalability.The PostgreSQL Project asks all users to download and begin testing 9.2 Beta as soon as possible.Major performance and scalability advances in this version include: Index-only scans, allowing users to avoid inefficient scans of base tables Enhanced read-only workload scaling to 64 cores and over 300,000 queries per second Improvements to data write speeds, including group commit Reductions in CPU power consumption Cascading replication, supporting geographically distributed standby databases PostgreSQL 9.2 will also offer many new features for application developers, including: JSON data support, enabling hybrid document-relational databases Range types, supporting new types of calendar, time-series and analytic applications Multiple improvements to ALTER and other statements, easing runtime database updates For a full listing of the features in version 9.2 Beta, please see the release notes. We depend on our community to help test the next version in order to guarantee that itis high-performance and bug-free. Please download PostgreSQL 9.2 Beta and try it with yourworkloads and applications as soon as you can, and give feedback to the PostgreSQL developers.More information on how to test and report issuesGet PostgreSQL 9.2 beta, including binariesand installers for Windows, Linux and Mac from our download page.Full documentation of the new version is available online, and also installs with PostgreSQL. PostgreSQL 9.2 beta 版源码下载地址http://www.postgresql.org/ftp/source/v9.2.0beta1/","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL : Tuples 被 freezing 的几种情况 ","slug":"20120506221006","date":"2012-05-06T14:10:06.000Z","updated":"2018-09-04T01:33:55.758Z","comments":true,"path":"20120506221006.html","link":"","permalink":"https://postgres.fun/20120506221006.html","excerpt":"","text":"根据 PostgreSQL 的 MVCC 机制，数据被插入时 PostgreSQL会分配给每行 tuples一个事务ID，即表上的隐含字段 Xmin， 而 PostgreSQL 的事务号由 32 bit 位 ( 40 亿) 组成，事务号分配完了后会循环，这样会造成过去的记录不可见的情况，为了解决这个问题，理论上在 20 亿事务之内需要 vacuum 每一个数据库的每一个表， 而vacuum 操作会替换某些老记录的xid 成 FrozenXID，这样即使事务号循环，这些被替换成 FrozenXID 的记录依然对当前事务可见，下面是模拟表数据的tuples 的xid 被替换成 FrozenXID 的几个例子。 Freezing 实验一 ( vacuum freeze )1.1 创建测试表并插入测试数据12345678mydb=&gt; create table test_vacuum1(id integer,name varchar(32)); CREATE TABLE mydb=&gt; insert into test_vacuum1 values (1,'a'); INSERT 0 1 mydb=&gt; insert into test_vacuum1 values (2,'b'); INSERT 0 1 mydb=&gt; insert into test_vacuum1 values (3,'c'); INSERT 0 1 1.2 查询表及tuples 的年龄123456789101112mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum1'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum1 | 3613 | 4 (1 row)mydb=&gt; select xmin,age(xmin),* from test_vacuum1; xmin | age | id | name ------+-----+----+------ 3614 | 5 | 1 | a 3615 | 4 | 2 | b 3616 | 3 | 3 | c (3 rows) 1.3 执行 vacuum freeze12345678910111213141516mydb=&gt; vacuum freeze test_vacuum1; VACUUM mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum1'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum1 | 3620 | 0 (1 row)mydb=&gt; select xmin,age(xmin),* from test_vacuum1; xmin | age | id | name ------+------------+----+------ 2 | 2147483647 | 1 | a 2 | 2147483647 | 2 | b 2 | 2147483647 | 3 | c (3 rows) 备注：对表 test_vacuum1 执行 vacuum freeze 后，test_vacuum1 上的所有记录被 frozen，即 xmin 值被修改，并且 tuples 的年龄为 2147483647，这个 XID 比较特殊，不参与普通 XID的比较，可以认为这个 XID 比 所有 XID 都老。 Freezing 实验二 ( vacuum )–2.1 创建表并插入测试数据12345678mydb=&gt; create table test_vacuum2 (id integer,name varchar(32)); CREATE TABLE mydb=&gt; insert into test_vacuum2 values (1,'a'); INSERT 0 1 mydb=&gt; insert into test_vacuum2 values (2,'b'); INSERT 0 1 mydb=&gt; insert into test_vacuum2 values (3,'c'); INSERT 0 1 2.2 查看表和 tuples 年龄12345678910111213mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum2'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum2 | 3623 | 4 (1 row)mydb=&gt; select xmin,age(xmin),* from test_vacuum2; xmin | age | id | name ------+-----+----+------ 3624 | 4 | 1 | a 3625 | 3 | 2 | b 3626 | 2 | 3 | c (3 rows) 2.3 设置 session 级 vacuum_freeze_min_age 参数，并 vacuum12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970mydb=&gt; set vacuum_freeze_min_age=10; SET mydb=&gt; vacuum test_vacuum2; VACUUMmydb=&gt; select xmin,age(xmin),* from test_vacuum2; xmin | age | id | name ------+-----+----+------ 3624 | 5 | 1 | a 3625 | 4 | 2 | b 3626 | 3 | 3 | c (3 rows)mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum2'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum2 | 3623 | 7 (1 row)mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum2'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum2 | 3623 | 8 (1 row)mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum2'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum2 | 3623 | 9 (1 row)mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum2'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum2 | 3623 | 10 (1 row)mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum2'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum2 | 3623 | 11 (1 row)mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum2'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum2 | 3623 | 12 (1 row)mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum2'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum2 | 3623 | 13 (1 row)mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum2'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum2 | 3623 | 14 (1 row)mydb=&gt; vacuum test_vacuum2; VACUUMmydb=&gt; select xmin,age(xmin),* from test_vacuum2; xmin | age | id | name ------+------------+----+------ 2 | 2147483647 | 1 | a 2 | 2147483647 | 2 | b 2 | 2147483647 | 3 | c (3 rows) 备注：发现 test_vaccum2 的记录 xmin 被替换成 frozenXID。 Freezing 实验三 ( autovacuum )3.1 创建测试表并插入数据 12345678910111213141516171819202122232425mydb=&gt; create table test_vacuum3(id integer,name varchar(32)); CREATE TABLE mydb=&gt; insert into test_vacuum3 select generate_series(1,1000),'francs'; INSERT 0 1000 mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum3'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum3 | 3651 | 2 (1 row)mydb=&gt; select xmin,age(xmin) from test_vacuum3 order by id limit 10; xmin | age ------+----- 3652 | 2 3652 | 2 3652 | 2 3652 | 2 3652 | 2 3652 | 2 3652 | 2 3652 | 2 3652 | 2 3652 | 2 (10 rows) 3.2 修改表的 autovacuum_freeze_min_age 参数1234567891011mydb=&gt; alter table test_vacuum3 set (autovacuum_freeze_min_age = 0); ALTER TABLEmydb=&gt; \\d+ test_vacuum3; Table \"mydb.test_vacuum3\" Column | Type | Modifiers | Storage | Description --------+-----------------------+-----------+----------+------------- id | integer | | plain | name | character varying(32) | | extended | Has OIDs: no Options: autovacuum_freeze_min_age=0 3.3 删除 250 条记录，触发 autovacuum12mydb=&gt; delete from test_vacuum3 where id &lt; 251; DELETE 250 备注：关于删除多少条记录会触发 autovacuum，可以参考之前写的 blog: https://postgres.fun/20110314162516.html 3.4 观察 CSV 日志 在上步 delete 数据后，等一会，如果看到以下日志，说明表 test_vacuum3 被 autovacuum 进程 vacuum了。 123452012-05-06 21:42:04.864 CST,,,32232,,4fa67fac.7de8,1,,2012-05-06 21:42:04 CST,3/397,3660,LOG,00000,\"automatic analyze of table \"\"mydb.mydb.test_vacuum3\"\" system usage: CPU 0.00s/0.00u sec elapsed 0.01 sec\",,,,,,,,,\"\" 2012-05-06 21:43:04.847 CST,,,32237,,4fa67fe8.7ded,1,,2012-05-06 21:43:04 CST,3/405,0,LOG,00000,\"automatic vacuum of table \"\"mydb.mydb.test_vacuum3\"\": index scans: 0 pages: 0 removed, 5 remain tuples: 0 removed, 750 remain system usage: CPU 0.00s/0.00u sec elapsed 0.00 sec\",,,,,,,,,\"\" 备注：修改表的 autovacuum_freeze_min_age 参数，目标是让表的 tuples 在 autovacuum 时即被 frozenXID替换。 3.5 再次查看表和 tuples 的年龄123456789101112131415161718mydb=&gt; select relname,relfrozenxid,age(relfrozenxid) from pg_class where relname='test_vacuum3'; relname | relfrozenxid | age --------------+--------------+----- test_vacuum3 | mydb=&gt; select xmin,age(xmin) from test_vacuum3 order by id limit 10; xmin | age ------+------------ 2 | 2147483647 2 | 2147483647 2 | 2147483647 2 | 2147483647 2 | 2147483647 2 | 2147483647 2 | 2147483647 2 | 2147483647 2 | 2147483647 2 | 2147483647 (10 rows) 备注：表 test_vacuum3 的 xmin 已经被 FrozenXID 替换，其中实验三需要触发两个条件，第一需要想办法触发 autovacuum，第二需设置事务时长，触发表上记录被 FrozenXID 替换。 参考 http://blog.163.com/digoal@126/blog/static/1638770402011830105342275/ http://www.postgresql.org/docs/9.1/static/sql-vacuum.html http://www.postgresql.org/docs/9.1/static/routine-vacuuming.html#VACUUM-FOR-WRAPAROUND","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"MVCC","slug":"MVCC","permalink":"https://postgres.fun/tags/MVCC/"}]},{"title":"PostgreSQL: Autovacuum 进程异常一例 ","slug":"20120506163200","date":"2012-05-06T08:32:00.000Z","updated":"2018-09-04T01:33:55.695Z","comments":true,"path":"20120506163200.html","link":"","permalink":"https://postgres.fun/20120506163200.html","excerpt":"","text":"今天在测试库上发现 autovacuum 无法正常启动，数据库版本 9.1.0，经查已经配置 autovacuum=on ，发现日志出现如下信息 数据库 CSVLOG1232012-03-08 11:14:59.017 CST,,,1155,,4f582407.483,1,,2012-03-08 11:14:15 CST,,0,LOG,00000,\"could not resolve \"\"localhost\"\": Temporary failure in name resolution\",,,,,,,,,\"\" 2012-03-08 11:14:59.017 CST,,,1155,,4f582407.483,2,,2012-03-08 11:14:15 CST,,0,LOG,00000,\"disabling statistics collector for lack of working socket\",,,,,,,,,\"\" 2012-03-08 11:14:59.018 CST,,,1155,,4f582407.483,3,,2012-03-08 11:14:15 CST,,0,WARNING,01000,\"autovacuum not started because of misconfiguration\",,\"Enable the \"\"track_counts\"\" option.\",,,,,,,\"\" 备注：注意这个ERROR，提示 autovacuum 进程没有正常启动是因为参数 track_counts 没有设置成 on，接着查看 $PGDATA/postgresql.conf， 发现 track_counts 参数已打开，并且这个参数默认是打开的，奇了怪了。 查看数据库参数12345678910111213[postgres@pgb ~]$ psql -h 127.0.0.1 psql (9.1.0) mydb=&gt; show autovacuum; autovacuum ------------ on (1 row)postgres=# show track_counts; track_counts -------------- off (1 row) 备注： 果然参数 track_counts 显示为 off，而在配置文件中明明是打开的，这是为啥呢？后来仔细查看 步骤1的报错，could not resolve “”localhost”，猜想可能是这里的问题。 Ping Localhostping localhost 不通，接着查看 /etc/hosts 文件。 查看 /etc/hosts123456[root@pgb ~]# cat /etc/hosts # Do not remove the following line, or various programs # that require network functionality will fail. #127.0.0.1 pgb localhost.localdomain localhost 192.168.1.25 pg1 192.168.1.26 pgb 备注：发现 127.0.0.1 这行注释了，于是将注释拿掉， Autovacuum 进程恢复1234[postgres@pgb pg_root]$ ps -ef | grep auto root 2260 1 0 14:09 ? 00:00:00 automount postgres 30865 30860 0 16:07 ? 00:00:00 postgres: autovacuum launcher process postgres 30911 4812 0 16:19 pts/3 00:00:00 grep auto 备注：autovacuum 进程已恢复正常。 查看CSVLOG122012-05-06 16:07:35.711 CST,,,30862,,4fa63147.788e,1,,2012-05-06 16:07:35 CST,,0,LOG,00000,\"database system was shut down at 2012-05-06 16:07:23 CST\",,,,,,,,,\"\" 2012-05-06 16:07:35.801 CST,,,30865,,4fa63147.7891,1,,2012-05-06 16:07:35 CST,,0,LOG,00000,\"autovacuum launcher started\",,,,,,,,,\"\" 总结开启 autovacuum 的步骤 设置 autovacuum = on ( 些参数默认打开 ) 设置 track_counts = on ( 些参数默认打开 ) 设置 /etc/hosts ， 能 ping 通 localhost","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"Autovacuum","slug":"Autovacuum","permalink":"https://postgres.fun/tags/Autovacuum/"}]},{"title":"PostgreSQL MVCC: future transaction should not be visible to the current transaction ","slug":"20120505215126","date":"2012-05-05T13:51:26.000Z","updated":"2018-09-04T01:33:55.648Z","comments":true,"path":"20120505215126.html","link":"","permalink":"https://postgres.fun/20120505215126.html","excerpt":"","text":"之前读到手册以下这段话一直不太理解，如下： 手册内容的一段话 PostgreSQL’s MVCC transaction semantics depend on being able to compare transaction ID (XID) numbers:a row version with an insertion XID greater than the current transaction’s XID is “in the future” and shouldnot be visible to the current transaction 备注：要理解这段话，首先得知道每行记录插入时 pg 会分配给每行 tuple 一个 xid, 这个 xid 为系统隐含字段 xmin，那么任何比 xmin 值要大的事务被认为是将来的，而且将来的事务在当前事务中不可见；这样翻译有点难懂。 后来通过实验，总算理解了。 实验一: 验证未来事务在当前事务不可见1.1 创建测试表并插入三条记录12345678mydb=&gt; create table test_29 (id integer,name varchar(32)); CREATE TABLEmydb=&gt; insert into test_29 values (1,'a'); INSERT 0 1 mydb=&gt; insert into test_29 values (2,'b'); INSERT 0 1 mydb=&gt; insert into test_29 values (3,'c'); INSERT 0 1 1.2 查询记录的 xmin12345678910111213141516171819mydb=&gt; select xmin,*,txid_current() from test_29; xmin | id | name | txid_current ------+----+------+-------------- 3338 | 1 | a | 3341 3339 | 2 | b | 3341 3340 | 3 | c | 3341 (3 rows)mydb=&gt; select txid_current(); txid_current -------------- 3342 (1 row)mydb=&gt; select txid_current(); txid_current -------------- 3343 (1 row) 1.3 再次插入三条记录，并查询记录 xmin1234567891011121314151617mydb=&gt; insert into test_29 values (4,'d'); INSERT 0 1 mydb=&gt; insert into test_29 values (5,'e'); INSERT 0 1 mydb=&gt; insert into test_29 values (6,'f'); INSERT 0 1mydb=&gt; select xmin,*,txid_current() from test_29; xmin | id | name | txid_current ------+----+------+-------------- 3338 | 1 | a | 3347 3339 | 2 | b | 3347 3340 | 3 | c | 3347 3344 | 4 | d | 3347 3345 | 5 | e | 3347 3346 | 6 | f | 3347 (6 rows) 备注：注意记录的 xmin 值。 1.4 查看控制文件：pg_controldata 输出1234567891011121314151617181920212223242526272829303132333435[postgres@pgb ~]$ pg_controldata pg_control version number: 903 Catalog version number: 201105231 Database system identifier: 5652547536925883229 Database cluster state: in production pg_control last modified: Wed 02 May 2012 09:27:56 PM CST Latest checkpoint location: 1/BC000020 Prior checkpoint location: 1/B8015934 Latest checkpoint's REDO location: 1/BC000020 Latest checkpoint's TimeLineID: 5 Latest checkpoint's NextXID: 0/3336 Latest checkpoint's NextOID: 66605 Latest checkpoint's NextMultiXactId: 1 Latest checkpoint's NextMultiOffset: 0 Latest checkpoint's oldestXID: 1670 Latest checkpoint's oldestXID's DB: 16386 Latest checkpoint's oldestActiveXID: 0 Time of latest checkpoint: Wed 02 May 2012 09:26:34 PM CST Minimum recovery ending location: 0/0 Backup start location: 0/0 Current wal_level setting: hot_standby Current max_connections setting: 100 Current max_prepared_xacts setting: 0 Current max_locks_per_xact setting: 64 Maximum data alignment: 4 Database block size: 8192 Blocks per segment of large relation: 1048576 WAL block size: 65536 Bytes per WAL segment: 67108864 Maximum length of identifiers: 64 Maximum columns in an index: 32 Maximum size of a TOAST chunk: 2000 Date/time type storage: 64-bit integers Float4 argument passing: by value Float8 argument passing: by reference 备注：Latest checkpoint s NextXID:值为 3336 1.5 关闭 PostgreSQL123[postgres@pgb ~]$ pg_ctl stop -m fast -D $PGDATA waiting for server to shut down...... done server stopped 1.6 再次查看 pg_controldata12345678910111213141516171819202122232425262728293031323334pg_control version number: 903 Catalog version number: 201105231 Database system identifier: 5652547536925883229 Database cluster state: shut down pg_control last modified: Wed 02 May 2012 09:29:34 PM CST Latest checkpoint location: 1/C0000020 Prior checkpoint location: 1/BC000020 Latest checkpoint's REDO location: 1/C0000020 Latest checkpoint's TimeLineID: 5 Latest checkpoint's NextXID: 0/3348 Latest checkpoint's NextOID: 66608 Latest checkpoint's NextMultiXactId: 1 Latest checkpoint's NextMultiOffset: 0 Latest checkpoint's oldestXID: 1670 Latest checkpoint's oldestXID's DB: 16386 Latest checkpoint's oldestActiveXID: 0 Time of latest checkpoint: Wed 02 May 2012 09:29:32 PM CST Minimum recovery ending location: 0/0 Backup start location: 0/0 Current wal_level setting: hot_standby Current max_connections setting: 100 Current max_prepared_xacts setting: 0 Current max_locks_per_xact setting: 64 Maximum data alignment: 4 Database block size: 8192 Blocks per segment of large relation: 1048576 WAL block size: 65536 Bytes per WAL segment: 67108864 Maximum length of identifiers: 64 Maximum columns in an index: 32 Maximum size of a TOAST chunk: 2000 Date/time type storage: 64-bit integers Float4 argument passing: by value Float8 argument passing: by reference 备注： Latest checkpoint s NextXID: 值发生了变化，值为 3348，这是因为关闭数据库时触发了 checkpoint。 1.7 我们尝试把 NextXID 设置成 3341 ，看看会出现什么情况1234[postgres@pgb ~]$ pg_resetxlog -x 3341 $PGDATA Transaction log reset [postgres@pgb ~]$ pg_ctl start -D $PGDATA server starting 1.8 再次查询1234567891011121314151617181920212223242526272829303132333435[postgres@pgb ~]$ psql -h 127.0.0.1 mydb mydb psql (9.1.0) Type \"help\" for help.mydb=&gt; select xmin,*,txid_current() from test_29; xmin | id | name | txid_current ------+----+------+-------------- 3338 | 1 | a | 3341 3339 | 2 | b | 3341 3340 | 3 | c | 3341 (3 rows)mydb=&gt; select xmin,*,txid_current() from test_29; xmin | id | name | txid_current ------+----+------+-------------- 3338 | 1 | a | 3342 3339 | 2 | b | 3342 3340 | 3 | c | 3342 (3 rows)mydb=&gt; select xmin,*,txid_current() from test_29; xmin | id | name | txid_current ------+----+------+-------------- 3338 | 1 | a | 3343 3339 | 2 | b | 3343 3340 | 3 | c | 3343 (3 rows)mydb=&gt; select xmin,*,txid_current() from test_29; xmin | id | name | txid_current ------+----+------+-------------- 3338 | 1 | a | 3344 3339 | 2 | b | 3344 3340 | 3 | c | 3344 (3 rows) 备注：上面一直只能查看到 test_29 表的前三条记录，后三条记录消失，那是因为后三条记录的 xmin 大于 3344，而当前事务ID 小于或等于 3344，所以后三条记录对于当前事务来说是未来的，所以在当前事务中不可见，接着查询看会出现什么情况。12345678910111213141516171819mydb=&gt; select xmin,*,txid_current() from test_29; xmin | id | name | txid_current ------+----+------+-------------- 3338 | 1 | a | 3345 3339 | 2 | b | 3345 3340 | 3 | c | 3345 3344 | 4 | d | 3345 (4 rows)mydb=&gt; select xmin,*,txid_current() from test_29; xmin | id | name | txid_current ------+----+------+-------------- 3338 | 1 | a | 3347 3339 | 2 | b | 3347 3340 | 3 | c | 3347 3344 | 4 | d | 3347 3345 | 5 | e | 3347 3346 | 6 | f | 3347 (6 rows) 备注：随着当前事务号的增加，后面的三条记录居然出现了。 按照上面 PostgreSQL MVCC 原理，那么可以模拟下数据库因为 Transaction ID Wraparound 失败，而造成所有数据消失的情况。 实验二: 模拟事务 “Transaction ID Wraparound” 失败，所有数据消失2.1 操作前数据库信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546[postgres@pgb ~]$ psql -h 127.0.0.1 mydb mydb psql (9.1.0) Type \"help\" for help.mydb=&gt; \\dt List of relations Schema | Name | Type | Owner --------+----------------+-------+------- mydb | ad_position_ad | table | mydb mydb | array_test | table | mydb mydb | phone | table | mydb mydb | phone_201202 | table | mydb mydb | test | table | mydb mydb | test_10 | table | mydb mydb | test_11 | table | mydb mydb | test_12 | table | mydb mydb | test_13 | table | mydb mydb | test_14 | table | mydb mydb | test_15 | table | mydb mydb | test_16 | table | mydb mydb | test_17 | table | mydb mydb | test_18 | table | mydb mydb | test_19 | table | mydb mydb | test_2 | table | mydb mydb | test_20 | table | mydb mydb | test_21 | table | mydb mydb | test_23 | table | mydb mydb | test_24 | table | mydb mydb | test_25 | table | mydb mydb | test_26 | table | mydb mydb | test_27 | table | mydb mydb | test_28 | table | mydb mydb | test_29 | table | mydb mydb | test_3 | table | mydb mydb | test_4 | table | mydb mydb | test_5 | table | mydb mydb | test_6 | table | mydb mydb | test_7 | table | mydb mydb | test_8 | table | mydb mydb | test_9 | table | mydb mydb | test_array | table | mydb mydb | test_integer | table | mydb mydb | test_name | table | mydb mydb | test_standby | table | mydb mydb | test_unlogged | table | mydb mydb | test_user | table | mydb (38 rows) 2.2 pg_controldata 输出1234567891011121314151617181920212223242526272829303132333435[postgres@pgb ~]$ pg_controldata pg_control version number: 903 Catalog version number: 201105231 Database system identifier: 5652547536925883229 Database cluster state: in production pg_control last modified: Sat 05 May 2012 09:25:43 PM CST Latest checkpoint location: 1/C4000270 Prior checkpoint location: 1/C4000020 Latest checkpoint's REDO location: 1/C4000240 Latest checkpoint's TimeLineID: 5 Latest checkpoint's NextXID: 0/3348 Latest checkpoint's NextOID: 66608 Latest checkpoint's NextMultiXactId: 1 Latest checkpoint's NextMultiOffset: 0 Latest checkpoint's oldestXID: 1670 Latest checkpoint's oldestXID's DB: 16386 Latest checkpoint's oldestActiveXID: 3348 Time of latest checkpoint: Sat 05 May 2012 09:25:43 PM CST Minimum recovery ending location: 0/0 Backup start location: 0/0 Current wal_level setting: hot_standby Current max_connections setting: 100 Current max_prepared_xacts setting: 0 Current max_locks_per_xact setting: 64 Maximum data alignment: 4 Database block size: 8192 Blocks per segment of large relation: 1048576 WAL block size: 65536 Bytes per WAL segment: 67108864 Maximum length of identifiers: 64 Maximum columns in an index: 32 Maximum size of a TOAST chunk: 2000 Date/time type storage: 64-bit integers Float4 argument passing: by value Float8 argument passing: by reference 2.3 关闭数据库123[postgres@pgb ~]$ pg_ctl stop -m fast -D $PGDATA waiting for server to shut down...... done server stopped 2.4 重置 xid，设置较小的 xid123[postgres@pgb ~]$ pg_resetxlog -x 100 $PGDATA Transaction log reset备注：这里将 xid 重置成 100, 看下接下来会发生什么情况。 2.5 重启数据库并查看12345678910[postgres@pgb ~]$ psql -h 127.0.0.1 mydb mydb psql (9.1.0) Type \"help\" for help.mydb=&gt; \\dt No relations found.mydb=&gt; select txid_current(); txid_current -------------- 100 (1 row) 备注：所有表消失。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"MVCC","slug":"MVCC","permalink":"https://postgres.fun/tags/MVCC/"}]},{"title":"PostgreSQL: Use Streaming Replication Migrating database ","slug":"20120430101049","date":"2012-04-30T02:10:49.000Z","updated":"2018-09-04T01:33:55.586Z","comments":true,"path":"20120430101049.html","link":"","permalink":"https://postgres.fun/20120430101049.html","excerpt":"","text":"最近有个项目需要跨 IDC 迁移，需将一个生产库迁移到另一个IDC，数据量不大，版本为 9.1.2，由于尽可能地减少停机时间，这次迁移采用的是 PostgreSQL 的流复制技术，即先搭建好备库，然后再主备切换，完成迁移，这种方法迁移，能将业务中断时间缩减在5分钟以内甚至更更快；流复制数据库迁移的步骤如下： 流复制搭建具体步骤可参考之前的 blog: PostgreSQL: Setting up streaming log replication (Hot Standby ) 主备切换 具体步骤可参考之前的两篇 blog: PostgreSQL HOT-Standby 的主备切换 主备切换异常一例 备注：上面只是主备切换的参考步骤，有些步骤可以不要，例如 VIP方面的设置，具体根据情况而定。 验证通知项目组开启应用，验证业务是否正常。","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"PostGIS 2.0.0 Released","slug":"20120427152307","date":"2012-04-27T07:23:07.000Z","updated":"2018-09-04T01:33:55.539Z","comments":true,"path":"20120427152307.html","link":"","permalink":"https://postgres.fun/20120427152307.html","excerpt":"","text":"PostGIS 2.0.0 Released 已经有段时间了，新版本 postgis 包括以下新特性，对 postgis 感兴趣的朋友可以下载测试下 The development process for 2.0 has been very long, but has resulted in a release with a number of exciting new features. PostGIS 2.0.0 新特性 Raster data and raster/vector analysis in the database Topological models to handle objects with shared boundaries PostgreSQL typmod integration, for an automagical geometry_columns table 3D and 4D indexing Index-based high performance nearest-neighbour searching Many more vector functions including ST_Split ST_Node ST_MakeValid ST_OffsetCurve ST_ConcaveHull ST_AsX3D ST_GeomFromGeoJSON ST_3DDistance Integration with the PostgreSQL 9.1 extension system Improved commandline shapefile loader/dumper Multi-file import support in the shapefile GUI Multi-table export support in the shapefile GUI A geo-coder optimized for free US Census TIGER (2010) data Download postgishttp://postgis.refractions.net/download/","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostGIS","slug":"PostGIS","permalink":"https://postgres.fun/tags/PostGIS/"}]},{"title":"PostgreSQL: 使用 pgcrypto 给敏感数据加密 ","slug":"20120426190509","date":"2012-04-26T11:05:09.000Z","updated":"2018-09-04T01:33:55.477Z","comments":true,"path":"20120426190509.html","link":"","permalink":"https://postgres.fun/20120426190509.html","excerpt":"","text":"今天有同事问我如何给数据库中的敏感数据加密问题，开始没相到什么方法，后来查了下手册，可以通过 PostgreSQL 的 pgcrypto 模块实现，下面是实验步骤： 环境准备创建外部模块 pgcrypto12mydb=# create extension pgcrypto ; CREATE EXTENSION 创建测试表12345678910111213141516mydb=&gt; create table test_user(id serial,username varchar(32),password text); NOTICE: CREATE TABLE will create implicit sequence \"test_user_id_seq\" for serial column \"test_user.id\" CREATE TABLEmydb=&gt; create unique index idx_test_user_username on test_user using btree (username); CREATE INDEXmydb=&gt; \\d test_user Table \"mydb.test_user\" Column | Type | Modifiers ----------+-----------------------+-------------------------------------------------------- id | integer | not null default nextval('test_user_id_seq'::regclass) username | character varying(32) | password | text | Indexes: \"idx_test_user_username\" UNIQUE, btree (username) 方法一: 使用 md5 加密插入测试用户信息1234mydb=&gt; insert into test_user(username,password) values ('user1',md5('123456')); INSERT 0 1 mydb=&gt; insert into test_user(username,password) values ('user2',md5('123456')); INSERT 0 1 查询用户信息，解密12345678910111213mydb=&gt; select * From test_user; id | username | password ----+----------+---------------------------------- 1 | user1 | e10adc3949ba59abbe56e057f20f883e 2 | user2 | e10adc3949ba59abbe56e057f20f883e (2 rows)mydb=&gt; select * From test_user where password=md5('123456'); id | username | password ----+----------+---------------------------------- 1 | user1 | e10adc3949ba59abbe56e057f20f883e 2 | user2 | e10adc3949ba59abbe56e057f20f883e (2 rows) 备注：使用 md5 加密后，如果两个用户的密码相同，那么 md5 加密后的密码也一样，如果破解了一个 md5 密码，那么很容易破解 md5 值相同的密码。 方法二: 使用 crypt() 加密使用 crypt() 函数增加两条用户信息1234mydb=&gt; insert into test_user(username,password) values ('user3',crypt('123456',gen_salt('md5'))); INSERT 0 1 mydb=&gt; insert into test_user(username,password) values ('user4',crypt('123456',gen_salt('md5'))); INSERT 0 1 查询新增用户信息12345mydb=&gt; select * From test_user where username in ('user3','user4'); id | username | password ----+----------+------------------------------------ 5 | user3 | $1$cS7Bs67A$5c2FTClGTOBYiHpG1HyvA/ 6 | user4 | $1$L6Rao5/l$7URcaCbT9Hrsrt9JcoBGq.(2 rows) 备注：虽然 user3，user4 使用相同的密码，但经过 crypt() 函数加密后，加密后的密码值不同，显然，这种加密方式要比 md5 安全。 查询，解密测试1234567891011mydb=&gt; select * From test_user where username ='user3' and password=crypt('123456',password); id | username | password ----+----------+------------------------------------ 5 | user3 | $1$cS7Bs67A$5c2FTClGTOBYiHpG1HyvA/ (1 row)mydb=&gt; select * From test_user where username ='user4' and password=crypt('123456',password); id | username | password ----+----------+------------------------------------ 6 | user4 | $1$L6Rao5/l$7URcaCbT9Hrsrt9JcoBGq. (1 row) 附: 函数解释 crypt()crypt(password text, salt text) returns text Calculates a crypt(3)-style hash of password. When storing a new password, you need to use gen_salt() to generate a new salt value. To check a password, pass the stored hash value as salt, and test whether the result matches the stored value. crypt() 函数支持的加密算法 gen_salt()gen_salt(type text [, iter_count integer ]) returns textGenerates a new random salt string for use in crypt(). The salt string also tells crypt() which algorithm to use.The type parameter specifies the hashing algorithm. The accepted types are: des, xdes, md5 and bf.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"The simple usage of Pgstattuple extension","slug":"20120426161452","date":"2012-04-26T08:14:52.000Z","updated":"2018-09-04T01:33:55.430Z","comments":true,"path":"20120426161452.html","link":"","permalink":"https://postgres.fun/20120426161452.html","excerpt":"","text":"pgstattuple 模块提供了统计信息函数，能精确查询表和索引的详细信息，包括dead tuples 信息，今天测试了 pgstattuple 的用法， 安装和使用比较简单，下面是实验过程。 安装 Pgstattuple12mydb=# create extension pgstattuple; CREATE EXTENSION 创建测试表创建测试表和插入测试数据，如下：123456789101112131415mydb=&gt; create table test_27 (id integer primary key ,name varchar(32)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_27_pkey\" for table \"test_27\" CREATE TABLEmydb=&gt; \\d test_27 Table \"mydb.test_27\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | not null name | character varying(32) | Indexes: \"test_27_pkey\" PRIMARY KEY, btree (id) mydb=&gt; insert into test_27 select generate_series(1,100000),'a'; INSERT 0 100000 查询表信息查询表信息，如下：1234567891011121314mydb=# \\x Expanded display is on.mydb=# select * from pgstattuple('mydb.test_27'); -[ RECORD 1 ]------+-------- table_len | 3629056 tuple_count | 100000 tuple_len | 3000000 tuple_percent | 82.67 dead_tuple_count | 0 dead_tuple_len | 0 dead_tuple_percent | 0 free_space | 16652 free_percent | 0.46 备注：上面显示了表的长度 table_len ，表的记录数 tuple_count，和 dead_tuple 等信息。123456789101112mydb=# \\dt+ mydb.test_27 List of relations Schema | Name | Type | Owner | Size | Description --------+---------+-------+-------+---------+------------- mydb | test_27 | table | mydb | 3568 kB | (1 row)mydb=# select 3629056/1024 kB; kb ------ 3544 (1 row) 查询索引信息123456789101112mydb=# select * From pgstatindex('mydb.test_27_pkey'); -[ RECORD 1 ]------+-------- version | 2 tree_level | 1 index_size | 1802240 root_block_no | 3 internal_pages | 0 leaf_pages | 219 empty_pages | 0 deleted_pages | 0 avg_leaf_density | 89.87 leaf_fragmentation | 0 删除数据12mydb=# delete from mydb.test_27 where id &lt; 50001; DELETE 49000 再次查询表信息1234567891011mydb=# select * from pgstattuple('mydb.test_27'); -[ RECORD 1 ]------+-------- table_len | 3629056 tuple_count | 50000 tuple_len | 1500000 tuple_percent | 41.33 dead_tuple_count | 49000 dead_tuple_len | 1470000 dead_tuple_percent | 40.51 free_space | 48652 free_percent | 1.34 备注：字段 tuple_count，tuple_len，dead_tuple_count 值发生了变化。 查询索引信息123456789101112mydb=# select * From pgstatindex('mydb.test_27_pkey'); -[ RECORD 1 ]------+-------- version | 2 tree_level | 1 index_size | 1802240 root_block_no | 3 internal_pages | 0 leaf_pages | 219 empty_pages | 0 deleted_pages | 0 avg_leaf_density | 89.87 leaf_fragmentation | 0 备注：字段详细信息可参考后面的附件，其中 leaf_fragmentation 字段可作为索引膨胀的依据。 查询表 page 信息 123456789101112131415161718mydb=# select * from pg_relpages('mydb.test_27'); -[ RECORD 1 ]---- pg_relpages | 443 mydb=# select relname,relpages from pg_class where relname='test_27'; relname | relpages ---------+---------- test_27 | 0 (1 row)mydb=# analyze mydb.test_27; ANALYZEmydb=# select relname,relpages from pg_class where relname='test_27'; relname | relpages ---------+---------- test_27 | 443 (1 row) 备注：表未分析前，使用 pg_relpages 函数就能精确查询表的 page 数据，而此时 pg_class 还没数据说明 pg_relpages 查询了表的 page 物理文件信息。 Vacuum 表1234567891011121314151617181920212223242526mydb=# select * From pgstattuple('mydb.test_27'); -[ RECORD 1 ]------+-------- table_len | 3629056 tuple_count | 50000 tuple_len | 1500000 tuple_percent | 41.33 dead_tuple_count | 49000 dead_tuple_len | 1470000 dead_tuple_percent | 40.51 free_space | 48652 free_percent | 1.34mydb=# vacuum mydb.test_27; VACUUM mydb=# select * From pgstattuple('mydb.test_27'); -[ RECORD 1 ]------+-------- table_len | 3629056 tuple_count | 50000 tuple_len | 1500000 tuple_percent | 41.33 dead_tuple_count | 0 dead_tuple_len | 0 dead_tuple_percent | 0 free_space | 1616652 free_percent | 44.55 备注：vacuum 后， dead_tuple 相关信息改变。 附一: Pgstattuple 函数信息 附二: Pgstatindex 函数信息 总结 pgstattuple 能精确查询表和索引的page 信息，包括表的 free_space ，dead_tuple_count 信息，和索引的deleted_pages ，leaf_fragmentation 信息，在SQL优化方面，这个模块比较有效，找出膨胀较大的索引和表。 由于 pgstattuple 模块会物理查询表和索引的 page 信息，如果是大表，这个步骤花费时间较长，今天只是在虚拟机上测试，这方面没有准确测试。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"RHEL6: Samba 服务配置 ","slug":"20120408201126","date":"2012-04-08T12:11:25.000Z","updated":"2018-09-04T01:33:55.383Z","comments":true,"path":"20120408201126.html","link":"","permalink":"https://postgres.fun/20120408201126.html","excerpt":"","text":"今天学习了 Samba 服务配置， Samba 服务可以用来实现 windows 和 Linux 实现目录共享，本文简单介绍下 RHEL6 环境下配置 Samba 服务的过程， 安装 Samba 服务1.1 Samba 需要安装的包 samba samba-common samba-client 1.2 查看已安装的 samba 相关包1234[root@redhat6 samba]# rpm -qa | grep sambasamba-common-3.5.10-114.el6.i686samba-winbind-clients-3.5.10-114.el6.i686samba-client-3.5.10-114.el6.i686 备注：系统上还少了 samba 包，接下来需要安装这个包。 1.3 安装 samba 包12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@redhat6 samba]# yum search sambaLoaded plugins: product-id, refresh-packagekit, security, subscription-managerUpdating certificate-based repositories.Repository rhel-source is listed more than once in the configuration======================================================== N/S Matched: samba ========================================================samba-client.i686 : Samba client programssamba-common.i686 : Files used by both Samba servers and clientssamba-winbind.i686 : Samba winbindsamba-winbind-clients.i686 : Samba winbind clientssblim-cmpi-samba.i686 : SBLIM WBEM-SMT Sambactdb.i686 : A Clustered Database based on Samba's Trivial Database (TDB)samba.i686 : Server and Client software to interoperate with Windows machines Name and summary matches only, use \"search all\" for everything.[root@redhat6 samba]# yum install samba.i686Loaded plugins: product-id, refresh-packagekit, security, subscription-managerUpdating certificate-based repositories.Repository rhel-source is listed more than once in the configurationSetting up Install ProcessResolving Dependencies--&gt; Running transaction check---&gt; Package samba.i686 0:3.5.10-114.el6 will be installed--&gt; Finished Dependency ResolutionDependencies Resolved==================================================================================================================================== Package Arch Version Repository Size====================================================================================================================================Installing: samba i686 3.5.10-114.el6 rhel-source 5.0 MTransaction Summary====================================================================================================================================Install 1 Package(s)Total download size: 5.0 MInstalled size: 17 MIs this ok [y/N]: yDownloading Packages:Running rpm_check_debugRunning Transaction TestTransaction Test SucceededRunning TransactionWarning: RPMDB altered outside of yum. Installing : samba-3.5.10-114.el6.i686 1/1Installed products updated'.Installed: samba.i686 0:3.5.10-114.el6Complete! 1.4 Samba的端口Nmbd: 137 138smbd: tcp 139 445备注：修改防火墙配置，打开 tcp 139，445 端口。 以匿名方式配置 Samba2.1 配置 samba 配置文件修改文件 /etc/samba/smb.conf， 修改 security 参数，部分内容如下12345678910[global] security = share passdb backend = tdbsam#============================ Share Definitions ==============================[francs_share]comment = My sharepath = /francs_sharepublic = yeswriteable = yesbrowseable = yes 备注：当 security 配置成 share 时，访问共享目录不需要输入密码。 2.2 共享目录相关参数说明 comment: 共享目录备注 path: 设置共享目录 public: 设置共享目录是否支持匿名访问 browseable: 设置用户是否可以在浏览器中看到目录 writeable: 设置用户是否可以在共享目录写入数据 valid user: 设置哪些用户可以访问共享目录 create mode: 设置用户在共享目录里创建文件的默认权限 directory mode: 设置用户在共享目录里创建文件夹的默认权限 2.3 创建目录12[root@redhat6 /]# mkdir -p /francs_share[root@redhat6 /]# chmod -R 777 /francs_share 2.4 查看 seliniux 运行模式12[root@redhat6 /]# getenforceEnforcing 2.5 测试在运行中输入 ”\\192.168.1.35\\” 可以看到共享目录，如图所示备注： 但是不能进入共享目录，双击目录时，报以下ERROR，原因是，RHEL6的selinux 挡住了，解决方法有两种： 1 禁用 selinux 2 配置 selinux2.6 方法一：禁用 selinux12345[root@redhat6 samba]# getenforceEnforcing[root@redhat6 samba]# setenforce 0[root@redhat6 samba]# getenforcePermissive 2.7 方法二：配置 selinux配置文件属性 ，给共享目录加上标签1[root@redhat6 francs_share]# semanage fcontext -a -t samba_share_t \"/francs_share(/.*)?\" 备注： smbd 服务可以对拥有具有 samba_share_1 属性的文件进行读写。 使标签生效12[root@redhat6 francs_share]# restorecon -R -v /francs_share/restorecon reset /francs_share context unconfined_u:object_r:default_t:s0-&gt;unconfined_u:object_r:samba_share_t:s0 重启 smb 服务123[root@redhat6 samba]# service smb restartShutting down SMB services: [ OK ]Starting SMB services: [ OK ] 2.8 再次访问测试配置好 selinux 后，可以进入共享目录 /francs_share ，并查看其中的子文件了。 以输入用户名/密码方式配置 samba 服务3.1 配置 samba 配置文件修改文件 /etc/samba/smb.conf 的 security 参数，部分内容如下12345678910[global] security = user passdb backend = tdbsam#============================ Share Definitions ==============================[francs_share]comment = My sharepath = /francs_sharepublic = yeswriteable = yesbrowseable = yes 备注：修改参数 security = user 参数。 3.2 增加用户 user2123456[root@redhat6 ~]# useradd user2[root@redhat6 ~]# passwd user2Changing password for user user2.New password:Retype new password:passwd: all authentication tokens updated successfully. 3.3 设置 samba 用户 user2 密码12345[root@redhat6 ~]# smbpasswd -a user2New SMB password:Retype new SMB password:Added user user2.[root@redhat6 ~]# 备注：这个密码是在 windows 平台上访问 Linux 共享目录的密码。 3.4 查看共享目录信息12345678910111213[root@redhat6 ~]# smbclient -U user2 -L localhostEnter user2's password:Domain=[MYGROUP] OS=[Unix] Server=[Samba 3.5.10-114.el6] Sharename Type Comment --------- ---- ------- francs_share Disk My share IPC$ IPC IPC Service (Samba Server Version 3.5.10-114.el6) user2 Disk Home DirectoriesDomain=[MYGROUP] OS=[Unix] Server=[Samba 3.5.10-114.el6] Server Comment --------- ------- Workgroup Master --------- ------- 3.5 重启 smb 服务123[root@redhat6 samba]# service smb restartShutting down SMB services: [ OK ]Starting SMB services: [ OK ] 3.6 连接测试访问成功，如图所示备注：在上图输入用户名和密码后，就可以访问目录 /francs_share,，并可在此目录进行创建，删除文件。 常见问题学习samba的时候，尝试用windows XP 连接 Linux 当一个用户登录成功后，想再试试其他用户能否成功登录就会提示不允许一个用户使用一个以上用户名与一个服务器或共享资源的多重连接，这时需要断掉当前用户重新登录，步骤如下。 Windows 机器执行 net 命令12345C:\\Documents and Settings\\ThinkPad&gt;net use * /del /y您有以下的远程连接: \\\\192.168.1.35\\myshare继续运行会取消连接。命令成功完成。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"LVM 管理之四： 缩减 VG 大小","slug":"20120331154340","date":"2012-03-31T07:43:40.000Z","updated":"2018-09-04T01:33:55.320Z","comments":true,"path":"20120331154340.html","link":"","permalink":"https://postgres.fun/20120331154340.html","excerpt":"","text":"今天学习了 pvmove 命令，pvmove 命令用来将一块盘上的数据移到另一块盘，可以移动整块盘的数据，也可以移动指定 extent 范围的数据， 接下来演示 pvmove 的一个例子，下面这个例子的 VG 由三个 PV 组成，我们想将其中一个 PV 的数据移动其它 PV中，空闲的PV 可以用来创建新的 VG。 1 显示 pv 使用信息12345[root@redhat6 ~]# pvs -o+pv_used PV VG Fmt Attr PSize PFree Used /dev/sda2 vg_redhat6 lvm2 a-- 14.51g 0 14.51g /dev/sdb vg_redhat6 lvm2 a-- 4.00g 2.62g 1.38g /dev/sdc vg_redhat6 lvm2 a-- 6.00g 5.60g 404.00m 备注：从上面看出，有三块 pv 。 2 移动 /dev/sdb 数据到 /dev/sdc 盘上1234567[root@redhat6 ~]# pvmove /dev/sdb /dev/sdc /dev/sdb: Moved: 0.8% /dev/sdb: Moved: 28.3% /dev/sdb: Moved: 42.5% /dev/sdb: Moved: 57.2% /dev/sdb: Moved: 72.8% /dev/sdb: Moved: 92.6% /dev/sdb: Moved: 100.0% 备注：上述命令将 /dev/sdb 文件移动 /dev/sdc 上。 3 再次查看 pv 信息12345[root@redhat6 ~]# pvs -o+pv_used PV VG Fmt Attr PSize PFree Used /dev/sda2 vg_redhat6 lvm2 a-- 14.51g 0 14.51g /dev/sdb vg_redhat6 lvm2 a-- 4.00g 4.00g 0 /dev/sdc vg_redhat6 lvm2 a-- 6.00g 4.22g 1.77g 备注：从上面看出 /dev/sdb 盘使用空间为 0，说明空间被腾出。 4 将 /dev/sdb 从 VG vg_redhat6 中移出1234567[root@redhat6 ~]# vgreduce /dev/vg_redhat6 /dev/sdb Removed \"/dev/sdb\" from volume group \"vg_redhat6\"[root@redhat6 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sda2 vg_redhat6 lvm2 a-- 14.51g 0 /dev/sdb lvm2 a-- 4.00g 4.00g /dev/sdc vg_redhat6 lvm2 a-- 6.00g 4.22g 备注： 根据pvs 信息， /dev/sdb 已经不是 VG vg_redhat6 的成员了。 那么 /dev/sdb 盘可以移除，或者它用。 5 在 /dev/sdb 上创建新的 VG123456[root@redhat6 ~]# vgcreate vg_pgroot /dev/sdb Volume group \"vg_pgroot\" successfully created [root@redhat6 ~]# vgs VG #PV #LV #SN Attr VSize VFree vg_pgroot 1 0 0 wz--n- 4.00g 4.00g vg_redhat6 2 4 0 wz--n- 20.50g 4.22g 6 再次查看 PV /dev/sdb 信息 [root@redhat6 ~]# pvs PV VG Fmt Attr PSize PFree /dev/sda2 vg_redhat6 lvm2 a-- 14.51g 0 /dev/sdb vg_pgroot lvm2 a-- 4.00g 4.00g /dev/sdc vg_redhat6 lvm2 a-- 6.00g 4.22g 备注：/dev/sdb 已经是 VG vg_pgroot 的成员了。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"LVM","slug":"LVM","permalink":"https://postgres.fun/tags/LVM/"}]},{"title":"LVM 管理之三：格式化输出","slug":"20120330174433","date":"2012-03-30T09:44:33.000Z","updated":"2018-09-04T01:33:55.258Z","comments":true,"path":"20120330174433.html","link":"","permalink":"https://postgres.fun/20120330174433.html","excerpt":"","text":"今天学习了下 LVM 输出格式化的内容，显示LVM信息常用 pvs,lvs,vgs 命令，如果想显示详细信息可以使用 pvdisplay ,vgdisplay ,lvdisplay，其中前面命令仅用于简单输出 lvm 对象相关信息，可以格式化输出；第二种命令会详细显示 lvm 对像信息，但不易于格式化输出。 PVS、LVS、VGS 使用1234567891011121314151617显示物理卷(pv)信息 [root@redhat6 lvm]# pvs PV VG Fmt Attr PSize PFree /dev/sda2 vg_redhat6 lvm2 a-- 14.51g 0 /dev/sdb vg_redhat6 lvm2 a-- 4.00g 2.81g显示逻辑卷组(vg)信息[root@redhat6 lvm]# vgs VG #PV #LV #SN Attr VSize VFree vg_redhat6 2 3 0 wz--n- 18.50g 2.81g显示逻辑卷(lv)信息[root@redhat6 lvm]# lvs LV VG Attr LSize Origin Snap% Move Log Copy% Convert lv_pgdata_01 vg_redhat6 -wi-ao 700.00m lv_root vg_redhat6 -wi-ao 13.10g lv_swap vg_redhat6 -wi-ao 1.91g 备注：前面的 pvs,lvs,vgs命令简单显示 LVM对像信息，都是默认的输出。 格式化控制2.1 显示额外信息1234567891011[root@redhat6 lvm]# pvs -v Scanning for physical volume names PV VG Fmt Attr PSize PFree DevSize PV UUID /dev/sda2 vg_redhat6 lvm2 a-- 14.51g 0 14.51g dpTW84-DHRy-PIA9-u1tY-YUH4-XQKw-8Hy2Zm /dev/sdb vg_redhat6 lvm2 a-- 4.00g 2.81g 4.00g YKIQwX-zc6H-TZ65-P3qr-sRfk-LX0p-T18vdG [root@redhat6 lvm]# vgs -v Finding all volume groups Finding volume group \"vg_redhat6\" VG Attr Ext #PV #LV #SN VSize VFree VG UUID vg_redhat6 wz--n- 4.00m 2 3 0 18.50g 2.81g E6cA2U-TL1x-ScCV-UnGU-3Kq4-1u6V-WUb5L4 备注：增加 -v 参数，会显示LVM 对像的额外信息，例如 UUID字段。 2.2 pvs 默认输出1234[root@redhat6 lvm]# pvs PV VG Fmt Attr PSize PFree /dev/sda2 vg_redhat6 lvm2 a-- 14.51g 0 /dev/sdb vg_redhat6 lvm2 a-- 4.00g 2.81g 2.3 只显示 pv_name,dev_size 字段1234[root@redhat6 lvm]# pvs -o pv_name,dev_size PV DevSize /dev/sda2 14.51g /dev/sdb 4.00g 备注：增加 -o 参数，指定需要输出的字段。 2.4 在pvs 默认输出上，增加字段12345678[root@redhat6 lvm]# pvs PV VG Fmt Attr PSize PFree /dev/sda2 vg_redhat6 lvm2 a-- 14.51g 0 /dev/sdb vg_redhat6 lvm2 a-- 4.00g 2.81g[root@redhat6 lvm]# pvs -o +pv_uuid PV VG Fmt Attr PSize PFree PV UUID /dev/sda2 vg_redhat6 lvm2 a-- 14.51g 0 dpTW84-DHRy-PIA9-u1tY-YUH4-XQKw-8Hy2Zm /dev/sdb vg_redhat6 lvm2 a-- 4.00g 2.81g YKIQwX-zc6H-TZ65-P3qr-sRfk-LX0p-T18vdG 备注：如果想在默认输出的前提下增加字段，可以使用 -o + 字段名。 2.5 vgs 显示卷组成员123456789[root@redhat6 lvm]# vgs VG #PV #LV #SN Attr VSize VFree vg_redhat6 2 3 0 wz--n- 18.50g 2.81g [root@redhat6 lvm]# vgs -o +lv_name VG #PV #LV #SN Attr VSize VFree LV vg_redhat6 2 3 0 wz--n- 18.50g 2.81g lv_root vg_redhat6 2 3 0 wz--n- 18.50g 2.81g lv_swap vg_redhat6 2 3 0 wz--n- 18.50g 2.81g lv_pgdata_01 字段排序3.1 根据 lv_size 字段排序1234567891011[root@redhat6 lvm]# lvs LV VG Attr LSize Origin Snap% Move Log Copy% Convert lv_pgdata_01 vg_redhat6 -wi-ao 700.00m lv_root vg_redhat6 -wi-ao 13.10g lv_swap vg_redhat6 -wi-ao 1.91g[root@redhat6 lvm]# lvs -O lv_size LV VG Attr LSize Origin Snap% Move Log Copy% Convert lv_pgdata_01 vg_redhat6 -wi-ao 700.00m lv_swap vg_redhat6 -wi-ao 1.91g lv_root vg_redhat6 -wi-ao 13.10g 备注：如果想根据输出字段排序，可以使用 -O 字段名 选项。 附 LVS、VGS、PVS 输出4.1 lvs 字段12345678910111213141516171819202122lv_all - All fields in this section. lv_uuid - Unique identifier. lv_name - Name. LVs created for internal use are enclosed in brackets. lv_path - Full pathname for LV. lv_attr - Various attributes - see man page. lv_major - Persistent major number or -1 if not persistent. lv_minor - Persistent minor number or -1 if not persistent. lv_read_ahead - Read ahead setting in current units. lv_kernel_major - Currently assigned major number or -1 if LV is not active. lv_kernel_minor - Currently assigned minor number or -1 if LV is not active. lv_kernel_read_ahead - Currently-in-use read ahead setting in current units. lv_size - Size of LV in current units. seg_count - Number of segments in LV. origin - For snapshots, the origin device of this LV. origin_size - For snapshots, the size of the origin device of this LV. snap_percent - For snapshots, the percentage full if LV is active. copy_percent - For mirrors and pvmove, current percentage in-sync. move_pv - For pvmove, Source PV of temporary LV created by pvmove. convert_lv - For lvconvert, Name of temporary LV created by lvconvert. lv_tags - Tags, if any. mirror_log - For mirrors, the LV holding the synchronisation log. modules - Kernel device-mapper modules required for this LV. 4.2 pvs 字段1234567891011pv_all - All fields in this section. pe_start - Offset to the start of data on the underlying device. pv_size - Size of PV in current units. pv_free - Total amount of unallocated space in current units. pv_used - Total amount of allocated space in current units. pv_attr - Various attributes - see man page. pv_pe_count - Total number of Physical Extents. pv_pe_alloc_count - Total number of allocated Physical Extents. pv_tags - Tags, if any. pv_mda_count - Number of metadata areas on this device. pv_mda_used_count - Number of metadata areas in use on this device. 4.3 vgs 字段1234567891011121314151617181920212223vg_all - All fields in this section. vg_fmt - Type of metadata. vg_uuid - Unique identifier. vg_name - Name. vg_attr - Various attributes - see man page. vg_size - Total size of VG in current units. vg_free - Total amount of free space in current units. vg_sysid - System ID indicating when and where it was created. vg_extent_size - Size of Physical Extents in current units. vg_extent_count - Total number of Physical Extents. vg_free_count - Total number of unallocated Physical Extents. max_lv - Maximum number of LVs allowed in VG or 0 if unlimited. max_pv - Maximum number of PVs allowed in VG or 0 if unlimited. pv_count - Number of PVs. lv_count - Number of LVs. snap_count - Number of snapshots. vg_seqno - Revision number of internal metadata. Incremented whenever it changes. vg_tags - Tags, if any. vg_mda_count - Number of metadata areas on this VG. vg_mda_used_count - Number of metadata areas in use on this VG. vg_mda_free - Free metadata area space for this VG in current units. vg_mda_size - Size of smallest metadata area for this VG in current units. vg_mda_copies - Target number of in use metadata areas in the VG.","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"LVM","slug":"LVM","permalink":"https://postgres.fun/tags/LVM/"}]},{"title":"LVM 管理之二：缩减 LV 大小","slug":"20120329212500","date":"2012-03-29T13:25:00.000Z","updated":"2018-09-04T01:33:55.211Z","comments":true,"path":"20120329212500.html","link":"","permalink":"https://postgres.fun/20120329212500.html","excerpt":"","text":"今天学习了 RHEL6 缩减 LV 大小，下面的例子目标是将LV “/dev/mapper/vg_redhat6-lv_pgdata_01” 缩小 86 M。1 当前LVM 信息1234567/dev/mapper/vg_redhat6-lv_root 13G 7.7G 4.6G 63% / tmpfs 250M 264K 250M 1% /dev/shm /dev/sda1 485M 31M 429M 7% /boot /dev/sr0 2.9G 2.9G 0 100% /media/RHEL_6.2 i386 Disc 1 /dev/mapper/vg_redhat6-lv_pgdata_01 786M 593M 154M 80% /database/skytf/pgdata1 2 卸载文件系统 ( unmount )12345[root@redhat6 ~]# umount /dev/mapper/vg_redhat6-lv_pgdata_013 检查文件系统( e2fsck )[root@redhat6 ~]# e2fsck /dev/mapper/vg_redhat6-lv_pgdata_01 e2fsck 1.41.12 (17-May-2010) /dev/mapper/vg_redhat6-lv_pgdata_01: clean, 236/49152 files, 154817/179200 blocks 3 缩小文件系统 ( resize2fs )1234[root@redhat6 ~]# resize2fs -f /dev/mapper/vg_redhat6-lv_pgdata_01 700M resize2fs 1.41.12 (17-May-2010) Resizing the filesystem on /dev/mapper/vg_redhat6-lv_pgdata_01 to 179200 (4k) blocks. The filesystem on /dev/mapper/vg_redhat6-lv_pgdata_01 is now 179200 blocks long. 4 缩小LV大小（ lvreduce ）123456[root@redhat6 ~]# lvreduce -L 700M /dev/mapper/vg_redhat6-lv_pgdata_01 WARNING: Reducing active logical volume to 700.00 MiB THIS MAY DESTROY YOUR DATA (filesystem etc.) Do you really want to reduce lv_pgdata_01? [y/n]: y Reducing logical volume lv_pgdata_01 to 700.00 MiB Logical volume lv_pgdata_01 successfully resized 5 查看当前 LV 大小12345[root@redhat6 ~]# lvs LV VG Attr LSize Origin Snap% Move Log Copy% Convert lv_pgdata_01 vg_redhat6 -wi-a- 700.00m lv_root vg_redhat6 -wi-ao 13.10g lv_swap vg_redhat6 -wi-ao 1.91g 备注： 逻辑卷 lv_pgdata_01 已经缩减成 700M了。 6 重新挂载文件系统，查看12345678910[root@redhat6 ~]# mount -t ext4 /dev/mapper/vg_redhat6-lv_pgdata_01 /database/skytf/pgdata1 [root@redhat6 ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/mapper/vg_redhat6-lv_root 13G 7.7G 4.6G 63% / tmpfs 250M 264K 250M 1% /dev/shm /dev/sda1 485M 31M 429M 7% /boot /dev/sr0 2.9G 2.9G 0 100% /media/RHEL_6.2 i386 Disc 1 /dev/mapper/vg_redhat6-lv_pgdata_01 688M 593M 61M 91% /database/skytf/pgdata1 备注：目录 /database/skytf/pgdata1 已成功缩减到 700M 左右。 7 总结 在缩减LV 大小前，首先得先缩减文件系统大小。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"LVM","slug":"LVM","permalink":"https://postgres.fun/tags/LVM/"}]},{"title":"SCP: WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! ","slug":"20120326140835","date":"2012-03-26T06:08:35.000Z","updated":"2018-09-04T01:33:55.148Z","comments":true,"path":"20120326140835.html","link":"","permalink":"https://postgres.fun/20120326140835.html","excerpt":"","text":"今天通过 scp 拷文件时出现以下ERROR， 根据提示信息可能是出于安全方面原因，密钥发生改变。 报错信息连接丢失，报错信息如下：123456789101112131415[postgres@skytf](mailto:postgres@skytf)-&gt; scp *.ddl [postgres@192.168.1.35:/pgdata/uims_ppa/1921/data05/pg_bakcup](mailto:postgres@192.168.1.35:/pgdata/uims_ppa/1921/data05/pg_bakcup) @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY! Someone could be eavesdropping on you right now (man-in-the-middle attack)! It is also possible that the RSA host key has just been changed. The fingerprint for the RSA key sent by the remote host is d3:94:17:c4:69:6f:28:5e:8e:a9:88:0d:88:f4:5e:df. Please contact your system administrator. Add correct host key in /home/postgres/.ssh/known_hosts to get rid of this message. Offending key in /home/postgres/.ssh/known_hosts:2 RSA host key for 192.168.1.35 has changed and you have requested strict checking. Host key verification failed. lost connection 解决方法解决方法比较简单，只要删除ssh client 端文件 ~/.ssh/known_hosts 中对应记录信息即可。12345678910[postgres@skytf](mailto:postgres@skytf)-&gt; cd ~/.ssh [postgres@skytf](mailto:postgres@skytf)-&gt; pwd /home/postgres/.ssh [postgres@skytf](mailto:postgres@skytf)-&gt; ll total 20K -r-------- 1 postgres postgres 668 Jul 4 2011 id_dsa -r-------- 1 postgres postgres 642 Jul 4 2011 id_dsa.pub -r-------- 1 postgres postgres 1.7K Jul 4 2011 id_rsa -r-------- 1 postgres postgres 434 Jul 4 2011 id_rsa.pub -rw------- 1 postgres postgres 1.6K Mar 26 13:44 known_hosts 修改文件 known_hosts，删除对应IP记录信息即可。12345192.168.1.35 ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA2Xhr9XQXTLXp+61nQRelAsms1SDbZH31QX u+wK/fUNCGVgxXoqwIq/tf8Cg+MgJlVRvLb3HHGK5nTGFnda/b49ITVi/TCOwEurzTIjMRm+RpDOzMWEvZN SeNIOixcExi52D9Q3U1zebMG1U5ecxmzkvZMnwdOQpFvf9d1bGPbRY3+cn2i9yZr/ccJqTms+QMImV0VK35 QOFIxHuDTLltkowewd8AuIwCytT+k7UqP7Pe+NYzWK5bReKJuvrQheGjmEpVD3uXoqyMRfl/SuDnUXFWIyP yfFO9NhHFT9NwM0GpiSyhCrS/oft5Ou+nmwpazYWYg0NAcMMjP1XjLOt38w== 备注：删除这条信息，重新 COPY 文件后不再报上述信息。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"PgBouncer: 如何获取客户端连接 IP ","slug":"20120320112403","date":"2012-03-20T03:24:03.000Z","updated":"2018-09-04T01:33:55.086Z","comments":true,"path":"20120320112403.html","link":"","permalink":"https://postgres.fun/20120320112403.html","excerpt":"","text":"由于 PostgreSQL 采用进程模式，大量连接对数据库性能会产生较大影响，在大量连接情况下，一般会在数据库和应用程序之间配置 pgbouncer，pgbouncer 可以配置在数据库主机上，也可以配置在单独一台服务器上，但是采用了 pgbouncer 后，在维护方面和以前有些区别： 环境信息PostgreSQL 版本： PostgreSQL 9.1.2pgbouncer 版本： pgbouncer version 1.4.2PostgreSQL 原库IP 192.168.1.12pgbouncer IP 192.168.1.10 区别一: pg_hba.conf 功能减弱采用了 pgbouncer 之后 pg_hba.conf 功能减弱，这里说减弱，是因为原库上 pg_hba.conf 可以配置，但已经形同虚设，不能发挥原有的作用，因为可以配置原库上的 pg_hba.conf ，配置成只允许 pgbouncer，原库 pg_hba.conf 配置如下。12345# TYPE DATABASE USER ADDRESS METHOD # \"local\" is for Unix domain socket connections only local all all trust#Pgbouncer IP host all all 192.168.1.10/32 md5 尽管在原库上配置了 pg_hba.conf ，只允许 pgbouncer主机连库， 但是不能限制其它IP通过 pg_bouncer 连接数据库，从这个角度来说，采用 pgbouncer 无疑是增加了安全漏洞，当然可以采用其它措施弥补。 区别二: 主库上无法查询应用服务器 IP在数据库维护过程中，经常需要查询某个库的业务服务器IP信息，查询哪些服务器在连接数据库，采用了 pgbouncer 后，这方面的查询没以前方便，例如在主库上查询连接 skytf 库的客户端 IP 信息，如下所示：1234567891011121314151617postgres=# select datname,usename ,client_addr,client_port from pg_stat_activity where datname='skytf'; datname | usename | client_addr | client_port -----------+-----------+-----------------+------------- skytf | skytf | 192.168.1.10 | 32119 skytf | skytf | 192.168.1.10 | 18583 skytf | skytf | 192.168.1.10 | 31723 skytf | skytf | 192.168.1.10 | 32363 skytf | skytf | 192.168.1.10 | 58929 skytf | skytf | 192.168.1.10 | 58930 skytf | skytf | 192.168.1.10 | 58931 skytf | skytf | 192.168.1.10 | 58932 skytf | skytf | 192.168.1.10 | 58933 skytf | skytf | 192.168.1.10 | 58934 skytf | skytf | 192.168.1.10 | 58935 skytf | skytf | 192.168.1.10 | 58936 skytf | skytf | 192.168.1.10 | 58937 skytf | skytf | 192.168.1.10 | 58938 从上面查询结果来看，上面的 clietn_addr 只显示 pgbouncer 的 IP，而无法显示真正的客户端 IP，那么如何获取客户端的IP呢？ 有两种方法： 方法一: 登陆 pgbouncer 控制台获取客户端IP信息登陆 pgbouncer 控制台，并且执行 show clients 命令。12345678910111213postgres@db&gt; psql -h 127.0.0.1 -p 1922 pgbouncer pgbouncer_admin psql (9.1.2, server 1.4.2/bouncer) WARNING: psql version 9.1, server version 1.4. Some psql features might not work. Type \"help\" for help.pgbouncer=# show clients; type | user | database | state | addr | port | local_addr | local_port | connect_tim e | request_time | ptr | link ------+-----------------+------------+--------+-----------------------+-------+------------------------+------------+--------------- ------+---------------------+------------+------------ C | skytf | skytf | active | ::ffff:192.168.1.15 | 57791 | ::ffff:192.168.173.215 | 1922 | 2012-03-20 06: 45:28 | 2012-03-20 10:36:31 | 0x1350c9d0 | 0x1352e330 备注：为了格式输出，上面只列出一条记录，例如上面可以看出连接 skytf 库的业务服务器IP为 192.168.1.15，显然通过 pgbouncer 查询客户端 IP 不太方便。 方法二: 通过 pgbouncer 日志查看另外一种方法可以通过 pgbouncer 日志查看， 如果 pgbouncer 日志功能没开启，那么需要在配置文件里配置 logfile 参数，这里不详细解释了。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PgBouncer","slug":"PgBouncer","permalink":"https://postgres.fun/tags/PgBouncer/"}]},{"title":"RHEL6 学习：配置 YUM 仓库","slug":"20120316164408","date":"2012-03-16T08:44:08.000Z","updated":"2018-09-04T01:33:55.023Z","comments":true,"path":"20120316164408.html","link":"","permalink":"https://postgres.fun/20120316164408.html","excerpt":"","text":"今天学习了 RHEL6 yum 源配置，RHEL6 的配置过程和 RHEL5 大体相同，以下是实验过程。 配置本地 YUM 源1.1 实验环境RHEL6：虚拟机 1.2 挂载光驱12[root@redhat6 yum.repos.d]# mount /dev/cdrom /mnt mount: block device /dev/sr0 is write-protected, mounting read-only 备注：将光盘 ISO 文件挂载到目录 /mnt 1.3 创建本地目录，用来 copy 光盘包1[root@redhat6 local_repo]# mkdir -p /opt/rpm/local_repo 1.4 copy RPM 包到指定目录12[root@redhat6 log]# cd /mnt/Packages/ [root@redhat6 Packages]# cp * /opt/rpm/local_repo 1.5 安装 createrepo 包123456[root@redhat6 local_repo]# rpm -ivh createrepo-0.9.8-4.el6.noarch.rpm warning: createrepo-0.9.8-4.el6.noarch.rpm: Header V3 RSA/SHA256 Signature, key ID fd431d51: NOKEY error: Failed dependencies: deltarpm is needed by createrepo-0.9.8-4.el6.noarch python-deltarpm is needed by createrepo-0.9.8-4.el6.noarch You have new mail in /var/spool/mail/root 1.5.1根据提示，安装需要的包1234567891011121314[root@redhat6 local_repo]# rpm -ivh deltarpm-3.5-0.5.20090913git.el6.i686.rpm warning: deltarpm-3.5-0.5.20090913git.el6.i686.rpm: Header V3 RSA/SHA256 Signature, key ID fd431d51: NOKEY Preparing... ########################################### [100%] 1:deltarpm ########################################### [100%] [root@redhat6 local_repo]# [root@redhat6 local_repo]# [root@redhat6 local_repo]# rpm -ivh python-deltarpm-3.5-0.5.20090913git.el6.i686.rpm warning: python-deltarpm-3.5-0.5.20090913git.el6.i686.rpm: Header V3 RSA/SHA256 Signature, key ID fd431d51: NOKEY Preparing... ########################################### [100%] 1:python-deltarpm ########################################### [100%] [root@redhat6 local_repo]# rpm -ivh createrepo-0.9.8-4.el6.noarch.rpm warning: createrepo-0.9.8-4.el6.noarch.rpm: Header V3 RSA/SHA256 Signature, key ID fd431d51: NOKEY Preparing... ########################################### [100%] 1:createrepo ########################################### [100%] 备注：createrepo 包安装成功 1.6 创建 repo 数据库123456789[root@redhat6 local_repo]# createrepo -d /opt/rpm/local_repo 1944/2804 - MAKEDEV-3.24-6.el6.i686.rpm iso-8859-1 encoding on Ville [Skytt?ville.skytta@iki.fi](mailto:Skytt?ville.skytta@iki.fi)&gt; - 2.8.2-22804/2804 - perl-Archive-Extract-0.38-119.el6_1.1.i686.rpm Saving Primary metadata Saving file lists metadata Saving other metadata Generating sqlite DBs Sqlite DBs complete 备注：这个步骤需要点时间。 1.7 编写 /etc/yum.repos.d/local_repo.repo 文件文件 /etc/yum.repos.d/local_repo.repo 内容如下。123456[rhel-source] name=Red Hat Enterprise Linux Local repo ##仓库描述 baseurl=file:///opt/rpm/local_repo ##软件仓库位置 enabled=1 ##是否启用 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release ##签名秘钥 YUM 源测试2.1 查找 ruby 相关包1234567891011121314[root@redhat6 yum.repos.d]# yum search ruby Loaded plugins: product-id, refresh-packagekit, security, subscription-manager Updating certificate-based repositories. Repository rhel-source is listed more than once in the configuration rhel-source | 2.7 kB 00:00 ... rhel-source/primary_db | 2.5 MB 00:00 ... ======================================================== N/S Matched: ruby ========================================================= ruby-irb.i686 : The Interactive Ruby ruby-libs.i686 : Libraries necessary to run Ruby ruby-qpid.i686 : Ruby language client for AMQP ruby-qpid-qmf.i686 : The QPID Management Framework bindings for ruby ruby.i686 : An interpreter of object-oriented scripting language saslwrapper.i686 : Ruby and Python wrappers for the cyrus sasl library. Name and summary matches only, use \"search all\" for everything. 2.2 安装包 ruby.i681234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@redhat6 yum.repos.d]# yum install ruby.i686Loaded plugins: product-id, refresh-packagekit, security, subscription-manager Updating certificate-based repositories. Repository rhel-source is listed more than once in the configuration Setting up Install Process Resolving Dependencies --&gt; Running transaction check ---&gt; Package ruby.i686 0:1.8.7.352-3.el6 will be installed --&gt; Processing Dependency: ruby-libs = 1.8.7.352-3.el6 for package: ruby-1.8.7.352-3.el6.i686 --&gt; Processing Dependency: libruby.so.1.8 for package: ruby-1.8.7.352-3.el6.i686 --&gt; Running transaction check ---&gt; Package ruby-libs.i686 0:1.8.7.352-3.el6 will be installed --&gt; Processing Dependency: libreadline.so.5 for package: ruby-libs-1.8.7.352-3.el6.i686 --&gt; Running transaction check ---&gt; Package compat-readline5.i686 0:5.2-17.1.el6 will be installed --&gt; Finished Dependency ResolutionDependencies Resolved==================================================================================================================================== Package Arch Version Repository Size ==================================================================================================================================== Installing: ruby i686 1.8.7.352-3.el6 rhel-source 532 k Installing for dependencies: compat-readline5 i686 5.2-17.1.el6 rhel-source 127 k ruby-libs i686 1.8.7.352-3.el6 rhel-source 1.6 MTransaction Summary ==================================================================================================================================== Install 3 Package(s)Total download size: 2.3 M Installed size: 7.7 M Is this ok [y/N]: y Downloading Packages: ------------------------------------------------------------------------------------------------------------------------------------ Total 16 MB/s | 2.3 MB 00:00 warning: rpmts_HdrFromFdno: Header V3 RSA/SHA256 Signature, key ID fd431d51: NOKEY Retrieving key from [file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release](file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release) Importing GPG key 0xFD431D51: Userid : Red Hat, Inc. (release key 2) &lt;[security@redhat.com](mailto:security@redhat.com)&gt; Package: redhat-release-server-6Server-6.2.0.3.el6.i686 (@anaconda-RedHatEnterpriseLinux-201111171035.i386/6.2) From : /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release Is this ok [y/N]: y Importing GPG key 0x2FA658E0: Userid : Red Hat, Inc. (auxiliary key) &lt;[security@redhat.com](mailto:security@redhat.com)&gt; Package: redhat-release-server-6Server-6.2.0.3.el6.i686 (@anaconda-RedHatEnterpriseLinux-201111171035.i386/6.2) From : /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release Is this ok [y/N]: y Running rpm_check_debug Running Transaction Test Transaction Test Succeeded Running Transaction Warning: RPMDB altered outside of yum. Installing : compat-readline5-5.2-17.1.el6.i686 1/3 Installing : ruby-libs-1.8.7.352-3.el6.i686 2/3 Installing : ruby-1.8.7.352-3.el6.i686 3/3 Installed products updated.Installed: ruby.i686 0:1.8.7.352-3.el6Dependency Installed: compat-readline5.i686 0:5.2-17.1.el6 ruby-libs.i686 0:1.8.7.352-3.el6Complete! 备注：本地 yum 源配置成功。 2.3 列出已安装的包1234567[root@redhat6 yum.repos.d]# yum list | grep ruby Repository rhel-source is listed more than once in the configuration ruby.i686 1.8.7.352-3.el6 @rhel-source ruby-libs.i686 1.8.7.352-3.el6 @rhel-source ruby-irb.i686 1.8.7.352-3.el6 rhel-source ruby-qpid.i686 0.7.946106-2.el6 rhel-source ruby-qpid-qmf.i686 0.12-6.el6 rhel-source 2.4 查看包信息12345678910111213141516171819[root@redhat6 yum.repos.d]# yum info ruby.i686 Loaded plugins: product-id, refresh-packagekit, security, subscription-manager Updating certificate-based repositories. Repository rhel-source is listed more than once in the configuration Installed Packages Name : ruby Arch : i686 Version : 1.8.7.352 Release : 3.el6 Size : 1.8 M Repo : installed From repo : rhel-source Summary : An interpreter of object-oriented scripting language URL : [http://www.ruby-lang.org/](http://www.ruby-lang.org/) License : Ruby or GPLv2 Description : Ruby is the interpreted scripting language for quick and easy : object-oriented programming. It has many features to process text : files and to do system management tasks (as in Perl). It is simple, : straight-forward, and extensible. 2.5 删除 yum 缓存数据123456[root@redhat6 yum.repos.d]# yum clean all Loaded plugins: product-id, refresh-packagekit, security, subscription-manager Updating certificate-based repositories. Repository rhel-source is listed more than once in the configuration Cleaning repos: rhel-source Cleaning up Everything","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"RHEL6 学习：Write Barriers","slug":"20120315140102","date":"2012-03-15T06:01:02.000Z","updated":"2018-09-04T01:33:54.961Z","comments":true,"path":"20120315140102.html","link":"","permalink":"https://postgres.fun/20120315140102.html","excerpt":"","text":"Write Barriers 是一种内核机制，可以保证文件系统元数据正确有序地写入持久化存储，哪怕持久化存储掉电的情况也能保证。当出现掉电的情况时，开启了Write Barriers 的文件系统通过 fsync () 将数据写入持久化存储。但是 Write Barriers 开启后，也会带来很大写性能下降， RHEL6 文件系统默认开启 Barriers，如果想关闭 Barriers 功能，可以在 mount 时加上 barrier=0 选项，下面是关闭 barrier 的测试。 如何关闭 Write Barriers ?1.1 关闭 barrier12[root@redhat6 skytf]# umount /database/skytf/pgdata1 [root@redhat6 skytf]# mount -t ext4 -o barrier=0 /dev/mapper/vg_redhat6-lv_pgdata_01 /database/skytf/pgdata1 备注： RHEL6 文件系统默认开启 Barriers ，如果想关闭 Barriers 功能，重新挂载文件系统并增加 -o barrier=0 即可。 1.2 查看 /var/log/messages12Mar 15 10:54:46 redhat6 kernel: EXT4-fs (dm-2): barriers disabled Mar 15 10:54:46 redhat6 kernel: EXT4-fs (dm-2): mounted filesystem with ordered data mode. Opts: 备注：从日志里已经显示了 “barriers disabled” 信息。 如何查看文件系统是否开启 Write Barriers ?2.1 方法一: mount -l12[root@redhat6 skytf]# mount -l | grep /database/skytf/pgdata1 /dev/mapper/vg_redhat6-lv_pgdata_01 on /database/skytf/pgdata1 type ext4 (rw,barrier=0) 备注：通过 mount -l 命令可以查看已挂载文件系统属性。 2.2 方法二: 查看 /etc/mtab 文件1234567891011[root@redhat6 skytf]# cat /etc/mtab /dev/mapper/vg_redhat6-lv_root / ext4 rw 0 0 proc /proc proc rw 0 0 sysfs /sys sysfs rw 0 0 devpts /dev/pts devpts rw,gid=5,mode=620 0 0 tmpfs /dev/shm tmpfs rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\" 0 0 /dev/sda1 /boot ext4 rw 0 0 none /proc/sys/fs/binfmt_misc binfmt_misc rw 0 0 sunrpc /var/lib/nfs/rpc_pipefs rpc_pipefs rw 0 0 gvfs-fuse-daemon /root/.gvfs fuse.gvfs-fuse-daemon rw,nosuid,nodev 0 0 /dev/mapper/vg_redhat6-lv_pgdata_01 /database/skytf/pgdata1 ext4 rw,barrier=0 0 0 备注：因为文件系统 mount 后会将信息写入文件 /etc/mtab 里，所以可以查看这个文件，当然如果 mount 文件系统时加选项 -n ，那么/etc/mtab文件里面就不会写入新挂载卷的信息。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"RHEL6 学习：磁盘配额","slug":"20120314174523","date":"2012-03-14T09:45:23.000Z","updated":"2018-09-04T01:33:54.914Z","comments":true,"path":"20120314174523.html","link":"","permalink":"https://postgres.fun/20120314174523.html","excerpt":"","text":"今天学习了 RHEL6 磁盘配额的内容，在生产环境下可能会用到，实验步骤如下： 磁盘配额主要步骤 修改 /etc/fstab ，打开目录配额选项。 重新 mount 需要限制配额的目录 创建配额配置文件 ( quotacheck ) 制定目录限额策略，例如目录使用大小，文件使用总数等。 磁盘配额2.1 修改/etc/fstab 打开目录配额选项 修改 /etc/fstab 文件，增加 userquota, grpquota 选项，文件如下。1234567891011121314# /etc/fstab # Created by anaconda on Thu Mar 8 22:18:35 2012 # # Accessible filesystems, by reference, are maintained under '/dev/disk' # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # /dev/mapper/vg_redhat6-lv_root / ext4 defaults 1 1 UUID=a0565a60-ddff-492e-aa25-ef8bca28e710 /boot ext4 defaults 1 2 /dev/mapper/vg_redhat6-lv_swap swap swap defaults 0 0 tmpfs /dev/shm tmpfs defaults 0 0 devpts /dev/pts devpts gid=5,mode=620 0 0 sysfs /sys sysfs defaults 0 0 proc /proc proc defaults 0 0 /dev/mapper/vg_redhat6-lv_pgdata_01 /database/skytf/pgdata1 ext4 defaults,usrquota,grpquota 0 0 2.2 重新 mount 需要限制配额的目录1[root@redhat6 ~]# mount -o remount /database/skytf/pgdata1 2.3 创建配额配置文件(quotacheck)1[root@redhat6 ~]# quotacheck -cug /dev/mapper/vg_redhat6-lv_pgdata_01 2.4 给指定用户制定目录配额策略 有两种方法制定用户限额策略。 2.4.1 使用 edquota 编辑1234[root@redhat6 ~]# edquota postgres Disk quotas for user postgres (uid 500): Filesystem blocks soft hard inodes soft hard /dev/mapper/vg_redhat6-lv_pgdata_01 20 40960 102400 5 6 0 2.4.2 使用 setquota 命令1[root@redhat6 pgdata1]# setquota -u postgres 40960 102400 5 6 /database/skytf/pgdata1 备注：上述命令是限制 postgresql 用户在 目录 /database/skytf/pgdata1 下最多使用 100 M空间，当使用空间达到 40 MB 时发出 warning 信息，同时最多创建 6 个文件，当创建第五个文件时发出 warning 信息。 2.5 查看当前限额情况1234567[root@redhat6 log]# repquota -s /database/skytf/pgdata1 * Report for user quotas on device /dev/mapper/vg_redhat6-lv_pgdata_01 Block grace time: 7days; Inode grace time: 7days Block limits File limits User used soft hard grace used soft hard grace ---------------------------------------------------------------------- postgres -- 16 40960 100M 1 5 6 2.6 开启配额123[root@redhat6 log]# quotaon -vaug /dev/mapper/vg_redhat6-lv_pgdata_01 [/database/skytf/pgdata1]: group quotas turned on /dev/mapper/vg_redhat6-lv_pgdata_01 [/database/skytf/pgdata1]: user quotas turned on 2.7 测试一 ( 文件 limit )12345678910111213141516[postgres@redhat6 pgdata1]$ cd /database/skytf/pgdata1 [postgres@redhat6 pgdata1]$ ll total 32 -rw-------. 1 root root 6144 Mar 14 15:53 aquota.group -rw-------. 1 root root 6144 Mar 14 16:36 aquota.user drwx------. 2 postgres postgres 16384 Mar 14 15:04 lost+found [postgres@redhat6 pgdata1]$ [postgres@redhat6 pgdata1]$ touch a.txt [postgres@redhat6 pgdata1]$ touch b.txt [postgres@redhat6 pgdata1]$ touch c.txt [postgres@redhat6 pgdata1]$ touch d.txt[postgres@redhat6 pgdata1]$ touch e.txt dm-2: warning, user file quota exceeded.[postgres@redhat6 pgdata1]$ touch f.txt dm-2: write failed, user file limit reached. touch: cannot touch \"f.txt\": Disk quota exceeded 备注：在目录 /database/skytf/pgdata1 下创建第五个文件时，发出 warning 信息，但第五个文件创建成功，当创建第六个文件时，提示 failed ，提示失败。 2.8 测试二: 配额测试1234567891011121314151617181920212223242526272829303132[postgres@redhat6 pgdata1]$ dd if=/dev/zero of=1.txt bs=1M count=10 10+0 records in 10+0 records out 10485760 bytes (10 MB) copied, 0.0504467 s, 208 MB/s [postgres@redhat6 pgdata1]$ dd if=/dev/zero of=2.txt bs=1M count=10 10+0 records in 10+0 records out 10485760 bytes (10 MB) copied, 0.0469055 s, 224 MB/s [postgres@redhat6 pgdata1]$ dd if=/dev/zero of=3.txt bs=1M count=10 10+0 records in 10+0 records out 10485760 bytes (10 MB) copied, 0.0476163 s, 220 MB/s[postgres@redhat6 pgdata1]$ dd if=/dev/zero of=4.txt bs=1M count=10 dm-2: warning, user block quota exceeded. 10+0 records in 10+0 records out 10485760 bytes (10 MB) copied, 0.0420879 s, 249 MB/s[postgres@redhat6 pgdata1]$ dd if=/dev/zero of=5.txt bs=1M count=10 dm-2: warning, user file quota exceeded. 10+0 records in 10+0 records out 10485760 bytes (10 MB) copied, 0.114665 s, 91.4 MB/s[postgres@redhat6 pgdata1]$ ll total 51232 -rw-rw-r--. 1 postgres postgres 10485760 Mar 14 17:01 1.txt -rw-rw-r--. 1 postgres postgres 10485760 Mar 14 17:01 2.txt -rw-rw-r--. 1 postgres postgres 10485760 Mar 14 17:01 3.txt -rw-rw-r--. 1 postgres postgres 10485760 Mar 14 17:01 4.txt -rw-rw-r--. 1 postgres postgres 10485760 Mar 14 17:02 5.txt -rw-------. 1 root root 6144 Mar 14 15:53 aquota.group -rw-------. 1 root root 6144 Mar 14 16:59 aquota.user drwx------. 2 postgres postgres 16384 Mar 14 15:04 lost+found 备注：创建五个文件，每个文件大小为 10M, 当使用空间达到 40 M 时，系统发出 warning 信息。 常见问题3.1 quotacheck 时异常12345[root@redhat6 ~]# quotacheck -cug /dev/mapper/vg_redhat6-lv_pgdata_01 quotacheck: Cannot create new quotafile /database/skytf/pgdata1/aquota.user.new: Permission denied quotacheck: Cannot initialize IO on new quotafile: Permission denied quotacheck: Cannot create new quotafile /database/skytf/pgdata1/aquota.group.new: Permission denied quotacheck: Cannot initialize IO on new quotafile: Permission denied 备注：解决方法，关闭 seliniux，关闭,启动 selinux 如下所示。 3.2 关闭 seliniux12/usr/sbin/setenforce 0 立刻关闭 SELINUX /usr/sbin/setenforce 1 立刻启用 SELINUX","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"RHEL6 学习: Swap 分区扩容 ","slug":"20120314144642","date":"2012-03-14T06:46:42.000Z","updated":"2018-09-04T01:33:54.851Z","comments":true,"path":"20120314144642.html","link":"","permalink":"https://postgres.fun/20120314144642.html","excerpt":"","text":"RedHat 企业版 6 默认安装会启用 LVM 管理方式，今天学习了给 swap 分区扩容，这里使用的是给已存在的 swap 逻辑分区扩容的方法，当然也可新建一个 swap 分区逻辑分区。本文采用的是前面的方法，下面是具体步骤。 一 环境信息1.1 测试环境平台：笔记本虚拟机系统：Red Hat Enterprise Linux Server release 6.2 1.2 查看 swap 使用情况12345[root@redhat6 ~]# free -m total used free shared buffers cached Mem: 714 665 49 0 38 460 -/+ buffers/cache: 165 549 Swap: 1439 0 1439 备注：swap 分区目前为 1439 MB。 1.3 查看硬盘使用情况123456[root@redhat6 ~]# df -hv Filesystem Size Used Avail Use% Mounted on /dev/mapper/vg_redhat6-lv_root 13G 3.5G 8.8G 29% / tmpfs 292M 100K 292M 1% /dev/shm /dev/sda1 485M 31M 429M 7% /boot 1.4 查看系统 VG 信息1234567891011121314151617181920[root@redhat6 ~]# vgdisplay --- Volume group --- VG Name vg_redhat6 System ID Format lvm2 Metadata Areas 1 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 1 Act PV 1 VG Size 14.51 GiB PE Size 4.00 MiB Total PE 3714 Alloc PE / Size 3714 / 14.51 GiB Free PE / Size 0 / 0 VG UUID E6cA2U-TL1x-ScCV-UnGU-3Kq4-1u6V-WUb5L4 1.5 查看系统 lv 信息123456789101112131415161718192021222324252627282930[root@redhat6 ~]# lvdisplay --- Logical volume --- LV Name /dev/vg_redhat6/lv_root VG Name vg_redhat6 LV UUID QFAl72-FSES-YKAH-Dax1-9FQH-kMmv-8vqju2 LV Write Access read/write LV Status available # open 1 LV Size 13.10 GiB Current LE 3354 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:0 --- Logical volume --- LV Name /dev/vg_redhat6/lv_swap VG Name vg_redhat6 LV UUID H26wg0-bbW2-IfHa-j250-RFFh-O0ze-zTe3VU LV Write Access read/write LV Status available # open 1 LV Size 1.41 GiB Current LE 360 Segments 1 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:1 备注：根据上面信息，swap 使用的是逻辑卷 vg_redhat6，而且 vg_redhat6 空间都已分配完成，那么要扩 swap 分区，只要扩卷组 vg_redhat6，之后再扩 lv /dev/vg_redhat6/lv_swap 即可。 二 Swap 分区扩容2.1 笔记本虚拟机加一块 4GB IDE 硬盘 此步略，硬盘加完重启系统后，硬盘信息如下:1234567891011121314151617181920212223242526272829303132[root@redhat6 ~]# fdisk -lDisk /dev/sdb: 4294 MB, 4294967296 bytes 255 heads, 63 sectors/track, 522 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000Disk /dev/sdb doesn t contain a valid partition tableDisk /dev/sda: 16.1 GB, 16106127360 bytes 255 heads, 63 sectors/track, 1958 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000d571a Device Boot Start End Blocks Id System /dev/sda1 * 1 64 512000 83 Linux Partition 1 does not end on cylinder boundary. /dev/sda2 64 1959 15215616 8e Linux LVMDisk /dev/mapper/vg_redhat6-lv_root: 14.1 GB, 14067695616 bytes 255 heads, 63 sectors/track, 1710 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000Disk /dev/mapper/vg_redhat6-lv_root doesn t contain a valid partition tableDisk /dev/mapper/vg_redhat6-lv_swap: 1509 MB, 1509949440 bytes 255 heads, 63 sectors/track, 183 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x00000000Disk /dev/mapper/vg_redhat6-lv_swap doesn t contain a valid partition table 备注：系统已经认出新加的盘 /dev/sdb， 容量为 4294 MB。 2.2 给VG vg_redhat6 扩容 4 GB1234567891011[root@redhat6 ~]# mkfs.ext4 -t ext4 -c /dev/sdbmke2fs 1.41.12 (17-May-2010)[root@redhat6 ~]# pvcreate /dev/sdb Writing physical volume data to disk \"/dev/sdb\" Physical volume \"/dev/sdb\" successfully created [root@redhat6 ~]# pvscan PV /dev/sda2 VG vg_redhat6 lvm2 [14.51 GiB / 0 free] PV /dev/sdb lvm2 [4.00 GiB] Total: 2 [18.51 GiB] / in use: 1 [14.51 GiB] / in no VG: 1 [4.00 GiB] [root@redhat6 ~]# vgextend vg_redhat6 /dev/sdb Volume group \"vg_redhat6\" successfully extended 2.3 再次查看 vg_redhat6 信息123456789101112131415161718192021[root@redhat6 ~]# vgdisplay --- Volume group --- VG Name vg_redhat6 System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 4 VG Access read/write VG Status resizable MAX LV 0 Cur LV 2 Open LV 2 Max PV 0 Cur PV 2 Act PV 2 VG Size 18.50 GiB PE Size 4.00 MiB Total PE 4737 Alloc PE / Size 3714 / 14.51 GiB Free PE / Size 1023 / 4.00 GiB VG UUID E6cA2U-TL1x-ScCV-UnGU-3Kq4-1u6V-WUb5L4 备注： VG vg_redhat6 扩容成功。 2.4 停用 swap12[root@redhat6 ~]# swapoff -v /dev/vg_redhat6/lv_swap swapoff on /dev/vg_redhat6/lv_swap 2.5 给 dev/vg_redhat6/lv_swap 扩容 512M123[root@redhat6 ~]# lvresize -L+512M /dev/vg_redhat6/lv_swap Extending logical volume lv_swap to 1.91 GiB Logical volume lv_swap successfully resized 2.6 格式化 swap 分区12345[root@redhat6 ~]# mkswap /dev/vg_redhat6/lv_swap mkswap: /dev/vg_redhat6/lv_swap: warning: dont erase bootbits sectors on whole disk. Use -f to force. Setting up swapspace version 1, size = 1998844 KiB no label, UUID=47f6e518-6189-4231-9aed-9f7ad01c29ca 2.7 启用 swap1234[root@redhat6 ~]# swapon -v /dev/vg_redhat6/lv_swap swapon on /dev/vg_redhat6/lv_swap swapon: /dev/mapper/vg_redhat6-lv_swap: found swap signature: version 1, page-size 4, same byte order swapon: /dev/mapper/vg_redhat6-lv_swap: pagesize=4096, swapsize=2046820352, devsize=2046820352 2.8 再次查看 swap 验证12345[root@redhat6 ~]# free -m total used free shared buffers cached Mem: 582 509 73 0 53 271 -/+ buffers/cache: 183 399 Swap: 1951 0 1951 备注： swap 分区成功扩容到 1951 MB。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"NFS 服务固定端口","slug":"20120306091612","date":"2012-03-06T01:16:12.000Z","updated":"2018-09-04T01:33:54.804Z","comments":true,"path":"20120306091612.html","link":"","permalink":"https://postgres.fun/20120306091612.html","excerpt":"","text":"前几天学习了下 NFS 服务配置，但之前没有使用固定 NFS 端口的方法，之后只能关闭防火墙实现NFS 配置，今天学习了下 NFS 服务固定端口的方法。 一 环境信息NFS SERVER 192.168.1.26NFS CLIENT 192.168.1.25 二 配置 NFS 服务端2.1 修改 /etc/exports1/database/pgdata1/shared 192.168.1.25(rw,anonuid=500,anongid=500) 2.2 显示共享目录信息123[root@pgb sysconfig]# exportfs -v/database/pgdata1/shared 192.168.1.25(rw,wdelay,root_squash,no_subtree_check,anonuid=500,anongid=500) 2.3 修改 /etc/sysconfig/nfs 文件1234MOUNTD_PORT=\"4002\"STATD_PORT=\"4003\"LOCKD_TCPPORT=\"4004\"LOCKD_UDPPORT=\"4004\" 备注：NFS 服务配置过程中，其中 mountd, statd 和 lockd 进程可以使用固定端口号。2.4 显示RPC信息12345678910111213141516171819202122232425262728[root@pgb sysconfig]# rpcinfo -p program vers proto port 100000 2 tcp 111 portmapper 100000 2 udp 111 portmapper 100024 1 udp 923 status 100024 1 tcp 926 status 100011 1 udp 927 rquotad 100011 2 udp 927 rquotad 100011 1 tcp 930 rquotad 100011 2 tcp 930 rquotad 100003 2 udp 2049 nfs 100003 3 udp 2049 nfs 100003 4 udp 2049 nfs 100021 1 udp 4004 nlockmgr 100021 3 udp 4004 nlockmgr 100021 4 udp 4004 nlockmgr 100021 1 tcp 4004 nlockmgr 100021 3 tcp 4004 nlockmgr 100021 4 tcp 4004 nlockmgr 100003 2 tcp 2049 nfs 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100005 1 udp 4002 mountd 100005 1 tcp 4002 mountd 100005 2 udp 4002 mountd 100005 2 tcp 4002 mountd 100005 3 udp 4002 mountd 100005 3 tcp 4002 mountd 2.5 修改防火墙,iptables 后面部分如下123456789101112131415161718NFS:portmapper-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 111 -j ACCEPT-A RH-Firewall-1-INPUT -p udp -m udp --dport 111 -j ACCEPTNFS:nfsd-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 2049 -j ACCEPT-A RH-Firewall-1-INPUT -p udp -m udp --dport 2049 -j ACCEPTNFS:mountd-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 4002 -j ACCEPT-A RH-Firewall-1-INPUT -p udp -m udp --dport 4002 -j ACCEPT-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 4003 -j ACCEPTNFS:nlockmgr-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 4004 -j ACCEPT-A RH-Firewall-1-INPUT -p udp -m udp --dport 4004 -j ACCEPT-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 111 -j ACCEPT-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 781 -j ACCEPT-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 778 -j ACCEPT-A RH-Firewall-1-INPUT -j REJECT --reject-with icmp-host-prohibitedCOMMIT 2.6 重启防火墙123456[root@pgb sysconfig]# service iptables restartFlushing firewall rules: [ OK ]Setting chains to policy ACCEPT: filter [ OK ]Unloading iptables modules: [ OK ]Applying iptables firewall rules: [ OK ]Loading additional iptables modules: ip_conntrack_netbios_ns [ OK ] 三 配置 NFS 客户端3.1 测试：mount 目录12[root@pg1 ~]# mkdir -p /mnt/pgb/shared[root@pg1 /]# mount -t nfs 192.168.1.26:/database/shared /mnt/pgb/shared/ 3.2 查看目录信息12345678[postgres@pg1 pgb]$ df -hvFilesystem Size Used Avail Use% Mounted on/dev/hda1 14G 11G 2.1G 85% /tmpfs 217M 0 217M 0% /dev/shm/dev/hdb3 1.9G 35M 1.8G 2% /archive/pganone 217M 104K 217M 1% /var/lib/xenstored192.168.1.26:/database/pgdata1/shared 2.0G 34M 1.9G 2% /mnt/pgb/shared 备注：NFS 配置成功。 3.3 设置开机自动挂载修改 /etc/fstab 文件，增加以下行1192.168.1.26:/database/pgdata1/shared /mnt/pgb/shared nfs defaults,rw 0 0","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"NFS","slug":"NFS","permalink":"https://postgres.fun/tags/NFS/"}]},{"title":"PostgreSQL 新特性之七 : oracle_fdw 实践 ","slug":"20120303174557","date":"2012-03-03T09:45:57.000Z","updated":"2018-09-04T01:33:54.742Z","comments":true,"path":"20120303174557.html","link":"","permalink":"https://postgres.fun/20120303174557.html","excerpt":"","text":"之前测试过 PostgreSQL 的 mysql_fdw， file_fdw ，今天有空，测试了 oracle_fdw, 什么是 oracle_fdw 呢？简单的说， 通过安装 oracle_fdw, 在 PostgreSQL 中可以访问 Oracle 库中的表，类似 dblink，下面是 oracle_fdw 的安装过程。 环境准备1.1 环境信息Oracle: 10.2.0.1PostgreSQL: PostgreSQL 9.1操作系统：虚拟机 ( Red Hat Enterprise Linux AS release 4 )备注： Oracle ,PostgreSQL 安装部分略。 1.2 postgres 编译1./configure --prefix=/opt/pgsql --with-pgport=1921 --with-segsize=8 --with-wal-segsize=64 --with-wal-blocksize=64 --with-perl --without-openssl --without-pam without-ldap --enable-thread-safety 备注：这里使用了 –without-ldap 1.3 postgres 环境变量修改 .bash_profile ，内容如下:123456789101112export PGPORT=1921 export PGDATA=/opt/pgdata/pg_root export LANG=en_US.utf8export PGHOME=/opt/pgsql export ORACLE_BASE=/app/oracle export ORACLE_HOME=/app/oracle/product/10g export LD_LIBRARY_PATH=$PGHOME/lib:$ORACLE_HOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib export DATE=`date +\"%Y%m%d%H%M\"` export PATH=$PGHOME/bin:$ORACLE_HOME/bin:$PATH:. export MANPATH=$PGHOME/share/man:$MANPATHalias rm='rm -i' alias ll='ls -lh' 备注：环境变量 里加入了 ORACLE_BASE, ORACLE_HOME ，以及环境变量 LD_LIBRARY_PATH 增加了 $PGHOME/lib。 安装 oracle_fdw2.1 下载 oracle_fdwhttp://pgxn.org/dist/oracle_fdw/备注：下载并解压到 /opt/soft_bak 目录。 2.2 install oraclel_fdw123[root@primary-01 oracle_fdw-0.9.3]# cd /opt/soft_bak/oracle_fdw-0.9.3 [root@primary-01 oracle_fdw-0.9.3]# source /home/postgres/.bash_profile [root@primary-01 oracle_fdw-0.9.3]# source /home/oracle/.bash_profile 备注：在安装 oracle_fdw 前需要先载入 PostgreSQL 和 Oracle 用户环境变量。123456789101112[root@primary-01 oracle_fdw-0.9.3]# make gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wformat-security -fno-strict-aliasing -fpic -I/app/oracle/product/10g/sdk/include -I/app/oracle/product/10g/oci/include -I/app/oracle/product/10g/rdbms/public -I. -I. -I/opt/pgsql/include/server -I/opt/pgsql/include/internal -D_GNU_SOURCE -c -o oracle_fdw.o oracle_fdw.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wformat-security -fno-strict-aliasing -fpic -I/app/oracle/product/10g/sdk/include -I/app/oracle/product/10g/oci/include -I/app/oracle/product/10g/rdbms/public -I. -I. -I/opt/pgsql/include/server -I/opt/pgsql/include/internal -D_GNU_SOURCE -c -o oracle_utils.o oracle_utils.c gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wformat-security -fno-strict-aliasing -fpic -shared -o oracle_fdw.so oracle_fdw.o oracle_utils.o -L/opt/pgsql/lib -Wl,-rpath,'/opt/pgsql/lib',--enable-new-dtags -L/app/oracle/product/10g -L/app/oracle/product/10g/bin -L/app/oracle/product/10g/lib -lclntsh[root@primary-01 oracle_fdw-0.9.3]# make install /bin/mkdir -p '/opt/pgsql/lib' /bin/mkdir -p '/opt/pgsql/share/extension' /bin/mkdir -p '/opt/pgsql/share/doc/extension' /bin/sh /opt/pgsql/lib/pgxs/src/makefiles/../../config/install-sh -c -m 755 oracle_fdw.so '/opt/pgsql/lib/oracle_fdw.so' /bin/sh /opt/pgsql/lib/pgxs/src/makefiles/../../config/install-sh -c -m 644 ./oracle_fdw.control '/opt/pgsql/share/extension/' /bin/sh /opt/pgsql/lib/pgxs/src/makefiles/../../config/install-sh -c -m 644 ./oracle_fdw--1.0.sql '/opt/pgsql/share/extension/' /bin/sh /opt/pgsql/lib/pgxs/src/makefiles/../../config/install-sh -c -m 644 ./README.oracle_fdw '/opt/pgsql/share/doc/extension/' 备注：如果出现上面信息，说明 oracle_fdw 编译安装成功。 2.3 test creating extension12postgres=# create extension oracle_fdw; ERROR: could not load library \"/opt/pgsql/lib/oracle_fdw.so\": libclntsh.so.10.1: cannot open shared object file: No such file or directory 备注：一开始创建 oraclre_fdw 时，提示少了 so 文件，这个比较好解决，只要将对应的 so 文件 copy 到 $PGHOME/lib 下即可，如下：1234567[root@primary-01 oracle_fdw-0.9.3]# cd /app/oracle/product/10g/lib [root@primary-01 lib]# ll libclntsh.so.10.1 -rwxr-xr-x 1 oracle oinstall 18M Jan 10 2010 libclntsh.so.10.1[root@primary-01 lib]# cp libclntsh.so.10.1 /opt/pgsql/lib/ [root@primary-01 lib]# chown postgres:postgres /opt/pgsql/lib/libclntsh.so.10.1[root@primary-01 lib]# ll /opt/pgsql/lib/libclntsh.so.10.1 -rwxr-xr-x 1 postgres postgres 18M Mar 3 14:29 /opt/pgsql/lib/libclntsh.so.10.1 2.4 再次创建 oracle_fdw123456789skytf=# create extension oracle_fdw; CREATE EXTENSION skytf=# \\dx List of installed extensions Name | Version | Schema | Description ------------+---------+------------+---------------------------------------- oracle_fdw | 1.0 | public | foreign data wrapper for Oracle access plpgsql | 1.0 | pg_catalog | PL/pgSQL procedural language (2 rows) 备注：再次测试 oracle_fdw，终于成功了。 Oracle 库创建只读用户3.1 oracle 创建只读用户并赋只读权限 ( On Oracle)123456789101112SQL&gt; CREATE USER read_only IDENTIFIED BY \"read_only\" 2 DEFAULT TABLESPACE TB_skytf 3 TEMPORARY TABLESPACE TEMP 4 PROFILE DEFAULT 5 ACCOUNT UNLOCK;User created.SQL&gt; GRANT CONNECT TO read_only;Grant succeeded.SQL&gt; grant select on skytf.test_1 to read_Only;Grant succeeded.SQL&gt; conn read_only/read_only; Connected. 3.2 测试 read_only 用户12345SQL&gt; select * from skytf.test_1; ID R ---------- - 1 a 2 b 备注： oracle 库中的 read_only 等下用于在 PG 中连接 Oracle 的只读用户。 部署 oracle_fdw 外部表4.1 create foreign server1234skytf=# CREATE SERVER oracle_srv skytf-# FOREIGN DATA WRAPPER oracle_fdw skytf-# OPTIONS (dbserver 'primary_1'); CREATE SERVER 4.2 tnsping test123456789[postgres@primary-01 ~]$ tnsping primary_1TNS Ping Utility for Linux: Version 10.2.0.1.0 - Production on 03-MAR-2012 15:55:15Copyright (c) 1997, 2005, Oracle. All rights reserved.Used parameter files: Used TNSNAMES adapter to resolve the alias Attempting to contact (DESCRIPTION= (ADDRESS= (PROTOCOL=TCP) (HOST=192.168.1.30) (PORT=1521)) (CONNECT_DATA= (SERVICE_NAME=MANUA))) OK (10 msec) [postgres@primary-01 ~]$ 备注： 以 postgres 用户 tnsping 测试下，并且也可以 sqlplus 测试下，看看是否能连 oracle 库。 4.3 create mapping users123456skytf=&gt; CREATE USER MAPPING FOR skytf skytf-&gt; SERVER oracle_srv skytf-&gt; OPTIONS (user 'read_only', password 'read_only'); CREATE USER MAPPINGskytf=# grant usage on foreign server oracle_srv to skytf; GRANT 4.4 create foreign table12345678skytf=# \\c skytf skytf You are now connected to database \"skytf\" as user \"skytf\".skytf=&gt; CREATE FOREIGN TABLE ft_test_1 ( skytf(&gt; id integer, skytf(&gt; name character varying(20) skytf(&gt; ) SERVER oracle_srv skytf-&gt; OPTIONS (schema 'skytf', table 'test_1'); CREATE FOREIGN TABLE 4.5 查询测试123456skytf=&gt; select * from ft_test_1; id | name ----+------ 1 | a 2 | b (2 rows) 备注：终于可以查询到对端 Oracle 库了，到了这步，说明 oracle_fdw 配置成功！ 常见问题常见问题一: 查询外部表报错123skytf=&gt; select * from ft_test_1; ERROR: error connecting to Oracle: OCIEnvCreate failed to create environment handle DETAIL: 备注： 在成功安装 orcle_fdw 后，查询外部表时老遇到上面 ERROR， 后来网上查了下，可能是在 PostgreSQL 的环境变量中没有下正确设置 Oracle 的环境变量 ，但检查了下，环境变量设置没问题，为了操作方便，后来将 PostgreSQL 的 group 设置成 oinstall ，这样 Oracle 用户和 postgres 用户属于同一组了，之后再重启 PostgreSQL 就正常了。 解决方法123[root@primary-01 bin]# usermod -g oinstall postgres [postgres@primary-01 ~]$ id uid=501(postgres) gid=500(oinstall) groups=500(oinstall) context=user_u:system_r:unconfined_t 常见问题二: 创建扩展报错缺少 so 文件，如下： postgres=# create extension oracle_fdw; ERROR: could not load library &quot;/opt/pgsql/lib/oracle_fdw.so&quot;: libclntsh.so.10.1: 备注： 一开始创建 oraclre_fdw 时，提示少了 so 文件。 解决方法 只要将对应的 so 文件 copy 到 $PGHOME/lib 下就行，详见 2.3 步骤。 总结oracle_fdw 的出现为 Oracle 转 PostgreSQL 项目提供了又一种方法，至于迁数据的效率今天没有环境具体测试。 参考 http://pgxn.org/dist/oracle_fdw/ http://www.postgresql.org/docs/9.1/static/sql-createserver.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"oracle_fdw","slug":"oracle-fdw","permalink":"https://postgres.fun/tags/oracle-fdw/"}]},{"title":"主备切换异常一例 \"Contrecord is requested by 10E/92000020\"","slug":"20120229162834","date":"2012-02-29T08:28:34.000Z","updated":"2018-09-04T01:33:54.679Z","comments":true,"path":"20120229162834.html","link":"","permalink":"https://postgres.fun/20120229162834.html","excerpt":"","text":"公司一套 Hot-Standby 环境主库需要内存扩容，为了尽可能减少停业务时间，常规的操作是做下 switch role ，即主备切换，将业务切换到备库，那么主库就有宽松的停机时间进行内存扩容，但今天有步操作不对，导致出现上面错误， 首先看下正确的步骤参考如下： 环境信息PostgreSQL: 9.1.1系统： Red Hat Enterprise Linux Server release 5.6 主备切换停主库VIP1/sbin/ifdown eth0:1 备注：主库上开了 eth0:1 这个IP权且称为 VIP，加入这个IP的目的是主备切换后不需要更改应用IP。 关闭主库 ( On Primary )1pg_ctl stop -m fast -D $PGDATA 备注：关闭主库，这步操作重要，为接下来激活备库做准备。 激活备库到主库状态 ( on slave )1touch $PGDATA/postgresql.trigger.1921 备注：激活备库很简单，只需要创建一个文件即可，执行上述操作后，recovery.conf 文件应该变成 recovery.done ，说明备库已经激活，接下来准备更改原主库角色。 修改原主库文件名 ( 重要 )1recovery.done --&gt; recovery.conf 备注： 这步很重要，在做主备切换时，PG 是根据 recovery 文件来区分的，简单的说 当备库准备激活成主库时: $PGDATA 目录下应该有 recovery.conf 文件，当备库成功激活时，recovery.conf 文件应该变成 recovery.done。 当备库激活后，需要将原主库变更成备库角色，此时目录 $PGDATA 下文件应该为 recovery.conf ，否则启动原来主库时，会产生冲突，即报以下ERROR，今天就是因为这么操作没有做，结果原主库不能 switch role 到备库，之后就是重做备库，COPY 200 多G 的数据库啊。。。。花了两小时重做 standby。 error122012-02-29 09:24:13.878 CST,,,4714,,4f4d7c99.126a,88,,2012-02-29 09:17:13 CST,,0,LOG,00000,\"contrecord is requested by 10E/92000020\",,,,,,,,,\"\" 2012-02-29 09:24:18.878 CST,,,4714,,4f4d7c99.126a,89,,2012-02-29 09:17:13 CST,,0,LOG,00000,\"contrecord is requested by 10E/92000020\",,,,,,,,,\"\" 激活原来的主库，让其转变成从库 (在原来的主库上执行)1nohup $PGHOME/bin/postgres -D $PGDATA -p $PGPORT 2&gt;/var/applog/pg_log/start_err_1921.log 2&gt;&amp;1 &amp; 启动备库 VIP1/sbin/ifup eth0:1 修改主库和备库 /etc/rc.d/rc.local 启动文件 通知业务检查应用是否正常。 备注：主备切换总共步骤就这些，上面步骤仅供参考。 异常检查 在主备切换时如遇通信问题，可检查主库备库文件 ~/.pgpass, pg_hba.conf 文件 。 参考 关于主备切换更详细的信息可参考以前写的 BLOG: PostgreSQL HOT-Standby 的主备切换","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"LVM 管理之一：扩容VG/LV","slug":"20120222164450","date":"2012-02-22T08:44:50.000Z","updated":"2018-09-04T01:33:54.632Z","comments":true,"path":"20120222164450.html","link":"","permalink":"https://postgres.fun/20120222164450.html","excerpt":"","text":"这节主要演练在线扩容 VG 和 LV 。 动态扩容 VG1.1 查看硬盘信息123456789101112131415161718192021222324[root@pgb lvm]# fdisk -lDisk /dev/hda: 19.3 GB, 19327352832 bytes 255 heads, 63 sectors/track, 2349 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/hda1 * 1 13 104391 83 Linux /dev/hda2 14 144 1052257+ 82 Linux swap / Solaris /dev/hda3 145 2349 17711662+ 83 LinuxDisk /dev/hdb: 2147 MB, 2147483648 bytes 16 heads, 63 sectors/track, 4161 cylinders Units = cylinders of 1008 * 512 = 516096 bytes Device Boot Start End Blocks Id System /dev/hdb1 1 1985 1000408+ 83 Linux /dev/hdb2 1986 4161 1096704 83 LinuxDisk /dev/hdd: 1073 MB, 1073741824 bytes 16 heads, 63 sectors/track, 2080 cylinders Units = cylinders of 1008 * 512 = 516096 bytes Device Boot Start End Blocks Id System /dev/hdd1 1 2080 1048288+ 83 Linux [root@pgb lvm]# pvscan PV /dev/hdb1 VG vg01_pgdata lvm2 [976.00 MB / 972.00 MB free] PV /dev/hdd1 VG vg01_pgdata lvm2 [1020.00 MB / 0 free] Total: 2 [1.95 GB] / in use: 2 [1.95 GB] / in no VG: 0 [0 ] 备注：根据 fdisk 和 pvscan 命令输出，知道 /dev/hdb2 还没有加入 VG， 可以使用，接下来将 /dev/hdb2 加入 VG vg01_pgdata。 1.2 查看 VG 信息1234567891011121314151617181920[root@pgb lvm]# vgdisplay --- Volume group --- VG Name vg01_pgdata System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 2 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 2 Act PV 2 VG Size 1.95 GB PE Size 4.00 MB Total PE 499 Alloc PE / Size 256 / 1.00 GB Free PE / Size 243 / 972.00 MB VG UUID B5pg8R-2AGm-6DEp-K7HK-TI1I-HC3h-gWx32m 1.3 格式化文件系统123456789101112131415161718192021[root@pgb lvm]# mkfs -t ext3 -c /dev/hdb2 mke2fs 1.39 (29-May-2006) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) 137088 inodes, 274176 blocks 13708 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=281018368 9 block groups 32768 blocks per group, 32768 fragments per group 15232 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376Checking for bad blocks (read-only test): done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 28 mounts or 180 days, whichever comes first. Use tune2fs -c or -i to override. 1.4 创建PV12[root@pgb lvm]# pvcreate /dev/hdb2 Physical volume \"/dev/hdb2\" successfully created 1.5 在线扩容 VG123456789[root@pgb lvm]# vgs VG #PV #LV #SN Attr VSize VFree vg01_pgdata 2 1 0 wz--n- 1.95G 972.00M[root@pgb lvm]# vgextend vg01_pgdata /dev/hdb2 Volume group \"vg01_pgdata\" successfully extended [root@pgb lvm]# vgs VG #PV #LV #SN Attr VSize VFree vg01_pgdata 3 1 0 wz--n- 2.99G 1.99G 1.6 再次查看 VG，查看是否扩容123456789101112131415161718192021[root@pgb lvm]# vgdisplay --- Volume group --- VG Name vg01_pgdata System ID Format lvm2 Metadata Areas 3 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 3 Act PV 3 VG Size 2.99 GB PE Size 4.00 MB Total PE 766 Alloc PE / Size 256 / 1.00 GB Free PE / Size 510 / 1.99 GB VG UUID B5pg8R-2AGm-6DEp-K7HK-TI1I-HC3h-gWx32m 备注：现在 vg01_pgdata 大小为 3 GB 左右，已成功扩容 1 GB。 动态扩容 LV 目标给已在线上使用的LV 扩容，在以下例子中，给目录 /database/pgdata1 扩容 512 M。2.1 查看目录使用情况12345678[root@pgb lvm]# df -hv Filesystem Size Used Avail Use% Mounted on /dev/hda3 17G 9.8G 5.8G 64% / /dev/hda1 99M 18M 76M 20% /boot tmpfs 217M 0 217M 0% /dev/shm none 217M 104K 217M 1% /var/lib/xenstored /dev/mapper/vg01_pgdata-lv_pgdata1 1008M 34M 924M 4% /database/pgdata1 2.2 查看所属 VG 信息123456789101112131415161718192021[root@pgb lvm]# vgdisplay --- Volume group --- VG Name vg01_pgdata System ID Format lvm2 Metadata Areas 3 Metadata Sequence No 3 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 3 Act PV 3 VG Size 2.99 GB PE Size 4.00 MB Total PE 766 Alloc PE / Size 256 / 1.00 GB Free PE / Size 510 / 1.99 GB VG UUID B5pg8R-2AGm-6DEp-K7HK-TI1I-HC3h-gWx32m 备注：从上面看出，VG vg01_pgdata 最大可用空间为 2.99 GB, 目前已分配 1 GB，还剩余 1.99 GB 可以分配。 2.3 增加 LV 大小123456789101112[root@pgb lvm]# lvextend -L +512M /dev/mapper/vg01_pgdata-lv_pgdata1 Extending logical volume lv_pgdata1 to 1.50 GB Logical volume lv_pgdata1 successfully resized [root@pgb lvm]# df -hv Filesystem Size Used Avail Use% Mounted on /dev/hda3 17G 9.9G 5.8G 64% / /dev/hda1 99M 18M 76M 20% /boot tmpfs 217M 0 217M 0% /dev/shm none 217M 104K 217M 1% /var/lib/xenstored /dev/mapper/vg01_pgdata-lv_pgdata1 1008M 34M 924M 4% /database/pgdata1 备注： LV 扩容成功，但目录 /database/pgdata1 大小仍然为 1G，没有变化。还需要 resize2fs 命令处理下。 2.4 resize2fs12345[root@pgb lvm]# resize2fs -f /dev/mapper/vg01_pgdata-lv_pgdata1 resize2fs 1.39 (29-May-2006) Filesystem at /dev/mapper/vg01_pgdata-lv_pgdata1 is mounted on /database/pgdata1; on-line resizing required Performing an on-line resize of /dev/mapper/vg01_pgdata-lv_pgdata1 to 524288 (4k) blocks. The filesystem on /dev/mapper/vg01_pgdata-lv_pgdata1 is now 524288 blocks long. 2.5 再次查看12345678[root@pgb lvm]# df -hv Filesystem Size Used Avail Use% Mounted on /dev/hda3 17G 9.8G 5.8G 64% / /dev/hda1 99M 18M 76M 20% /boot tmpfs 217M 0 217M 0% /dev/shm none 217M 104K 217M 1% /var/lib/xenstored /dev/mapper/vg01_pgdata-lv_pgdata1 1.5G 34M 1.4G 3% /database/pgdata1 备注：目录 /database/pgdata1 空间果然变大了。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"LVM","slug":"LVM","permalink":"https://postgres.fun/tags/LVM/"}]},{"title":"Linux: LVM 创建 ","slug":"20120222163314","date":"2012-02-22T08:33:14.000Z","updated":"2018-09-04T01:33:54.570Z","comments":true,"path":"20120222163314.html","link":"","permalink":"https://postgres.fun/20120222163314.html","excerpt":"","text":"关于Linux 的逻辑卷知识，以前前接触过一些，今天再次温习下，主要是演练下 LVM 的创建过程及维护命令，LVM 理论部分和虚拟机增加硬盘部分略。 1 环境准备虚拟机：Red Hat Enterprise Linux Server release 5.5增加两块 IDE 硬盘，一块为 2GB，另一块为 1GB。 2查看新增硬盘情况123456789101112131415161718Disk /dev/hda: 19.3 GB, 19327352832 bytes 255 heads, 63 sectors/track, 2349 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/hda1 * 1 13 104391 83 Linux /dev/hda2 14 144 1052257+ 82 Linux swap / Solaris /dev/hda3 145 2349 17711662+ 83 LinuxDisk /dev/hdb: 2147 MB, 2147483648 bytes 16 heads, 63 sectors/track, 4161 cylinders Units = cylinders of 1008 * 512 = 516096 bytes Device Boot Start End Blocks Id System /dev/hdb1 1 1985 1000408+ 83 Linux /dev/hdb2 1986 4161 1096704 83 LinuxDisk /dev/hdd: 1073 MB, 1073741824 bytes 16 heads, 63 sectors/track, 2080 cylinders Units = cylinders of 1008 * 512 = 516096 bytes Device Boot Start End Blocks Id System /dev/hdd1 1 2080 1048288+ 83 Linux 备注： /dev/hdb， /dev/hdd 就是新增的盘， /dev/hdb 为 2147 MB，/dev/hdd 为 1073 MB。 3 创建PV ( pvcreat )1234567[root@pgb ~]# pvcreate /dev/hdb1 /dev/hdd1 /dev/cdrom: open failed: Read-only file system Attempt to close device '/dev/cdrom' which is not open. Physical volume \"/dev/hdb1\" successfully created /dev/cdrom: open failed: Read-only file system Attempt to close device '/dev/cdrom' which is not open. Physical volume \"/dev/hdd1\" successfully created 备注：在创建 VG 之前，首先要将物理设备标识成 LVM 可识别的物理设备，另外 pvcreate 命令会清除设备上的所有数据。 4 查看 PV 信息1234[root@pgb ~]# pvscan PV /dev/hdb1 lvm2 [976.96 MB] PV /dev/hdd1 lvm2 [1023.72 MB] Total: 2 [1.95 GB] / in use: 0 [0 ] / in no VG: 2 [1.95 GB] 备注：上面显示 /dev/hdb1， /dev/hdd1 已经创建成 PV 了。 5 创建VG ( vgcreate )123456789[root@pgb ~]# vgcreate vg01_pgdata /dev/hdb1 /dev/hdd1 /dev/cdrom: open failed: Read-only file system /dev/cdrom: open failed: Read-only file system Attempt to close device '/dev/cdrom' which is not open. /dev/cdrom: open failed: Read-only file system Attempt to close device '/dev/cdrom' which is not open. /dev/cdrom: open failed: Read-only file system Attempt to close device '/dev/cdrom' which is not open. Volume group \"vg01_pgdata\" successfully created 6 显示 VG 信息1234567891011121314151617181920[root@pgb ~]# vgdisplay --- Volume group --- VG Name vg01_pgdata System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 1 VG Access read/write VG Status resizable MAX LV 0 Cur LV 0 Open LV 0 Max PV 0 Cur PV 2 Act PV 2 VG Size 1.95 GB PE Size 4.00 MB Total PE 499 Alloc PE / Size 0 / 0 Free PE / Size 499 / 1.95 GB VG UUID B5pg8R-2AGm-6DEp-K7HK-TI1I-HC3h-gWx32m 备注: 逻辑卷 vg01_pgdata 创建成功，大小为 1.95 GB。 7 创建LV12[root@pgb lvm]# lvcreate -L 1G -n lv_pgdata1 vg01_pgdata Logical volume \"lv_pgdata1\" created 8 扫描 lv 逻辑单元12[root@pgb lvm]# lvscan ACTIVE '/dev/vg01_pgdata/lv_pgdata1' [1.00 GB] inherit 9 查看LV信息123456789101112131415[root@pgb lvm]# lvdisplay /dev/vg01_pgdata/lv_pgdata1 --- Logical volume --- LV Name /dev/vg01_pgdata/lv_pgdata1 VG Name vg01_pgdata LV UUID pXsfbX-MAdM-6FQ5-cUz6-IKXC-hRJM-sNQ9yp LV Write Access read/write LV Status available # open 0 LV Size 1.00 GB Current LE 256 Segments 2 Allocation inherit Read ahead sectors auto - currently set to 256 Block device 253:0 备注：LV创建成功，大小为 1 GB，接下来可以格式化并挂载 LV 了。 10 格式化文件系统123456789101112131415161718192021[root@pgb lvm]# mkfs -t ext3 -c /dev/vg01_pgdata/lv_pgdata1 mke2fs 1.39 (29-May-2006) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) 131072 inodes, 262144 blocks 13107 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=268435456 8 block groups 32768 blocks per group, 32768 fragments per group 16384 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376Checking for bad blocks (read-only test): done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 30 mounts or 180 days, whichever comes first. Use tune2fs -c or -i to override. 11 目录挂载1234567891011[root@pgb lvm]# mkdir -p /database/pgdata1[root@pgb lvm]# mount -t ext3 /dev/vg01_pgdata/lv_pgdata1 /database/pgdata1[root@pgb lvm]# chown -R postgres:postgres /database/pgdata1[root@pgb lvm]# df -hv Filesystem Size Used Avail Use% Mounted on /dev/hda3 17G 9.8G 5.8G 64% / /dev/hda1 99M 18M 76M 20% /boot tmpfs 217M 0 217M 0% /dev/shm none 217M 104K 217M 1% /var/lib/xenstored /dev/mapper/vg01_pgdata-lv_pgdata1 1008M 34M 924M 4% /database/pgdata1 备注：到了这步， LVM 配置成功，目录挂载成功。 12 设置开机自动挂载修改文件 /etc/fstab,增加以下行1/dev/vg01_pgdata/lv_pgdata1 /database/pgdata1 ext3 defaults 0 0 13 再次查看 VG [root@pgb lvm]# vgdisplay --- Volume group --- VG Name vg01_pgdata System ID Format lvm2 Metadata Areas 2 Metadata Sequence No 2 VG Access read/write VG Status resizable MAX LV 0 Cur LV 1 Open LV 1 Max PV 0 Cur PV 2 Act PV 2 VG Size 1.95 GB PE Size 4.00 MB Total PE 499 Alloc PE / Size 256 / 1.00 GB Free PE / Size 243 / 972.00 MB VG UUID B5pg8R-2AGm-6DEp-K7HK-TI1I-HC3h-gWx32m 备注：分配了(Alloc PE) 1GB，还剩余 (Free PE) 972 MB。 14 常见 LVM 命令 功能 PV VG LV 搜索 pvscan vgscan lvscan 建立 pvcreate vgcreate lvcreate 列出 pvdisplay vgdisplay lvdisplay 增加 vgextend lvextend 减少 vgreduce lvreduce 删除 vgremove lvremove","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"LVM","slug":"LVM","permalink":"https://postgres.fun/tags/LVM/"}]},{"title":"软 RAID0 创建和管理","slug":"20120221165748","date":"2012-02-21T08:57:48.000Z","updated":"2018-12-04T00:32:50.608Z","comments":true,"path":"20120221165748.html","link":"","permalink":"https://postgres.fun/20120221165748.html","excerpt":"","text":"RAID 分为硬件 RAID 和软件 RAID， 硬件 RAID 是通过 RAID 卡来实现的，软件RAID是通过软件实现的，企业级应用一般用硬件RAID， 今天只是学习下软件 RAID 的创建，管理等。 环境准备 虚拟机环境，添加两块 1GB IDE 盘，计划做 RAID0。 查看硬盘信息123456789101112131415[root@pgb ~]# fdisk -lDisk /dev/hda: 16.1 GB, 16106127360 bytes 255 heads, 63 sectors/track, 1958 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/hda1 * 1 1827 14675346 83 Linux /dev/hda2 1828 1958 1052257+ 82 Linux swap / SolarisDisk /dev/hdb: 1073 MB, 1073741824 bytes 16 heads, 63 sectors/track, 2080 cylinders Units = cylinders of 1008 * 512 = 516096 bytesDisk /dev/hdb doesn't contain a valid partition tableDisk /dev/hdd: 1073 MB, 1073741824 bytes 16 heads, 63 sectors/track, 2080 cylinders Units = cylinders of 1008 * 512 = 516096 bytesDisk /dev/hdd doesn't contain a valid partition table 备注： 虚拟计添加硬盘步骤略。 RAID 设备创建和管理1.1 创建软件 RAID 0123456789101112[root@pgb ~]# mdadm -C -v /dev/md1 -l0 -n2 /dev/hdb /dev/hdd mdadm: chunk size defaults to 64K mdadm: /dev/hdb appears to contain an ext2fs file system size=1048576K mtime=Thu Jan 1 08:00:00 1970 mdadm: /dev/hdd appears to contain an ext2fs file system size=1048576K mtime=Thu Jan 1 08:00:00 1970 Continue creating array? y mdadm: array /dev/md1 started.[root@pgb ~]# ll /dev/md* brw-r----- 1 root disk 9, 0 Feb 14 22:14 /dev/md0 brw-r----- 1 root disk 9, 1 Feb 14 23:21 /dev/md1 1.2 扫描 RAID 信息12[root@pgb ~]# mdadm -Ds ARRAY /dev/md1 level=raid0 num-devices=2 metadata=0.90 UUID=4140c28c:ace28b95:93c51a55:8451fbc3 1.3 停 /dev/md112[root@pgb ~]# mdadm -Ss mdadm: stopped /dev/md1 1.4 启动 RAID /dev/md11[root@pgb ~]# mdadm -A /dev/md1 /dev/hdb /dev/hddmdadm: /dev/md1 has been started with 2 drives. 1.5 查看硬盘 RAID 信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@pgb ~]# mdadm --examine /dev/hdb /dev/hdb: Magic : a92b4efc Version : 0.90.00 UUID : 4140c28c:ace28b95:93c51a55:8451fbc3 Creation Time : Sat Feb 14 23:22:06 2015 Raid Level : raid0 Used Dev Size : 0 Raid Devices : 2 Total Devices : 2 Preferred Minor : 1 Update Time : Sat Feb 14 23:22:06 2015 State : active Active Devices : 2 Working Devices : 2 Failed Devices : 0 Spare Devices : 0 Checksum : 592584c8 - correct Events : 1 Chunk Size : 64K Number Major Minor RaidDevice State this 0 3 64 0 active sync /dev/hdb 0 0 3 64 0 active sync /dev/hdb 1 1 22 64 1 active sync /dev/hdd [root@pgb ~]# mdadm --examine /dev/hdd /dev/hdd: Magic : a92b4efc Version : 0.90.00 UUID : 4140c28c:ace28b95:93c51a55:8451fbc3 Creation Time : Sat Feb 14 23:22:06 2015 Raid Level : raid0 Used Dev Size : 0 Raid Devices : 2 Total Devices : 2 Preferred Minor : 1 Update Time : Sat Feb 14 23:22:06 2015 State : active Active Devices : 2 Working Devices : 2 Failed Devices : 0 Spare Devices : 0 Checksum : 592584dd - correct Events : 1 Chunk Size : 64K Number Major Minor RaidDevice State this 1 22 64 1 active sync /dev/hdd 0 0 3 64 0 active sync /dev/hdb 1 1 22 64 1 active sync /dev/hdd 1.6 查看阵列 /dev/md1 信息12345678910111213141516171819202122[root@pgb ~]# mdadm -D /dev/md1 /dev/md1: Version : 0.90 Creation Time : Sat Feb 14 23:22:06 2015 Raid Level : raid0 Array Size : 2097024 (2048.22 MiB 2147.35 MB) Raid Devices : 2 Total Devices : 2 Preferred Minor : 1 Persistence : Superblock is persistent Update Time : Sat Feb 14 23:22:06 2015 State : clean Active Devices : 2 Working Devices : 2 Failed Devices : 0 Spare Devices : 0 Chunk Size : 64K UUID : 4140c28c:ace28b95:93c51a55:8451fbc3 Events : 0.1 Number Major Minor RaidDevice State 0 3 64 0 active sync /dev/hdb 1 22 64 1 active sync /dev/hdd 备注：从上面看出阵列 /dev/md1 由 /dev/hdb ， /dev/hdd 两个成员组成，UUID 为 4140c28c:ace28b95:93c51a55:8451fbc3 1.7 创建 RAID 配置文件 /etc/mdadm.conf配置文件 /etc/mdadm.conf 本不存在，需要手工创建，创建这个文件便于 RAID 设备的维护。 123[root@pgb ~]# mdadm -Ds ARRAY /dev/md1 level=raid0 num-devices=2 metadata=0.90 UUID=4140c28c:ace28b95:93c51a55:8451fbc3[root@pgb ~]# mdadm -Ds &gt;&gt; /etc/mdadm.conf 修改文件 /etc/mdadm.conf， 增加 device 内容，修改后内容如下12ARRAY /dev/md1 level=raid0 num-devices=2 metadata=0.90 UUID=4140c28c:ace28b95:93c51a55:8451fbc3 devices=/dev/hdb,/dev/hdd 备注：通过创建 /etc/mdadm.conf 文件后，那么启动RAID 时不需要指定 RAID设备和 RAID 成员。 1.8 重启阵列 /dev/md1 测试12345[root@pgb ~]# mdadm -Ssmdadm: stopped /dev/md1[root@pgb ~]# mdadm -As mdadm: /dev/md1 has been started with 2 drives.[root@pgb ~]# mdadm -Ds ARRAY /dev/md1 level=raid0 num-devices=2 metadata=0.90 UUID=4140c28c:ace28b95:93c51a55:8451fbc3 1.9 mdadm 常用参数12345678-A, --assemble 激活RAID -C, --create 创建RAID -s, --scan 扫描RAID设备 -S, --stop 停止正在运行的RAID 设备 RAID 设备使用接着介绍 RAID 设备分区，文件系统格式化，目录挂载。 2.1检查 RAID 设备是否存在12345[root@pgb ~]# fdisk -l /dev/md1Disk /dev/md1: 2147 MB, 2147352576 bytes 2 heads, 4 sectors/track, 524256 cylinders Units = cylinders of 8 * 512 = 4096 bytesDisk /dev/md1 doesn't contain a valid partition table 备注：这里看出 /dev/md1 设备容量为 2147MB。 2.2 RAID 文件系统格式化123456789101112131415161718192021[root@pgb ~]# mkfs -t 'ext3' -c /dev/md1 mke2fs 1.39 (29-May-2006) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) 262144 inodes, 524256 blocks 26212 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=536870912 16 block groups 32768 blocks per group, 32768 fragments per group 16384 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Checking for bad blocks (read-only test): done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 31 mounts or 180 days, whichever comes first. Use tune2fs -c or -i to override. 2.3 目录挂载123456789[root@pgb ~]# mkdir -p /database/pgdata1[root@pgb ~]# mount -t 'ext3' /dev/md1 /database/pgdata1[root@pgb ~]# df -hv Filesystem Size Used Avail Use% Mounted on /dev/hda1 14G 5.0G 8.0G 39% / tmpfs 217M 0 217M 0% /dev/shm none 217M 104K 217M 1% /var/lib/xenstored /dev/md1 2.0G 36M 1.9G 2% /database/pgdata1[root@pgb ~]# chown -R postgres:postgres /database 备注： 设备 /dev/md1 成功挂载，挂载点为 /database/pgdata1，容量 2 个 GB。 2.4 设置开机自动挂载1/dev/md1 /database/pgdata1 ext3 defaults 0 0 总结 今天只是学习下软件 RAID0 的创建与管理，纯属学习，RAID1，RAID10 有空时也练习下。 与硬件RAID相比，软件RAID 有很多不足，例如系统资源占用高，不支持硬盘执插拨，不支持远程阵列管理等。 企业级应用硬件 RAID 用得较多。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"MySQL_Fdw 实践: 宕机一例 ","slug":"20120220205211","date":"2012-02-20T12:52:11.000Z","updated":"2018-12-04T00:26:24.538Z","comments":true,"path":"20120220205211.html","link":"","permalink":"https://postgres.fun/20120220205211.html","excerpt":"","text":"这两天突然想起前段时间 MySQL 转 PG 项目实施过程中曾经导致过 PostgreSQL 宕机的情况，但很快 PG 自己恢复，下面来模拟下当时的情形。 1 环境信息PostgreSQL: 9.1.1MySQL : 5.5.15 2 创建测试表 (on MySQL )123456789101112131415161718192021222324252627mysql&gt; create table test_1 (id integer ,name varchar(32)); Query OK, 0 rows affected (0.02 sec)mysql&gt; desc test_1; +-------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+-------+ | id | int(11) | YES | | NULL | | | name | varchar(32) | YES | | NULL | | +-------+-------------+------+-----+---------+-------+ 2 rows in set (0.00 sec)mysql&gt; insert into test_1 values (1,'a'); Query OK, 1 row affected (0.00 sec)mysql&gt; insert into test_1 values (2,'b'); Query OK, 1 row affected (0.00 sec)mysql&gt; insert into test_1 values (3,'c'); Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_1; +------+------+ | id | name | +------+------+ | 1 | a | | 2 | b | | 3 | c | +------+------+ 3 rows in set (0.00 sec) 备注：为了演示，创建一张名为 test_1 的表，两个字段，并插入三条数据。 3 查看 foreign server ( On PostgreSQL)123456adsystem=# \\des List of foreign servers Name | Owner | Foreign-data wrapper -----------+----------+---------------------- mysql_svr | adsystem | mysql_fdw (1 row) 4 创建外部表 ( On PostgreSQL)12adsystem=# create foreign table ft_test_1 (id integer) server mysql_svr OPTIONS (database '51mrp_adsystem', table 'test_1'); CREATE FOREIGN TABLE 备注：在 PG 库中创建外部表时，故意只创建一张字段，与 MySQL 中的表 test_1 表结构不一样。 5 查询外部表 ( On PostgreSQL)12345adsystem=# select * from ft_test_1; The connection to the server was lost. Attempting reset: WARNING: terminating connection because of crash of another server process DETAIL: The postmaster has commanded this server process to roll back the current transaction and exit, because another server process exited abnormally and possibly corrupted shared memory. HINT: In a moment you should be able to reconnect to the database and repeat your command. Failed. 备注：当查询外部表 ft_test_1 时， PostgreSQL 服务出现连接中断的情况，当前 session 中断，查看数据库服务器日志如下。 6 csv 日志信息 ( On PostgreSQL)123456789101112132012-02-20 20:18:21.807 CST,,,13434,,4f41ebff.347a,2,,2012-02-20 14:45:19 CST,,0,LOG,00000,\"server process (PID 20995) was terminated by signal 11: Segmentation fault\",,,,,,,,,\"\" 2012-02-20 20:18:21.807 CST,,,13434,,4f41ebff.347a,3,,2012-02-20 14:45:19 CST,,0,LOG,00000,\"terminating any other active server processes\",,,,,,,,,\"\" 2012-02-20 20:18:21.810 CST,,,13439,,4f41ebff.347f,2,,2012-02-20 14:45:19 CST,1/0,0,WARNING,57P02,\"terminating connection because of crash of another server process\",\"The postmaster has commanded this server process to roll back the current transaction and exit, because another server process exited abnormally and possibly corrupted shared memory.\",\"In a moment you should be able to reconnect to the database and repeat your command.\",,,,,,,\"\" 2012-02-20 20:18:21.813 CST,\"adsystem\",\"adsystem\",21086,\"[local]\",4f423a0d.525e,1,\"authentication\",2012-02-20 20:18:21 CST,2/0,0,WARNING,57P02,\"terminating connection because of crash of another server process\",\"The postmaster has commanded this server process to roll back the current transaction and exit, because another server process exited abnormally and possibly corrupted shared memory.\",\"In a moment you should be able to reconnect to the database and repeat your command.\",,,,,,,\"\" 2012-02-20 20:18:21.815 CST,,,13434,,4f41ebff.347a,4,,2012-02-20 14:45:19 CST,,0,LOG,00000,\"all server processes terminated; reinitializing\",,,,,,,,,\"\" 2012-02-20 20:18:21.857 CST,,,21087,,4f423a0d.525f,1,,2012-02-20 20:18:21 CST,,0,LOG,00000,\"database system was interrupted; last known up at 2012-02-20 17:45:19 CST\",,,,,,,,,\"\" 2012-02-20 20:18:21.858 CST,,,21087,,4f423a0d.525f,2,,2012-02-20 20:18:21 CST,,0,LOG,00000,\"database system was not properly shut down; automatic recovery in progress\",,,,,,,,,\"\" 2012-02-20 20:18:21.872 CST,,,21087,,4f423a0d.525f,3,,2012-02-20 20:18:21 CST,,0,LOG,00000,\"consistent recovery state reached at 2/571E7284\",,,,,,,,,\"\" 2012-02-20 20:18:21.872 CST,,,21087,,4f423a0d.525f,4,,2012-02-20 20:18:21 CST,,0,LOG,00000,\"redo starts at 2/571E7284\",,,,,,,,,\"\" 2012-02-20 20:18:21.872 CST,,,21087,,4f423a0d.525f,5,,2012-02-20 20:18:21 CST,,0,LOG,00000,\"record with zero length at 2/571E871C\",,,,,,,,,\"\" 2012-02-20 20:18:21.872 CST,,,21087,,4f423a0d.525f,6,,2012-02-20 20:18:21 CST,,0,LOG,00000,\"redo done at 2/571E7284\",,,,,,,,,\"\" 2012-02-20 20:18:21.887 CST,,,13434,,4f41ebff.347a,5,,2012-02-20 14:45:19 CST,,0,LOG,00000,\"database system is ready to accept connections\",,,,,,,,,\"\" 2012-02-20 20:18:21.889 CST,,,21090,,4f423a0d.5262,1,,2012-02-20 20:18:21 CST,,0,LOG,00000,\"autovacuum launcher started\",,,,,,,,,\"\" 备注：PostgreSQL 出现短时间的中断，但又很快恢复正常，从日志文件来看，数据库首先是异常关闭，然后自动恢复正常，整个处理过程在 80 ms 左右，下面看下 PostgreSQL 正常的创建外部表的方法。 7 正确的方法 ( On PostgreSQL)12345678910111213adsystem=# drop foreign table ft_test_1; DROP FOREIGN TABLEadsystem=# create foreign table ft_test_1 (id integer, name varchar(32)) server mysql_svr OPTIONS (database '51mrp_adsystem', table 'test_1'); CREATE FOREIGN TABLEadsystem=# select * from ft_test_1; id | name ----+------ 1 | a 2 | b 3 | c (3 rows) 备注：当外部表和源 MySQL 表的表结构一致时，上述故障不再发生，查询正常。 8 总结 在使用 mysql_fdw 迁移 MySQL 库数据到 PG 时，外部表的结构和 MySQL 的表结构必需一致；这个一致包括三方面： 字段类型一致; 字段总数一样，否则查询外部表时可能导致 PG 宕机; 字段顺序一致，否则数据可能出现问题。 在做 MySQL 转 PG项目时，外部表创建脚本务必需要仔细检查，并在测试环境全面测试。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"mysql_fdw","slug":"mysql-fdw","permalink":"https://postgres.fun/tags/mysql-fdw/"}]},{"title":"记一次修改数据类型粗心操作导致数据异常","slug":"20120218105054","date":"2012-02-18T02:50:54.000Z","updated":"2018-09-04T01:33:54.398Z","comments":true,"path":"20120218105054.html","link":"","permalink":"https://postgres.fun/20120218105054.html","excerpt":"","text":"近在做一次 MySQL 迁 PG 的项目，晚上实施时犯了小错误，在对一个小表数据修改类型时，操作失误，还好开发人员发现及时，没对业务产生严重影响，后来在测试库上做了下测试，重现了整个过程，具体如下： 1 创建测试表12345678910111213CREATE TABLE ad_tmp ( xxxx bigint , xxxx character varying(20) , xxxx integer , xxxxxxx character varying(20) , platform character varying(20) , xxxxxxxxxxx integer , config character varying(20) , is_check character varying(20) , status character varying(20) , xxxxxxxxxxxxx bigint , xxxxxxxx bigint , xxxxxxxxxxx integer); 备注：创建表之后，插入了 200 多条数据，数据插入步骤略。 2 查看表数据12345678910111213mydb=&gt; select count(*) from ad_tmp; count ------- 275 (1 row)mydb=&gt; select distinct config from ad_tmp; config -------- 2 0 1 (3 rows) 3 更改表结构类型 ( character varying(20) –&gt; smallint )12mydb=&gt; alter table ad_tmp alter column config type smallint using platform::smallint; ALTER TABLE 4 字段类型修改后，核实数据12345mydb=&gt; select distinct config from ad_tmp; config -------- 3 (1 row) 备注： 字段 config 的类型由 character varying(20) 更改成 smallint 后，发现数据居然诡异都变成 3 了，细心的朋友可能已经发现这异常原因了，原因很简单，这里就不说明了。可以说是一次低级失误吧，今天记录下。 5 总结 生产库上的操作一定要谨慎，不管什么时候; 晚上数据库维护的脚本一定要仔细检查，包括每一步； 如果有临时情况发生需要修改脚本的地方，也需要仔细检查，切莫因为粗心影响整个项目实施。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL 整型字段 “Integer out of range” 一例","slug":"20120214154926","date":"2012-02-14T07:49:26.000Z","updated":"2018-09-04T01:33:53.835Z","comments":true,"path":"20120214154926.html","link":"","permalink":"https://postgres.fun/20120214154926.html","excerpt":"","text":"今天检查数据库时，发现一个日志库根目录使用率比较高，由于 /var/applog/pg_log 用来存放 postgresql 日志，猜测 postgresql 有大量日志，经查，果然目录 /var/applog/pg_log 下有大量日志，每天的日志有 2G 左右，且日志里有大量以下内容。 数据库 CSV 日志12012-02-14 14:35:30.640 CST,\"accessstat\",\"accessstat\",18107,\"192.168.169.71:43743\",4ed49343.46bb,5331,\"INSERT\",2011-11-29 16:09:39 CST,168/45813,0,ERROR,22003,\"integer out of range\",,,,,,\"INSERT INTO tbl_tmp(imsi, termipaddr, accessipaddr, networkID, conntype, smsCenter, cellId, dialTime, connectTime, dialRatio, connectRatio, exitReason, reconnresult) VALUES ('460028469626829', '117.136.7.71', '192.168.169.71', 0, 0, '', 0, 2306, 221, -1, -1, 1, 1); 备注：猜测可能有整型字段长度不够，于是查询手册确认下。 手册上 ERROR 代码表123456 Error CodeMeaningCondition Name22003NUMERIC VALUE OUT OF RANGEnumeric_value_out_of_range 执行报错的 SQL12345678910111213141516171819202122232425262728INSERT INTO tbl_tmp (xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx, xxxxxxxxxx) VALUES ('460021452274418', '117.136.7.72', '192.168.169.70', 0, 0, '8613800452500', 0, 56, 1098, -1, -1, 0, 0); 备注：从 INSERT 语句来看，推测是主键 seq 的问题，seq 类型为 integer。 查看表结构1234567891011121314151617181920212223accessstat=&gt; \\d tbl_tmp Table \"skytf.tbl_tmp\" Column | Type | Modifiers --------------+-----------------------+--------------------------------------------------------------------- seq | integer | not null default nextval('tbl_tmp_seq_seq'::regclass) xxxx | character varying(16) | not null xxxxxxxxxx | character varying(16) | not null xxxxxxxxxxxx | character varying(16) | not null xxxxxxxxx | smallint | not null xxxxxxxx | smallint | not null xxxxxxxxx | character varying(16) | not null xxxxxx | integer | not null xxxxxxxx | integer | not null xxxxxxxxxxx | integer | not null xxxxxxxxx | smallint | not null xxxxxxxxxxxx | smallint | not null xxxxxxxxxx | smallint | not null xxxxxxxxxxxx | smallint | not null xxxxxxxx | date | not null default now() Indexes: \"tbl_tmp_pkey\" PRIMARY KEY, btree (seq) Inherits: tbl_tmp Tablespace: \"tbs_accessstat_idx\" 查看序列 next 值12345accessstat=&gt; select nextval('tbl_tmp_seq_seq'); nextval ------------ 2148607080 (1 row) 数据类型 integer 范围是 ( -2147483648 to +2147483647 )12345accessstat=&gt; select 2148607080-2147483647; ?column? ---------- 1123433 (1 row) 备注：可见 pk 已经超出了 integer 的范围。 处理方法最有效的方法是将字段 seq 类型由 integer 扩为 bigint 类型，由于 pg 扩字段需要重写表，那么还得想办法清理历史数据，由于这张表是日志表，和开发人员确认后，可以清理历史数据，只保留当月数据即可。 更改字段类型( integer -&gt; bigint )123accessstat=&gt; alter table tbl_tmp alter column seq type bigint; ALTER TABLE Time: 83110.885 ms 备注：建议清理历史数据后再执行修改字段操作。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"Linux: Crontab 命令权限控制 ","slug":"20120213131906","date":"2012-02-13T05:19:06.000Z","updated":"2018-09-04T01:33:53.773Z","comments":true,"path":"20120213131906.html","link":"","permalink":"https://postgres.fun/20120213131906.html","excerpt":"","text":"crontab 用来任务定时调度，在 Linux 下可以通过创建文件 /etc/cron.allow 或者 /etc/cron.deny 来控制权限，如果 /etc/cron.allow 文件存在，那么只有这个文件中列出的用户可以使用 cron， 同时/etc/cron.deny文件被忽略； 如果 /etc/cron.allow 文件不存在，那么文件 /cron.deny 中列出的用户将不能用使用 cron。 下面简单演示下限制用户使用 crontab 命令的情况。 1 修改文件 /etc/cron.deny12[root@pgb etc]# ll /etc/cron.deny -rw-r--r-- 1 root root 10 Feb 12 21:52 /etc/cron.deny 备注：添加要限制的用户，只需要写入用户名即可。 2 /etc/cron.deny 文件内容12[root@pgb etc]# cat /etc/cron.deny postgres 3 重启 crond 服务12345[root@pgb cron]# service crond status crond (pid 3834) is running...[root@pgb etc]# service crond restart Stopping crond: [ OK ] Starting crond: [ OK ] 4 切换到 postgres 用户测试1234[root@pgb ~]# su - postgres [postgres@pgb ~]$ crontab -l You (postgres) are not allowed to use this program (crontab) See crontab(1) for more information 备注：切换到 postgres 用户后，就不能使用 crontab 命令了。 5 /var/log/cron 日志信息1Feb 12 21:46:25 pgb crontab[6209]: (postgres) AUTH (crontab command not allowed) 6 crontab 内容存放目录1234[root@pgb etc]# ll /var/spool/cron total 8 -rw------- 1 postgres root 15 Feb 12 21:52 postgres -rw------- 1 root root 111 Feb 12 15:18 root 7 root 用户的 crontab123456[root@pgb etc]# crontab -l ###ntpdate 5 * * * * /usr/sbin/ntpdate asia.pool.ntp.org ;/sbin/hwclock --systohc &gt;&gt; /root/sync_date.log 2&gt;&amp;1[root@pgb etc]# cat /var/spool/cron/root###ntpdate 5 * * * * /usr/sbin/ntpdate asia.pool.ntp.org ;/sbin/hwclock --systohc &gt;&gt; /root/sync_date.log 2&gt;&amp;1 [root@pgb etc]# 8 其它说明如果用户创建 crontab 任务，在脚本中应指定 SHELL，PATH，HOME 变量， 否则很容易出现脚本，命令找不到的情况。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"Linux: 同步系统时间和硬件时间","slug":"20120212151254","date":"2012-02-12T07:12:54.000Z","updated":"2018-12-04T00:33:09.640Z","comments":true,"path":"20120212151254.html","link":"","permalink":"https://postgres.fun/20120212151254.html","excerpt":"","text":"Linux 系统有两个时间，一个是操作系统时间，还有个硬件时间，并且这两个时间不会自动同步，其中系统时间可以通过ntpdate 命令来同步，硬件时钟可以通过 hwclock 命令来同步，下面是简单实验过程： 同步 Linux 系统时间1234567[root@pgb init.d]# date -s \" 2012-02-12 12:00:00\" Sun Feb 12 12:00:00 CST 2012[root@pgb init.d]# ntpdate asia.pool.ntp.org 12 Feb 14:15:21 ntpdate[3453]: step time server 116.193.83.174 offset 8115.182172 sec[root@pgb init.d]# date Sun Feb 12 14:15:22 CST 2012 更改硬件时钟1234[root@pgb init.d]# hwclock --show Fri 20 Feb 2015 06:54:27 PM CST -0.930090 seconds[root@pgb init.d]# /sbin/hwclock --systohc 显示当前硬件时钟12[root@pgb init.d]# hwclock --show Sun 12 Feb 2012 02:16:25 PM CST -0.953096 seconds 备注： –systohc set the hardware clock to the current system time 将硬件时间和当前系统时间保持同步。 加入 Crontab每小时同步系统时间和硬件时间，如下：12###ntpdate 5 * * * * /usr/sbin/ntpdate asia.pool.ntp.org ;/sbin/hwclock --systohc &gt;&gt; /root/sync_date.log 2&gt;&amp;1 附: 网络时间服务器微软公司授时主机(美国)time.windows.com台警大授时中心(台湾)asia.pool.ntp.org中科院授时中心(西安)210.72.145.44网通授时中心(北京)219.158.14.130 Hwclock 命令说明 NAME hwclock - query and set the hardware clock (RTC)SYNOPSIS hwclock -r or hwclock –show hwclock -w or hwclock –systohc hwclock -s or hwclock –hctosys hwclock -a or hwclock –adjust hwclock -v or hwclock –version hwclock –set –date=newdate hwclock –getepoch hwclock –setepoch –epoch=yearDESCRIPTION hwclock is a tool for accessing the Hardware Clock. You can display the current time, set the Hardware Clock to a specified time, set the Hardware Clock to the System Time, and set the System Time from the Hardware Clock. You can also run hwclock periodically to insert or remove time from the Hardware Clock to compensate for sys- tematic drift (where the clock consistently gains or loses time at a certain rate if left to run). Ntpdate 命令说明 NAME ntpdate - set the date and time via NTP Disclaimer: The functionality of this program is now available in the ntpd program. See the -q command line option in the ntpd - Network Time Protocol (NTP) daemon page. After a suitable period of mourning, the ntpdate program is to be retired from this distribution ntpdate sets the local date and time by polling the Network Time Protocol (NTP) server(s) given as the server arguments to determine the correct time. It must be run as root on the local host. A number of samples are obtained from each of the servers specified and a subset of the NTP clock filter and selection algorithms are applied to select the best of these. Note that the accuracy and reliability of ntpdate depends on the number of servers, the number of polls each time it is run and the interval between runs. 参考资料 http://blog.sina.com.cn/s/blog_721cd3390101anth.html http://www.cnblogs.com/kaiyuanlee/articles/1921634.html","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"Linux 如何修改 SSH 远程连接端口?","slug":"20120209200852","date":"2012-02-09T12:08:52.000Z","updated":"2018-09-04T01:33:53.663Z","comments":true,"path":"20120209200852.html","link":"","permalink":"https://postgres.fun/20120209200852.html","excerpt":"","text":"ssh 远程连接的默认端口为 22 ，为了提高安全性，经常需要修改 ssh 端口，下面是将 ssh 端口修改成 20104 的简单步骤，本文写的是简单的内容，高手就不需要看了。 修改 Sshd 服务配置文件修改 sshd 服务配置文件 “/etc/ssh/sshd_config”， 在尾部增加一行 Port=20104，注意 /etc/ssh 目录下还有个 ssh_config 配置文件，这是 ssh 客户端配置文件。 重启 Sshd 服务1234567891011121314[root@pgb ~]# netstat -anp | grep ssh tcp 0 0 :::20103 :::* LISTEN 2829/sshd unix 2 [ ACC ] STREAM LISTENING 10394 3684/ssh-agent /tmp/ssh-GXpRDx3649/agent.3649 unix 2 [ ACC ] STREAM LISTENING 70931 1540/2 /tmp/ssh-dobWNI1540/agent.1540 unix 2 [ ACC ] STREAM LISTENING 73624 2838/3 /tmp/ssh-EqhrRm2838/agent.2838[root@pgb ~]# service sshd restartStopping sshd: [ OK ] Starting sshd: [ OK ][root@pgb ~]# netstat -anp | grep ssh tcp 0 0 :::20104 :::* LISTEN 2897/sshd unix 2 [ ACC ] STREAM LISTENING 10394 3684/ssh-agent /tmp/ssh-GXpRDx3649/agent.3649 unix 2 [ ACC ] STREAM LISTENING 70931 1540/2 /tmp/ssh-dobWNI1540/agent.1540 unix 2 [ ACC ] STREAM LISTENING 73624 2838/3 /tmp/ssh-EqhrRm2838/agent.2838 备注： sshd 服务重启后， sshd 服务端口已成为 20104 了。 修改防火墙配置/etc/sysconfig/iptables 文件增加以下一行：1-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 20104 -j ACCEPT 下面是本机的 “/etc/sysconfig/iptables” 文件内容。1234567891011121314151617181920212223242526# Firewall configuration written by system-config-securitylevel # Manual customization of this file is not recommended. *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] :RH-Firewall-1-INPUT - [0:0] -A INPUT -j RH-Firewall-1-INPUT -A FORWARD -j RH-Firewall-1-INPUT -A RH-Firewall-1-INPUT -i lo -j ACCEPT -A RH-Firewall-1-INPUT -p icmp --icmp-type any -j ACCEPT -A RH-Firewall-1-INPUT -p 50 -j ACCEPT -A RH-Firewall-1-INPUT -p 51 -j ACCEPT -A RH-Firewall-1-INPUT -p udp --dport 5353 -d 224.0.0.251 -j ACCEPT -A RH-Firewall-1-INPUT -p udp -m udp --dport 631 -j ACCEPT -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 631 -j ACCEPT -A RH-Firewall-1-INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 1921 -j ACCEPT -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 1922 -j ACCEPT -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 1923 -j ACCEPT -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 1929 -j ACCEPT -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 1930 -j ACCEPT -A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 20104 -j ACCEPT -A RH-Firewall-1-INPUT -j REJECT --reject-with icmp-host-prohibited COMMIT 4 重载防火墙配置12[root@pgb ~]# iptables-restore /etc/sysconfig/iptables [root@pgb ~]# 测试这里是通过使用 Windows 下的 SecueCRT 远程登陆工具测试的，测试通过。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"PostgreSQL: 如何删除单表重复数据？ ","slug":"20120206161509","date":"2012-02-06T08:15:09.000Z","updated":"2018-09-04T01:33:53.616Z","comments":true,"path":"20120206161509.html","link":"","permalink":"https://postgres.fun/20120206161509.html","excerpt":"","text":"今天在导入一张表数据时遇到重复数据问题，以前在维护 Oracle 时也遇到过， Oracle 库去重的方法很多，常用的是根据 rowid 进行去重，那么 PostgreSQL 库如何去除单表重复数据呢，可以通过 ctid 进行，下面是实验过程： 关于 ctid 的解释，可以参考之前的blog: https://postgres.fun/20101210100526.html 环境准备创建测试表并插入初始数据，如下：123456789101112131415161718mydb=&gt; create table test_name (id integer,name varchar(32)); CREATE TABLEmydb=&gt; insert into test_name values (1,'francs'); INSERT 0 1 mydb=&gt; insert into test_name values (1,'francs'); INSERT 0 1 mydb=&gt; insert into test_name values (1,'francs'); INSERT 0 1 mydb=&gt; insert into test_name values (2,'fpZhou'); INSERT 0 1 mydb=&gt; insert into test_name values (2,'fpZhou'); INSERT 0 1 mydb=&gt; insert into test_name values (3,'A'); INSERT 0 1 mydb=&gt; insert into test_name values (4,'B'); INSERT 0 1 mydb=&gt; insert into test_name values (5,'C'); INSERT 0 1 备注：实验场景，目标是去除 id 字段重复的数据。 查询初始化数据123456789101112mydb=&gt; select * from test_name; id | name ----+-------- 1 | francs 1 | francs 1 | francs 2 | fpZhou 2 | fpZhou 3 | A 4 | B 5 | C (8 rows) 删除重复数据查询重复数据情况123456mydb=&gt; select distinct id ,count(*) from test_name group by id having count(*) &gt; 1; id | count ----+------- 1 | 3 2 | 2 (2 rows) 备注：上面查询出 id 为1 的记录有 3条， id 为 2的记录有两条。 查询重复的数据（即将删除的数据）123456789mydb=&gt; select * from test_name a where a.ctid not in (select min(ctid) from test_name b group by id); id | name ----+-------- 1 | francs 1 | francs 2 | fpZhou (3 rows) 备注：上面查询列出重复的数据（即将删除的数据）。 12345678910111213mydb=&gt; select ctid,* from test_name where name='francs'; ctid | id | name -------+----+-------- (0,1) | 1 | francs (0,2) | 1 | francs (0,3) | 1 | francs (3 rows)mydb=&gt; select min(ctid) from test_name where name='francs'; min ------- (0,1) (1 row) 删除重复的数据123mydb=&gt; delete from test_name a mydb-&gt; where a.ctid not in (select min(ctid) from test_name b group by id); DELETE 3 查询验证123456789mydb=&gt; select * from test_name order by id; id | name ----+-------- 1 | francs 2 | fpZhou 3 | A 4 | B 5 | C (5 rows) 上述实验是通过表记录的 tid 来进行去重的，也可以根据业务需要，对表上的字段进行分析做为去重字段。 嵌套SQL删除重复数据也可以使用以下SQL 删除重复数据123456delete from test_name a where a.ctid &lt;&gt; ( select max(b.ctid) from test_name b where a.id = b.id ); 备注：在表数据量较大的情况下，这种删除方法效率很高。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: Oldest xmin is far in the past 处理一例","slug":"20120118174735","date":"2012-01-18T09:47:35.000Z","updated":"2018-09-04T01:33:53.554Z","comments":true,"path":"20120118174735.html","link":"","permalink":"https://postgres.fun/20120118174735.html","excerpt":"","text":"快过年了，今天对数据库进行健康检查，发现海外一数据库中出现大量以下日志，而且已经报了十几天，CSV 日志如下： 数据库 CSV 日志12342012-01-18 15:22:39.098 CST,,,8871,,4f16733e.22a7,1,,2012-01-18 15:22:38 CST,81/42114625,0,WARNING,01000,\"oldest xmin is far in the past\",,\"Close open transactions soon to avoid wraparound problems.\",,,,,, 2012-01-18 15:22:39.109 CST,,,8871,,4f16733e.22a7,2,,2012-01-18 15:22:38 CST,81/42114627,0,WARNING,01000,\"oldest xmin is far in the past\",,\"Close open transactions soon to avoid wraparound problems.\",,,,,, 2012-01-18 15:22:39.119 CST,,,8871,,4f16733e.22a7,3,,2012-01-18 15:22:38 CST,81/42114629,0,WARNING,01000,\"oldest xmin is far in the past\",,\"Close open transactions soon to avoid wraparound problems.\",,,,,, 2012-01-18 15:22:39.119 CST,,,8871,,4f16733e.22a7,4,,2012-01-18 15:22:38 CST,81/42114631,0,WARNING,01000,\"oldest xmin is far in the past\",,\"Close open transactions soon to avoid wraparound problems.\",,,,,, 备注：根据提示信息， 猜测有尚未提交的事务，导致 autovacuum 进程不能 vacuum。 查询数据库对像年龄1234567891011121314postgres=# select age(relfrozenxid) from pg_class where relkind='r' order by 1 desc limit 10; age ----------- 341105277 341105277 341105277 341105277 341105277 341105277 341105277 315987817 315987814 315987811 (10 rows) 备注：数据库对像年龄已经达到 3 亿。 发现异常进程12345678910111213141516171819202122232425262728postgres=# select procpid, datname,usename,query_start from pg_stat_activity where current_query !='&lt;IDLE&gt;' order by query_start; procpid | datname | usename | query_start ---------+------------+----------+------------------------------- 27694 | db_oversea | oversea | 2011-09-12 07:35:56.615883+08 5019 | db_oversea | oversea | 2012-01-18 15:40:47.063949+08 5927 | db_oversea | oversea | 2012-01-18 15:40:47.290211+08 1552 | db_oversea | oversea | 2012-01-18 15:40:47.32765+08 8227 | db_oversea | oversea | 2012-01-18 15:40:47.381145+08 7758 | db_oversea | oversea | 2012-01-18 15:40:47.391125+08 7759 | db_oversea | oversea | 2012-01-18 15:40:47.45002+08 8094 | db_oversea | oversea | 2012-01-18 15:40:47.812721+08 4804 | db_oversea | oversea | 2012-01-18 15:40:47.821056+08 6689 | db_oversea | oversea | 2012-01-18 15:40:47.90369+08 8506 | db_oversea | oversea | 2012-01-18 15:40:47.915079+08 9039 | postgres | postgres | 2012-01-18 15:43:47.834318+08 (12 rows) postgres=# select procpid,datname,usename,current_query,query_start from pg_stat_activity where procpid=27694; procpid | datname | usename | current_query | query_start ---------+------------+---------+----------------------------------------------------+------------------------------- 27694 | skytf | skytf | SELECT | 2011-09-12 07:35:56.615883+08 : mrp.*,res.name as displayname : FROM : skytf.tbl_app as mrp,tbl_app_resource as res : where mrp.appid=res.appid and : mrp.status='1' and res.status='1' : order by id desc 备注：上面发现进程 27694 是去年的，至今仍未提交，经和应用人员确认后，可以 kill。 Kill 进程 2769412345postgres=# select pg_terminate_backend(27694); pg_terminate_backend ---------------------- t (1 row) 发现 Autovacuum 进程12345678910[postgres@skytf-db](mailto:postgres@skytf-db)-&gt; ps -ef | grep auto root 3523 1 0 2010 ? 00:00:01 automount postgres 4002 3949 0 2010 ? 00:10:54 postgres: autovacuum launcher process postgres 10001 3949 0 16:16 ? 00:00:00 postgres: autovacuum worker process skytf postgres 10002 3949 0 16:16 ? 00:00:00 postgres: autovacuum worker process skytf postgres 10003 3949 0 16:16 ? 00:00:00 postgres: autovacuum worker process skytf postgres 10004 3949 0 16:16 ? 00:00:00 postgres: autovacuum worker process skytf postgres 10005 3949 0 16:16 ? 00:00:00 postgres: autovacuum worker process skytf postgres 10006 3949 0 16:16 ? 00:00:00 postgres: autovacuum worker process skytf postgres 10007 3949 0 16:16 ? 00:00:00 postgres: autovacuum worker process skytf 进程 kill 后再次查看年龄1234567891011121314postgres=# select age(relfrozenxid) from pg_class where relkind='r' order by 1 desc limit 10; age ---------- 50003395 50003393 50003382 50003381 50003381 50003381 50003381 50003374 50003374 50003374 (10 rows) 备注：进程 kill 后再次查看年龄，数据库对像年龄已经循环，当前最大值为 50003395，并且日志也不再报错。 参考http://www.postgresql.org/docs/9.0/static/routine-vacuuming.html#VACUUM-FOR-WRAPAROUND","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL: 如何查询表和索引的表空间","slug":"20120113155633","date":"2012-01-13T07:56:33.000Z","updated":"2018-09-04T01:33:53.491Z","comments":true,"path":"20120113155633.html","link":"","permalink":"https://postgres.fun/20120113155633.html","excerpt":"","text":"在数据库运维工作中，经常会有数据目录使用率较高需要调整的情况，通常会给数据库建立多个表空间，并分别位于不同的盘上，这时需要做的工作就是调整库中现有表和索引的表空间，下面简单总结下这块维护工作的内容，以下都是基于 PostgreSQL 9.0.1 做的测试。 查询表的表空间PostgreSQL 提供类似” “命令很方便得到相关信息，命令如下：1234567891011skytf=&gt; \\d test_2 Table \"skytf.test_2\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | obj_id | integer | not null name | character varying(64) | Indexes: \"idx_hash_name\" hash (name) \"idx_test_2\" btree (id, obj_id) Tablespace: \"tbs_skytf_idx\" 备注：如果这个表的表空间为当前数据库的默认表空间，那么上面则不会显示 Tablespace 信息，相反，则会显示这张有的表空间，例如上面的表 test_2 的表空间为 tbs_skytf_idx，而表空间 “tbs_skytf_idx” 不是数据库 skytf 的默认表空间， 那么如何查询数据库的默认表空间呢，可以通过以下命令查询。 查询数据库的默认表空间1234567891011skytf=&gt; select datname,dattablespace from pg_database where datname='skytf'; datname | dattablespace ---------+--------------- skytf | 14203070 (1 row)skytf=&gt; select oid,spcname from pg_tablespace where oid=14203070; oid | spcname ----------+----------- 14203070 | tbs_skytf (1 row) 备注：通过以上查出数据库 skytf 的默认表空间为 tbs_skytf。 批量查询表和索引的表空间查询表和索引所在的表空间12345select relname, relkind, relpages,pg_size_pretty(pg_relation_size(a.oid)), tb.spcname from pg_class a, pg_tablespace tb where a.reltablespace = tb.oid and a.relkind in ('r', 'i') order by a.relpages desc; 备注：上面只取了部分结果，这个查询能够查询表和索引所处的表空间，但是有一点需要注意，这个查询仅显示表空间不是数据库默认表空间的数据库对像，而我们通常需要查出位于数据库默认表空间的对像，显然上面的查询不是我们想要的，接下来看另一个查询。 查询位于默认数据库表空间的对像12345select relname, relkind, relpages,pg_size_pretty(pg_relation_size(a.oid)),reltablespace,relowner from pg_class a where a.relkind in ('r', 'i') and reltablespace='0' order by a.relpages desc; 备注：这个查询加入限制条件 reltablespace=’0’，即可查找出位于当前数据库默认表空间的数据库表和索引。 通常这才是我们想要的结果，接下来可以把部分表转移到其它表空间上去，转移的方法可以用 “ALTER TABLE move tablespace “或者重建索引移表空间等方法，这里不详细介绍。 查询在某个表空间上的对像123456select relname, relkind, relpages,pg_size_pretty(pg_relation_size(a.oid)),reltablespace,relowner from pg_class a, pg_tablespace tb where a.relkind in ('r', 'i') and a.reltablespace=tb.oid and tb.spcname='tablespace_name' order by a.relpages desc; 关于 reltablespace手册上对于 pgclass 视图的 reltablespace 字段解释 The tablespace in which this relation is stored. If zero, the database is default tablespace isimplied. (Not meaningful if the relation has no on-disk file.)","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"How to drop a role in PostgreSQL Server ?","slug":"20120109212600","date":"2012-01-09T13:26:00.000Z","updated":"2018-09-04T01:33:53.444Z","comments":true,"path":"20120109212600.html","link":"","permalink":"https://postgres.fun/20120109212600.html","excerpt":"","text":"在 PostgreSQL 数据库中，” role “ 可以理解为 user，即数据库用户， 当删除 PostgreSQL 的角色时， PostgreSQL 会谨慎对待，当这个用户还拥有数据库对像，或者这个用户在某些数据库对像上还拥有权限时，则不能删除，下面简单演示下： 环境准备创建新用户并赋予权限12345678CREATE ROLE readonly LOGIN ENCRYPTED PASSWORD 'readonly' nosuperuser noinherit nocreatedb nocreaterole ;grant connect on database skytf to readonly; grant usage on schema skytf to readonly; grant select on all tables in schema skytf to readonly;grant connect on database beha_db to readonly; grant usage on schema beha_db to readonly; grant select on all tables in schema beha_db to readonly; 备注：这步创建一个新用户 readonly，并赋予数据库 skytf 和 beha_db 所有表的查询权限。 查询用户 readonly 权限信息123456789101112131415161718192021222324252627skytf=&gt; \\c beha_db beha_db You are now connected to database \"beha_db\" as user \"beha_db\".select grantor,grantee,table_schema,table_name,privilege_type from information_schema.table_privileges where grantee='readonly'; grantor | grantee | table_schema | table_name | privilege_type -----------+----------+--------------+----------------------------------+---------------- beha_db | readonly | beha_db | tbl_beha_db_stat_20111119 | SELECT beha_db | readonly | beha_db | tbl_beha_db_stat_20111123 | SELECT beha_db | readonly | beha_db | tbl_beha_db_stat_20111225 | SELECT beha_db | readonly | beha_db | tbl_beha_db_stat_20120210 | SELECT beha_db | readonly | beha_db | tbl_beha_db_stat_20120328 | SELECT beha_db | readonly | beha_db | tbl_beha_db_stat_20120514 | SELECT beha_db | readonly | beha_db | tbl_beha_db_stat_20120701 | SELECT beha_db=&gt; \\c skytf skytf You are now connected to database \"skytf\" as user \"skytf\".skytf=&gt; select grantor,grantee,table_schema,table_name,privilege_type from skytf-&gt; information_schema.table_privileges where grantee='readonly'; grantor | grantee | table_schema | table_name | privilege_type ---------+----------+--------------+------------------------------+---------------- skytf | readonly | skytf | test_65 | SELECT skytf | readonly | skytf | table_b | SELECT skytf | readonly | skytf | test_null | SELECT skytf | readonly | skytf | test_14 | SELECT skytf | readonly | skytf | test_aesc | SELECT 尝试删除用户尝试删除用户 readonly，如下：123456postgres=# drop role readonly; ERROR: role \"readonly\" cannot be dropped because some objects depend on it DETAIL: privileges for database beha_db privileges for database skytf 2506 objects in database beha_db 265 objects in database skytf 备注：当删除用户 readonly 时， ERROR 产生，信息如上，根据提示信息，很容易知道是因为用户 readonly 上还拥有数据库对像，那么如何处理以上问题呢，下面提供两种方法： 方法一：使用 “DROP OWNED” 删除用户对像和权限Drop owned 命令用来删除用户的对像信息和权限信息，这个命令具有很大风险，因为此命令作用是删除用户下的所有对像，使用此命令时，务必谨慎，不要删错数据库对像，关于此命令详细信息，请参考文档 http://www.postgresql.org/docs/9.1/static/sql-drop-owned.html ，下面继续演示： 连接 beha_db 库，删除用户 readonly 的权限信息12345beha_db=&gt; \\c beha_db postgres You are now connected to database \"beha_db\" as user \"postgres\". beha_db=# drop owned by readonly; DROP OWNED 连接 skytf 库，删除用户 readonly 的权限信息12345postgres=# \\c skytf postgres You are now connected to database \"skytf\" as user \"postgres\".. skytf=# drop owned by readonly; DROP OWNED 尝试再次删除用户 readonly123postgres=# drop role readonly; ERROR: role \"readonly\" cannot be dropped because some objects depend on it DETAIL: privileges for database beha_db 备注：提示用户 readonly 还有权限信息存在，删除失败。 删除 readonly 用户的数据库连接权限1234postgres=# revoke connect on database beha_db from readonly; REVOKE postgres=# revoke connect on database skytf from readonly; REVOKE 再次删除用户，终于删除成功12postgres=# drop role readonly; DROP ROLE 方法二：REVOKE 权限信息创建用户并赋予权限12345CREATE ROLE readonly LOGIN ENCRYPTED PASSWORD 'readonly' nosuperuser noinherit nocreatedb nocreaterole ;grant connect on database skytf to readonly; grant usage on schema skytf to readonly; grant select on all tables in schema skytf to readonly; REVOKE权限12345678skytf=# \\c skytf skytf You are now connected to database \"skytf\" as user \"skytf\".skytf=&gt; revoke connect on database skytf from readonly; REVOKE skytf=&gt; revoke select on all tables in schema skytf from readonly; REVOKEskytf=&gt; revoke usage on schema skytf from readonly; REVOKE 删除用户12postgres=# drop role readonly; DROP ROLE 备注：在删除用户前，需先删除用户的对像和权限信息，才能删除用户，关于这一点，可以参考手册上的 附：手册上的解释 Drop Role A role cannot be removed if it is still referenced in any database of the cluster; an error will beraised if so. Before dropping the role, you must drop all the objects it owns (or reassign their ownership)and revoke any privileges the role has been granted. 总结PostgreSQL 在处理删除用户的动作时持谨慎态度，当删除数据库时，首先得保证被删除的用户没有数据库对像，同时 被删除的用户不拥有任何数据库对像的权限，只有保证了以上两点才能删除成功，而 Oracle 处理此问题没有这么复杂，当删除用户时，提供一个 cascade 属性，删除用户时可以级连删除这个用户拥有的数据库一切对像。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 如何处理导出的数据出现中文乱码问题？ ","slug":"20120106214224","date":"2012-01-06T13:42:24.000Z","updated":"2018-09-04T01:33:53.382Z","comments":true,"path":"20120106214224.html","link":"","permalink":"https://postgres.fun/20120106214224.html","excerpt":"","text":"在数据库维护过程中，经常有需求导出生产库的部分数据，并且要求将数据保存为 Excel 形式， 对于 Oracle 来说，这是非常简单的工作，因为有 plsqldev 等图形化工具，且功能非常强大，可以导出 Excel， 对于 PG 来说，这方面的支持就少了，首先图形化界面工具少，另外常用的PG GUI工具 pgadmin 功能非常简单，没有导出数据到 excel 的功能，尽管如此, PostgreSQL 本身提供的 copy 命令可以实现此功能，可以将数据导成 csv 格式。但是在某些 Linux 环境下导出的 csv 文件，传输到 windows 环境下打开中文却显示乱码， 本文介绍两种解决 csv 文件在 windows 下显示为乱码的方法。 方法一: 设置客户端字符集一般 PostgreSQL 建库都是用的 UTF8 字符集， 在 UTF8 字符集情况下如果中文不能正常显示，可以设置客户端字符集，修改成 “ GBK “ ，命令如下： 修改客户端字符集12345678postgres=# show client_encoding; client_encoding ----------------- UTF8 (1 row)postgres=# set client_encoding='GBK'; SET copy 导出数据到 GBK 编码类型的 csv 文件12skytf=# copy skytf.test_2 to '/home/postgres/script/tf/skytf.test_2.csv' with csv header; COPY 1000000 备注：此时通过 sftp 将 csv 文件传到 windows 本机，就不会出现乱码了。 方法二: 使用 Iconv 更改文件编码iconv 是 linux 命令，用来转换文件的编码的 ，手册解释如下 “Convert encoding of given files from one encoding to another”，我们可以使用 iconv 命令转换文件的编码，如果 utf8 编码的文件中文显示为乱码，可以使用 iconv 命令将 UTF8 格式文件转换成 gb18030，参考步骤： 导出数据到 utf8 编码类型文件。1234567891011skytf=# set client_encoding='UTF8'; SETskytf=# show client_encoding; client_encoding ----------------- UTF8 (1 row)skytf=# copy skytf.test_2 to '/home/postgres/script/tf/skytf.test_2.csv' with csv header; COPY 1000000 将文件编码由 utf8 转换成 gb180301iconv -f utf-8 -t gb18030 skytf.test_2.csv -o skytf.test_2_gbk.csv 附：Iconv 命令参考12345678910111213141516171819[postgres@tf]$ iconv --help Usage: iconv [OPTION...] [FILE...] Convert encoding of given files from one encoding to another.Input/Output format specification: -f, --from-code=NAME encoding of original text -t, --to-code=NAME encoding for outputInformation: -l, --list list all known coded character setsOutput control: -c omit invalid characters from output -o, --output=FILE output file -s, --silent suppress warnings --verbose print progress information -?, --help Give this help list --usage Give a short usage message -V, --version Print program versionMandatory or optional arguments to long options are also mandatory or optional for any corresponding short options.For bug reporting instructions, please see:","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 新特性之六 : mysql_fdw 实践","slug":"20111221133204","date":"2011-12-21T05:32:04.000Z","updated":"2018-09-04T01:33:53.335Z","comments":true,"path":"20111221133204.html","link":"","permalink":"https://postgres.fun/20111221133204.html","excerpt":"","text":"最近有个 MySql 项目需要转成 PG， 这段时间在做迁移联调， PostgreSQL 9.1 之后支持多种外部表，可以打通与多类数据库的连接，例如 oracle, mysql, nosql 等，同时还可以打通与文本文件的连接，可以在数据库里读文件内容，这里介绍下 PostgreSQL 打通与 Mysql 连接的方法，即 mysql_fdw 的应用。 硬件环境笔记本虚拟机PostgreSQL IP 192.168.1.26/1921MysSQL IP 192.168.1.25/3306备注：PostgreSQL 版本 9.1 or later ，PostgreSQL 和 Mysql 安装本文省略。 MySQL库准备MySQL 库上创建数据库和表，如下：1234567891011121314151617181920212223242526272829mysql&gt; create database skytf; Query OK, 1 row affected (0.02 sec)mysql&gt; use skytf; Database changedmysql&gt; create table test_1 (id integer,name varchar(32)); Query OK, 0 rows affected (0.02 sec)mysql&gt; insert into test_1 values (1,'a'); Query OK, 1 row affected (0.00 sec)mysql&gt; insert into test_1 values (2,'b'); Query OK, 1 row affected (0.00 sec)mysql&gt; insert into test_1 values (3,'c'); Query OK, 1 row affected (0.00 sec)mysql&gt; select * from test_1; +------+------+ | id | name | +------+------+ | 1 | a | | 2 | b | | 3 | c | +------+------+ 3 rows in set (0.00 sec) mysql&gt; grant select on skytf.* to ['skytf'@'192.168.%.%'](mailto:'skytf'@'192.168.%.%') identified by 'skytf'; Query OK, 0 rows affected (0.03 sec)mysql&gt; flush privileges; Query OK, 0 rows affected (0.00 sec) 配置 PostgreSQL4.1 增加 PostgreSQL 环境变量1234export PGHOME=/opt/pgsql9.1 export MYSQLHOME=/opt/mysqlexport LD_LIBRARY_PATH=$MYSQLHOME/lib:$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib export PATH=$MYSQLHOME/bin:$PGHOME/bin:/opt/pgbouncer1.4.2/bin:$PATH:. 备注： 修改好后, 执行 source .bash_profile。 mysql_fdw 部署mysql_fdw 下载下载地址： http://pgxn.org/dist/mysql_fdw/ 之后解压到目录 /opt/soft_bak/mysql_fdw-1.0.0 编译并安装123# cd /opt/soft_bak/mysql_fdw-1.0.0 # make USE_PGXS=1 # make USE_PGXS=1 install 详细信息1234567891011[root@pgb mysql_fdw-1.0.0]# make USE_PGXS=1gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wformat-security -fno-strict-aliasing -fwrapv -fpic -I/opt/mysql/include -I. -I. -I/opt/pgsql9.1/include/server -I/opt/pgsql9.1/include/internal -D_GNU_SOURCE -I/usr/include/libxml2 -c -o mysql_fdw.o mysql_fdw.c mysql_fdw.c: In function ‘mysqlPlanForeignScan’: mysql_fdw.c:370: 警告：此函数中的 ‘rows’ 在使用前可能未初始化 gcc -O2 -Wall -Wmissing-prototypes -Wpointer-arith -Wdeclaration-after-statement -Wendif-labels -Wformat-security -fno-strict-aliasing -fwrapv -fpic -shared -o mysql_fdw.so mysql_fdw.o -L/opt/pgsql9.1/lib -L/usr/lib -Wl,-rpath,'/opt/pgsql9.1/lib',--enable-new-dtags -rdynamic -L/opt/mysql/lib -lmysqlclient -lz -lcrypt -lnsl -lm -lmygcc[root@pgb mysql_fdw-1.0.0]# make USE_PGXS=1 install /bin/mkdir -p '/opt/pgsql9.1/lib' /bin/mkdir -p '/opt/pgsql9.1/share/extension' /bin/sh /opt/pgsql9.1/lib/pgxs/src/makefiles/../../config/install-sh -c -m 755 mysql_fdw.so '/opt/pgsql9.1/lib/mysql_fdw.so' /bin/sh /opt/pgsql9.1/lib/pgxs/src/makefiles/../../config/install-sh -c -m 644 ./mysql_fdw.control '/opt/pgsql9.1/share/extension/' /bin/sh /opt/pgsql9.1/lib/pgxs/src/makefiles/../../config/install-sh -c -m 644 ./mysql_fdw--1.0.sql '/opt/pgsql9.1/share/extension/' [root@pgb mysql_fdw-1.0.0]# ll 备注：安装成功后会在目录 /opt/pgsql9.1/share/extension/ 产生 mysql_fdw.so , mysql_fdw.control , mysql_fdw–1.0.sql 文件。 创建 extension mysql_fdw12skytf=# CREATE EXTENSION mysql_fdw; CREATE EXTENSION 可能出现的ERROR12345ERROR: could not open extension control file \"/opt/pgsql9.1/share/extension/mysql_mdw.control\": No such file or directory postgres=#ERROR: could not stat file \"/opt/pgsql9.1/share/extension/mysql_mdw--1.0.sql\": No such file or directory postgres=# create extension mysql_mdw;ERROR: could not load library \"/opt/pgsql9.1/lib/mysql_fdw.so\": libmysqlclient.so.16: cannot open shared object file: No such file or directory 备注：在创建 extension 时可能出现以上 ERROR， 这时只要将复制一份以上文件即可。 创建 server 赋权给普通用户1234567skytf=# CREATE SERVER mysql_svr_25 skytf-# FOREIGN DATA WRAPPER mysql_fdw skytf-# OPTIONS (address '192.168.1.25', port '3306'); CREATE SERVERskytf=# grant usage on foreign server mysql_svr_25 to skytf; GRANT 备注：在赋权限后，普通用户 skytf 就可以使用 foreign server mysql_svr_25 创建外部表了。 查看 server12345678skytf=# \\c skytf skytf You are now connected to database \"skytf\" as user \"skytf\". skytf=&gt; \\des List of foreign servers Name | Owner | Foreign-data wrapper --------------+----------+---------------------- mysql_svr_25 | postgres | mysql_fdw 创建匹配用户 ( mapping user )1234skytf=# CREATE USER MAPPING FOR skytf skytf-# SERVER mysql_svr_25 skytf-# OPTIONS (username 'skytf', password 'skytf'); CREATE USER MAPPING 备注：如果不创建 mapping user ， 会出现 ERROR: user mapping not found for “skytf” 类似信息。 查看匹配用户123456skytf=&gt; \\deu List of user mappings Server | User name --------------+----------- mysql_svr_25 | skytf (1 row) 创建外部表123456skytf=&gt; CREATE FOREIGN TABLE test_1 ( skytf(&gt; id integer, skytf(&gt; name character varying(20) skytf(&gt; ) SERVER mysql_svr_25 skytf-&gt; OPTIONS (database 'skytf', table 'test_1'); CREATE FOREIGN TABLE 备注：到了这里就已经打通了 Mysql 和 PostgreSQL 的连接。 查询1234567skytf=&gt; select * from test_1; id | name ----+------ 1 | a 2 | b 3 | c (3 rows) 备注：果然可以查到了 Mysql 数据库的数据了。 总结 利用 mysql_fdw 可以创建 PostgreSQL 与 Mysql 的连接，可以实现 PG 与 Mysql 的数据同步； 在 Mysql 转 PG 项目中，mysql_fdw 是非常有效的方法，具体迁移效率之后再做测试。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"mysql_fdw","slug":"mysql-fdw","permalink":"https://postgres.fun/tags/mysql-fdw/"}]},{"title":"PgBouncer 参数解释之一 server_reset_query","slug":"20111211170614","date":"2011-12-11T09:06:14.000Z","updated":"2018-09-04T01:33:53.272Z","comments":true,"path":"20111211170614.html","link":"","permalink":"https://postgres.fun/20111211170614.html","excerpt":"","text":"Pgbouncer 有个重要参数为 server_reset_query ，这个参数可以控制 pgbouncer 连接池复用的行为，起初对这个参数的意思感到非常的不理解，今天做了下实验，总算对这个参数明白一二。 手册上的解释手册上 server_reset_query 的参数解释如下： Query sent to server on connection release, before making it available to other clients. At that moment no transaction is in progress so it should not include ABORT or ROLLBACK.A good choice for Postgres 8.2 and below is:server_reset_query = RESET ALL; SET SESSION AUTHORIZATION DEFAULT;for 8.3 and above its enough to do:server_reset_query = DISCARD ALL;When transaction pooling is used, the server_reset_query should be empty, as clients should not use any session features.Default: DISCARD ALL 备注：手册上的这段话之前看过很多次，一直没怎么懂，只知道当 pgbouncer 的 pool_mode 设置为 session 模式时，建议将server_reset_query 设置成 “DISCARD ALL”，当 pgbouncer 的 pool_mode 设置为 transaction 时，这个参数应该设置成空，即 server_reset_query=’’， 在实验之前，先来看下 pgbouncer 的三种模式。 PgBouncer 的三种模式首先看看 pgbouncer 连接池的三种模式。 Session pooling (会话池) : 最为礼貌的方式，当客户端向 pgbouncer 发出连接请求时pgbouncer 将分配连接，当客户端断开连接时，Pgbouncer 将连接回收到连接池，session 模式是以客户端连接为周期。 Transaction pooling (事务池): 事务池将以事务为周期，当客户端发出事务执行的请求时, pgbouncer 才分分配一个连接, 当事务结束时，pgbouncer 将连接回收到连接池。 Statement pooling ( 语句池)： 语句池是最为激进的模式，当客户端一条语句执行完时， pgbouncer 会立即将连接回收到连接池，所以含有多个语句的事务不能在这种模式下运行，因为事务的连接会被中断。 当pgbouncer 模式为 session 时，建议将 server_reset_query 设置成 “DISCARD ALL” ，为了理解这个参数下面做了两个实验说明下。 场景一: server_reset_query=’’当server_reset_query 设置为 ‘’ 时，编写 config1921.ini ，设置 server_reset_query， pool_mode 参数。 config1921.ini 文件配置123456789101112131415161718192021222324[databases] mydb = host=127.0.0.1 dbname=mydb port=1923 pool_size=2 mydb_test = host=127.0.0.1 dbname=mydb_test port=1923 pool_size=2 [pgbouncer] pool_mode = session listen_port = 1921 unix_socket_dir = /opt/pgbouncer/etc listen_addr = * auth_type = md5 auth_file = /opt/pgbouncer1.4.2/etc/1921/user1921.txt #logfile = /dev/null logfile = /var/applog/pgbouncer_log/pgbouncer_1921.log pidfile = /opt/pgbouncer1.4.2/etc/1921/pgbouncer1921.pid max_client_conn = 1000 reserve_pool_timeout = 0 admin_users = pgbouncer_admin stats_users = pgbouncer_guest ignore_startup_parameters = extra_float_digits tcp_keepalive = 1 tcp_keepidle = 30 tcp_keepcnt = 3 tcp_keepintvl = 5 server_reset_query='' 备注：这里将 mydb 的pool_size 设置为 2, 是为了后面的测试。同时将参数 pool_mode 设置为 session 模式，server_reset_query 值为空。 Reload pgbouncer1234567[postgres@pgb 1921]$ psql -h 127.0.0.1 -p 1921 pgbouncer pgbouncer_admin psql (9.1.0, server 1.4.2/bouncer) WARNING: psql version 9.1, server version 1.4. Some psql features might not work. Type \"help\" for help.pgbouncer=# reload; RELOAD 备注：RELOAD pgbouncer ，使配置生效。 开启 session A123456789101112131415[postgres@pgb load_test]$ psql -h 127.0.0.1 -p 1921 mydb mydb psql (9.1.0) Type \"help\" for help.mydb=&gt; show work_mem; work_mem ---------- 1MB (1 row)mydb=&gt; set work_mem='2MB'; SETmydb=&gt; show work_mem; work_mem ---------- 2MB (1 row) 开启 session B1234567891011121314151617[postgres@pgb ~]$ psql -h 127.0.0.1 -p 1921 mydb mydb psql (9.1.0) Type \"help\" for help.mydb=&gt; mydb=&gt; show work_mem; work_mem ---------- 1MB (1 row)mydb=&gt; set work_mem=\"3MB\"; SETmydb=&gt; show work_mem; work_mem ---------- 3MB (1 row)mydb=&gt; 开启 session C1234[postgres@pgb ~]$ psql -h 127.0.0.1 -p 1921 mydb mydb psql (9.1.0) Type \"help\" for help.mydb=&gt; show work_mem; 备注：当开启 session c 并执行命令时，session C 会处于等侍状态，这是因为配置中已将 mydb 的 poo_size 设置成了 2，最多允许给数据库 mydb 分配两个池子， 结束会话 A1mydb=&gt; \\q 再次查看会话C12345mydb=&gt; show work_mem; work_mem ---------- 2MB (1 row) 备注：在结束会话A后，会话 c的 “ show work_mem” 命令等侍消失，并且显示值为 2MB。因为将会话 A结束时， pgbouncer 将连接回收到连接池中并分配给了会话C，此时并没有清除会话A设置的 session 级属性。所以在会话C 看到的是会话A设置的参数。 场景二: server_reset_query = DISCARD ALL当 server_reset_query 设置为 ‘DISCARD ALL’时 ，看下情况。将参数 server_reset_query 设置成 “DISCARD ALL” config1921.ini 配置123456789101112131415161718192021222324[databases] mydb = host=127.0.0.1 dbname=mydb port=1923 pool_size=2 mydb_test = host=127.0.0.1 dbname=mydb_test port=1923 pool_size=2 [pgbouncer] pool_mode = session listen_port = 1921 unix_socket_dir = /opt/pgbouncer/etc listen_addr = * auth_type = md5 auth_file = /opt/pgbouncer1.4.2/etc/1921/user1921.txt #logfile = /dev/null logfile = /var/applog/pgbouncer_log/pgbouncer_1921.log pidfile = /opt/pgbouncer1.4.2/etc/1921/pgbouncer1921.pid max_client_conn = 1000 reserve_pool_timeout = 0 admin_users = pgbouncer_admin stats_users = pgbouncer_guest ignore_startup_parameters = extra_float_digits tcp_keepalive = 1 tcp_keepidle = 30 tcp_keepcnt = 3 tcp_keepintvl = 5 server_reset_query = DISCARD ALL reload pgbouncer1234567[postgres@pgb 1921]$ psql -h 127.0.0.1 -p 1921 pgbouncer pgbouncer_admin psql (9.1.0, server 1.4.2/bouncer) WARNING: psql version 9.1, server version 1.4. Some psql features might not work. Type \"help\" for help.pgbouncer=# reload; RELOAD 开启 Session A12345678910111213141516[postgres@pgb load_test]$ psql -h 127.0.0.1 -p 1921 mydb mydb psql (9.1.0) Type \"help\" for help.mydb=&gt; show work_mem; work_mem ---------- 1MB (1 row)mydb=&gt; set work_mem=\"2MB\"; SET mydb=&gt; show work_mem; work_mem ---------- 2MB (1 row)mydb=&gt; 开启 Session B123456789101112131415[postgres@pgb ~]$ psql -h 127.0.0.1 -p 1921 mydb mydb psql (9.1.0) Type \"help\" for help.mydb=&gt; show work_mem; work_mem ---------- 1MB (1 row)mydb=&gt; set work_mem=\"3MB\"; SET mydb=&gt; show work_mem; work_mem ---------- 3MB (1 row) 开启SESSION C1234[postgres@pgb ~]$ psql -h 127.0.0.1 -p 1921 mydb mydb psql (9.1.0) Type \"help\" for help.mydb=&gt; show work_mem; 备注：由于 mydb 库的 poo_size 值为 2, 达到了上限，这里依然是处于 wait 状态。 结束 SESSION A1mydb=&gt; \\q 查看 SESSION C12345mydb=&gt; show work_mem; work_mem ---------- 1MB (1 row) 备注：在结束会话A，再次查看会话C时，这时的 work_mem 值依然为初始值，说明当pgbouncer 在回收 SESSION A 的连接，并将连接分配给 SESSION C 时，将 SESSION A 的 sessoin 级设置的值丢弃了，这也正是因为 server_reset_query 参数设置成 “DISCARD ALL “。 总结根据上面的实验应该理解 pgbouncer 的参数 server_reset_query 的意思了。 当 pgbouncer 为 session 模式时，应该将参数值设置 “DISCARD ALL” ，这样当 pgbouncer 回收 连接并分配给其它客户端时，之前客户端设置的 session 级别的值将被丢弃，否则将会带来不可控的影响。 当 pgbouncer 为 Transaction 模式时，由于pgbouncer 连接池的分配是以事务为周期的，当连接池被 pgbouncer回收并分配给其它客户端时，session 级别设置的参数将不会保留。所以这个参数可以设置为空。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PgBouncer","slug":"PgBouncer","permalink":"https://postgres.fun/tags/PgBouncer/"}]},{"title":"PostgreSQL: 业务框架SQL导致 PostgreSQL 负载高一例","slug":"20111210154027","date":"2011-12-10T07:40:27.000Z","updated":"2018-09-04T01:33:53.210Z","comments":true,"path":"20111210154027.html","link":"","permalink":"https://postgres.fun/20111210154027.html","excerpt":"","text":"最近刚上了新业务，在业务前期业务量不是很大， 但数据库负载有点高，达到 3 到 4左右，接着到数据库里查询一番，发现业务的慢SQL 很少，奇怪的是有个非常大的SQL在跑，之所以奇怪是因为这个SQL 查的是 PostgreSQL 的系统表，显然不是业务系统发起，而且这个SQL每天执行的量比较大，下面是详细信息。 问题 SQL12345678910111213141516171819202122232425262728293031323334353637383940SELECT conname, consrc, contype, indkey FROM ( SELECT conname, CASE WHEN contype='f' THEN pg_catalog.pg_get_constraintdef(oid) ELSE 'CHECK (' || consrc || ')' END AS consrc, contype, conrelid AS relid, NULL AS indkey FROM pg_catalog.pg_constraint WHERE contype IN ('f', 'c') UNION ALL SELECT pc.relname, NULL, CASE WHEN indisprimary THEN 'p' ELSE 'u' END, pi.indrelid, indkey FROM pg_catalog.pg_class pc, pg_catalog.pg_index pi WHERE pc.oid=pi.indexrelid AND EXISTS ( SELECT 1 FROM pg_catalog.pg_depend d JOIN pg_catalog.pg_constraint c ON (d.refclassid = c.tableoid AND d.refobjid = c.oid) WHERE d.classid = pc.tableoid AND d.objid = pc.oid AND d.deptype = 'i' AND c.contype IN ('u', 'p') ) ) AS sub WHERE relid = (SELECT oid FROM pg_catalog.pg_class WHERE relname='coin_businesses' AND relnamespace = (SELECT oid FROM pg_catalog.pg_namespace WHERE nspname='freecoin')) 备注：根据 pg_stat_activity 的 client_addr 字段，知道了发起这个 SQL的源IP。 查询结果 conname | consrc | contype | indkey ----------------------+--------+---------+-------- coin_businesses_pkey | | p | 1 备注：于是找来开发人员，和开发人员沟通后，原来这个语句是他们用的一个框架发起，会不停地查询数据库结构。 解决方法后来开发人员将这个查询做到缓存，不需要时时查询数据库，这个SQL解决之后，数据库平均负载下降40% 左右。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL: 通过函数初始化三层结构分区表","slug":"20111203202443","date":"2011-12-03T12:24:43.000Z","updated":"2018-09-04T01:33:53.147Z","comments":true,"path":"20111203202443.html","link":"","permalink":"https://postgres.fun/20111203202443.html","excerpt":"","text":"PostgreSQL 没有提供像 Oracle 里那样比较智能的分区表功能，比如插入时需要指定子表，但是在一定程序上也能构建分区表，平常用得比较多的是两层结构，通过时间进行分区，今天介绍下构建三导分区表的方法，三层结构分区别可参考以下。 一 体系结构三层结构分区表结构 备注：三层结构分区表包括三层，第一层为父表, 第二层通过某字段取模分区分了多个表，上图分为128个表， 第三层为年月表，根据时间字段分区，这样便实现了三层结构分区。接下来介绍下创建三层结构分区表的具体步骤。 二 创建三层结构分区表2.1 创建父表 ( 第一层 )12345678910111213141516create table tbl_log( sky_imsi character varying(50) not null, xxx character varying(12) , xxx character varying(30) not null, xxx integer , xxx character varying , xxx character varying(20) , trade_time timestamp without time zone not null, col1 character varying(20), --预留字段1 col2 character varying (30), --预留字段2 col3 character varying (50), --预留字段3 remark character varying(100) , --备注 CONSTRAINT pk_tbl_log PRIMARY KEY (order_id) );create index idx_tbl_log on tbl_log(sky_imsi); create index idx_tbl_log_ctime on tbl_log using btree (trade_time); 2.2 创建第二层表12345create table tbl_log_128_0 ( like tbl_log including constraints including defaults including indexes ) inherits ( tbl_log ); create table tbl_log_128_1 ( like tbl_log including constraints including defaults including indexes ) inherits ( tbl_log ); create table tbl_log_128_2 ( like tbl_log including constraints including defaults including indexes ) inherits ( tbl_log ); ...... create table tbl_log_128_127 ( like tbl_log including constraints including defaults including indexes ) inherits ( tbl_log ); 备注：上面创建了 128 张第二层表。 2.3 创建年月配置表并初始化配置数据123456789create table tmp_date ( table_suff character varying(6) primary key );insert into tmp_date values ('201112'); insert into tmp_date values ('201201'); insert into tmp_date values ('201202'); insert into tmp_date values ('201203'); insert into tmp_date values ('201204'); insert into tmp_date values ('201205'); insert into tmp_date values ('201206'); insert into tmp_date values ('201207'); 备注： 这个表为配置表，用来存年月信息，初始化数据时只要输入年月信息即可，这里共配置了8个月的分区，下面的 function 会用到这个配置表来创建第三层表。 2.4 创建第三层子表和约束关系的 function12345678910111213141516171819202122232425262728293031323334CREATE OR REPLACE FUNCTION func_create_table() RETURNS void LANGUAGE plpgsql AS $function$ declare v_next_month char(6); v_second_tabname varchar(64); v_third_tabname varchar(64); rec_date RECORD; v_date varchar(6); v_cur_date varchar(64); v_tmo_date varchar(64);begin for i in 0..127 loop v_second_tabname := 'tbl_log_128_' || i::text; --raise notice 'i=%',i; FOR rec_date in ( select table_suff from tmp_date order by tmp_date ) LOOP v_date := rec_date.table_suff; -- raise notice 'date=% ', rec_date.table_suff; v_third_tabname :=v_second_tabname || '_' || v_date; v_cur_date := v_date || '01'; v_tmo_date := to_char(v_cur_date::date + interval '1 months','yyyymmdd'); --raise notice 'v_cur_date=% ', v_cur_date; --raise notice 'v_tmo_date=% ', v_tmo_date; execute 'create table ' || v_third_tabname || '( like ' || v_second_tabname || ' including constraints including indexes including defaults) inherits (' || v_second_tabname || ')' ; execute 'alter table ' || v_third_tabname || ' add constraint ' || v_third_tabname || E'_check check (((trade_time &gt;='' || v_cur_date || E'') AND (trade_time &lt; '' || v_tmo_date || E'')))'; end loop;end loop;end $function$; 2.5 执行函数 func_create_table()1select func_create_table(): 三 总结 这里只简单介绍了三层结构分构表的初始化过程，平常维护成本还是较高的;通过函数创建分区表和约束关系大大减少了工作量。 关于分区表的查询性能这里不做介绍，之后的博文里会做这方面的分析。","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"PostgreSQL: 给大表 Varchar 字段扩长的方法","slug":"20111203164349","date":"2011-12-03T08:43:49.000Z","updated":"2018-09-04T01:33:53.085Z","comments":true,"path":"20111203164349.html","link":"","permalink":"https://postgres.fun/20111203164349.html","excerpt":"","text":"昨天有个日志库有个需求，需要将日志表的一个 varchar 字段扩长，在数据库中给字段增长是一个非常普遍的需求，但在 PostgreSQL 里，这却是件蛋疼的事，因为 PostgreSQL 在给字段增长的场景，大多数据情况下需要重写表，这里可以参考我以前写的 blog： https://postgres.fun/20110215200654.html 考虑到直接修改原表字段类型会对所有表数据进行重写，这个耗时是非常长的，而且在 ALTER TABLE 过程中会锁表，表上所有的查询，插入等操作都不能进行，于是想到了个釜底抽薪的方法，可以解决这个问题，接下来往下看。 场景介绍父表： tbl_tmp_log子表： 1158 张日表，如 tbl_tmp_log_20111203 ，日表大小在 23 G左右需求： 修改 tbl_tmp_log 表的 refer 字段类型，从 character varying(2000) 扩容到 character varing(5000) 环境信息父表表定义12345678910111213141516171819202122232425skytf=&gt; \\d tbl_tmp_log Table \"public.tbl_tmp_log\" Column | Type | Modifiers -----------+-----------------------------+------------------------------------------------------------- xxx | bigint | not null default nextval('tbl_tmp_log_id_seq'::regclass) xxx | character varying(50) | xxx | character varying(50) | xxx | timestamp without time zone | not null default now() xxx | character varying(2000) | xxx | character varying(10) | xxx | integer | refer | character varying(2000) | xxx | character varying(50) | xxx | character varying | xxx | character varying(50) | xxx | character varying(50) | xxx | boolean | xxx | integer | xxx | character varying(50) | xxx | integer | Indexes: \"pk_tbl_tmp_log_id\" PRIMARY KEY, btree (id) \"idx_tbl_tmp_log_ntime\" btree (ntime), tablespace \"tbs_skytf_idx\" Number of child tables: 1158 (Use d+ to list them.) Tablespace: \"tbs_skytf_idx\" 子表信息123456789101112131415161718skytf=&gt; \\dt+ List of relations Schema | Name | Type | Owner | Size | Description ---------+-------------------------------+-------+---------+------------+------------- skytf | tbl_tmp_log_20111108 | table | skytf | 26 GB | skytf | tbl_tmp_log_20111109 | table | skytf | 26 GB | skytf | tbl_tmp_log_20111110 | table | skytf | 24 GB | skytf | tbl_tmp_log_20111111 | table | skytf | 25 GB | skytf | tbl_tmp_log_20111112 | table | skytf | 28 GB | skytf | tbl_tmp_log_20111113 | table | skytf | 26 GB | skytf | tbl_tmp_log_20111114 | table | skytf | 23 GB | skytf | tbl_tmp_log_20111115 | table | skytf | 23 GB | skytf | tbl_tmp_log_20111116 | table | skytf | 23 GB | skytf | tbl_tmp_log_20111117 | table | skytf | 23 GB | skytf | tbl_tmp_log_20111118 | table | skytf | 24 GB | skytf | tbl_tmp_log_20111119 | table | skytf | 25 GB | skytf | tbl_tmp_log_20111207 | table | skytf | 0 bytes | ...省略 备注：为了便于显示，只列出少数表，实际上有 1158 张子表。 实施过程清理历史数据由于这是日志库，数据仓库会实时抽取数据，那么只要是同步到仓库里的数据，在生产环境下是可以清除的，所以接下来向仓库部门请求核对 2011-11-08 到 2011-12-01 的数据，核对无误后，这些数据都可以清除了。 数据清理后的表情况12345678910skytf=&gt; \\dt+ List of relations Schema | Name | Type | Owner | Size | Description ---------+-------------------------------+-------+---------+------------+------------- skytf | tbl_tmp_log_20111202 | table | skytf | 6805 MB | skytf | tbl_tmp_log_20111203 | table | skytf | 0 bytes | skytf | tbl_tmp_log_20111204 | table | skytf | 0 bytes | skytf | tbl_tmp_log_20111205 | table | skytf | 0 bytes | skytf | tbl_tmp_log_20111206 | table | skytf | 0 bytes | skytf | tbl_tmp_log_20111207 | table | skytf | 0 bytes | 备份表 tbl_tmp_log_20111202 表结构表 tbl_tmp_log_20111202 的表结构和权限信息可以通 pg_dump 导出 如1pg_dump -h 127.0.0.1 -p 1921 -E UTF8 -t \"skytf.tbl_tmp_log_20111202\" -s -v skytf &gt; tbl_tmp_log_20111202.ddl 重命名日表 tbl_tmp_log_20111202 的表名和索引123alter table tbl_tmp_log_20111202 rename to tbl_tmp_log_20111202_bak; alter index tbl_tmp_log_20111202_pkey rename to tbl_tmp_log_20111202_pkey_bak; alter index idx_tbl_tmp_log_20111202_ntime rename to idx_tbl_tmp_log_20111202_ntime_bak; 取消日表 tbl_tmp_log_20111202 和父表的继承关系1alter table tbl_tmp_log_20111202_bak no inherit public.tbl_tmp_log; 备注：这里取消表 tbl_tmp_log_20111202_bak 和父表的继承关系是因为接下来打算给父表 tbl_tmp_log 扩字段长度，如果不取消这张表的继承关系，那么这张表的数据是需要重写的。 创建同名表 tbl_tmp_log_20111202在步骤 3.3,3.4 结束后，迅速执行步骤 3.2 导出的建表脚本, 脚本中包括表 tbl_tmp_log_20111202 的建表语句，索引创建语句和权限信息，所以执行后，和原表的信息是一样的。只不过是一张空表而已。 给父亲表字段扩容12skytf=&gt; alter table public.tbl_tmp_log alter column refer type character varying(5000);ALTER TABLE Time: 83031.807 ms 备注：在步骤 3.5 完成后，接下来终于可以给字段扩容了，语句如上，耗时 83 秒，这个时间已经算很少了。这个字段扩容语句可以同时给了 1000 多张字表同样字段也扩容了。 将备份表数据插回新表1insert into tbl_tmp_log_20111202 select * from tbl_tmp_log_20111202_bak; 删除备份表1drop table tbl_tmp_log_20111202_bak; 备注：到了这步，就算完成了字段扩容操作。 总结 以上是实现给大表 varchar 字段扩长的一个方法，当然方法也有很多。 这种方法对生产库的影响非常小的，对生产库的影响在一分钟左右。 上面方法的一个重要步骤是恰当地拿掉了备份表和父表的继承关系，正因为这样，避免了重写 6 G的日表数据。 由于这次的场景是日志库，如果碰到了业务库的分区表需要扩字段长度，还得另外考虑方案。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 通过角色赋权","slug":"20111129112223","date":"2011-11-29T03:22:23.000Z","updated":"2018-09-04T01:33:53.022Z","comments":true,"path":"20111129112223.html","link":"","permalink":"https://postgres.fun/20111129112223.html","excerpt":"","text":"在开始话题之前，先讲解下 PostgreSQL 里的用户和角色的一些知识，在 PostgreSQL 里没有区分用户和角色的概念，CREATE USER 为 CREATE ROLE 的别名，这两个命令几乎是完全相同的，唯一的区别是”CREATE USER”命令创建的用户的 LOGIN 属性默认为 on , 而 CREATE ROLE 命令创建的用户的 NOLOGIN 属性默认为 on。 在 Oracle 里，可以通过角色赋权，例如新建一个用户后，可以把 connect ，RESOURCE 等角色赋给新建的用户，在PostgreSQL 里也可以实现通过角色赋权，PostgreSQL 是通过角色继承的方式实现，下面看下具体实现过程。 创建 CP 角色创建角色和创建用户的语法一样，如下创建 CP 角色。12postgres=# create role cp login nosuperuser nocreatedb nocreaterole noinherit encrypted password 'cp'; CREATE ROLE 给 CP 角色赋权给 CP 用户赋予数据库 skytf 连接权限和相关表的查询权限1234567891011postgres=# grant connect on database skytf to cp; GRANT postgres=# \\c skytf skytf; You are now connected to database \"skytf\" as user \"skytf\". skytf=&gt; grant usage on schema skytf to cp; GRANT skytf=&gt; grant select on skytf.test_1 to cp; GRANT 创建用户 cp_1123456skytf=&gt; \\c postgres postgres You are now connected to database \"postgres\" as user \"postgres\". postgres=# create role cp_1 login nosuperuser nocreatedb nocreaterole inherit encrypted password 'cp_1'; CREATE ROLE 备注：这里创建 cp_1 用户，并开启 inherit 属性。 将 cp 角色赋给 cp_1 用户方法一12skytf=# grant cp to cp_1; GRANT ROLE 方法二12postgres=# create role cp_2 login nosuperuser nocreatedb nocreaterole inherit encrypted password 'cp_2' in role cp; CREATE ROLE 备注：上面提供了两种方法将 cp 角色的权限赋给其它用户，第二种方法是在创建用户时即赋予了角色权限。 测试 cp_1 用户的权限1234567skytf=# \\c skytf cp_1; You are now connected to database \"skytf\" as user \"cp_1\". skytf=&gt; select * from skytf.test_1 limit 1; id ---- (0 rows) 查询角色、用户信息123456postgres=# \\du Role name | Attributes | Member of ------------------+---------------------------------------------------+----------- cp | No inheritance | &#123;&#125; cp_1 | | &#123;cp&#125; cp_2 | | &#123;cp&#125; 备注：\\du 命令显示用户信息， “ Member of “ 项表示 cp_1, cp_2 用户属于 cp 用户组。 附 CREATE ROLE 语法 CREATE ROLENameCREATE ROLE – define a new database roleSynopsisCREATE ROLE name [ [ WITH ] option [ … ] ]where option can be:SUPERUSER | NOSUPERUSER | CREATEDB | NOCREATEDB | CREATEROLE | NOCREATEROLE | CREATEUSER | NOCREATEUSER | INHERIT | NOINHERIT | LOGIN | NOLOGIN | CONNECTION LIMIT connlimit | [ ENCRYPTED | UNENCRYPTED ] PASSWORD ‘password’ | VALID UNTIL ‘timestamp’ | IN ROLE role_name [, …] | IN GROUP role_name [, …] | ROLE role_name [, …] | ADMIN role_name [, …] | USER role_name [, …] | SYSID uid Description","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.1新特性之五：同步复制 ( Synchronous Replication )","slug":"20111119164657","date":"2011-11-19T08:46:57.000Z","updated":"2018-09-04T01:33:52.960Z","comments":true,"path":"20111119164657.html","link":"","permalink":"https://postgres.fun/20111119164657.html","excerpt":"","text":"PostgreSQL9.0 版本开始提供非常酷的流复制技术，即备库可以实时同步主库，如果备库机器性能足够好，从库延迟时间可以是毫秒级；尽管如此， PostgreSQL9.0 的流复制技术依然是异步的，当主库 crash 时，从库存在数据丢失的危险，到了 PostgreSQL 9.1 ，可以支持同步复制了，下面仅列出PostgreSQL 9.1 搭建 HOT-Standby 的关键参数，因为大部分步骤和 PostgreSQL9.0 的搭建类似，关于 9.0 版本搭建 HOT-Standby 可以参考之前的blog: PostgreSQL: Setting up streaming log replication (Hot Standby ) 环境信息主节点: 192.168.1.25备节点： 192.168.1.26PostgreSQL 版本： 9.1操作系统: Red Hat Enterprise Linux Server release 5.5 同步复制配置创建 replicaton role ( On primary)1CREATE ROLE replication_role REPLICATION LOGIN PASSWORD 'pwd_replication' 备注：9.1 版本以后 replication 角色不再需要是超级用户，只需要新赋予 “REPLICATION” 权限。 修改 pg_hba.conf ( On primary)，增加以下记录1host replication replication_role 192.168.1.25/32 md5 主库备份，并将数据文件 copy 到 standby 节点（略） 配置 recovery.conf ( On standby )123standby_mode = 'on' #标记PG为STANDBY SERVER primary_conninfo = 'host=192.168.1.26 port=1923 user=replication_role application_name=mydb_standby1' trigger_file = '/opt/pgdata/pg_root/postgresql.trigger.1923' 备注: recovery.conf 的参数 primary_conninfo 里面和9.0 版本配置的不一样，这里多了个 application_name 参数，primary 节点的 synchronous_standby_names 参数会用到从库的 application_name 参数值，接着往下看就明白了。 设置 postgresql.conf ( on primary) 1synchronous_standby_names = 'mydb_standby1' 这个参数的值为从库节点 recovery.conf 文件的参数 primary_conninfo 的 application_name 属性值。 修改主库的 synchronous_standby_names 参数，表示主库开启同步复制功能，这意味着当主库上发生数据变化时，主库会先等侍这一变化发送到从库并提交，之后主库才显示已提交，这样即使主节点 crash , 也能保证数据不会丢失。 这个参数可以配置多个 standby 节点，参数列表中排最最前面的优先级最高，排第一的为 stadnby 同步节点，当排第一的 standby 节点 crash 时，排第二的 standby 节点可以顶上去，所以指定大于1的 standby 节点提高了系统的高可用性。 上面参数修改后，可以看到日志 ( on primary )12011-11-19 15:54:48.742 CST,\"replication_role\",\"\",19250,\"192.168.1.25:50092\",4ec76003.4b32,3,\"streaming 1/1801B114\",2011-11-19 15:51:31 CST,1/0,0,LOG,00000,\"standby \"\"mydb_standby1\"\" is now the synchronous standby with priority 1\",,,,,,,,,\"mydb_standby1\" 备注：如果主库参数 synchronous_standby_names 设置的值找不到对应的从库信息，那么当在主库上提交 SQL时，SQL 会一直处于等待状态，因为它找不到同步 standby 节点 ，这所以这个参数需要小心配置。 新增 pg_stat_replication 视图12345678910111213141516171819202122232425262728293031323334353637383940414243mydb=# \\d pg_stat_replication View \"pg_catalog.pg_stat_replication\" Column | Type | Modifiers ------------------+--------------------------+----------- procpid | integer | usesysid | oid | usename | name | application_name | text | client_addr | inet | client_hostname | text | client_port | integer | backend_start | timestamp with time zone | state | text | sent_location | text | write_location | text | flush_location | text | replay_location | text | sync_priority | integer | sync_state | text | mydb=&gt; \\c mydb postgres You are now connected to database \"mydb\" as user \"postgres\". mydb=# select * from pg_stat_replication ; procpid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | state | sent_location | write_location | flush_location | replay_location | sync_priority | sync_state ---------+----------+------------------+------------------+--------------+-----------------+-------------+-------------------------- -----+-----------+---------------+----------------+----------------+-----------------+---------------+------------ 19250 | 16403 | replication_role | mydb_standby1 | 192.168.1.25 | | 50092 | 2011-11-19 15:51:31.54513 7+08 | streaming | 1/1801B304 | 1/1801B304 | 1/1801B304 | 1/1801B304 | 1 | sync (1 row) mydb=# \\c mydb mydb You are now connected to database \"mydb\" as user \"mydb\". mydb=&gt; select * from pg_stat_replication ; procpid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | state | se nt_location | write_location | flush_location | replay_location | sync_priority | sync_state ---------+----------+------------------+------------------+-------------+-----------------+-------------+---------------+-------+--- ------------+----------------+----------------+-----------------+---------------+------------ 19250 | 16403 | replication_role | mydb_standby1 | | | | | | | | | | | (1 row) 备注：9.1 新增视图 pg_stat_replication 记录主节点上所有 wal 发送进程信息， 使用超级用户时，可以看到这个视图的详细的信息，而普通用户看到的信息更少，这应该是 PostgreSQL 基于安全性的考虑。 开启同步模式的风险若开启同步模式，第一个standby 节点的状态直接影响到主库，若第一个 standby 节点 crash ，而又没有其它 standby 节点顶上，那么主库上的所有操作将会被 HANG 住，直到从点恢复；主库才恢复正常，如果是很重要的业务，建议至少配置两个 standby 节点，提高高可用性。 后续( 性能测试 )理论上开启了同步功能会对主库的性能有所影响，今天没有详细地测试。 参考资料 PostgreSQL: Setting up streaming log replication (Hot Standby ) http://www.postgresql.org/docs/9.1/static/runtime-config-replication.html#RUNTIME-CONFIG-REPLICATION-MASTER http://www.postgresql.org/docs/9.1/static/warm-standby.html#SYNCHRONOUS-REPLICATION","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"数据库信息被篡改一例","slug":"20111114232711","date":"2011-11-14T15:27:11.000Z","updated":"2018-09-04T01:33:52.897Z","comments":true,"path":"20111114232711.html","link":"","permalink":"https://postgres.fun/20111114232711.html","excerpt":"","text":"这几天有套应用发生了匪夷所思的现象，数据库信息频繁被篡改，一开始我们怀疑可能是应用被攻击了，但依然没有可靠的证据，接连几天找不到原因，今天下午应用人员告诉我说很多用户都登陆不了系统了，数据库是不是出问题了，下面是今天的分析过程。 数据库信息数据库负载为 2 左右，对于 8 核的机器来说还不算高，IO，CPU使用率在合理范围之内。 主机层面查看当前会话1234567891011121314[postgres@db](mailto:postgres@db)&gt; ps -ef | grep UPDATE postgres 393 3708 0 Nov12 ? 00:01:24 postgres: mpc_db_account mpc_db_account 192.168.104.50(38165) UPDATE waiting postgres 403 3708 0 Nov12 ? 00:01:23 postgres: mpc_db_account mpc_db_account 192.168.104.50(38172) UPDATE waiting postgres 465 3708 0 Nov12 ? 00:01:24 postgres: mpc_db_account mpc_db_account 192.168.104.50(45733) UPDATE waiting postgres 478 3708 0 Nov12 ? 00:01:30 postgres: mpc_db_account mpc_db_account 192.168.104.50(45740) UPDATE waiting postgres 516 3708 0 Nov12 ? 00:01:29 postgres: mpc_db_account mpc_db_account 192.168.104.50(45747) UPDATE waiting postgres 8114 3708 0 15:06 ? 00:00:00 postgres: mpc_db_account mpc_db_account xxx.xxx.xxx.xx(40477) UPDATE waiting postgres 8236 3708 0 15:08 ? 00:00:00 postgres: mpc_db_account mpc_db_account xxx.xxx.xxx.xx(51622) UPDATE waiting postgres 8781 8319 0 15:19 pts/0 00:00:00 grep UPDATE postgres 15199 3708 0 Nov13 ? 00:00:46 postgres: mpc_db_account mpc_db_account xxx.xxx.xxx.xx(50177) UPDATE waiting postgres 15207 3708 0 Nov13 ? 00:05:28 postgres: mpc_db_account mpc_db_account xxx.xxx.xxx.xx(50184) UPDATE postgres 15215 3708 0 Nov13 ? 00:00:59 postgres: mpc_db_account mpc_db_account xxx.xxx.xxx.xx(50190) UPDATE waiting postgres 15223 3708 0 Nov13 ? 00:00:48 postgres: mpc_db_account mpc_db_account xxx.xxx.xxx.xx(50197) UPDATE waiting postgres 23476 3708 0 Nov12 ? 00:01:13 postgres: mpc_db_account mpc_db_account xxx.xxx.xxx.xx(43681) UPDATE waiting 备注：这里发现有十几个 update 会话处于等待状态，只有会话 15207 在执行，猜想可能是这个会话有问题。 查看 15207 会话详细信息123456789postgres=# select datname,current_query,query_start,client_a from pg_stat_activity where current_query !='&lt;IDLE&gt;' and procpid=15207; datname |current_query | query_start | client_addr | client_port ----------------+------------------------------------------------------------------------------------------------------------------- mpc_db_account | update tmp_table set column1 = '[?au]..v..&lt;&lt;kool-boy&gt;&gt;...', sex = 1, birthday = '1994-11-24', mobile = '', province = '', city = '', signature = 'sunlei '--', head_icon = 30, country_code = 286 where user_id = 139957561 | 2011-11-14 14:51:07.147826+08 | xxx.xxx.xxx.xx | 50184 (1 row) 备注：根据执行时间（ query_start ）来看，这个会话跑了有 30 分钟左右了还没跑完，这个 update 语句是带where 条件的，应该只更新一条记录，为什么执行了这么久还没跑完呢？觉得非常奇怪, 接下来想进一步分析，但业务因为部分用户登陆不了系统，项目经理建议先重启业务，回头再查原因。 出现转机业务重启后，应用恢复正常，接着开发人员在测试环境上进行了测试，幸运的是他们带来了意外的发现，他们发现在修改字段值时，加上单引号，再加上注释符 – 后，表上所有的记录都被更新了，正如上面的 signature 字段值全被更新成 sunlei 了，后来了解了下情况，目前业务上基本上没有对客户端用户输入的信息做处理，没对特殊字符做转义。接下来详细测试下。 模拟测试创建测试表并插入数据12345678910111213141516171819skytf=&gt; create table test_66 (id integer, name varchar(32)); CREATE TABLEskytf=&gt; insert into test_66 select generate_series(1,10),'a'; INSERT 0 10skytf=&gt; select * from test_66; id | name ----+------ 1 | a 2 | a 3 | a 4 | a 5 | a 6 | a 7 | a 8 | a 9 | a 10 | a (10 rows) 更新数据1234567891011121314151617skytf=&gt; update test_66 set name='aaa '--' where id=1; UPDATE 10skytf=&gt; select * from test_66; id | name ----+------ 1 | aaa 2 | aaa 3 | aaa 4 | aaa 5 | aaa 6 | aaa 7 | aaa 8 | aaa 9 | aaa 10 | aaa (10 rows) 备注：虽然带有 where 条件的更新，却全表更新了表数据，原因是注释符 “–” 后面的语句被注释掉了，如果此时程序发送分号，那么语句被执行，全表数据被更新。 测试结论上面证实了本文的问题属于软件问题，没对对客户端输入的文字进行合法性检查，也没有对特殊字符进行转义处理，同时这也给 SQL 注入带来了很大的隐患。 对特殊字符进行转义处理举例如下1234567891011121314151617181920212223242526272829303132333435skytf=&gt; update test_66 set name=E'bbb'bbb' where id=1; UPDATE 1skytf=&gt; select * from test_66; id | name ----+--------- 2 | aaa 3 | aaa 4 | aaa 5 | aaa 6 | aaa 7 | aaa 8 | aaa 9 | aaa 10 | aaa 1 | bbb'bbb (10 rows)skytf=&gt; update test_66 set name=E'bbb'--' where id=2; UPDATE 1skytf=&gt; select * from test_66; id | name ----+--------- 3 | aaa 4 | aaa 5 | aaa 6 | aaa 7 | aaa 8 | aaa 9 | aaa 10 | aaa 1 | bbb'bbb 2 | bbb'-- (10 rows) 解决方法为了减少此类情况和 SQL 注入的情况，建议程序里对客户端输入的文字进行合法性检查，对特殊字符进行转义处理；同时也要增加应用程序的安全性，例如权限， 不使用动态SQL，以及密码需加密等。 参考文档 http://technet.microsoft.com/zh-cn/cc562985.aspx","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL: System Information Functions","slug":"20111108195515","date":"2011-11-08T11:55:15.000Z","updated":"2018-09-04T01:33:52.835Z","comments":true,"path":"20111108195515.html","link":"","permalink":"https://postgres.fun/20111108195515.html","excerpt":"","text":"在生产库维护过程中，经常需要查看数据库的一些信息，包括表，索引等， PostgreSQL 一方面提供反斜线加字母的组合显示数据库信息，例如” d 显示表和视图信息 “，同时 PostgreSQL 还提供强大的系统信息查询函数，下面是系统信息查询函数的用法举例。 1 查看 PostgreSQL 版本信息12345skytf=&gt; select version(); version ------------------------------------------------------------------------------------------------------------------- PostgreSQL 9.0.1 on x86_64-unknown-linux-gnu, compiled by GCC gcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-48), 64-bit (1 row) 2 查看当前会话对应的 PostgreSQL 服务进程12345postgres=# select pg_backend_pid(); pg_backend_pid ---------------- 9610 (1 row) 3 查询当前事务ID12345postgres=# select txid_current(); txid_current -------------- 21195195 (1 row) 4 查询 PostgreSQL 服务启动时间12345postgres=# select pg_postmaster_start_time(); pg_postmaster_start_time ------------------------------- 2011-09-30 11:06:00.132932+08 (1 row) 5 查看当前连的接数据库12345skytf=&gt; select current_database(); current_database ------------------ skytf (1 row) 6 查看当前 schema12345skytf=&gt; select current_schema(); current_schema ---------------- skytf (1 row) 7 查看当前连接的用户12345skytf=&gt; select current_user; current_user -------------- skytf (1 row) 8 查看函数的定义123456skytf=&gt; select oid,proname from pg_proc where proname='create_table'; oid | proname ----------+-------------- 14290044 | create_table (1 row)skytf=&gt; select pg_get_functiondef(14290044); 9 查看索引创建语句1234567891011skytf=&gt; select oid ,relname from pg_class where relname='game_appid_key'; oid | relname ----------+---------------- 14311899 | game_appid_key (1 row)skytf=&gt; select pg_get_indexdef(14311899); pg_get_indexdef ---------------------------------------------------------------- CREATE UNIQUE INDEX game_appid_key ON game USING btree (appid) (1 row) 10 查看视图创建语句12345skytf=&gt; select pg_get_viewdef('view_activity'); pg_get_viewdef ----------------------------------------------------------------------------------------------------------------------------------- SELECT activity.id, activity.identity_code, activity.skyid, activity.category, activity.body, activity.create_time FROM activity; (1 row) 备注：以上只列举出常见的查询函数，还有很多就不演示了，详细信息可以查看手册。 11 参考 http://www.postgresql.org/docs/9.1/static/functions-info.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Process 23178 Still Waiting For ShareLock on Transaction 693345717 After 1000.363 ms","slug":"20111104143745","date":"2011-11-04T06:37:45.000Z","updated":"2018-09-04T01:33:52.772Z","comments":true,"path":"20111104143745.html","link":"","permalink":"https://postgres.fun/20111104143745.html","excerpt":"","text":"昨天一生产库出现异常，短时间内负载较高( LOAD 达到 5 ), 查看了下数据库日志，发现了有大量以下日志。生产库日志12011-11-03 06:50:30.459 CST,\"mpc_db_account\",\"mpc_db_account\",23178,\"192.168.104.51:51196\",4eb112db.5a8a,65,\"UPDATE waiting\",2011-11-02 17:52:27 CST,42/1784416,693346019,LOG,00000,\"process 23178 still waiting for ShareLock on transaction 693345717 after 1000.363 ms\",,,,,,\"update mpc_user_basic_info set nickname = '/&lt;u?o/ay', sex = 1, birthday = '1991-01-01', mobile = '', province = '', city = '', signature = '', head_icon = 0, country_code = 452 where user_id = 229898411\",,,\"\" 备注：信息 process 23178 still waiting for ShareLock on transaction 693345717 after 1000.363 ms虽然知道这说明数据库有行锁等侍，但 transaction 后面的数值 693345717 是什么含义呢？是指事务ID么？ 带着这个疑问在测试环境做一下测试。 创建测试表创建测试表并插入数据123456789101112skytf=&gt; create table test_65 (id integer,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_65 values (1,'a'); INSERT 0 1 skytf=&gt; insert into test_65 values (2,'b'); INSERT 0 1 skytf=&gt; insert into test_65 values (3,'c'); INSERT 0 1 skytf=&gt; insert into test_65 values (4,'d'); INSERT 0 1 skytf=&gt; insert into test_65 values (5,'e'); INSERT 0 1 session A123456789101112131415skytf=&gt; begin; BEGIN skytf=&gt; select txid_current(); txid_current -------------- 21192006 (1 row) skytf=&gt; update test_65 set name='aaa' where id=1; UPDATE 1 skytf=&gt; update test_65 set name='bbb' where id=2; UPDATE 1 skytf=&gt; update test_65 set name='ccc' where id=3; UPDATE 1 备注：首先开启 session A, 更新三条记录，并不提交，此时的事务ID为 21192006。 session B123456789skytf=&gt; begin; BEGIN skytf=&gt; select txid_current(); txid_current -------------- 21192007 skytf=&gt; update test_65 set name='aaa' where id=1; 备注：再开一个事务 session B, 更新 id=1 的记录，此时 session B 在等付, 事务ID为 21192007。 session Csession 主要负责监控当前会话12345678910111213141516skytf=&gt; select datname,procpid,current_query from pg_stat_activity where datname='skytf'; datname | procpid | current_query ---------+---------+----------------------------------------------------------------------------------- skytf | 7890 | select datname,procpid,current_query from pg_stat_activity where datname='skytf'; skytf | 16706 | &lt;IDLE&gt; in transaction skytf | 16656 | update test_65 set name='aaa' where id=1; (3 rows)skytf=&gt; select database,relation,pid,mode, granted,transactionid from pg_locks where relation=16920668; database | relation | pid | mode | granted | transactionid ----------+----------+-------+------------------+---------+--------------- 14203071 | 16920668 | 16656 | AccessShareLock | t | 14203071 | 16920668 | 16656 | RowExclusiveLock | t | 14203071 | 16920668 | 16706 | RowExclusiveLock | t | 14203071 | 16920668 | 16656 | ExclusiveLock | t | (4 rows)) 备注：事务 session C, 查询当前会话信息和锁等侍情况。 数据库日志 (csv log)12011-11-04 11:07:11.120 CST,\"skytf\",\"skytf\",16656,\"127.0.0.1:53640\",4eb3566b.4110,1,\"UPDATE waiting\",2011-11-04 11:05:15 CST,61/402,21192007,LOG,00000,\"process 16656 still waiting for ShareLock on transaction 21192006 after 1000.962 ms\",,,,,,\"update test_65 set name='aaa' where id=1;\",,,\"psql\" 备注： 注意信息 “process 16656 still waiting for ShareLock on transaction 21192006 after 1000.962 ms” ,transaction 后面的事务值为 21192006, 正好是会话 A 的事务ID。 原因分析出现 “process 16656 still waiting for ShareLock on transaction 21192006 after 1000.962 ms” 问题的原因很多的，个人觉得主要是以下方面的原因： 应用程序逻辑设计不合理，没有及时提交更新(update)事务； 被更新的表上索引太多，降低表上的更新操作（ update/delete/insert ）性能； 数据库负载高，导致 update 语句执行效率降低，耗时增加，当前端页面没及时返回时，出现重复更新同一记录的情况。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL: 数组类型(Array) 的使用","slug":"20111103105752","date":"2011-11-03T02:57:52.000Z","updated":"2018-09-04T01:33:52.725Z","comments":true,"path":"20111103105752.html","link":"","permalink":"https://postgres.fun/20111103105752.html","excerpt":"","text":"PostgreSQL 支持数组类型，包括一维数组和多维数组，在某些应用场合数组的应用还是很需要的，这里简单介绍下一维数组的使用及有关数组函数和操作符的使用。 #定义数组12345678910111213mydb=&gt; create table test_array(id serial primary key, phone int8[]); NOTICE: CREATE TABLE will create implicit sequence \"test_array_id_seq\" for serial column \"test_array.id\" NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_array_pkey\" for table \"test_array\" CREATE TABLE mydb=&gt; \\d test_array Table \"mydb.test_array\" Column | Type | Modifiers --------+----------+--------------------------------------------------------- id | integer | not null default nextval('test_array_id_seq'::regclass) phone | bigint[] | Indexes: \"test_array_pkey\" PRIMARY KEY, btree (id) 数组元素插入数组元素插入有两种方式 ，如下： 1234567891011121314mydb=&gt; insert into test_array(phone) values ('&#123;1,2&#125;'); INSERT 0 1 mydb=&gt; insert into test_array(phone) values ('&#123;2,3&#125;'); INSERT 0 1mydb=&gt; insert into test_array(phone) values (array[3,4,5]); INSERT 0 1mydb=&gt; select * From test_array; id | phone ----+--------- 1 | &#123;1,2&#125; 2 | &#123;2,3&#125; 3 | &#123;3,4,5&#125; (3 rows) 数组元素的引用12345678910mydb=&gt; select phone from test_array where id=1; phone ------- &#123;1,2&#125; (1 row)mydb=&gt; select phone[1],phone[2] from test_array where id=1; phone | phone -------+------- 1 | 2 数组操作符数组操作符如下图。 示例如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647equal mydb=&gt; select array[1,2]=array[1.1,2.1]::int[]; ?column? ---------- t (1 row)not equal mydb=&gt; select array[1,2] &lt;&gt; array[1,2,3]; ?column? ---------- t (1 row) less than mydb=&gt; select ARRAY[1,2,3] &lt; ARRAY[1,2,4]; ?column? ---------- t (1 row) greater than mydb=&gt; select ARRAY[1,4,3] &gt; ARRAY[1,2,4]; ?column? ---------- t (1 row) contains mydb=&gt; select ARRAY[1,4,3] @&gt; ARRAY[3,1]; ?column? ---------- t (1 row) is contained by mydb=&gt; select ARRAY[2,7] &lt;@ ARRAY[1,7,4,2,6]; ?column? ---------- t (1 row) overlap (have elements in common) mydb=&gt; select ARRAY[1,4,3] &amp;&amp; ARRAY[2,1]; ?column? ---------- t 常见数组函数将数据元素追加到数组12345mydb=&gt; select array_append(array[2,3,4],5); array_append -------------- &#123;2,3,4,5&#125; (1 row) 连接两个数组12345mydb=&gt; select array_cat(array[1,2],array[3,4]); array_cat ----------- &#123;1,2,3,4&#125; (1 row) 获得数组的维度12345678910mydb=&gt; select array_ndims(array[1,2,3]); array_ndims ------------- 1 (1 row)mydb=&gt; select array_ndims(array[[1,2,3],[4,5,6]]); array_ndims ------------- 2 (1 row) 获得数组的长度12345678910mydb=&gt; select array_length(array[1,2,3],1); array_length -------------- 3 (1 row)mydb=&gt; select array_length(array[[1,2],[2,3]],1); array_length -------------- 2 (1 row) intarray 模块的数组函数获取元素个数据总和12345678910mydb=&gt; select icount(array[1,2]); icount -------- 2 (1 row)mydb=&gt; select icount(array[[1,2],[2,3]]); icount -------- 4 (1 row) 排序123456789101112131415mydb=&gt; select sort_asc(array[4,8,7]); sort_asc ---------- &#123;4,7,8&#125; (1 row)mydb=&gt; select sort_desc(array[4,8,7]); sort_desc ----------- &#123;8,7,4&#125; (1 row)mydb=&gt; select sort_desc(array[[4,8,7],[8,9,7]]); sort_desc ------------------- &#123;&#123;9,8,8&#125;,&#123;7,7,4&#125;&#125; (1 row) intarray 模块的数组操作符 表数据123456789mydb=&gt; select * from test_array; id | phone ----+--------- 1 | &#123;1,2&#125; 2 | &#123;2,3&#125; 3 | &#123;3,4,5&#125; 4 | &#123;4,5,6&#125; 5 | &#123;4,5,7&#125; (5 rows) 查找包括相同元素的记录123456mydb=&gt; select id ,phone from test_array where phone &amp;&amp; array[1,2]::int8[]; id | phone ----+------- 1 | &#123;1,2&#125; 2 | &#123;2,3&#125; (2 rows) 查找数组元素的交集12345mydb=&gt; select array[1,2,3] &amp; array[3,4,5]; ?column? ---------- &#123;3&#125; (1 row) 索引的使用数组支持创建 GiST 和 GIN 类型索引，这两类索引的选择要根据场合，简单的说， GIN 类型索引在查询上要比 GiST 类型索引快，但在 update 的时候要慢些，所以 GIN 类型索引适合表数据不太变化的场合，而 GiST 索引适用于表数据经常需要 UPDATE 的场景。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 如何获取一维数组的相同元素并根据相似度排序","slug":"20111102222409","date":"2011-11-02T14:24:09.000Z","updated":"2018-09-04T01:33:52.663Z","comments":true,"path":"20111102222409.html","link":"","permalink":"https://postgres.fun/20111102222409.html","excerpt":"","text":"今天开发有个需求，表中有一个列为一维数组类型，现在需要找出表中具有相同元素的数据，描述起来可能有点费力，下面举个例子就明白了。 需求演示测试表12345678910111213141516mydb=&gt; \\d test_array; Table \"mydb.test_array\" Column | Type | Modifiers --------+----------+----------- id | integer | phone | bigint[] |mydb=&gt; select * from test_array; id | phone ----+------------- 1 | &#123;1,2&#125; 2 | &#123;1,2,3&#125; 3 | &#123;2,3&#125; 4 | &#123;1,2,3,4&#125; 5 | &#123;1,2,3,4,5&#125; 6 | &#123;4,5,6&#125; 备注: 给出一个 id, 然后找出与这个 id 对应的 phone 数组含有相同元素的记录，相同的元素越多，我们就认为这两个元素越相似，并根据相似度降序排序。 找出与 id=1 的 phone 数组含有相同的元素的记录1234567mydb=&gt; select id,phone from test_array where phone &amp;&amp; (select phone from test_array where id=1) and id!=1; id | phone ----+------------- 2 | &#123;1,2,3&#125; 3 | &#123;2,3&#125; 4 | &#123;1,2,3,4&#125; 5 | &#123;1,2,3,4,5&#125; 备注：上面SQL虽然能成功找出具有相同元素的记录，但是不能根据相似度排序，今天总结了下，有以下方法实现上面功能。 方法一：使用 intarray 模块使用intarray模块比较 int4[] 的数组类型。 安装 intarray 模块12mydb=# create extension intarray; CREATE EXTENSION 备注：intarray模块里有个 “&amp;” 函数，可以找到数组元素的相同部分， 具体信息可查阅手册 http://www.postgresql.org/docs/9.1/static/intarray.html &amp; 操作符使用12345mydb=&gt; select array[1,2,3] &amp; array[1,2]; ?column? ---------- &#123;1,2&#125; (1 row) 2.3 不支持 int8 类型的数组12345mydb=&gt; select array[11111111111,2,3] &amp; array[11111111111,2]; ERROR: operator does not exist: bigint[] &amp; bigint[] LINE 1: select array[11111111111,2,3] &amp; array[11111111111,2]; ^ HINT: No operator matches the given name and argument type(s). You might need to add explicit type casts. 备注：intarray 模块虽然能比较并获得数组的相同元素，但仅支持 int4 数组类型。 &amp; 操作符代码123456CREATE OPERATOR &amp; ( LEFTARG = _int4, RIGHTARG = _int4, COMMUTATOR = &amp;, PROCEDURE = _int_inter ); 备注：可以在 $PGHOME/share/extension 目录下查阅intarray–1.0.sql 文件。 方法二：自定义函数创建 intersection 函数，对 int8[] 数组类型进行比较创建函数，如下：1234567CREATE OR REPLACE FUNCTION intersection(anyarray, anyarray) RETURNS anyarray as $$ SELECT ARRAY( SELECT $1[i] FROM generate_series( array_lower($1, 1), array_upper($1, 1) ) i WHERE ARRAY[$1[i]] &amp;&amp; $2 ); $$ language sql; 备注：这里我们开发组的一名同事找到的，感谢这位同事。 测试12345mydb=&gt; select intersection(array[11111111111,2,3],array[11111111111,2,3]); intersection ------------------- &#123;11111111111,2,3&#125; (1 row) 备注：这次果然没报错了，这种方法虽然功能实现了，但效率如何呢？下面简单测试下。 性能测试创建测试表并插入数据12345678910111213141516171819202122mydb=&gt; create table array_test (skyid serial primary key,phone_list int8[]); NOTICE: CREATE TABLE will create implicit sequence \"array_test_skyid_seq\" for serial column \"array_test.skyid\" NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"array_test_pkey\" for table \"array_test\" CREATE TABLE mydb=&gt; insert into array_test(phone_list) select regexp_split_to_array(id1||';'||id2||';'||id3||';'||id4,';')::int8[] from phone ; INSERT 0 100000 mydb=&gt; select * from array_test limit 10; skyid | phone_list -------+--------------- 1 | &#123;1,2,3,4&#125; 2 | &#123;2,3,4,5&#125; 3 | &#123;3,4,5,6&#125; 4 | &#123;4,5,6,7&#125; 5 | &#123;5,6,7,8&#125; 6 | &#123;6,7,8,9&#125; 7 | &#123;7,8,9,10&#125; 8 | &#123;8,9,10,11&#125; 9 | &#123;9,10,11,12&#125; 10 | &#123;10,11,12,13&#125; (10 rows) 执行SQL12345678910mydb=&gt; select t2.skyid,t2.phone_list, array_length(intersection(t1.phone_list,t2.phone_list),1) mydb-&gt; from array_test t1, array_test t2 mydb-&gt; where t1.skyid=1 and t1.skyid!=t2.skyid and t1.phone_list &amp;&amp; t2.phone_list mydb-&gt; ; skyid | phone_list | array_length -------+------------+-------------- 2 | &#123;2,3,4,5&#125; | 3 3 | &#123;3,4,5,6&#125; | 2 4 | &#123;4,5,6,7&#125; | 1 (3 rows) 查看执行计划1234567891011121314151617mydb=&gt; explain analyze select t2.skyid,t2.phone_list, array_length(intersection(t1.phone_list,t2.phone_list),1) mydb-&gt; from array_test t1, array_test t2 mydb-&gt; where t1.skyid=8 and t1.skyid!=t2.skyid and t1.phone_list &amp;&amp; t2.phone_list mydb-&gt; order by array_length(intersection(t1.phone_list,t2.phone_list),1) desc; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------- Sort (cost=3743.94..3745.19 rows=500 width=110) (actual time=1279.393..1279.423 rows=6 loops=1) Sort Key: (array_length(intersection(t1.phone_list, t2.phone_list), 1)) Sort Method: quicksort Memory: 17kB -&gt; Nested Loop (cost=0.00..3721.53 rows=500 width=110) (actual time=0.651..1279.292 rows=6 loops=1) Join Filter: ((t1.skyid &lt;&gt; t2.skyid) AND (t1.phone_list &amp;&amp; t2.phone_list)) -&gt; Index Scan using array_test_pkey on array_test t1 (cost=0.00..8.28 rows=1 width=57) (actual time=0.236..0.275 rows=1 loops=1) Index Cond: (skyid = 8) -&gt; Seq Scan on array_test t2 (cost=0.00..2087.00 rows=100000 width=57) (actual time=0.013..608.045 rows=100000 loops=1) Total runtime: 1279.619 ms (9 rows) 创建 gin 索引12mydb=&gt; create index concurrently idx_array_test_phone_list on array_test using gin (phone_list); CREATE INDEX 再次查看PLAN123456789101112131415161718192021mydb=&gt; explain analyze select t2.skyid,t2.phone_list, array_length(intersection(t1.phone_list,t2.phone_list),1) mydb-&gt; from array_test t1, array_test t2 mydb-&gt; where t1.skyid=7 and t1.skyid!=t2.skyid and t1.phone_list &amp;&amp; t2.phone_list mydb-&gt; order by array_length(intersection(t1.phone_list,t2.phone_list),1) desc; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------------- Sort (cost=1070.18..1071.43 rows=500 width=110) (actual time=1.185..1.215 rows=6 loops=1) Sort Key: (array_length(intersection(t1.phone_list, t2.phone_list), 1)) Sort Method: quicksort Memory: 17kB -&gt; Nested Loop (cost=19.88..1047.77 rows=500 width=110) (actual time=0.854..1.117 rows=6 loops=1) Join Filter: (t1.skyid &lt;&gt; t2.skyid) -&gt; Index Scan using array_test_pkey on array_test t1 (cost=0.00..8.28 rows=1 width=57) (actual time=0.231..0.239 rows=1 loops=1) Index Cond: (skyid = 7) -&gt; Bitmap Heap Scan on array_test t2 (cost=19.88..905.74 rows=500 width=57) (actual time=0.226..0.264 rows=7 loops=1) Recheck Cond: (t1.phone_list &amp;&amp; phone_list) -&gt; Bitmap Index Scan on idx_array_test_phone_list (cost=0.00..19.75 rows=500 width=0) (actual time=0.123..0.123 rows=7 loops=1) Index Cond: (t1.phone_list &amp;&amp; phone_list) Total runtime: 1.399 ms (12 rows) 备注：由于测试是在虚拟机上进行，数据量并不大，但从上面看出上面的SQL在创建了 gin 类型索引后，执行时间在1.3 毫秒左右，效率显著提高。 参考 http://blog.163.com/digoal@126/blog/static/163877040201192624726272/ http://www.itfingers.com/Question/756871/postgres-function-to-return-the-intersection-of-2-arrays/zh http://www.postgresql.org/docs/9.1/static/intarray.html http://www.postgresql.org/docs/9.1/static/indexes-types.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Like 查询优化一例","slug":"20111025211743","date":"2011-10-25T13:17:43.000Z","updated":"2018-09-04T01:33:52.600Z","comments":true,"path":"20111025211743.html","link":"","permalink":"https://postgres.fun/20111025211743.html","excerpt":"","text":"今天发现一生产库上的负载较高，是因为一个与 like 有关的 SQL 引起的，虽然情形比较简单，但也记录下，这个查询语句走了全表扫描非常慢，可以改下SQL，优化后，执行时间由原来的 4秒可降到 1 毫秒以内，详细信息，如下。 SQL 语句12select user_id, account, nickname, sex, city, head_icon, country_code from tmp_user where 1=1 and user_id &lt;&gt; 226102033 and sex=1 and province like '%云南%' limit 20; 备注：从数据库日志来看，每天有大量类似语句，执行时间都在1秒以下，有的甚至达到 4秒。 表信息表结构1234567891011121314151617181920212223242526272829skytf=&gt; \\d tmp_user Table \"skytf.tmp_user\" Column | Type | Modifiers --------------+--------------------------------+----------------------------------------- user_id | integer | not null account | character varying(32) | not null nickname | character varying(64) | sex | smallint | default 1 birthday | character varying(20) | default '1991-01-01'::character varying email | character varying(20) | id_num | character varying(40) | mobile | character varying(20) | country | character varying(32) | default '中国'::character varying province | character varying(20) | city | character varying(16) | signature | character varying(128) | head_icon | integer | default 0 state | integer | default 0 reg_time | timestamp(0) without time zone | country_code | integer | default 86 Indexes: \"tmp_user_pkey\" PRIMARY KEY, btree (user_id) \"tbl_mpc_user_info_username_key\" UNIQUE, btree (account) \"tmp_user_birthday\" btree (birthday) \"tmp_user_city_index\" btree (city) \"tmp_user_nickname_index\" btree (nickname) \"tmp_user_province_index\" btree (province) \"tmp_user_reg_time_idx\" btree (reg_time) \"tmp_user_sex_index\" btree (sex, city, birthday, country, id_num) \"tmp_user_signature\" btree (signature) 备注：在字段 province 上有索引。 表大小12345skytf=&gt; \\dt+ tmp_user List of relations Schema | Name | Type | Owner | Size | Description ----------------+---------------------+-------+----------------+---------+------------- skytf | tmp_user | table | skytf | 2628 MB | 优化前的执行计划123456789skytf=&gt; explain analyze select user_id, account, nickname, sex, city, head_icon, country_code from tmp_user skytf-&gt; where 1=1 and user_id &lt;&gt; 226102033 and sex=1 and province like '%广西%' limit 20; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------ Limit (cost=0.00..511445.19 rows=1 width=61) (actual time=514.228..4489.034 rows=3 loops=1) -&gt; Seq Scan on tmp_user (cost=0.00..511445.19 rows=1 width=61) (actual time=514.225..4489.025 rows=3 loops=1) Filter: ((user_id &lt;&gt; 226102033) AND ((province)::text ~~ '%广西%'::text) AND (sex = 1)) Total runtime: 4489.073 ms (4 rows) 优化后的执行计划12345678910skytf=&gt; explain analyze select user_id, account, nickname, sex, city, head_icon, country_code from tmp_user skytf-&gt; where 1=1 and user_id &lt;&gt; 226102033 and sex=1 and province like '山东%' limit 20; QUERY PLAN --------------------------------------------------------------------------------------------------------------------------- Limit (cost=0.00..6.36 rows=1 width=61) (actual time=0.104..0.312 rows=1 loops=1) -&gt; Index Scan using tmp_user_province_index on tmp_user (cost=0.00..6.36 rows=1 width=61) (actual time=0.103..0.311 rows=1 loops=1) Index Cond: (((province)::text &gt;= '山东'::text) AND ((province)::text &lt; '山丝'::text)) Filter: ((user_id &lt;&gt; 226102033) AND ((province)::text ~~ '山东%'::text) AND (sex = 1)) Total runtime: 0.342 ms 备注：优化后，从计划看出已经走索引 tmp_user_province_index 了，时间下降到 0.3 毫秒。 总结 当检索条件为 like ‘%something%’ 时，这时用不到字段上的索引，会走全表扫描； 当检索条件为 like ‘something%’ 时，这时可以用到检索列上的索引。 在这种情况下， oracle 和 pg 类似，这里不测试了。","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Opitimize","slug":"Opitimize","permalink":"https://postgres.fun/tags/Opitimize/"}]},{"title":"How to Migrate Oracle to PostgreSQL","slug":"20111025155557","date":"2011-10-25T07:55:57.000Z","updated":"2018-09-04T01:33:52.538Z","comments":true,"path":"20111025155557.html","link":"","permalink":"https://postgres.fun/20111025155557.html","excerpt":"","text":"最近在做 oracle 转 PostgreSQL 项目调研，部分业务需要由 oracle 数据库迁移到 PostgreSQL 平台，这几天在做这方面的调研，暂时有几分心得。 Oracle 转 pg 显然是一个比较复杂的工程，需要考虑的事情很多，如果是比较繁忙且数据量大的系统，这个工作量和难度是很大的。不管是大库还是小库，总有些理论是相同的。 前期调研 了解业务：在项目开始前，需要和开发人员，项目经理进行充分的沟通，了解业务，了解原系统，了解数据分布； 表定义需改写: oracle 和 pg 数据类型不同，所以表定义脚本需要写； 哪些表需要迁数据？哪些表只需迁结构？ 存储过程，函数需改写 是否使用 job? 数据库用户权限如何迁移？ 大表如何迁移？ 备注：前期需要调研的内容很多，这里只列出了想到的一部分，在真正实施过程中肯定会遇到更多的问题。 数据库方案调研 是否需要做高可用？ 日志表是否需要分区？ 是否需要做连接池？ 根据业务特点进行个性化 postgresql 参数设置。 硬件资源需求（CPU, 内存，硬盘或存储） 数据类型对比Oracle 和 PG 数据类型对照表，如下： 数据库表迁移测试接下来做个测试：是从oracle 库到 pg 库的数据迁移, 仅以迁移一张表为例。目标将 oracle 库中的表 sup.TBL_SUP_WEATHER_UPDATE_LOG 迁移到 pg 库。这里 pg 库已创建。 oracle 库中的表结构12345678910111213141516171819202122232425262728293031323334SQL&gt; desc sup.TBL_SUP_WEATHER_UPDATE_LOG; Name Null? Type ----------------------------------------- -------- ---------------------------- ID NOT NULL NUMBER(11) APP_ID NOT NULL NUMBER(10) APP_VER NUMBER(10) SHORT_NAME VARCHAR2(255) HSMAN VARCHAR2(100) HSTYPE VARCHAR2(100) IMEI VARCHAR2(100) IMSI VARCHAR2(100) PLAT VARCHAR2(30) REQ_TYPE NUMBER(1) FEE_TYPE NUMBER(1) WIDTH NUMBER(4) HEIGHT NUMBER(4) PROVIDER NUMBER(1) MSG_CEN VARCHAR2(20) PORTV NUMBER(10) VMV NUMBER(10) IP_ADDR VARCHAR2(15) LOCAL_ADDRESS VARCHAR2(15) LOCAL_PORT NUMBER(5) RESPONSE_CODE NUMBER(3) FILE_LEN NUMBER(10) REQ_MD5 VARCHAR2(32) RESP_MD5 VARCHAR2(32) REQ_START_POS NUMBER(10) RESP_START_POS NUMBER(10) CHECK_INTERVAL NUMBER(10) CHECK_AFTER_TIMES NUMBER(5) RES_POLICY NUMBER(1) RES_MD5 VARCHAR2(32) CREATE_TIME DATE 在pg库中建表12345678910111213141516171819202122232425262728293031323334create table TBL_SUP_WEATHER_UPDATE_LOG ( ID integer NOT NULL, APP_ID integer NOT NULL , APP_VER integer, SHORT_NAME VARCHAR(255), HSMAN VARCHAR(100), HSTYPE VARCHAR(100), IMEI VARCHAR(100), IMSI VARCHAR(100), PLAT VARCHAR(30) , REQ_TYPE integer, FEE_TYPE integer, WIDTH integer, HEIGHT integer, PROVIDER integer, MSG_CEN VARCHAR(20), PORTV integer, VMV integer, IP_ADDR VARCHAR(15), LOCAL_ADDRESS VARCHAR(15), LOCAL_PORT integer, RESPONSE_CODE integer, FILE_LEN integer, REQ_MD5 VARCHAR(32), RESP_MD5 VARCHAR(32), REQ_START_POS integer, RESP_START_POS integer, CHECK_INTERVAL integer, CHECK_AFTER_TIMES integer, RES_POLICY integer, RES_MD5 VARCHAR(32), CREATE_TIME timestamp without time zone ); 备注：根据 oracle 和 pg 数据类型对应表，修改表定义中的数据类型即可得到相应的 pg 建表脚本。接下来几个脚本是用来取 oracle 数据的，并将数据导入到 pg 库中。 TBL_SUP_WEATHER_UPDATE_LOG.sql 脚本1234567891011set termout off set feedback off set verify off set echo off set pagesize 0 set trims on set linesize 1000spool \"TBL_SUP_WEATHER_UPDATE_LOG.log\" @ \"SELECT_TBL_SUP_WEATHER_UPDATE_LOG.sql\" spool off exit 备注：这个脚本功能是将 oracle 数据导入到文本文件，为了使得 spool 出来的文本仅包含数据，脚本做了格式化； SELECT_TBL_SUP_WEATHER_UPDATE_LOG.sql 脚本1select /*+ FULL(A) PARALLEL(A 8) */ nvl(to_char(ID ), 'N') || chr(9)||nvl(to_char(APP_ID ), 'N') || chr(9)|| nvl(to_char(APP_VER ), 'N') || chr(9)|| nvl(to_char(SHORT_NAME ), 'N') || chr(9)|| nvl(to_char(HSMAN ), 'N') || chr(9)|| nvl(to_char(HSTYPE ), 'N') || chr(9)|| nvl(to_char(IMEI ), 'N') || chr(9)|| nvl(to_char(IMSI ), 'N') || chr(9)|| nvl(to_char(PLAT ), 'N') || chr(9)|| nvl(to_char(REQ_TYPE ), 'N') || chr(9)|| nvl(to_char(FEE_TYPE ), 'N') || chr(9)|| nvl(to_char(WIDTH ), 'N') || chr(9)|| nvl(to_char(HEIGHT ), 'N') || chr(9)|| nvl(to_char(PROVIDER ), 'N') || chr(9)|| nvl(to_char(MSG_CEN ), 'N') || chr(9)|| nvl(to_char(PORTV ), 'N') || chr(9)|| nvl(to_char(VMV ), 'N') || chr(9)|| nvl(to_char(IP_ADDR ), 'N') || chr(9)|| nvl(to_char(LOCAL_ADDRESS ), 'N') || chr(9)|| nvl(to_char(LOCAL_PORT ), 'N') || chr(9)|| nvl(to_char(RESPONSE_CODE ), 'N') || chr(9)|| nvl(to_char(FILE_LEN ), 'N') || chr(9)|| nvl(to_char(REQ_MD5 ), 'N') || chr(9)|| nvl(to_char(RESP_MD5 ), 'N') || chr(9)|| nvl(to_char(REQ_START_POS ), 'N') || chr(9)|| nvl(to_char(RESP_START_POS ), 'N') || chr(9)|| nvl(to_char(CHECK_INTERVAL ), 'N') || chr(9)|| nvl(to_char(CHECK_AFTER_TIMES), 'N') || chr(9)|| nvl(to_char(RES_POLICY ), 'N') || chr(9)|| nvl(to_char(RES_MD5 ), 'N') || chr(9)|| nvl(to_char(CREATE_TIME ), 'N') from sup.TBL_SUP_WEATHER_UPDATE_LOG A; 备注：这里需要注意两点： 字段为空的用 “N” 填充，否则，在导入到 pg库中会报错； 在使用 nvl 参数时，date, NUMBER 字段需要转换成字符类型,为了方便，在使用 nvl 函数时, 建议所有字段都先转换成字符类型。 为了将 oracle 数据导出到 pg 能读取的格式，花了一番功夫才整出以上取数据 SQL。 执行脚本 TBL_SUP_WEATHER_UPDATE_LOG.sql 取数据123456789[oracle@db_sup](mailto:oracle@db_sup)-&gt; sqlplus \" /as sysdba\"SQL*Plus: Release 10.2.0.4.0 - Production on Tue Oct 25 10:43:36 2011Copyright (c) 1982, 2007, Oracle. All Rights Reserved.Connected to: Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 - 64bit Production With the Partitioning, OLAP, Data Mining and Real Application Testing options SQL&gt; @\"TBL_SUP_WEATHER_UPDATE_LOG.sql\"Disconnected from Oracle Database 10g Enterprise Edition Release 10.2.0.4.0 - 64bit Production With the Partitioning, OLAP, Data Mining and Real Application Testing options 备注：这里产生数据文件 TBL_SUP_WEATHER_UPDATE_LOG.log , 大小为 340M, 接下来将这个数据文件传到 pg 主机上。 文件列表12345[oracle@db_sup](mailto:oracle@db_sup)-&gt; ll total 340M -rw-r--r-- 1 oracle oinstall 1.6K Oct 25 10:43 SELECT_TBL_SUP_WEATHER_UPDATE_LOG.sql -rw-r--r-- 1 oracle oinstall 340M Oct 25 10:47 TBL_SUP_WEATHER_UPDATE_LOG.log -rw-r--r-- 1 oracle oinstall 207 Oct 25 10:26 TBL_SUP_WEATHER_UPDATE_LOG.sql 将数据导入到 PG库12345678910111213141516171819202122232425262728[postgres@db]$ psql -h 127.0.0.1 skytf_bak postgres psql (9.0.1) Type \"help\" for help.skytf_bak=# select count(*) from skytf.tbl_sup_weather_update_log; count ------- 0 (1 row)skytf_bak=# copy skytf.tbl_sup_weather_update_log from '/home/postgres/script/tf/TBL_SUP_WEATHER_UPDATE_LOG.log'; COPY 1479485skytf_bak=# select count(*) from skytf.tbl_sup_weather_update_log; count --------- 1479485 (1 row)skytf_bak=# select * From skytf.tbl_sup_weather_update_log limit 1; id | app_id | app_ver | short_name | hsman | hstype | imei | imsi | plat | req_type | fee_t ype | width | height | provider | msg_cen | portv | vmv | ip_addr | local_address | local_port | response_code | file_len | req_md5 | resp_md5 | req_start_pos | resp_start_pos | check_interval | check_after_times | res_policy | res_ md5 | create_time -------+--------+---------+------------+----------+----------+----------------------+----------------------+------+----------+------ -----------------+--------------------- 86712 | 2914 | 9000 | tianqi | b~1&amp;qqM= | zdzFow== | (Bzzb^zyL5zyftyy%fzy | )dzyLlyy$tzy$dyy~fzy | MTK | 0 | 2 | 240 | 320 | | 8613800775500 | 101081180 | 1964 | 211.138.250.103 | 192.168.171.39 | 6011 | 404 | | | | 0 | 0 | 10 | 10 | 1 | 683def7566d4099f 63589514dda23d9d | 2011-09-11 00:52:26 备注：到了这步，表 tbl_sup_weather_update_log 已成功从 oracle 库迁移到 pg库中。 难点如何将大表从 oracle 库中迁移到 PostgreSQL? 可以利用第三方工具，如 “ESF Database Migration Toolkit “, 也可以使用本文的方法，或许还有其它方法，但是我们需要最快的方法，尽可能的减少迁移时间。 备注：本文只是 Oracle 转 PostgreSQL 初始经验总结。 参考资料 http://wiki.postgresql.org/wiki/Oracle_to_Postgres_Conversion","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"探索：PostgreSQL 的 UPDATE 操作","slug":"20111022113035","date":"2011-10-22T03:30:35.000Z","updated":"2018-09-04T01:33:52.491Z","comments":true,"path":"20111022113035.html","link":"","permalink":"https://postgres.fun/20111022113035.html","excerpt":"","text":"根据 PostgreSQLL 的 MVCC 机制，在执行 update 命令更新数据时， PG 会在原有基础上复制一份新的复本 tuples 出来，然后在新的 tuples 上进行更新，下面验证下这个过程。 创建测试表1234skytf=&gt; create table test_59(id integer,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_59 values (1,'a'),(2,'b'),(3,'c'); INSERT 0 3 查询表的 ctid1234567skytf=&gt; select ctid, * from test_59; ctid | id | name -------+----+------ (0,1) | 1 | a (0,2) | 2 | b (0,3) | 3 | c (3 rows) 备注： ctid 为记录上逻辑标识，ctid 有两个字段组成，第一个字段表示 table 的逻辑 PAGE 编号，第二个字段表示每个逻辑 PAGE 上的第 N 条记录; 例如 ctid =(0,3) 表示这条记录位于这个表的第 0 PAGE 页的第三条记录。 更新记录更新表中的一条记录，如下：12skytf=&gt; update test_59 set name='aaa' where id=1; UPDATE 1 再次查询表的 ctid123456789101112skytf=&gt; select ctid, * from test_59; ctid | id | name -------+----+------ (0,2) | 2 | b (0,3) | 3 | c (0,4) | 1 | aaa (3 rows)skytf=&gt; select ctid, * from test_59 where id=1; ctid | id | name -------+----+------ (0,4) | 1 | aaa 备注：id=1 这条记录的 ctid 由原来的 (0,1) 变成现在的 (0,4) 了，说明在 update 时，是在这条记录的复本上进行 update, 同时老记录标识不可见，的记录依然在PAGE里，VACUUM之后才会将老记录清除，这里不再详述。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"MVCC","slug":"MVCC","permalink":"https://postgres.fun/tags/MVCC/"}]},{"title":"探索：PostgreSQL 的 MVCC 机制对性能的影响","slug":"20111021102003","date":"2011-10-21T02:20:03.000Z","updated":"2018-09-04T01:33:52.428Z","comments":true,"path":"20111021102003.html","link":"","permalink":"https://postgres.fun/20111021102003.html","excerpt":"","text":"今天思考了下 PostgreSQL 的 MVCC原理，PG在 delete 记录时， 不会立刻在物理上删除记录，而将原始记录保留在原来 page, 只是改变下状态位；而 update 记录时，会先复制一份新的记录，并在复制的这份上修改；想到：这种机制在大量数据更新后，是否会影响查询性能？ 下面实验模拟下，模拟两个场景： 场景一：大量数据更新后，全表扫描的性能是否有变化？ 场景二：大量数据更新后，走索引的查询SQL性能是否有变化？ 为了准确的测试，在测试前将 PostgreSQL 的 autovacuum 参数设置成 off 。 场景一：测试全表扫描创建测试表并插入数据123456skytf=&gt; create table test_59 ( id integer primary key ,name varchar(32)); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_59_pkey\" for table \"test_59\" CREATE TABLEskytf=&gt; insert into test_59 select generate_series(1,10000000),'francs'; INSERT 0 10000000 表分析123456789101112131415skytf=&gt; \\dt+ test_59 List of relations Schema | Name | Type | Owner | Size | Description --------+---------+-------+-------+--------+------------- skytf | test_59 | table | skytf | 422 MB | (1 row)skytf=&gt; analyze test_59; ANALYZEskytf=&gt; select relname,relpages,reltuples from pg_class where relname='test_59'; relname | relpages | reltuples ---------+----------+----------- test_59 | 54055 | 1e+07 (1 row) 备注：原表大小为 422M, 共有 54055 PAGE。 执行计划12345678skytf=&gt; explain ( analyze on, buffers on ) select count(*) from test_59; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------- Aggregate (cost=179055.00..179055.01 rows=1 width=0) (actual time=11958.853..11958.854 rows=1 loops=1) Buffers: shared hit=54055 -&gt; Seq Scan on test_59 (cost=0.00..154055.00 rows=10000000 width=0) (actual time=0.008..5984.120 rows=10000000 loops=1) Buffers: shared hit=54055 Total runtime: 11958.889 ms(5 rows) 备注：”shared hit” 值为 54055, 扫描了 54055 PAGE。耗时为 11.9秒左右； 全表更新123456789101112131415161718skytf=&gt; update test_59 set name='aaa'; UPDATE 10000000skytf=&gt; \\dt+ test_59 List of relations Schema | Name | Type | Owner | Size | Description --------+---------+-------+-------+--------+------------- skytf | test_59 | table | skytf | 768 MB | (1 row)skytf=&gt; analyze test_59; ANALYZEskytf=&gt; select relname,relpages,reltuples from pg_class where relname='test_59'; relname | relpages | reltuples ---------+----------+----------- test_59 | 98302 | 1e+07 (1 row) 备注：表表更新后，表大小增加了,为 768M, PAGE也增加了，为 98302。 再次 EXPLAIN12345678skytf=&gt; explain ( analyze on, buffers on ) select count(*) from test_59; QUERY PLAN -------------------------------------------------------------------------------------------------------------------------------- Aggregate (cost=223302.00..223302.01 rows=1 width=0) (actual time=14442.960..14442.961 rows=1 loops=1) Buffers: shared hit=71327 read=26975 written=27 -&gt; Seq Scan on test_59 (cost=0.00..198302.00 rows=10000000 width=0) (actual time=2452.108..8477.221 rows=10000000 loops=1) Buffers: shared hit=71327 read=26975 written=27 Total runtime: 14443.009 ms(5 rows) 备注：再次观察PLAN，发现扫描的 Buffers 总数为 “71327+26975=98302” , 耗时也增加了，为 14.4 秒左右, 可见大批量数据修改后，全表扫描的SQL扫描的块更多，耗时也增加了。 vacuum12345678910skytf=&gt; vacuum test_59; VACUUMskytf=&gt; \\dt+ test_59 List of relations Schema | Name | Type | Owner | Size | Description --------+---------+-------+-------+--------+------------- skytf | test_59 | table | skytf | 769 MB | Time: 24472.248 ms 查看执行计划，如下：123456789skytf=&gt; explain ( analyze on, buffers on ) select count(*) from test_59; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------ Aggregate (cost=223410.00..223410.01 rows=1 width=0) (actual time=11867.036..11867.037 rows=1 loops=1) Buffers: shared hit=85158 read=13252 -&gt; Seq Scan on test_59 (cost=0.00..198410.00 rows=10000000 width=0) (actual time=61.639..5903.459 rows=10000000 loops=1) Buffers: shared hit=85158 read=13252 Total runtime: 11867.089 ms (5 rows) 备注：vaccum后，表大小没变，全表扫描扫描的PAGE数也没变； 场景二：测试走索引的查询索引查询1234567skytf=&gt; explain ( analyze on, buffers on ) select * from test_59 where id=1; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------- Index Scan using test_59_pkey on test_59 (cost=0.00..5.91 rows=1 width=8) (actual time=0.017..0.020 rows=1 loops=1) Index Cond: (id = 1) Buffers: shared hit=6 Total runtime: 0.048 ms(4 rows) 备注：索引索引只扫描了 6个 page。 批量更新 function12345678910CREATE or replace FUNCTION fun_update_test_59() RETURNS INTEGER AS $$ DECLARE i INTEGER ; BEGIN for i in 1..10000 loop update test_59 set name='test' where id=1; end loop; return 1; END; $$ LANGUAGE 'plpgsql'; 备注：计划只更新 id=1 的这条记录，这里用了一个 function 反复更新同一记录； 执行批量更新函数，如下。123456skytf=&gt; select fun_update_test_59() ; fun_update_test_59 -------------------- 1 (1 row)Time: 2191.491 ms 再次查看PLAN12345678skytf=&gt; explain ( analyze on, buffers on ) select * from test_59 where id=1; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------- Index Scan using test_59_pkey on test_59 (cost=0.00..5.91 rows=1 width=8) (actual time=0.036..1.555 rows=1 loops=1) Index Cond: (id = 1) Buffers: shared hit=59 Total runtime: 1.579 ms (4 rows) 备注：id=1 的这条记录被更新 10000 次以后，再次索引扫描却扫描了 59 个 page， 说明数据更新后，对走索引的扫描语句的性能也有一定影响； 再次执行 explain analyze，如下：12345678skytf=&gt; explain ( analyze on, buffers on ) select * from test_59 where id=1; QUERY PLAN ---------------------------------------------------------------------------------------------------------------------- Index Scan using test_59_pkey on test_59 (cost=0.00..5.91 rows=1 width=8) (actual time=0.020..0.022 rows=1 loops=1) Index Cond: (id = 1) Buffers: shared hit=4 Total runtime: 0.049 ms (4 rows) 备注：再次”explain analyze” 后，发现扫描的块又回到了 4, 这是因为再次查询后，需要的数据都在内存里。 总结 PG大批量 UPDATE 数据后，全表扫描SQL，索引扫描SQL将扫描更多的块，性能会降低。 可以推测PG大批量 DELETE 数据后 , 全表扫描SQL，索引扫描SQL扫描块不变，性能不会有太大影响。 对于ORACLE，根据以前的经验，UPDATE 大量数据后，由于更新的数据不在原表 PAGE保存，而保存在UNDO 表空间，性能不会有太大影响。 对于ORACLE，根据以前的经验，DELETE大量数据后, 由于表的 HWM 不会降低，性能不会有太大影响。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"MVCC","slug":"MVCC","permalink":"https://postgres.fun/tags/MVCC/"}]},{"title":"PostgreSQL : 慎用 db_link","slug":"20111011104119","date":"2011-10-11T02:41:19.000Z","updated":"2018-09-04T01:33:52.366Z","comments":true,"path":"20111011104119.html","link":"","permalink":"https://postgres.fun/20111011104119.html","excerpt":"","text":"今天由于业务需要，需要在生产库上创建一个 db_link, 每天取源端库一张表一天数据，表大小为2G左右，考虑到这个数据每天只取一次，而且只取一天数据，于是准备在库上创建 db_link, 但在测试过程中发现性能问题。 在创建完 db_link 后，在库上执行 select * from view_xxx limit 1 时，发现十分钟都没有执行完, 而且数据库负载很高，达到 20, 同时 IO 等侍也很高，达到 20， 下面在测试环境下模拟下这种情形。 环境准备主机： 8核 8G源库 mpc_db_statistic源库表 mpc_log_register（表大小: 193 M）测试库 skytf视图 view_mpc_log_register 准备在数据库 skytf 创建视图 view_mpc_log_register 取源库 mpc_db_statistic 的数据 mpc_log_register。 mpc_log_register 表结构1234567891011121314151617181920212223Table \"mpc_db_statistic.mpc_log_register\" Column | Type | Modifiers ---------------+--------------------------------+---------------------------------------------------------------------- record_id | integer | not null default nextval('mpc_log_register_record_id_seq'::regclass) client_ip | character varying(64) | not null client_type | integer | default 0 reg_time | timestamp(0) without time zone | reg_account | character varying(32) | reg_userid | integer | default 0 update_time | timestamp(0) without time zone | default now() imsi | character varying | imei | character varying | manufacturers | character varying | model | character varying | appid | integer | scr_width | integer | scr_height | integer | sms_center | character varying | mem_size | integer | access_ip | character varying(32) | Indexes: \"mpc_log_register_pkey\" PRIMARY KEY, btree (record_id) \"log_register_client_id_index\" btree (client_ip) 创建视图1234567891011121314151617181920212223CREATE OR REPLACE VIEW view_mpc_log_register AS SELECT * FROM dblink('dbname=mpc_db_statistic host=localhost port=1921 user=mpc_db_statistic password=mpc_db_statistic' , 'select * from mpc_log_register') as t1 ( record_id integer , client_ip character varying(64) , client_type integer , reg_time timestamp(0) without time zone , reg_account character varying(32) , reg_userid integer , update_time timestamp(0) without time zone , imsi character varying , imei character varying , manufacturers character varying , model character varying , appid integer , scr_width integer , scr_height integer , sms_center character varying , mem_size integer , access_ip character varying(32) ); 配置 pg_hba.conf1host mpc_db_statistic mpc_db_statistic 127.0.0.1/32 md5 备注：这行需要加到 pg_hba.conf ；另外 # IPv4 local connections: 部分第一行，如果不加这行，那么在查询视图时可能会报以下ERROR：123ERROR: password is required DETAIL: Non-superuser cannot connect if the server does not request a password. HINT: Target servers authentication method must be changed. 查询视图测试1234567skytf=&gt; select record_id,client_ip from view_mpc_log_register limit 1; NOTICE: identifier \"dbname=mpc_db_statistic host=localhost port=1921 user=mpc_db_statistic password=mpc_db_statistic\" will be truncated to \"dbname=mpc_db_statistic host=localhost port=1921 user=mpc_db_st\" record_id | client_ip -----------+--------------------- 235215 | 117.136.25.45:30119 (1 row)Time: 16697.482 ms 查询当前进程123456postgres=# select datname,procpid,current_query from pg_stat_activity where current_query !='&lt;IDLE&gt;'; datname | procpid | current_query ------------------+---------+-------------------------------------------------------------------------------------------- skytf | 29265 | select record_id,client_ip from view_mpc_log_register where record_id=1 limit 1; postgres | 29853 | select datname,procpid,current_query from pg_stat_activity where current_query !='&lt;IDLE&gt;'; mpc_db_statistic | 29946 | select * from mpc_log_register 备注：一开始查询，可以看到有两个进程，一个是 skytf 库查询视图进程，另一个是 mpc_db_statistic库的全表扫描进程, 奇怪了，进程 29265 明明是根据索引字段 record_id 来查询的，为什么到了目标库就是全表扫描 select * from mpc_log_register？ 继续查询当前进程123456postgres=# select datname,procpid,current_query from pg_stat_activity where current_query !='&lt;IDLE&gt;'; datname | procpid | current_query ----------+---------+-------------------------------------------------------------------------------------------- skytf | 29265 | select record_id,client_ip from view_mpc_log_register where record_id=1 limit 1; postgres | 29853 | select datname,procpid,current_query from pg_stat_activity where current_query !='&lt;IDLE&gt;'; (2 rows) 备注：过一会儿，发现目标库进程 29946 执行完毕，此时还有进程 29265 在跑。 查看内存使用SQL执行过程中内存使用情况12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485[root@ ~]# free -m total used free shared buffers cached Mem: 16053 656 15396 0 8 501 -/+ buffers/cache: 146 15906 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 792 15260 0 8 501 -/+ buffers/cache: 282 15771 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 849 15203 0 8 501 -/+ buffers/cache: 339 15714 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1012 15041 0 8 501 -/+ buffers/cache: 501 15551 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1126 14926 0 8 501 -/+ buffers/cache: 616 15436 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1166 14886 0 8 501 -/+ buffers/cache: 656 15396 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1209 14843 0 8 501 -/+ buffers/cache: 699 15354 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1318 14734 0 8 514 -/+ buffers/cache: 795 15257 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1495 14558 0 8 546 -/+ buffers/cache: 940 15113 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1585 14467 0 8 563 -/+ buffers/cache: 1014 15039 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1672 14380 0 8 578 -/+ buffers/cache: 1085 14968 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1762 14290 0 8 595 -/+ buffers/cache: 1158 14895 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1858 14194 0 8 614 -/+ buffers/cache: 1235 14817 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 1952 14100 0 8 632 -/+ buffers/cache: 1311 14741 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 2044 14008 0 8 650 -/+ buffers/cache: 1385 14668 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 2140 13912 0 8 669 -/+ buffers/cache: 1462 14590 Swap: 16386 31 16355[root@ ~]# free -m total used free shared buffers cached Mem: 16053 658 15394 0 8 501 -/+ buffers/cache: 148 15904 Swap: 16386 31 16355 备注：发现内存 used 值开始由 656M 一直增加到 2140 M, 当 SQL执行完后，又再次回到 658 M, 推测SQL通过 db_link 查询远端数据时，会将远端表先取到本地库内存里，这个步骤非常耗时，耗IO，同时本地库负载也比异常高。 结果出来了1234567skytf=&gt; select record_id,client_ip from view_mpc_log_register limit 1; NOTICE: identifier \"dbname=mpc_db_statistic host=localhost port=1921 user=mpc_db_statistic password=mpc_db_statistic\" will be truncated to \"dbname=mpc_db_statistic host=localhost port=1921 user=mpc_db_st\" record_id | client_ip -----------+--------------------- 235215 | 117.136.25.45:30119 (1 row)Time: 16697.482 ms 备注：这张表不太大，只有193M，都花了 16 秒左右。 执行计划1234567skytf=&gt; explain select record_id,client_ip from view_mpc_log_register where record_id=1 limit 1; QUERY PLAN ----------------------------------------------------------------------- Limit (cost=0.00..2.50 rows=1 width=150) -&gt; Function Scan on dblink t1 (cost=0.00..12.50 rows=5 width=150) Filter: (record_id = 1) (3 rows) 备注： 走的是函数索引。 大表测试由于前面源表大小才 193M，效果不明显，后来拿了张2G大小的表做实验，通过视图取一条数据，后来数据库负载直接升到40左右，连数据库主机都 SSH 不上了。汗， PostgreSQL 的 db_link 得慎用。 总结 PostgreSQL 的 db_link 要慎用，没有特殊的需求不要使用，如果业务需要可以通过其它方式代替； PostgreSQL 的 db_link 在使用过程中会先把对端表数据全表扫描到本地库，尽管查询视图时用到了索引字段，但依然不会走索引扫描，而是走 “Function Scan” ，从而带来严重的性能问题 （负载高，IO高）。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"ERROR: database is not accepting commands to avoid wraparound data loss in database","slug":"20110930171458","date":"2011-09-30T09:14:58.000Z","updated":"2018-09-04T01:33:52.319Z","comments":true,"path":"20110930171458.html","link":"","permalink":"https://postgres.fun/20110930171458.html","excerpt":"","text":"今天有个日志库遭遇 ERROR: database is not accepting commands to avoid wraparound data loss in database ,HINT: Stop the postmaster and use a standalone backend to VACUUM in &quot;mydb&quot;.,根据提示，数据库已经不接受事务，需要关闭后以单用户模式 vacuum full。 真是杯具啊，生产库遭受这样的错误，无疑是致命的打击，日志库有几百G数据，所有库 vacuum full 至少需要好几小时，显然这无法接受的，迅速将故障报给德哥，还好德哥反应快，很快搭建了套只有表结构的日志库，让业务继续跑起来，这样可以有充足的时间维护这个故障库,下面是处理过程。 日志分析2011-09-28号时的日志 ( WARNING )12WARNING: database \"mydb\" must be vacuumed within 177009986 transactions HINT: To avoid a database shutdown, execute a database-wide VACUUM in \"mydb\". 备注： 这是故障的初级阶段，如果在这时发现问题，可以手工修复而不需要停库维护了；这时只需要在提示的库上用超级用户登陆进行全库 Vacuum 就可以了。 2011-09-29 号的日志 ( ERROR )1232011-09-29 09:42:33.273 CST,,,30165,,4e83cd09.75d5,1,,2011-09-29 09:42:33 CST,138/13556266,0,ERROR,54000,\"database is not accepting commands to avoid wraparound data loss in database \"\"mydb\"\"\",,\"Stop the postmaster and use a standalone backend to vacuum that database. You might also need to commit or roll back old prepared transactions.\",,,,,,,\"\" 2011-09-29 09:42:33.274 CST,\"mydb\",\"mydb\",10928,\"192.168.171.50:34763\",4e813f57.2ab0,247465,\"INSERT\",2011-09-27 11:13:27 CST,94/34368952,0,ERROR,54000,\"database is not accepting commands to avoid wraparound data loss in database \"\"mydb\"\"\",,Stop the postmaster and use a standalone backend to vacuum that database. 备注：到了这个程度，根据提示，数据库需要停库以单用户模式进行 vacuum , 下面是单用户模式对数据库进行维护的例子。 关闭数据库123[postgres@pgb ~]$ pg_ctl stop -m fast -D $PGDATA waiting for server to shut down...... done server stopped 单用户模式维护单用户模式维护数据库 mydb1234[postgres@pgb ~]$ postgres --single -D /database/pgdata1923_9.1/pg_root mydbPostgreSQL stand-alone backend 9.1.0 backend&gt; vacuum full analyze; backend&gt; Crtl+D 退出 备注：后来在这个日志库上对所有日志库进行以上操作，重启后数据库不再报以上ERROR，恢复正常。 原因分析PostgreSQL 使用的是 MVCC 机制, 可以分配 2^32 ( 40 亿) 次方事务， 当事务号分配完后，将归0，PG是这样处理的: 在达到 20 亿事务后需要对整个数据库进行 vacuum。 这个操作一般 autovacuum 可以完成，不需要手工干预，当 autovacuum 由于某种原因不能对数据库表进行 vacuum时，会出现开头 “WARNING” 的提示信息，如果再不继续处理，接着会出现“ERROR” 信息，这时唯一处理的方法是：先停库 ，再以单用户模式 VACUUM FULL。今天的故障原因猜测是有长时间未提交事务导致 autovacuum 表失败； 后期计划 后期计划将长时间事务进行监控； 参考资料 http://www.postgresql.org/docs/9.0/static/routine-vacuuming.html#VACUUM-FOR-WRAPAROUND http://blog.163.com/digoal@126/blog/static/163877040201041111516154/","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL9.1: Converting a INDEXED varchar column to text still requires rewrite a index","slug":"20110923105316","date":"2011-09-23T02:53:16.000Z","updated":"2018-09-04T01:33:52.272Z","comments":true,"path":"20110923105316.html","link":"","permalink":"https://postgres.fun/20110923105316.html","excerpt":"","text":"上一篇blog讨论了PostgreSQL9.1的新特性之一, 即将字段类型由 varchar 转换成 text 类型时不再需要重写表了, 原文地址 https://postgres.fun/20110916165650.html ,但是有种情况例外，当这个字段被索引时，索引需要重写，下面是测试过程。 环境准备创建测试表12345678[postgres@pgb 16386]$ psql mydb mydb psql (9.1.0) Type \"help\" for help.mydb=&gt; create table test_13 (id integer,name varchar(32)); CREATE TABLEmydb=&gt; insert into test_13 select generate_series(1,1000000),'aaa'; INSERT 0 1000000 创建索引并分析表12345mydb=&gt; create index idx_test_13_name on test_13 using btree (name); CREATE INDEXmydb=&gt; analyze test_13; ANALYZE 查询表，索引统计信息1234567891011mydb=&gt; select relname,relowner,relfilenode,relpages,reltuples from pg_class where relname='test_13'; relname | relowner | relfilenode | relpages | reltuples ---------+----------+-------------+----------+----------- test_13 | 16384 | 32793 | 4425 | 1e+06 (1 row)mydb=&gt; select relname,relowner,relfilenode,relpages,reltuples from pg_class where relname='idx_test_13_name'; relname | relowner | relfilenode | relpages | reltuples ------------------+----------+-------------+----------+----------- idx_test_13_name | 16384 | 32796 | 2198 | 1e+06 (1 row) 修改字段类型 ( varchar –&gt; text )123456mydb=&gt; \\timing Timing is on.mydb=&gt; alter table test_13 alter column name type text; ALTER TABLE Time: 9192.565 ms 备注：表上有100万数据，将字段类型由 varchar 修改成 text 类型时，花费了 9 秒，猜测重写了表，接着往下看。 123456mydb=&gt; \\d test_13 Table \"mydb.test_13\" Column | Type | Modifiers --------+---------+----------- id | integer | name | text | 再次查看表，索引信息1234567891011mydb=&gt; select relname,relowner,relfilenode,relpages,reltuples from pg_class where relname='test_13'; relname | relowner | relfilenode | relpages | reltuples ---------+----------+-------------+----------+----------- test_13 | 16384 | 32793 | 4425 | 1e+06 (1 row)mydb=&gt; select relname,relowner,relfilenode,relpages,reltuples from pg_class where relname='idx_test_13_name'; relname | relowner | relfilenode | relpages | reltuples ------------------+----------+-------------+----------+----------- idx_test_13_name | 16384 | 32797 | 2198 | 1e+06 (1 row) 备注：索引的 relfilenode 发生了变化，由原来的 32796 变成了 32797, 说明索引被重写; 而表的 relfilenode 没有变化，依然是 32793 , 说明表数据没有被重写。接下来做进一步分析，看看将 text 类型字段转换成 varchar 会是什么情况。 修改字段类型 ( text –&gt; varchar )123mydb=&gt; alter table test_13 alter name type varchar(32); ALTER TABLE Time: 13660.555 ms 备注， 将字段类型由 text 改成成 varchar 时，花费了 13 秒左右，说明重写了表。 再次查看表，索引信息1234567891011mydb=&gt; select relname,relowner,relfilenode,relpages,reltuples from pg_class where relname='test_13'; relname | relowner | relfilenode | relpages | reltuples ---------+----------+-------------+----------+----------- test_13 | 16384 | 32802 | 4425 | 1e+06 (1 row)mydb=&gt; select relname,relowner,relfilenode,relpages,reltuples from pg_class where relname='idx_test_13_name'; relname | relowner | relfilenode | relpages | reltuples ------------------+----------+-------------+----------+----------- idx_test_13_name | 16384 | 32805 | 2198 | 1e+06 (1 row) 备注：表的 relfilenode 发生了变化，由原来的 32793 变化成 32802, 说明表被重写了。索引的 relfilenode 也发生了变化，由原来的 32797 变化成了 32805, 说明也被重写了。 总结 当类型为 varchar 的字段上没有索引时，将其转换成 text 时，不需要重写表和索引。 当类型为 varchar 的字段上有 btree 索引时, 不需要重写表数据，但需要重写索引。 其它索引类型没有测试，有兴趣的朋友可以测试下，推测是和第2点情况一样。 当类型为 text 的字段转换成 varchar 类型时，表和索引需要重写。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL9.1新特性之四：Converting a varchar column to text no longer requires a rewrite of the table","slug":"20110916165650","date":"2011-09-16T08:56:50.000Z","updated":"2018-09-04T01:33:52.209Z","comments":true,"path":"20110916165650.html","link":"","permalink":"https://postgres.fun/20110916165650.html","excerpt":"","text":"今天看到了 PostgreSQL9.1 一个令人兴奋的新特性，9.1 将字断类型从 varchar 修改成 text 类型，将不会重写表数据。而扩字断长度依然会重写表数据，但是这些在以后版本可能会改进，下面测试下： 9.0 版本上测试查看版本12345skytf=&gt; select version(); version ------------------------------------------------------------------------------------------------------------------- PostgreSQL 9.0.1 on x86_64-unknown-linux-gnu, compiled by GCC gcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-48), 64-bit (1 row) 创建测试表并插入数据1234567891011121314mydb=&gt; create table test_47 (id integer, name varchar(32)); CREATE TABLEmydb=&gt; insert into test_47 select generate_series(1,5000000),'francs'; INSERT 0 5000000mydb=&gt; timing Timing is on.mydb=&gt; \\d test_47 Table \"mydb.test_47\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | 修改字段 name 类型12345678910skytf=&gt; alter table test_47 alter column name type text; ALTER TABLE Time: 18925.293 msskytf=&gt; \\d test_47 Table \"skytf.test_47\" Column | Type | Modifiers --------+---------+----------- id | integer | name | text | 备注：9.0 版本将字段类型由 varchar 修改成 text 时，花费了 19 秒左右，可见重写了这个字段。 9.1 版本上测试查看版本12345mydb=&gt; select version(); version -------------------------------------------------------------------------------------------------------- PostgreSQL 9.1.0 on i686-pc-linux-gnu, compiled by gcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-48), 32-bit (1 row) 创建测试表并插入数据1234567mydb=&gt; create table test_47 (id integer, name varchar(32)); CREATE TABLEmydb=&gt; insert into test_47 select generate_series(1,5000000),'francs'; INSERT 0 5000000mydb=&gt; timing Timing is on. 修改字段 name 类型12345678910mydb=&gt; alter table test_47 alter column name type text; ALTER TABLE Time: 122.347 msmydb=&gt; \\d test_47 Table \"mydb.test_47\" Column | Type | Modifiers --------+---------+----------- id | integer | name | text | 备注：9.1版本将字段类型由 varchar 修改成 text 类型很快就完成了，果然没有重写表数据。 总结平常在数据库维护过程中扩字断及修改数据类型是非常痛苦的事情,因为要重写表，如果表比较大，这无疑对生产系统业务是致使的打击，虽然 9.1 版本只在字段类型由 varchar 转换成 text 这种情况下不用重写表，但这依然令人兴奋，因为这至少说明这是 PG 核心组在这方面正在努力改进，相信在以后的版本中，像扩字段这样的操作不用重写表, 下面总结下修改表结构几种情况： 字段类型扩长，会重写表， 如 varchar(30)扩成 varchar(128)； 修改字段类型，会重写表, 如 integer 类型转换成 varchar ; 字段类型由 varchar 转换成 text 类型不会重写表。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"编译 PostgreSQL \"--Without-ldap\" 报错一例","slug":"20110916134302","date":"2011-09-16T05:43:02.000Z","updated":"2018-09-04T01:33:52.163Z","comments":true,"path":"20110916134302.html","link":"","permalink":"https://postgres.fun/20110916134302.html","excerpt":"","text":"由于准备做 oracle_fdw 实验, 今天打算重新安装 PostgreSQL 9.1, 所以计划将 PostgreSQL 配置成没有安装 ldap 的模式, 但在编译过程中遇到了错误，以下是详细信息。 环境信息PostgreSQL: Postgresql 9.1OS : Red Hat Enterprise Linux Server release 5.5 (Tikanga)硬件： 笔记本上的虚拟机 安装 PostgreSQL执行 configure 配置 PostgreSQL1./configure --prefix=/opt/pgsql9.1 --with-pgport=1923 --with-segsize=8 --with-wal-segsize=64 --with-wal-blocksize=64 --with-perl --with-python --with-openssl --with-pam --with-libxml --with-libxslt --enable-thread-safety without-ldap 备注： 这个配置脚本不安装 ldap 模块，增加了 “–without-ldap” 参数，其它的配置以前经常用到，都正常。 然后执行 gmake world 进行编译， 这步遇到错误，错误信息如下，只贴出后面的主要信息G。12345678910111213141516171819202122232425262728293031utils/time/tqual.o utils/time/snapmgr.o utils/fmgrtab.o ../../src/timezone/localtime.o ../../src/timezone/strftime.o ../../src/timezone/pgtz.o ../../src/port/libpgport_srv.a -lxslt -lxml2 -lpam -lssl -lcrypto -lcrypt -ldl -lm -o postgres libpq/auth.o: In function `InitializeLDAPConnection': auth.c:(.text+0x2c4): undefined reference to `ldap_init' auth.c:(.text+0x2e7): undefined reference to `ldap_set_option' auth.c:(.text+0x329): undefined reference to `ldap_start_tls_s' auth.c:(.text+0x339): undefined reference to `ldap_unbind' auth.c:(.text+0x37e): undefined reference to `ldap_unbind' libpq/auth.o: In function `CheckLDAPAuth': auth.c:(.text+0x163a): undefined reference to `ldap_simple_bind_s' auth.c:(.text+0x1647): undefined reference to `ldap_unbind' auth.c:(.text+0x1690): undefined reference to `ldap_simple_bind_s' auth.c:(.text+0x174c): undefined reference to `ldap_search_s' auth.c:(.text+0x1768): undefined reference to `ldap_count_entries' auth.c:(.text+0x1783): undefined reference to `ldap_first_entry' auth.c:(.text+0x1792): undefined reference to `ldap_get_dn' auth.c:(.text+0x17c3): undefined reference to `ldap_memfree' auth.c:(.text+0x17ce): undefined reference to `ldap_msgfree' auth.c:(.text+0x17d9): undefined reference to `ldap_unbind_s' auth.c:(.text+0x1929): undefined reference to `ldap_get_option' auth.c:(.text+0x1964): undefined reference to `ldap_err2string' auth.c:(.text+0x19c2): undefined reference to `ldap_get_option' auth.c:(.text+0x19fd): undefined reference to `ldap_err2string' auth.c:(.text+0x1a42): undefined reference to `ldap_msgfree' auth.c:(.text+0x1a5e): undefined reference to `ldap_count_entries' auth.c:(.text+0x1b0a): undefined reference to `ldap_count_entries' collect2: ld 返回 1 gmake[2]: * [postgres] 错误 1 gmake[2]: Leaving directory `/opt/soft_bak/postgresql-9.1.0/src/backend' gmake[1]: * [all-backend-recurse] 错误 2 gmake[1]: Leaving directory `/opt/soft_bak/postgresql-9.1.0/src' gmake: * [world-src-recurse] 错误 2 备注：遇到这种问题非常无语，通常在编译过程中报错都是由于操作系统包没打全，但这个 ERROR 应该不是这原因，后来网上查了些资料，有人说在由于以前执行过安装 ldap模块的 configure 命令，会产生 很多临时文件，所以再次执行不安装 ldap 模块的 configure 命令时会报错，解决方法是在 configure 之前先执行 make distclean 命令。 执行 Make Distcleanmake distclean 命令是将先前configure 生成的文件全部删除掉，包括Makefile。在 configure 操作前先执行了下 make distclean , 之后编译就没报错了。 重新编译gamke world 终于执行成功,下面是部分日志信息123456789101112131415161718gmake -C unaccent all gmake[2]: Entering directory `/opt/soft_bak/postgresql-9.1.0/contrib/unaccent' gmake[2]: Nothing to be done for `all'. gmake[2]: Leaving directory `/opt/soft_bak/postgresql-9.1.0/contrib/unaccent' gmake -C vacuumlo all gmake[2]: Entering directory `/opt/soft_bak/postgresql-9.1.0/contrib/vacuumlo' gmake[2]: Nothing to be done for `all'. gmake[2]: Leaving directory `/opt/soft_bak/postgresql-9.1.0/contrib/vacuumlo' gmake -C sslinfo all gmake[2]: Entering directory `/opt/soft_bak/postgresql-9.1.0/contrib/sslinfo' gmake[2]: Nothing to be done for `all'. gmake[2]: Leaving directory `/opt/soft_bak/postgresql-9.1.0/contrib/sslinfo' gmake -C xml2 all gmake[2]: Entering directory `/opt/soft_bak/postgresql-9.1.0/contrib/xml2' gmake[2]: Nothing to be done for `all'. gmake[2]: Leaving directory `/opt/soft_bak/postgresql-9.1.0/contrib/xml2' gmake[1]: Leaving directory `/opt/soft_bak/postgresql-9.1.0/contrib' PostgreSQL, contrib, and documentation successfully made. Ready to install. 备注：接下来安装 PostgreSQL ，建库就很顺利了，这里不再详述。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL9.1新特性之三：基于文件访问的 SQL/MED","slug":"20110915133126","date":"2011-09-15T05:31:26.000Z","updated":"2018-09-04T01:33:52.100Z","comments":true,"path":"20110915133126.html","link":"","permalink":"https://postgres.fun/20110915133126.html","excerpt":"","text":"SQL/MED((Management of External Data) )早在 PostgreSQL8.4 版本已谈到，但是到 9.1 版本已经可以创建外部表（ foreign tables）, 并通过外部表访问外部数据，这也是 SQL/MED 的主要目的之一。 SQL/MED 可以访问多种类型的外部数据，可以是操作系统的文件，也可以是其它类型数据库如 oracle ,MySQL 等，关于支持的外部数据类型，详见 http://wiki.postgresql.org/wiki/Foreign_data_wrappers 这篇 blog 主要讨论如何在数据库里直接访问文件系统上的 CSV 格式的数据文件。 环境准备在测试库上 copy 表12skytf=# copy ( select * from skytf.activity limit 10000) to '/home/postgres/script/tf/skytf.activity.csv' ; COPY 10000 备注：在公司的测试库上取表 skytf.activity 少量数据，注意格式为 csv。 传送文件将步骤1产生的文件上传到笔记本虚拟机，因为 PostgreSQL9.1 是装在笔记本虚拟机上。 部署 File_fdw加载 file_fdw 外部模块123456789mydb=# create extension file_fdw with schema mydb; CREATE EXTENSIONmydb=# \\dx List of installed extensions Name | Version | Schema | Description ----------+---------+------------+------------------------------------------- file_fdw | 1.0 | mydb | foreign-data wrapper for flat file access plpgsql | 1.0 | pg_catalog | PL/pgSQL procedural language (2 rows) 备注：以 postgres 用户连接虚拟机上的库 mydb, 创建外部模块 file_fdw。 创建 SERVER (外部服务器)123456789mydb=# create server ser_file FOREIGN DATA WRAPPER file_fdw; CREATE SERVERmydb=# \\des List of foreign servers Name | Owner | Foreign-data wrapper ----------+----------+---------------------- ser_file | postgres | file_fdw (1 row) 注：以 postgres 用户连接虚拟机上的库 mydb, 并创建 SERVER;Server 可以理解为外部服务器，是本地库访问外部数据的桥梁。 创建外部表123456789mydb=# CREATE FOREIGN TABLE fr_activity ( mydb(# id character varying(32) NOT NULL, mydb(# identity_code integer NOT NULL, mydb(# skyid integer NOT NULL, mydb(# category character varying(50), mydb(# body character varying(500) NOT NULL, mydb(# create_time timestamp without time zone mydb(# ) server ser_file options (filename '/home/postgres/script/tf/skytf.activity.csv', format 'csv') ; CREATE FOREIGN TABLE 备注：这里创建外部表，并指定外部 Server 为 ser_file，在测试过程中，发现外部表有很多限制, 目前发现了以下： default 值不支持； 索引不支持； 到了这步，终于完成了外部表创建的所有步骤，接下来验证下是否真的可以访问操作系统文件数据。 数据查询12345mydb=# select * from fr_activity limit 1; id | identity_code | skyid | category | body | create_time ----------------------------------+---------------+-----------+----------------+--------------+------------------------- 000000002a3554d5012a5518bf1f65d7 | -472428551 | 133257042 | SKY_VISIT_FROM | $&#123;fromSkyid&#125; | 2010-08-09 12:25:39.106 (1 row) 备注： 果然文件’/home/postgres/script/tf/skytf.activity.csv’ 数据可以直接访问了。 执行计划12345678mydb=# explain select count(*) from fr_activity; QUERY PLAN ---------------------------------------------------------------------------- Aggregate (cost=6429.76..6429.77 rows=1 width=0) -&gt; Foreign Scan on fr_activity (cost=0.00..6277.30 rows=60983 width=0) Foreign File: /home/postgres/script/tf/skytf.activity.csv Foreign File Size: 1463602 (4 rows) 备注：外部表的PLAN里显示了外部文件的信息，包括文件路径，大小等。 附一: CREATE SERVER 语法 CREATE SERVERNameCREATE SERVER – define a new foreign serverSynopsisCREATE SERVER server_name [ TYPE ‘server_type’ ] [ VERSION ‘server_version’ ] FOREIGN DATA WRAPPER fdw_name [ OPTIONS ( option ‘value’ [, … ] ) ]DescriptionCREATE SERVER defines a new foreign server. The user who defines the server becomes its owner.A foreign server typically encapsulates connection information that a foreign-data wrapper uses to access an external data resource. Additional user-specific connection information may be specified by means of user mappings.The server name must be unique within the database.Creating a server requires USAGE privilege on the foreign-data wrapper being used. 附二: CREATE FOREIGN TABLE 语法 CREATE FOREIGN TABLENameCREATE FOREIGN TABLE – define a new foreign tableSynopsisCREATE FOREIGN TABLE [ IF NOT EXISTS ] table_name ( [ { column_name data_type [ NULL | NOT NULL ] } [, … ]] ) SERVER server_name[ OPTIONS ( option ‘value’ [, … ] ) ]DescriptionCREATE FOREIGN TABLE will create a new foreign table in the current database. The table will be owned by the user issuing the command.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"file_fdw","slug":"file-fdw","permalink":"https://postgres.fun/tags/file-fdw/"}]},{"title":"PostgreSQL9.1 新特性之二: Writeable Common Table Expressions","slug":"20110914154355","date":"2011-09-14T07:43:55.000Z","updated":"2018-09-04T01:33:52.053Z","comments":true,"path":"20110914154355.html","link":"","permalink":"https://postgres.fun/20110914154355.html","excerpt":"","text":"PostgreSQL9.1 的这个特性可以重新利用数据更新（/Delete/Insert/Update）的结果集，并做进一步的处理，解释起来比较困难，先看看下面的例子。 备份数据创建测试表1234567891011121314151617mydb=&gt; create table test_1 (id integer, name varchar(32)); CREATE TABLEmydb=&gt; insert into test_1 values (1,'a'),(2,'b'),(3,'c'); INSERT 0 3mydb=&gt; create table test_1_bak as select * from test_1 where 1=2; SELECT 0mydb=&gt; select * From test_1; id | name ----+------ 1 | a 2 | b 3 | c (3 rows)mydb=&gt; select * from test_1_bak; id | name ----+------ (0 rows) 场景一：CTE 基本用法删除表 test_1 数据，并将删除的数据备份在表 test_1_bak12345678910111213141516mydb=&gt; WITH deleted as (delete from test_1 where id=1 returning id,name) mydb-&gt; insert into test_1_bak select * From deleted; INSERT 0 1mydb=&gt; select * from test_1; id | name ----+------ 2 | b 3 | c (2 rows)mydb=&gt; select * from test_1_bak; id | name ----+------ 1 | a (1 row) 备注：这个功能可以使用的历史数据归档上，比如删除上月数据时，可以将删除的数据归档到历史表上，节省操作步骤，提升效率。 场景二：显示已删除的数据删除数据后，显示删除的数据，如下：1234567891011mydb=&gt; WITH test_1_deleted as (delete from test_1 where id=2 returning *) mydb-&gt; select * from test_1_deleted; id | name ----+------ 2 | b (1 row)mydb=&gt; select * from test_1; id | name ----+------ 3 | c (1 row) 备注：这个功能可以应用在清理数据时，将日志打出，确保删除的数据无误。当然，如果删除的数据量比较大，这点就不提倡了。 场景三：CTE 处理复杂的SQL逻辑可以将稍复杂的业务逻辑放在一个SQL里实现，简化步骤，提高效率，如下所示；1234567891011121314151617181920212223242526272829303132333435363738394041mydb=&gt; insert into test_1 select generate_series(4,10),'c'; INSERT 0 7mydb=&gt; select * from test_1; id | name ----+------ 3 | c 4 | c 5 | c 6 | c 7 | c 8 | c 9 | c 10 | c (8 rows)mydb=&gt; create table test_2 (id integer , name varchar(32)); CREATE TABLEmydb=&gt; insert into test_2 select generate_series(1,10000),'AAA'; INSERT 0 10000mydb=&gt; select * From test_2 limit 3; id | name ----+------ 1 | AAA 2 | AAA 3 | AAAmydb=&gt; WITH test_1_deleted as (delete from test_1 where id&lt;7 returning *) , mydb-&gt; delete_count as (select count(*) as total from test_1_deleted) mydb-&gt; update test_2 mydb-&gt; set name='this is a test' mydb-&gt; from delete_count d mydb-&gt; where id=d.total; UPDATE 1mydb=&gt; select * From test_2 where name='this is a test'; id | name ----+---------------- 4 | this is a test","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.1 新特性之一: 日志表的使用","slug":"20110914131818","date":"2011-09-14T05:18:18.000Z","updated":"2018-09-04T01:33:51.991Z","comments":true,"path":"20110914131818.html","link":"","permalink":"https://postgres.fun/20110914131818.html","excerpt":"","text":"PostgreSQL9.1的新特性之一：日志表，往日志表写数据时会比普通表快很多，因为日志表写入时不写 WAL 日志，这是它的优点；同时，日志表的使用还有些限制，比如在主库日志表上写数据时，数据不会自动复制到从库，在日志表上创建索引时也不会产生WAL日志，并且目前日志表不支持创建 GIST 索引。同时日志表还有个致命的缺点，在数据库异常关闭时，日志表数据可能会丢失，下面测试下日志表表的特性。 环境准备修改 postgresql.conf1checkpoint_timeout = 120min 为了测试效果，先修改 checkpoint_timeout 参数，设置较大值, 排除 PostgreSQL自动 checkpoint 对测试结果的影响！ 创建普通表和日志表1234mydb=&gt; create table test ( id integer, name varchar(32)); CREATE TABLEmydb=&gt; create unlogged table test_unlogged ( id integer, name varchar(32)); CREATE TABLE 插入数据测试12345678mydb=&gt; timing Timing is on.mydb=&gt; insert into test select generate_series(1,1000000),'AAA'; INSERT 0 1000000 Time: 8675.356 msmydb=&gt; insert into test_unlogged select generate_series(1,1000000),'AAA'; INSERT 0 1000000 Time: 1657.043 ms 备注：这是在笔记本虚拟机上的测试，从上面看出，向日志表里写数据比向普通表写数据要快很多。 测试一: 数据库非正常关闭数据库非正常关闭123[postgres@pgb pg_root]$ pg_ctl stop -m immediate -D/database/pgdata1923_9.1/pg_root -p 1923 waiting for server to shut down.... done server stopped 数据库再次启动，并连接12345678[postgres@pgb tf]$ /opt/pgsql9.1/bin/psql -p 1923 psql (9.1.0) Type \"help\" for help.mydb=&gt; select count(*) from test_unlogged; count ------- 0 (1 row) 备注：数据库非正常关闭并启动后，日志表 test_unlogged 数据完全丢失。 测试二: 数据库正常关闭3.1 向日志表插入数据123mydb=&gt; insert into test_unlogged select generate_series(1,1000000),'AAA'; INSERT 0 1000000 Time: 1657.043 ms 正常关闭后123[postgres@pgb pg_root]$ pg_ctl stop -m fast -D /database/pgdata1923_9.1/pg_root waiting for server to shut down...... done server stopped 数据库再次启动，并连接123456789[postgres@pgb tf]$ /opt/pgsql9.1/bin/psql -p 1923 psql (9.1.0) Type \"help\" for help.postgres=# \\c mydb mydbmydb=&gt; select count(*) from test_unlogged; count -------- 1000000 (1 row) 备注：数据库正常关闭后，数据没有丢失。 测试三: Checkpoint 后非正常关闭数据库向日志表插入数据12345678mydb=&gt; insert into test_unlogged select generate_series(1,1000000),'AAA'; INSERT 0 1000000 Time: 1648.446 msmydb=&gt; select count(*) from test_unlogged; count --------- 2000000 (1 row) 主动发出 checkpoint 命令1234567mydb=&gt; checkpoint; ERROR: must be superuser to do CHECKPOINTmydb=&gt; \\c postgres postgres You are now connected to database \"postgres\" as user \"postgres\".postgres=# checkpoint; CHECKPOINT 数据库非正常关闭123[postgres@pgb pg_root]$ pg_ctl stop -m immediate -D /database/pgdata1923_9.1/pg_root -p 1923 waiting for server to shut down.... done server stopped 数据库连接后，再次查看数据12345mydb=&gt; select count(*) from test_unlogged; count -------- 2000000 (1 row) 备注： 往日志表 test_unlogged 写入数据，并执行 checkpoint 操作后，数据库非正常关闭后，数据没有丢失。 五 总结 PostgreSQL 9.1 的日志表写入速度比普通表要快很多，经测试至少快了5倍以上，因为日志表写数据时，不产生 WAL日志。 在日志表上创建索引时，也不会写 WAL日志。 目前 PostgreQL 版本不支持在日志表上创建 GIST 索引。 在主库日志表上写数据时，不会自动复制到从库； 在生产库上使用日志表需谨慎，在数据库异常关闭时，数据可能会丢失; 在短时间数据维护时，可以临时使用日志表加载数据，提高数据加载速度。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 数据同步","slug":"20110902161746","date":"2011-09-02T08:17:46.000Z","updated":"2018-09-04T01:33:51.944Z","comments":true,"path":"20110902161746.html","link":"","permalink":"https://postgres.fun/20110902161746.html","excerpt":"","text":"今天接到个需求，有个业务需要数据同步，源库和目标库都是 PostgreSQL , 由于表都是日志表，表数据只有插入，不会更新；所以只需要将源库新增数据同步到目标库即可，下面是同步脚本，供参考。 同步需求源库: source_db目标库： des_db源库表： skytf.hk_tbl_charge_oversea skytf.hk_tbl_charge_oversea_dsm 目标库表： hk_tbl_charge_oversea hk_tbl_charge_oversea_dsm需求：每天将源库指定表的前一天数据同步到目标库 编写同步脚本这套脚本有两个功能； 每天定时执行，同步源库 source_db 指定表的前一天数据； 如果由于种种原因，数据同步失败，可以用这套脚本补数据。 exec_sync_crp.sh 脚本代码，如下：123456789101112131415#!/bin/bashexport PGPORT=1921 export PGDATA=/database/pgdata1921 export LANG=en_US.utf8 export PGHOME=/app/pgsql export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib export DATE=`date +\"%Y%m%d%H%M\"` export PATH=$PGHOME/bin:$PATH:. export MANPATH=$PGHOME/man:$MANPATH yesterday=`date -d '-1 day' +%F` script_dir=/home/postgres/script/tf/sync_script #Call script sync_crp.sh $&#123;script_dir&#125;/sync_crp.sh $&#123;yesterday&#125; 备注：这个脚本调用来调用脚本 sync_crp_part.sh sync_crp.sh 脚本代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#!/bin/bashif [ $# != 1 ]; then echo Usage: ./sync_crp.sh arg1 echo Examples: echo To get the date of 2011-09-01, enter: echo ' sync_crp.sh 2011-09-01 &gt; 2011-09-01.out' exit 0 fi export PGPORT=1921 export PGDATA=/database/pgdata1921 export LANG=en_US.utf8 export PGHOME=/app/pgsql export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib export DATE=`date +\"%Y%m%d%H%M\"` export PATH=$PGHOME/bin:$PATH:. export MANPATH=$PGHOME/man:$MANPATHtoday=$1 nextday=`date -d \"$&#123;today&#125; +1 days\" +%F` host_ip='192.168.xx.xx' v_email=\"[Francs3@163.com](mailto:Francs3@163.com)\" #Make SQL strings, the sql of getting database source_db data sql1=\"select * from skytf.hk_tbl_charge_oversea where createtime &gt;= '$&#123;today&#125;' and createtime &lt;'$&#123;nextday&#125;' \" sql2=\"select * from skytf.hk_tbl_charge_oversea_dsm where createtime &gt;= '$&#123;today&#125;' and createtime &lt;'$&#123;nextday&#125;' \" ################## The real get data process ###################### ## 同步表 crp.hk_tbl_charge_oversea echo \"`date +%F %T` Begin sync report data from data source_db to crp database ! \" psql -h 192.168.xx.xx -p 1921 source_db francs -c \"copy ($sql1) to stdout \" | psql des_db des_db -c \"copy crp.hk_tbl_charge_oversea from stdin\"if [ $? -ne 0 ] then echo \"`date +%F %T` ERROR: Sync table hk_tbl_charge_oversea error \" echo -e \"`date +%F %T`nIP: $&#123;host_ip&#125;nHostname: `hostname`nnAuthor: francs(DBA)\" | mutt -s \" ERROR: Sync table hk_tbl_charge_oversea error \" $&#123;v_email&#125; else echo \"`date +%F %T` Table hk_tbl_charge_oversea is finished \" fi ## 同步表 crp.hk_tbl_charge_oversea_dsm psql -h 192.168.xx.xx -p 1921 source_db francs -c \"copy ($sql2) to stdout \" | psql des_db des_db -c \"copy crp.hk_tbl_charge_oversea_dsm from stdin\"if [ $? -ne 0 ] then echo \"`date +%F %T` ERROR: Sync table hk_tbl_charge_oversea_dsm error \" echo -e \"`date +%F %T`nIP: $&#123;host_ip&#125;nHostname: `hostname`nnAuthor: francs(DBA)\" | mutt -s \" ERROR: Sync table hk_tbl_charge_oversea_dsm error \" $&#123;v_email&#125; else echo \"`date +%F %T` Table hk_tbl_charge_oversea_dsm is finished \" fi 备注：脚本里只同步了两张表，如果还有其它表，可以继续加代码，如果表名一样，可以写个 for 循环，由于这里源表名，目标表名不一样，所以没用 for 循环, 这个脚本按天同步数据，如果需要补数据,只要加上日期参数即可； 设置任务计划加入 crontab 定时执行，如下12# Sync crp data from data warehouse 6* * * * /home/postgres/script/tf/sync_script/exec_sync_crp.sh &gt;&gt; /home/postgres/script/tf/sync_script/exec_sync_crp.log 2&gt;&amp;1","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Shell 脚本 \"Eval\" 用法一例","slug":"20110831141620","date":"2011-08-31T06:16:20.000Z","updated":"2018-09-04T01:33:51.881Z","comments":true,"path":"20110831141620.html","link":"","permalink":"https://postgres.fun/20110831141620.html","excerpt":"","text":"今天查看 shell 脚本时，发现其中有个关键字”eval” , 以前没看到过这个用法，下面举个例子来了解下它的用法。 脚本内容test.sh 脚本内容如下：123456789#将变量的变量值赋给另一变量 v1=\"francs\" v2=\"fpZhou\"c=1 echo $&#123;c&#125;vname=v$c echo $&#123;vname&#125;eval vvv=\"$\"$vname echo $&#123;vvv&#125; 执行脚本1234[postgres@192_168_1_26 tf]$ test.sh 1 v1 francs 语法1语法：eval cmdLine eval会对后面的 cmdLine 进行两遍扫描，如果第一遍扫描后 cmdLine 是个普通命令，则执行此命令；如果 cmdLine 中含有变量的间接引用，则保证间接引用的语义。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"The Limit on max_standby_streaming_delay is currently 35 minutes","slug":"20110830175342","date":"2011-08-30T09:53:42.000Z","updated":"2018-09-04T01:33:51.834Z","comments":true,"path":"20110830175342.html","link":"","permalink":"https://postgres.fun/20110830175342.html","excerpt":"","text":"有一套系统刚搭建好 HOT-Standby, 为了降低主库压力，准备让数据仓库在备库上抽取，这里需要修改参数 max_standby_streaming_delay ，否则在备库上查询时间超过 30 秒时会报错，关于这个报错可以参考之前写的blog: https://postgres.fun/20110530103706.html ，但今天在修改这个参数时，出现了奇怪的一幕，以下为详细信息。 修改 postgresql.conf 文件设置参数1max_standby_streaming_delay = 3600s 备注：修改参数 max_standby_streaming_delay，准备修改成一小时，当在备库上执行查询操作时，恰当备库需要应用主库发来的WAL日志，此时两者发生冲突，那么允许备库上的SQL的最大执行时间为 1 小时，超过一小时后，备库会 cancel 备库上的查询语句。 参数修改后，重新载入12[postgres@db-192-168-1-25](mailto:postgres@db-192-168-1-25)-&gt; pg_ctl reload -D $PGDATA server signaled 再次查询 max_standby_streaming_delay 值12345netpk=&gt; show max_standby_streaming_delay; max_standby_streaming_delay ----------------------------- 30s (1 row) 备注：这里出现了奇怪的一幕，明明将参数 max_standby_streaming_delay 值设置成了 3600 秒，而仍然显示默认的 30 秒，这个参数修改后是不需要重启 PostgreSQL 的。 尝试重启备库由于是备库，参数 max_standby_streaming_delay 在线修改不成功，于是准备重启下备库, 接下来报了下面这个 FATAL。 1[postgres@db-192-168-1-25](mailto:postgres@db-192-168-1-25)-&gt; FATAL: 3600000 is outside the valid range for parameter \"max_standby_streaming_delay\" (-1 .. 2147483) 备注: 奇怪了，根据提示，参数 max_standby_streaming_delay 值超出了设置范围，难道这个参数有限制，于是迅速查看手册，遗憾的是，9.0 上的手册没有提到 max_standby_streaming_delay 参数有限制值, 没办法，只有问 GOOGLE 老师了, 还好运气不错。 查询官网查询官网，http://archives.postgresql.org/ The limit on max_standby_streaming_delay is currently 35 minutes(around) - or you have to set it to unlimited. This is because the GUCis limited to MAX_INT/1000, unit milliseconds.Is there a reason for the /1000, or is it just an oversight thinkingthe unit was in seconds?If we can get rid of the /1000, it would make the limit over threeweeks, which seems much more reasonable. Or if we made it a 64-bitcounter it would go away completely.备注：运气不错，查到了相关的回复，上面是说 max_standby_streaming_delay 参数当前的限制值为 35 分钟， 也可以将值设为 “-1” ，表示不限制。这个限制可能会在后期版本中修改。 总结 合理的设置 max_standby_streaming_delay 的值，主要还是要看备库的用途，是高可用，还是用来分担主库负载，比如跑些报表，数据抽取之类的，如果是高可用，设置较小的值比较合理；因为备库需要时刻同步主库的数据；如果是为了分担主库负载 ，那么可以设置较大的值（35 min）, 那么长时间的报表，或者仓库抽取之类的工作可以进行； $PGDATA/postgresql.conf 参数修改后，最好还是登陆到数据库里用” show parameter “ 命令去查看下，因为有些参数有范围限制，即使设置超出范围的参数值，在 reload 也没报错，max_standby_streaming_delay 参数就是这样。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"几种客户端连接 PostgreSQL 不输入密码的方法","slug":"20110825112431","date":"2011-08-25T03:24:31.000Z","updated":"2018-09-04T01:33:51.787Z","comments":true,"path":"20110825112431.html","link":"","permalink":"https://postgres.fun/20110825112431.html","excerpt":"","text":"平常工作中，有时需要异地连接 PostgreSQL 数据库做些维护，例如异地备份等,如果备份脚本写在异地机器，备份的时候会弹出密码输入提示，那么脚本就不能后台执行，这里总结了几种不弹出密码输入提示的方法。 测试环境目标库IP: 192.168.1.25/1921 ； 数据库 skytf客户端IP： 192.168.1.26在 192.168.1.26 连接数据库 skytf , 弹出密码提示postgres@db6-&gt; psql -h 192.168.1.25 -p 1921 skytf skytfPassword for user skytf: 方法一：设置环境变量 PGPASSWORDPGPASSWORD 是 PostgreSQL 系统环境变量，在客户端设置这后，那么在客户端连接远端数据库时，将优先使用这个密码。 测试12345678[postgres@db6](mailto:postgres@db6)-&gt; export PGPASSWORD=skytf[postgres@db6](mailto:postgres@db6)-&gt; psql -h 192.168.1.25 -p 1921 skytf skytf psql (9.1beta3, server 9.0.1) WARNING: psql version 9.1, server version 9.0. Some psql features might not work. Type \"help\" for help. skytf=&gt; \\q 备注：设置环境变量 PGPASSWORD ，连接数据库不再弹出密码输入提示。 但是从安全性方面考虑，这种方法并不推荐， 方法二：设置 .pgpass 密码文件通过在客户端 /home/postgres 目录下创建隐藏文件 .pgpass ，从而避免连接数据库时弹出密码输入提示。 创建密码文件 .pgpass ( on 客户端 ) 修改 /home/postgres/.pgpass 文件，增加以下，这个文件如果没有则新建一个。 格式1hostname:port:database:username:password 范例1192.168.1.25:1921:skytf:skytf:skytf 权限1Chmod 600 .pgpass 连接测试123456[postgres@db6](mailto:postgres@db6)-&gt; psql -h 192.168.1.25 -p 1921 skytf skytf psql (9.1beta3, server 9.0.1) WARNING: psql version 9.1, server version 9.0. Some psql features might not work. Type \"help\" for help.skytf=&gt; 备注：在/home/postgres 目录创建了密码文件 .pgpass 文件后，并正确配置连接信息，那么客户端连接数据时会优先使用 .pgass文件， 并使用匹配记录的密码，从而不跳出密码输入提示,这种方法比方法一更安全，所以推荐使用创建 .pgpass 文件方式。 方法三：修改 pg_hba.conf修改认证文件 $PGDATA/pg_hba.conf, 添加以下行， 并 reload 使配置立即生效。12host skytf skytf 172.16.3.174/32 trust[postgres@192_168_1_26 pg_root]$ pg_ctl reload -D $PGDATAserver signaled 服务端 pg_hba.conf 的配置1234# IPv4 local connections: host all all 127.0.0.1/32 trust host skytf skytf 172.16.3.174/32 trust host all all 0.0.0.0/0 md5 客户端再次连接测试123456[postgres@db6](mailto:postgres@db6)-&gt; psql -h 192.168.1.25 -p 1921 skytf skytf psql (9.1beta3, server 9.0.1) WARNING: psql version 9.1, server version 9.0. Some psql features might not work. Type \"help\" for help.skytf=&gt; \\q 备注：修改服务端 pg_hba.conf 并 reload 后，不再弹出密码输入提示。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL HOT-Standby 的主备切换","slug":"20110824223133","date":"2011-08-24T14:31:33.000Z","updated":"2018-09-04T01:33:51.725Z","comments":true,"path":"20110824223133.html","link":"","permalink":"https://postgres.fun/20110824223133.html","excerpt":"","text":"这节将介绍下 PostgreSQL HOT-Standby 的主备切换，虽然 PostgreSQL 的主备 切换不太方便，没能像 Oracle DataGuard 一样提供切换命令，但是仍然有方法实现 这点在官网手册中有指出，但没给出详细的指导步骤。今天在测试过程中遇到不少问题，后经德哥指点，终于完成本实验，下面是详细过程。 环境信息PostgreSQL 版本： PostgreSQL 9.1beta3OS ： Red Hat Enterprise Linux Server release 5.5硬件环境 : 笔记本上的两台虚拟机主库 IP : 192.168.1.25主库 PORT : 1921备库 IP : 192.168.1.26备库 PORT : 1921 备注：这节不详细介绍 HOT-Standby 的搭建，关于搭建的内容可以参考之前的BLOG, PostgreSQL: Setting up streaming log replication (Hot Standby ) 如何判断主库和备库有时在论坛上会有人问，如何区分主库和备库，这里提供两种方法。 方法一: 根据主机进程判断1234[postgres@pg1 pg_root]$ ps -ef | grep \"wal\" postgres 17715 17684 0 20:41 ? 00:00:00 postgres: wal writer process postgres 17746 17684 0 20:43 ? 00:00:00 postgres: wal sender process repuser 192.168.1.26(43246) streaming 0/700178A8 postgres 17819 17590 0 21:00 pts/2 00:00:00 grep wal 备注：这里显示了wal 日志发送进程”wal sender process”，说明是主库。 123[postgres@pgb pg_xlog]$ ps -ef | grep wal postgres 29436 29386 0 20:43 ? 00:00:00 postgres: wal receiver process streaming 0/700178A8 postgres 29460 29289 0 21:00 pts/3 00:00:00 grep wal 备注：这里显示了 wal 日志接收进程 “wal receiver process” ，说明是备库； 方法二: 根据 pg_controldata 输出1pg_controldata 输出数据库服务的当前状态，可以根据 \"Database cluster state: \" 的信息来判断, 如果值为 “in production” 说明是主库，如果值为 “in archive recovery” 说明是备库。 主库的 pg_controldata 输出1234567891011[postgres@pg1 pg_root]$ pg_controldata pg_control version number: 903 Catalog version number: 201105231 Database system identifier: 5640897481082175487 Database cluster state: in production...备库的 pg_controldata 输出 [postgres@pgb pg_xlog]$ pg_controldata pg_control version number: 903 Catalog version number: 201105231 Database system identifier: 5640897481082175487 Database cluster state: in archive recovery... Recovery.conf 文件recovery.conf 是一个配置文件，用于主库，备库切换时的参数配置，可以从 $PGHOME/share 目录下复制一份 recovery.conf.sample 到备库 $PGDATA 目录，里面有众多参数，这里只介绍用于切换时的关键参数。 123standby_mode = '' --标记PG为STANDBY SERVER primary_conninfo = '' --标识主库信息 trigger_file = '' --标识触发器文件 主备切换4.1 创建备库recovery.conf 文件（ On Slave ）1cp $PGHOME/share/recovery.conf.sample $PGDATA/recovery.conf 配置以下参数123standby_mode = 'on' --标记PG为STANDBY SERVER primary_conninfo = 'host=192.168.1.25 port=1921 user=repuser ' --标识主库信息 trigger_file = '/opt/pgdata/pg_root/postgresql.trigger.1921' --标识触发器文件 4.2 关闭主库(on Primary)123[postgres@pg1 pg_root]$ pg_ctl stop -m fast -D $PGDATA waiting for server to shut down....... done server stopped 4.3 激活备库到主库状态 ( on slave )激活备库只要创建一个文件即可，根据备库 recovery.conf 配置文件的参数 trigger_file 值，创建这个 trigger 文件即可。 例如 “touch /opt/pgdata/pg_root/postgresql.trigger.1921 “ 1[postgres@pgb pg_root]$ touch /opt/pgdata/pg_root/postgresql.trigger.1921 过一会儿发现 recovery.conf 文件变成 recovery.done ，说明备库已经激活。123456789101112131415161718192021[postgres@pgb pg_root]$ ll total 176K -rw------- 1 postgres postgres 168 Aug 24 10:24 backup_label.old drwx------ 5 postgres postgres 4.0K Aug 15 10:03 base drwx------ 2 postgres postgres 4.0K Aug 24 20:50 global drwx------ 2 postgres postgres 4.0K Aug 15 10:03 pg_clog -rw------- 1 postgres postgres 4.5K Aug 24 10:39 pg_hba.conf -rw------- 1 postgres postgres 1.6K Aug 15 10:03 pg_ident.conf drwx------ 4 postgres postgres 4.0K Aug 15 10:03 pg_multixact drwx------ 2 postgres postgres 4.0K Aug 24 20:42 pg_notify drwx------ 2 postgres postgres 4.0K Aug 15 10:03 pg_serial drwx------ 2 postgres postgres 4.0K Aug 15 10:03 pg_stat_tmp drwx------ 2 postgres postgres 4.0K Aug 15 10:03 pg_subtrans drwx------ 2 postgres postgres 4.0K Aug 21 20:21 pg_tblspc drwx------ 2 postgres postgres 4.0K Aug 15 10:03 pg_twophase -rw------- 1 postgres postgres 4 Aug 15 10:03 PG_VERSION drwx------ 3 postgres postgres 4.0K Aug 24 21:20 pg_xlog -rw------- 1 postgres postgres 19K Aug 24 10:24 postgresql.conf -rw------- 1 postgres postgres 51 Aug 24 20:42 postmaster.opts -rw------- 1 postgres postgres 69 Aug 24 20:42 postmaster.pid -rw-r--r-- 1 postgres postgres 4.7K Aug 24 20:42 recovery.conf 查看从库CSV日志(正在激活成主库)1232011-08-24 21:20:55.130 CST,,,29388,,4e54f1c5.72cc,11,,2011-08-24 20:42:45 CST,1/0,0,LOG,00000,\"selected new timeline ID: 6\",,,,,,,,,\"\" 2011-08-24 21:20:58.119 CST,,,29388,,4e54f1c5.72cc,12,,2011-08-24 20:42:45 CST,1/0,0,LOG,00000,\"archive recovery complete\",,,,,,,,,\"\" 2011-08-24 21:20:58.495 CST,,,29386,,4e54f1c3.72ca,5,,2011-08-24 20:42:43 CST,,0,LOG,00000,\"database system is ready to accept connections\",,,,,,,,,\"\" 说明从库已经为OPEN状态，可以进行读写操作。 4.4 激活原来的主库，让其转变成从库 (在原来的主库上执行) 创建 $PGDATA/recovery.conf 文件，配置以下参数1234recovery_target_timeline = 'latest' standby_mode = 'on' --标记PG为STANDBY SERVER primary_conninfo = 'host=192.168.1.26 port=1921 user=repuser ' --标识主库信息 trigger_file = '/opt/pgdata/pg_root/postgresql.trigger.1921' --标识触发器文件 创建密码文件 /home/postgres/.pgpass 密码文件，输入以下内容1192.168.1.26:1921:replication:repuser:rep123us345er 修改 pg_hba.conf (现在的主库上增加)，添加以下配置1host replication repuser 192.168.1.25/32 md5 将原来的主库（现在的从库）启动12[postgres@pg1 pg_root]$ pg_ctl start -D $PGDATA server starting 查看从库日志， 发现大量 FATAL 错误信息12342011-08-24 21:31:59.178 CST,,,17889,,4e54fd4f.45e1,1,,2011-08-24 21:31:59 CST,,0,FATAL,XX000,\"timeline 6 of the primary does not match recovery target timeline 5\",,,,,,,,,\"\" 2011-08-24 21:32:04.208 CST,,,17891,,4e54fd54.45e3,1,,2011-08-24 21:32:04 CST,,0,FATAL,XX000,\"timeline 6 of the primary does not match recovery target timeline 5\",,,,,,,,,\"\" 2011-08-24 21:32:09.135 CST,,,17892,,4e54fd59.45e4,1,,2011-08-24 21:32:09 CST,,0,FATAL,XX000,\"timeline 6 of the primary does not match recovery target timeline 5\",,,,,,,,,\"\" 2011-08-24 21:32:14.136 CST,,,17895,,4e54fd5e.45e7,1,,2011-08-24 21:32:14 CST,,0,FATAL,XX000,\"timeline 6 of the primary does not match recovery target timeline 5\",,,,,,,,,\"\" 备注：出现了大量 FATAL,XX000,”timeline 6 of the primary does not match recovery target timeline 5” 估计是时间线有问题，网上查了下资料也没啥结果，后来咨询了下德哥，只要将从库 $PGDATA/pg_xlog 一个文件考过来就行。 将主库文件 00000006.history 复制到从库123[postgres@pgb pg_xlog]$ scp 00000006.history [postgres@192.168.1.25:/opt/pgdata/pg_root/pg_xlog](mailto:postgres@192.168.1.25:/opt/pgdata/pg_root/pg_xlog) [postgres@192.168.1.25's](mailto:postgres@192.168.1.25's) password: 00000006.history 再次查看从库日志1232011-08-24 21:36:04.819 CST,,,17948,,4e54fe44.461c,1,,2011-08-24 21:36:04 CST,,0,FATAL,XX000,\"timeline 6 of the primary does not match recovery target timeline 5\",,,,,,,,,\"\" 2011-08-24 21:36:09.742 CST,,,17885,,4e54fd44.45dd,5,,2011-08-24 21:31:48 CST,1/0,0,LOG,00000,\"new target timeline is 6\",,,,,,,,,\"\" 2011-08-24 21:36:09.824 CST,,,17977,,4e54fe49.4639,1,,2011-08-24 21:36:09 CST,,0,LOG,00000,\"streaming replication successfully connected to primary\",,,,,,,,,\"\" 备注：根据日志信息，说明从库已经恢复正常； 验证主库上创建一张表12skytf=&gt; create table test_11 (id integer,name varchar(32)); CREATE TABLE 从库上查询 [postgres@pgb pg_root]$ psql psql (9.1beta3) Type &quot;help&quot; for help. skytf=&gt; \\d List of relations Schema | Name | Type | Owner --------+--------------------+-------+---------- public | pg_stat_statements | view | postgres skytf | pgbench_accounts | table | skytf skytf | pgbench_branches | table | skytf skytf | pgbench_history | table | skytf skytf | pgbench_tellers | table | skytf skytf | test_11 | table | skytf skytf | test_stadnby | table | skytf 备注：可见表 test_11 迅速从主库上同步过来了，到此为止，库切换完成。 总结 Hot-Standby 切换步骤比较多，有些配置可以提前做好的，例如 .pgpass, pg_hba.conf 等; 主，备切换时，务必先将主库关闭，否则一旦从库被激活时，而主库尚未关闭，会有问题; 主，备切换可作为生产库迁移的一种方式，因为这最大限度减少了业务停机时间。","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/tags/PG高可用性/"}]},{"title":"PostgreSQL 9.1 : 使用 CREATE EXTENSION 加载外部模块","slug":"20110822155518","date":"2011-08-22T07:55:18.000Z","updated":"2018-09-04T01:33:51.662Z","comments":true,"path":"20110822155518.html","link":"","permalink":"https://postgres.fun/20110822155518.html","excerpt":"","text":"PostgreSQL 有很多外部模块可以加载，例如 dblink, pg_buffercache 等，在 9.1 版本以前，如果对应的 postgresql-contrib 已经安装，只需要将对应的 sql 文件导入到目标库即可，例如，要在 数据库 skytf 里安装 dblink 模块，只需要执行以下操作就行；12cd $PGHOME/share/contrib psql -d skytf -U postgresql -f dblink.sql 导入成功之后，那么 dblink 模块即加载成功。在 9.1 版本以后，将有些不同，模块加载环节 PostgreSQL 提供命令 “CREATE EXTENSION” 来替代以上操作。 手册上的解释 CREATE EXTENSIONNameCREATE EXTENSION – install an extensionSynopsisCREATE EXTENSION [ IF NOT EXISTS ] extension_name [ WITH ] [ SCHEMA schema ] [ VERSION version ] [ FROM old_version ]Description CREATE EXTENSION loads a new extension into the current database. There must not be an extension of the same name already loaded.Loading an extension essentially amounts to running the extension’s script file. The script will typically create new SQL objects such as functions, data types, operators and index support methods. CREATE EXTENSION additionally records the identities of all the created objects, so that they can be dropped again if DROP EXTENSION is issued.Loading an extension requires the same privileges that would be required to create its component objects. For most extensions this means superuser or database owner privileges are needed. The user who runs CREATE EXTENSION becomes the owner of the extension for purposes of later privilege checks, as well as the owner of any objects created by the extension’s script. 备注：上面的内容就不翻译了， 执行 “CREATE EXTENSION “ 时需要使用超级用户来执行。更多信息请查阅 http://www.postgresql.org/docs/9.1/static/sql-createextension.html 加载 dblink 模块12345[postgres@pg1 ~]$ psql skytf psql (9.1beta3) Type \"help\" for help.skytf=# create extension dblink; CREATE EXTENSION 备注：这个命令执行之后，即加载了 “/opt/pgsql/share/extension/“ 目录下的 dblnk 模块，而不需要手工指定SQL脚本。这是与之前版本的不同。 9.1 之前版本迁移到9.1 版本需要的操作1CREATE EXTENSION module_name FROM unpackaged; 备注：如果将9.1版本以下的数据库迁移到 9.1 版本，并且老库使用了外部模块，那么在数据库迁移后，需要执行以上命令，上面这个命令的作用是更新老库加载的对象信息。如果这步不执行，老库里加载的模块对象在9 .1 版本上将不可用。 查询已安装的 extension1234567skytf=# select extname,extowner,extnamespace,extrelocatable,extversion from pg_extension; extname | extowner | extnamespace | extrelocatable | extversion --------------------+----------+--------------+----------------+------------ plpgsql | 10 | 11 | f | 1.0 pg_stat_statements | 10 | 2200 | t | 1.0 dblink | 10 | 2200 | t | 1.0 (3 rows) 备注：可以通过查询 pg_extension 视图来查看数据库已安装的 extension, 关于所有 extension 信息可以查阅视图 pg_available_extensions 和 pg_available_extension_versions，这里不详细说明了。 pg_extension 字段解释","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"索引坏块一例：Right sibling's left-link doesn't match: block 817 links..","slug":"20110822110107","date":"2011-08-22T03:01:07.000Z","updated":"2018-09-04T01:33:51.615Z","comments":true,"path":"20110822110107.html","link":"","permalink":"https://postgres.fun/20110822110107.html","excerpt":"","text":"今天开发人员反映数据库异常，报错信息 “right sibling’s left-link doesn’t match: block 817 links..”，对生产环境产生一定影响，于是迅速查看数据库日志，发现有大量以下错误，见 CSV 日志。 数据库日志12011-08-21 13:11:04.607 CST,\"skytf\",\"skytf\",3905,\"192.168.170.38:12421\",4e50930a.f41,2,\"UPDATE\",2011-08-21 13:09:30 CST,174/119132,175573360,ERROR,XX000,right sibling's left-link doesn't match: block 817 links to 45366 instead of expected 70930 in index \"\"t_tmp_win_date_win_modify_time_idx\"\"\",,,,,,\" 备注： CSV 日志里有大量以上日志, 猜想索引 “t_tmp_win_date_win_modify_time_idx” 可能异常。 查看手册PostgreSQL 错误代码表12Class XX ― Internal Error XX000 INTERNAL ERROR internal_error 备注：查看手册 PostgreSQL 错误代码表，是内部错误。 异常 SQL出错时，应用发起的语句1234567891011121314151617181920212223242526update t_tmp set win = win + 1, date_win = (select CASE date(date_win_modify_time) WHEN CURRENT_DATE THEN date_win + 1 ELSE 1 END from t_tmp where appid = $1 and skyid = $2), week_win = (select CASE date_trunc('week', week_win_modify_time) WHEN date_trunc('week', now()) THEN week_win + 1 ELSE 1 END from t_tmp where appid = $3 and skyid = $4), date_win_modify_time = now(), week_win_modify_time = now(), strength_value = strength_value + $5 WHERE appid = $6 and skyid = $7 备注： 这个语句是 update 语句，更新非常频繁，且业务逻辑复杂。 报错代码含义 What does the above mean?It means you have got a corrupted index. REINDEX will probably fix it.You should try to figure out what caused the problem though … 备注：说是索引被损坏，需要重建索引。 查询异常索引信息12345678910111213skytf=&gt; \\dt+ t_tmp List of relations Schema | Name | Type | Owner | Size | Description --------+-------------+-------+-------+---------+------------- skytf | t_tmp | table | skytf | 2207 MB | 角色信息表 (1 row)skytf=&gt; \\di+ t_tmp_win_date_win_modify_time_idx List of relations Schema | Name | Type | Owner | Table | Size | Description --------+------------------------------------------+-------+-------+-------------+---------+------------- skytf | t_tmp_win_date_win_modify_time_idx | index | skytf | t_tmp | 1016 MB | (1 row) 备注：表 t_tmp 大小为 2G,而索引 t_tmp_win_date_win_modify_time_idx 就有 1个G，索引大得出奇，估计是膨胀太厉害了。 索引重建123456789101112skytf=&gt; drop index t_tmp_win_date_win_modify_time_idx; DROP INDEX skytf=&gt; CREATE INDEX concurrently t_tmp_win_date_win_modify_time_idx ON t_tmp USING btree (win DESC, date_win_modify_time); CREATE INDEX skytf=&gt; \\di+ t_tmp_win_date_win_modify_time_idx List of relations Schema | Name | Type | Owner | Table | Size | Description --------+------------------------------------------+-------+-------+-------------+-------+------------- skytf | t_tmp_win_date_win_modify_time_idx | index | skytf | t_tmp | 36 MB | (1 row) 备注：索引 t_tmp_win_date_win_modify_time_idx 重建后，大小迅速降为 36 MB, 可见索引膨胀太厉害了。继续观察，索引重建后，上面错误信息不再抛出。 总结 生产环境，对于更新非常频繁的表，需要定期重建索引, 以保证索引不会迅速膨胀， 保证索引性能。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"使用 pg_stat_statements 记录运行中的 SQL 信息","slug":"20110821181927","date":"2011-08-21T10:19:27.000Z","updated":"2018-09-04T01:33:51.569Z","comments":true,"path":"20110821181927.html","link":"","permalink":"https://postgres.fun/20110821181927.html","excerpt":"","text":"pg_stat_statements 属于一个 Extension 模块，用来记录数据库所有的SQL语句的运行信息，这个模块需要设置参数 shared_preload_libraries 的值为 “pg_stat_statements” , 并且需要重启 pg 服务。 pg_stat_statements 模块用于记录 SQL 的运行信息，这个和 Oracle 的视图 v$sql 类似，v$sql 里详细记录了数据库 SQL的运行状态，包括SQL的执行时间，执行次数，逻辑读，解析等信息, 下面介绍下 pg_stat_statements 模块的下载和使用。 pg_stat_statements 视图 从上面看出， pg_stat_statements 视图提供了语句所涉及的行数,执行时间shared_buffer 命中的数据块等信息。 加载 pg_stat_statements 模块设置 postgresql.conf 模块并重启 PG 服务1234shared_preload_libraries = 'pg_stat_statements' custom_variable_classes = 'pg_stat_statements' pg_stat_statements.max = 10000 pg_stat_statements.track = all 备注：参数 shared_preload_libraries 修改后需要重启PG服务. 加载 pg_stat_statements 模块12skytf=# create extension pg_stat_statements; CREATE EXTENSION 备注：9.1 版本以后，需要使用 “CREATE EXTENSION” 加载外部模块。 使用 pgbench 执行SQL1pgbench -c 10 -T 30 -n -M prepared -d skytf -U skytf -f script_1.sql 这里用的是默认的 pgbench 脚本，关于 pgbench 的使用，可以参考之前写的一篇 blog : https://postgres.fun/20110820103909.html 查询执行时间 TOP SQL查询执行总时间排前五位的 SQL 语句信息1234567891011skytf=# SELECT query, calls, total_time, rows, 100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent FROM pg_stat_statements ORDER BY total_time DESC LIMIT 5; query | calls | total_time | rows | hit_percent ----------------------------------------------------------------------+-------+------------------+------+---------------------- UPDATE pgbench_branches SET bbalance = bbalance + $1 WHERE bid = $2; | 3119 | 147.078177 | 3119 | 99.9780330821782396 UPDATE pgbench_tellers SET tbalance = tbalance + $1 WHERE tid = $2; | 3119 | 90.9786530000002 | 3119 | 100.0000000000000000 UPDATE pgbench_accounts SET abalance = abalance + $1 WHERE aid = $2; | 3119 | 7.25809 | 3119 | 97.8061756156419679 alter table pgbench_accounts add primary key (aid) | 1 | 7.232948 | 0 | 13.3546405966033716 vacuum analyze pgbench_accounts | 1 | 7.0277 | 0 | 40.3064903846153846 (5 rows) 关于 pg_stat_statements_reset() 函数 pg_stat_statements_reset() pg_stat_statements_reset discards all statistics gathered so far by pg_stat_statements. By default, this function can only be executed by superusers. 备注：上面的解释很明白， pg_stat_statements_reset()函数将会丢弃所有的语句统计信息。 pg_stat_statements_reset 函数测试12345678910111213skytf=# select pg_stat_statements_reset(); pg_stat_statements_reset -------------------------- (1 row)skytf=# SELECT query, calls, total_time, rows, 100.0 * shared_blks_hit / skytf-# nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent skytf-# FROM pg_stat_statements ORDER BY total_time DESC LIMIT 5; query | calls | total_time | rows | hit_percent ------------------------------------+-------+------------+------+------------- select pg_stat_statements_reset(); | 1 | 0.146981 | 1 | (1 row) 备注：在运行了函数 pg_stat_statements_reset() 之后，视图 pg_stat_statements 的信息被清空了。 总结 pg_stat_statements 用于分析语句的执行状态，在比较繁忙的生产库上应用对性能分析会有很大帮助; pg_stat_statements 模块加载会消耗部分内存，可以通过 pg_stat_statements.max * track_activity_query_size来计算。这个值是比较小的, 假如 pg_stat_statements.max 值为 10000, 也就消耗了 10 M内存。 加载 pg_stat_statements 模块需要重启 PG服务，这点需要注意。 附一: pg_stat_statements 配置参数 pg_stat_statements.max (integer) pg_stat_statements.max is the maximum number of statements tracked by the module (i.e., the maximum number of rows in the pg_stat_statements view). If more distinct statements than that are observed, information about the least-executed statements is discarded. The default value is 1000. This parameter can only be set at server start. pg_stat_statements.track (enum) pg_stat_statements.track controls which statements are counted by the module. Specify top to track top-level statements (those issued directly by clients), all to also track nested statements (such as statements invoked within functions), or none to disable. The default value is top. Only superusers can change this setting. pg_stat_statements.track_utility (boolean) pg_stat_statements.track_utility controls whether utility commands are tracked by the module. Utility commands are all those other than SELECT, INSERT, UPDATE and DELETE. The default value is on. Only superusers can change this setting. pg_stat_statements.save (boolean) pg_stat_statements.save specifies whether to save statement statistics across server shutdowns. If it is off then statistics are not saved at shutdown nor reloaded at server start. The default value is on. This parameter can only be set in the postgresql.conf file or on the server command line.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"使用 pgbench 进行数据库压力测试","slug":"20110820103909","date":"2011-08-20T02:39:09.000Z","updated":"2018-09-04T01:33:51.506Z","comments":true,"path":"20110820103909.html","link":"","permalink":"https://postgres.fun/20110820103909.html","excerpt":"","text":"pgbench 是对 PostgreSQL 进行压力测试的一款简单程序, SQL 命令可以在一个连接中顺序地执行，通常会开多个数据库 Session, 并且在测试最后形成测试报告，得出每秒平均事务数，pgbench 可以测试 select,update,insert,delete 命令，用户可以编写自己的脚本进行测试，下面是 pgbench 的简单使用过程。 pgbench 初始化1234567语法： pgbench -i [ other-options ] dbname [postgres@pg1 bin]$ pgbench -i -s 10 skytf -U skytf NOTICE: table \"pgbench_branches\" does not exist, skipping NOTICE: table \"pgbench_tellers\" does not exist, skipping NOTICE: table \"pgbench_accounts\" does not exist, skipping NOTICE: table \"pgbench_history\" does not exist, skipping creating tables... 数据情况123456table # of rows --------------------------------- pgbench_branches 10 pgbench_tellers 100 pgbench_accounts 1000000 pgbench_history 0 pgbench 事务脚本123456789101112131415set nbranches :scale set ntellers 10 * :scale set naccounts 100000 * :scale setrandom delta -5000 5000 setrandom aid 1 :naccounts setrandom bid 1 :nbranches setrandom tid 1 :ntellersBEGIN; UPDATE pgbench_accounts SET abalance = abalance + :delta WHERE aid = :aid; SELECT abalance FROM pgbench_accounts WHERE aid = :aid; UPDATE pgbench_tellers SET tbalance = tbalance + :delta WHERE tid = :tid; UPDATE pgbench_branches SET bbalance = bbalance + :delta WHERE bid = :bid; INSERT INTO pgbench_history (tid, bid, aid, delta, mtime) VALUES (:tid, :bid, :aid, :delta, CURRENT_TIMESTAMP); END; 备注：上面是 pgbench 自带的事务脚本，事务包含有一个 select、一个 insert、 3 个 update 语句，用户也可以自定义事务脚本。 使用 pgbench 压力测试测试服务器配置：8核16G数据量: pgbench_accounts 120 M , 另外两张表比较小。 测试一 ( 开一个session )1234567891011nohup pgbench -c 1 -T 30 -d skytf -U skytf -f script_1.sql &gt; script_1.out &amp;pghost: pgport: 1921 nclients: 1 duration: 30 dbName: skytf transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 1 number of threads: 1 duration: 30 s number of transactions actually processed: 30966 tps = 1032.164322 (including connections establishing) tps = 1032.259389 (excluding connections establishing) 测试二 ( 开 10 个session )1234567891011nohup pgbench -c 10 -T 30 -d skytf -U skytf -f script_1.sql &gt; script_1.out &amp;pghost: pgport: 1921 nclients: 10 duration: 30 dbName: skytf transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 10 number of threads: 1 duration: 30 s number of transactions actually processed: 78146 tps = 2604.131434 (including connections establishing) tps = 2606.302653 (excluding connections establishing) 测试三 ( 增加 -j 参数 )1234567891011nohup pgbench -c 10 -T 30 -j 5 -d skytf -U skytf -f script_1.sql &gt; script_1.out &amp;pghost: pgport: 1921 nclients: 10 duration: 30 dbName: skytf transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 10 number of threads: 5 duration: 30 s number of transactions actually processed: 67551 tps = 2251.026043 (including connections establishing) tps = 2251.931874 (excluding connections establishing) 测试四 ( 开 50 个session )1234567891011nohup pgbench -c 50 -T 30 -d skytf -U skytf -f script_1.sql &gt; script_1.out &amp;pghost: pgport: 1921 nclients: 50 duration: 30 dbName: skytf transaction type: Custom query scaling factor: 1 query mode: simple number of clients: 50 number of threads: 1 duration: 30 s number of transactions actually processed: 49133 tps = 1635.045134 (including connections establishing) tps = 1641.632876 (excluding connections establishing) 备注：测试二的 tps 为 2604， 为测试的最大事务值, 其中每个事务中有一个 select，一个 insert, 3 个 update 语句。 tps = 2604 /s 平均处理时间 0.38 毫秒 (包含网络开销)QPS = 13020/s 平均处理时间 0.07 毫秒 (包含网络开销) 总结pgbench 的自带脚本在测试过程中，发现了有 updae waiting 的情况，说明对测试结果是有影响的；在生产的实际测试过程需要根据实际情况进行测试。 附一: pgbench 初始化参数1234567pgbench accepts the following command-line initialization arguments:-F fillfactor Create the pgbench_accounts, pgbench_tellers and pgbench_branches tables with the given fillfactor. Default is 100.-i Required to invoke initialization mode.-s scale_factor Multiply the number of rows generated by the scale factor. For example, -s 100 will create 10,000,000 rows in the pgbench_accounts table. Default is 1. 附二: pgbench 压力测试参数1234567891011121314151617181920212223242526272829303132333435pgbench accepts the following command-line benchmarking arguments:-c clients Number of clients simulated, that is, number of concurrent database sessions. Default is 1.-C Establish a new connection for each transaction, rather than doing it just once per client session. This is useful to measure the connection overhead.-d Print debugging output.-D varname=value Define a variable for use by a custom script (see below). Multiple -D options are allowed.-f filename Read transaction script from filename. See below for details. -N, -S, and -f are mutually exclusive.-j threads Number of worker threads within pgbench. Using more than one thread can be helpful on multi-CPU machines. The number of clients must be a multiple of the number of threads, since each thread is given the same number of client sessions to manage. Default is 1.-l Write the time taken by each transaction to a log file. See below for details.-M querymode Protocol to use for submitting queries to the server:simple: use simple query protocol.extended: use extended query protocol.prepared: use extended query protocol with prepared statements.The default is simple query protocol. (See Chapter 46 for more information.)-n Perform no vacuuming before running the test. This option is necessary if you are running a custom test scenario that does not include the standard tables pgbench_accounts, pgbench_branches, pgbench_history, and pgbench_tellers.-N Do not update pgbench_tellers and pgbench_branches. This will avoid update contention on these tables, but it makes the test case even less like TPC-B.-s scale_factor Report the specified scale factor in pgbench is output. With the built-in tests, this is not necessary; the correct scale factor will be detected by counting the number of rows in the pgbench_branches table. However, when testing custom benchmarks (-f option), the scale factor will be reported as 1 unless this option is used.-S Perform select-only transactions instead of TPC-B-like test.-t transactions Number of transactions each client runs. Default is 10.-T seconds Run the test for this many seconds, rather than a fixed number of transactions per client. -t and -T are mutually exclusive.-v Vacuum all four standard tables before running the test. With neither -n nor -v, pgbench will vacuum the pgbench_tellers and pgbench_branches tables, and will truncate pgbench_history.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"pgbench","slug":"pgbench","permalink":"https://postgres.fun/tags/pgbench/"}]},{"title":"使用 Auto_explain 模块记录历史执行计划","slug":"20110809135703","date":"2011-08-09T05:57:03.000Z","updated":"2018-09-04T01:33:51.459Z","comments":true,"path":"20110809135703.html","link":"","permalink":"https://postgres.fun/20110809135703.html","excerpt":"","text":"今天有个库负载突然比较高，达到 50 左右，平常这个库是很空闲的，负载大部分时候都维持在1以下，于是查看数据库日志和当前活动会话，有个语句比较多，而且执行时间在 1.2s 左右, 于是单独分析这个 SQL， 奇怪的是，在数据库端执行这语句效率比较高，执行计划正确，索引也走对了，执行时间为零点几毫秒。于是怀疑这个语句的历史执行计划可能与当前的不一样；可 PostgreSQL 似乎没有提供系统 view 查询历史执行计划信息，网上查了下也没有结果，后来发现 auto_explain 模块可以记录SQL语句的执行计划到日志里，汗，虽然不是记录到系统表里，但总算也提供一种查询历史 PLAN 的方式。 Auto_explain 模块介绍auto_explain 模块能够自动记录慢查询语句的执行计划，而不需要手工执行 Explain 命令，在比较大的业务系统中，这个模块专门用来跟踪尚未优化的查询SQL。 Auto_explain 模块启用如果已经编译并安装了 auto_explain 模块，只要配置参数 shared_preload_libraries , 和 custom_variable_classes 参数，如下所示。 postgresql.conf 配置以下参数：123shared_preload_libraries = 'auto_explain' custom_variable_classes = 'auto_explain' auto_explain.log_min_duration = '0' auto_explain.log_min_duration 参数在配置文件里 PostgreSQL.conf 没有，需要手工添加，参数修改后，需要重启 PostgreSQL 服务。 其中 auto_explain.log_min_duration 设置成了 0, 表示记录所有语句的PLAN，这是为了测试需要，在生产情况时，可以根据需求设置，一般设置成 1000ms。 auto_explain 的其它配置参数 auto_explain.log_min_duration (integer)：这个参数用来控制执行时间在指定值以上的SQL的执行计划被记录。O 表示记录所有的语句的PLAN，默认什为 -1，表示不记录语句的PLAN。 auto_explain.log_analyze (boolean) auto_explain.log_verbose (boolean) auto_explain.log_buffers (boolean) auto_explain.log_format (enum) auto_explain.log_nested_statements (boolean) 备注：这里主要介绍了参数 auto_explain.log_min_duration, 关于其它参数的用法可参考文档 http://www.postgresql.org/docs/9.0/static/auto-explain.html Auto_explain 模块使用创建测试表12345678skytf=&gt; create table test_1 (id integer ,name varchar(32)); CREATE TABLE skytf=&gt; insert into test_1 select generate_series(1,100000) ,'francs'; INSERT 0 100000skytf=&gt; create index idx_test_1_id on test_1 using btree (id); CREATE INDEX 测试一: session一执行查询12345skytf=&gt; select * from test_1 where id=1; id | name ----+-------- 1 | francs (1 row) csvlog 日志记录了索引扫描信息12342011-08-09 13:17:12.250 CST,\"skytf\",\"skytf\",10706,\"127.0.0.1:54993\",4e40aaf6.29d2,19,\"SELECT\",2011-08-09 11:35:18 CST,1/21,0,LOG,00000,\"duration: 0.001 ms plan: Query Text: select * from test_1 where id=1; Index Scan using idx_test_1_id on test_1 (cost=0.00..8.28 rows=1 width=11) Index Cond: (id = 1)\",,,,,,,,,\"psql\" 备注：当SESSION一执行查询后，PG的日志里记录了语句的PLAN具体信息。 测试二: session一执行查询12345skytf=&gt; select count(*) from test_1 where id&gt; 10; count ------- 99990 (1 row) csv日志记录了全表扫描信息 2011-08-09 13:19:18.094 CST,&quot;skytf&quot;,&quot;skytf&quot;,10706,&quot;127.0.0.1:54993&quot;,4e40aaf6.29d2,22,&quot;SELECT&quot;,2011-08-09 11:35:18 CST,1/24,0,LOG,00000,&quot;duration: 50.475 ms plan: Query Text: select count(*) from test_1 where id&gt; 10; Aggregate (cost=1990.98..1990.99 rows=1 width=0) -&gt; Seq Scan on test_1 (cost=0.00..1741.00 rows=99991 width=0) Filter: (id &gt; 10)&quot;,,,,,,,,,&quot;psql&quot; 常见错误在配置了参数 shared_preload_libraries = ‘auto_explain’后重启PG时，可能会报模块不存在等ERROR，这说明在编译PG时没有安装 auto_explain 模块，也就是在安装PG过程中需要使用 “gmake world” 进行编译，并且在安装时需要使用”gmake install-world”, 关于这方面的详细信可以参考文档，这里不做详细说明。 总结由于PG没有系统视图可以查询语句的历史执行计划信息， 而 auto_explain 可以记录语句的历史PLAN，确实提供了一种不错的方法， 但在比较繁忙的业务系统中，如果开启这个模块，应该会稍微增加数据库压力，但不会太大。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"正责表达式量词符号的逃逸","slug":"20110727143100","date":"2011-07-27T06:31:00.000Z","updated":"2018-09-04T01:33:51.397Z","comments":true,"path":"20110727143100.html","link":"","permalink":"https://postgres.fun/20110727143100.html","excerpt":"","text":"今天有开发人员问我，需要取出下面字符串 + 号之前的字符串。原字符串：”[爱情狂人]性感大片+w(160)h(128)”目标取出：“[爱情狂人]性感大片” ,即 + 号之前的字符串。 当然最先想到的是可以先找出 + 号的位置，然后通过 substring 函数来取，但这方法不太灵活，于是想到使用正责表达式，应该可以实现。 创建测试表12345678910111213skytf=&gt; create table test_44 (id integer, name varchar(64)); CREATE TABLEskytf=&gt; insert into test_44 values (1,'[爱情狂人]性感大片+w(160)h(128)'); INSERT 0 1skytf=&gt; insert into test_44 values (2,'[爱情狂人]性感大片?w(160)h(128)'); INSERT 0 1skytf=&gt; select * from test_44; id | name ----+--------------------------------- 1 | [爱情狂人]性感大片+w(160)h(128) 2 | [爱情狂人]性感大片?w(160)h(128) (2 rows) 使用正责表达式检索尝试使用正责表达式检索，如下：12345skytf=&gt; select substring(name from '(.*)+') from test_44 where id=1; substring ----------- (1 row) 备注：由于 + 号是量词符合，检索不出，需要加上逃逸字符。 使用逃逸字符检索1234567891011skytf=&gt; select substring(name from E'(.*)+') from test_44 where id=1; substring -------------------- [爱情狂人]性感大片 (1 row)skytf=&gt; select substring(name from E'(.*)?') from test_44 where id=2; substring -------------------- [爱情狂人]性感大片 (1 row) 备注：使用逃逸字符号，检索成功。 使用 Position 函数检索12345skytf=&gt; select substring(name from 1 for position ('+' in name ) -1 ) from test_44 where id=1; substring -------------------- [爱情狂人]性感大片 (1 row) 备注：些方法先通过 position 函数找出 + 号的位置，然后再通过 substring 取出，这种方法用得比较多，特别是在 Oracle 环境下。 总结关于正责表达式的逃逸还有很多情况，也很复杂，这里不一一举例，有兴趣的朋友可以自己实验下。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Unable to cancel query by using \"pg_cancel_backend\"","slug":"20110725223745","date":"2011-07-25T14:37:45.000Z","updated":"2018-09-04T01:33:51.350Z","comments":true,"path":"20110725223745.html","link":"","permalink":"https://postgres.fun/20110725223745.html","excerpt":"","text":"今天在海外 PostgreSQL 库上杀进程, 杀了很久都没成功；数据库为8.3版本, 只能使用 pg_cancel_backend 来杀, 以下为详细过程： 查看进程12345678postgres=# select procpid, datname,current_query ,waiting from pg_stat_activity where current_query !='&lt;IDLE&gt;'; procpid | datname | current_query | waiting ---------+----------+------------------------------------------------------------------------------------------------------+--------- 24110 | postgres | select procpid, datname,current_query ,waiting from pg_stat_activity where current_query !='&lt;IDLE&gt;'; | f 8048 | skytf | select * From tbl_conn_rec limit 1; | f 10787 | skytf | select ip from tbl_conn_rec limit 1; | f 1131 | skytf | select max(createtime) from tbl_conn_rec; | f (4 rows) 备注： 表 tbl_conn_rec 比较大，有20G左右，现在需要将进程，1131, 8048,10787, 都杀掉。 尝试杀会话12345678910postgres=# select pg_cancel_backend(1131); pg_cancel_backend ------------------- t (1 row)postgres=# select pg_cancel_backend(1131); pg_cancel_backend ------------------- t (1 row) 备注：由于8.3 版本没有 pg_terminate_backend, 所以只能使用 pg_cancel_backend 杀查询会话。 再次查看进程，进程未消失 postgres=# select procpid, datname,current_query ,waiting from pg_stat_activity where current_query !=&apos;&lt;IDLE&gt;&apos;; procpid | datname | current_query | waiting ---------+----------+------------------------------------------------------------------------------------------------------+--------- 24110 | postgres | select procpid, datname,current_query ,waiting from pg_stat_activity where current_query !=&apos;&lt;IDLE&gt;&apos;; | f 8048 | skytf | select * From tbl_conn_rec limit 1; | f 10787 | skytf | select ip from tbl_conn_rec limit 1; | f 1131 | skytf | select max(createtime) from tbl_conn_rec; | f (4 rows) 备注：进程还在，说明杀进程失败，虽然 pg_cancel_backend 返回为真，但实际上进程没有杀成功。 疑似 BUG网上查了资料，说是BUG ，如下： The following bug has been logged online:Bug reference: 5459Logged by: Mason HaleEmail address: [hidden email]PostgreSQL version: 8.3.8Operating system: Redhat EL 5.1-64 bitDescription: Unable to cancel query while in send()Details:ISSUE: unable to cancel queries using pg_cancel_backend(), that are insend() function call, waiting on client receipt of data.EXPECTED RESULT: expect to be able to cancel most/all queries usingpg_cancel_backend() as superuser, perhaps with some wait time, but not anhour or more.= SYMPTOM =A SELECT query was running over 18 hours on our PostgreSQL 8.3.8 server.Verified that it was not waiting on any locks via pg_stat_activity.Attempted to cancel the query using pg_cancel_backend(), which returned ‘t’.However more than an hour later the process was still active, using about 6%of CPU and 5% of RAM.Terminated the client process that was running the query (from anotherserver) did not cause the query process on the pgsql server to stop. In thiscase the client was connecting via a ssh tunnel through an intermediate‘gateway’ server.Connection path was: CLIENT –&gt; SSH GATEWAY –&gt; DB SERVER 备注：上面描述的和今天的问题一样，只是操作系统版本为 “ Red Hat Enterprise Linux Server release 4.2”, 今天运气不错，第一次遇到了 PostgreSQL 的 bug。大概过了几小时后，前面那三个查询会话自己运行完了。 后期计划 后期准备将这个库升为9.0。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PGCon：2011 PostgreSQL 全国大会有感 ( 第一届 广州 )","slug":"20110717182023","date":"2011-07-17T10:20:23.000Z","updated":"2018-09-04T01:33:51.287Z","comments":true,"path":"20110717182023.html","link":"","permalink":"https://postgres.fun/20110717182023.html","excerpt":"","text":"第一届中国 PostgreSQL 用户大会在广州圆满成功，这次有幸能和德哥一起参加，实属不易！在大会中有幸能亲眼听到PG社区重量级人物 David Fetter ,和 Tastuo Ishii 的分享以及各界人士的演讲并认识一些朋友，受益非浅，下面是一些大会的照片。 这位是李元佳(Galy Lee)，即这次大会的组织者，首先应该感谢 galy, 为了这次大会的顺利进行 ,galy 付出了很大的努力, 大会上他有两个 topic，其中 “PostgreSQL原理简介” 分享很精彩，里面详细介绍了 PostgreSQL 里的体系结构，系统进程及其作用，以及块的物理结构等内核知识，这些东西曾经经常浮在偶脑海里，一直没有系统地找到这些的资料，今天听了他的课，茅塞顿开! 第二位出场的就是德哥了，德哥分享的主题有 &lt;&lt; PostgreSQL开源数据库应用实践 &gt;&gt; ,&lt;&lt; PostgreSQL简介 &gt;&gt;, 德哥在 PostgreSQL 企业及应用方面有着丰富的实战经验，大家对他的分享也比较感兴趣，分享的内容也让更多的人了解PG, 同时也因为 PG在杭州斯凯公司的广泛应用也让更多人对PG的企业级应用有了信心！ 这位是石井达夫( Tastuo Ishii), pgpool 的开发者，他为大家讲解了 pgpool的原理，以及在企业级的使用,演讲用的是英文，英文听的能力有限，还好有翻译，再根据 PPT的容易，基本上能理解。 下面这位是泥鳅哥，给大家分享的主题是 &lt;&lt; SQL语言解析器的实现–为BDB添加SQL支持 &gt;&gt; ,这块内容偏开发，说实话，这块分享俺听不太懂，不过在国内，中国非常缺少能够为 PG 社区提交代码的开发人员，这次大会有好几个课题偏PG内核开发，希望在PG代码贡献名单列表中将出现越来越多的的名单来自中国。 这位是来自美国PG社区的 David Fetter , 给大家分享了 &lt;&lt; 数据库虚拟化 &gt;&gt; 方面的话题，即在虚拟机中搭建 PostgreSQL 数据库的一些内容。 个人觉得在虚拟机中搭建企业级的数据库可能还是有风险，不过在未来的云计算的发展，也很难预测。 David Fetter 非常地幽默，在会后，还给大家每人发了个印着大象图标的徽章，就是他胸前的那个，有点小，仔细看还是能看到的。 接下来进入了大会的最后一个环节，由 David Fetter , 石井达夫, 以及阿弟分别讲述 美国 ,日本还有中国 PostgreSQL 的社区的情况，以及和所有与会人员讨论如何扩大中国 PostgreSQL社区 的发展。参会者很多提出了很多宝贵的意见，我想， 在这次国内 PostgreSQL 大会的影响下，国内 PostgreSQL 社区将会活跃起来！ 大会入场前给每人发的书，书的内容为本次大会嘉宾分享的PPT的内容，真的是非常地细心，给听会者很大的方便，再次感谢 Galy Lee ！ 关于大会的具体信息，参照以下链接：http://wiki.postgresql.org/wiki/Pgconchina2011","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"Understanding Bitmap Heap Scan","slug":"20110706170227","date":"2011-07-06T09:02:27.000Z","updated":"2018-09-04T01:33:51.240Z","comments":true,"path":"20110706170227.html","link":"","permalink":"https://postgres.fun/20110706170227.html","excerpt":"","text":"PostgreSQL 里表访问方式有 Bitmap heap scan, 这个访问方式有些特别，要理解它首先得理解 Sequence Scan Seq ScanSequence Scan 可以翻译成全表扫描，如果表上没有任何索引，如果查询这张表将会走 Seq Scan , Seq Scan 将从表的第一行开始顺序扫描，一直扫描到最后满足查询条件的记录。 创建测试表123456789101112131415161718skytf=&gt; create table test_43(id integer,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_43 select generate_series(1,100000),'a'; INSERT 0 100000skytf=&gt; select * From test_43 limit 1; id | name ----+------ 1 | a (1 row)skytf=&gt; \\d test_43 Table \"skytf.test_43\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Seq Scan 例子123456skytf=&gt; explain select * From test_43 where id=1; QUERY PLAN ------------------------------------------------------------ Seq Scan on test_43 (cost=0.00..830.62 rows=155 width=86) Filter: (id = 1) (2 rows) Bitmap Heap Scan 当 PostgreSQL 需要合并索引访问的结果子集时 会用到这种方式 ，通常情况是在用到 “or”，“and” 时会出现“Bitmap heap scan”。 Bitmap heap scan 举例1234567891011121314151617181920212223242526skytf=&gt; create index concurrently idx_test_43_id on test_43 using btree ( id ); CREATE INDEXskytf=&gt; analyze test_43; ANALYZEskytf=&gt; \\d test_43 Table \"skytf.test_43\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Indexes: \"idx_test_43_id\" btree (id) \"idx_test_43_name\" btree (name) skytf=&gt; explain select * from test_43 where id=1 or id=2; QUERY PLAN ----------------------------------------------------------------------------------- Bitmap Heap Scan on test_43 (cost=4.53..8.42 rows=2 width=6) Recheck Cond: ((id = 1) OR (id = 2)) -&gt; BitmapOr (cost=4.53..4.53 rows=2 width=0) -&gt; Bitmap Index Scan on idx_test_43_id (cost=0.00..2.26 rows=1 width=0) Index Cond: (id = 1) -&gt; Bitmap Index Scan on idx_test_43_id (cost=0.00..2.26 rows=1 width=0) Index Cond: (id = 2) 备注：以上只是列出 “Bitmap Heap Scan” 的简单情况，在生产环境中，出现“Bitmap Heap Scan” 的情况将会很多，而且更复杂，有些是因为统计信息没有及时生成, 需要具体分析。 “Bitmap Heap Scan” 解释 A plain indexscan fetches one tuple-pointer at a time from the index, and immediately visits that tuple in the table. A bitmap scan fetches all the tuple-pointers from the index in one go, sorts them using an in-memory “bitmap” data structure, and then visits the table tuples in physical tuple-location order. The bitmap scan improves locality of reference to the table at the cost of more bookkeeping overhead to manage the “bitmap” data structure — and at the cost that the data is no longer retrieved in index order, which doesn’t matter for your query but would matter if you said ORDER BY. 上面的意思是说，普通的索引扫描（ index scan） 一次只读一条索引项，那么一个 PAGE面有可能被多次访问；而 bitmap scan 一次性将满足条件的索引项全部取出，并在内存中进行排序, 然后根据取出的索引项访问表数据。","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"使用降序组合索引优化一例","slug":"20110706132359","date":"2011-07-06T05:23:59.000Z","updated":"2018-09-04T01:33:51.193Z","comments":true,"path":"20110706132359.html","link":"","permalink":"https://postgres.fun/20110706132359.html","excerpt":"","text":"今天发现有个库负载很高, 达到10左右，经查有个SQL比较慢，执行时间在 1.5 秒左右，而且并发量很大，SQL如下。 低性能 SQL1234567select * from (select * from tbl_table where appid = 324016 and status = 0 and manual_status = 0) t where 1 = 1 and C8 = 1 and (C0 &gt; 0 and C0 &lt; 12) order by C0 desc, skyid desc offset 5 limit 5 执行计划12345678910111213141516---------------------------------------------------------------------------------------------------------------------------- Limit (cost=56947.84..56947.85 rows=5 width=463) (actual time=1401.016..1401.018 rows=5 loops=1) -&gt; Sort (cost=56947.83..57023.30 rows=30188 width=463) (actual time=1401.010..1401.011 rows=10 loops=1) Sort Key: tbl_table.c0, tbl_table.skyid Sort Method: top-N heapsort Memory: 30kB -&gt;Bitmap Heap Scan on tbl_table (cost=22005.42..56295.47 rows=30188 width=463) (actual time=1129.283..1370.956 rows=57884 loops=1) Recheck Cond: ((c8 = 1) AND (appid = 324016) AND (c0 &gt; 0) AND (c0 &lt; 12)) Filter: ((status = 0) AND (manual_status = 0)) -&gt; BitmapAnd (cost=22005.42..22005.42 rows=30190 width=0) (actual time=1126.349..1126.349 rows=0 loops=1) -&gt; Bitmap Index Scan on tbl_table_c8_idx (cost=0.00..3222.01 rows=100481 width=0) (actual time=213.694..213.694 rows=144600 loops=1) Index Cond: (c8 = 1) -&gt; Bitmap Index Scan on tbl_table_appid_key (cost=0.00..9064.41 rows=343740 width=0) (actual time=481.639..481.639 rows=358735 loops=1) Index Cond: (appid = 324016) -&gt; Bitmap Index Scan on tbl_table_c0_idx (cost=0.00..9695.86 rows=319749 width=0) (actual time=420.901..420.901 rows=324980 loops=1) Index Cond: ((c0 &gt; 0) AND (c0 &lt; 12)) Total runtime: 1401.181 ms (15 rows) 备注：从PLAN可以看出，这个SQL执行时间为 1401 ms, 时间大部分花在 “Bitmap Heap Scan on tbl_table “ 上, 而且需要将三个”Bitmap Index Scan on” 的子集进行合并，所以执行时间比较长,大概 1.3 秒的时间花在这两步上。 查看表大小123456skytf=&gt; \\dt+ tbl_table List of relations Schema | Name | Type | Owner | Size | Description --------+-------------+-------+-------+--------+------------- skytf | tbl_table | table | skytf | 583 MB | 角色信息表 (1 row) 表结构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475skytf=&gt; \\d tbl_table Table \"skytf.tbl_table\" Column | Type | Modifiers ----------------------+-----------------------------+---------------------------------------------------------- id | integer | not null default nextval('tbl_table_id_seq'::regclass) appid | integer | not null skyid | bigint | not null strength_value | integer | default 1000 win | integer | default 0 lost | integer | default 0 title | character varying | bitmap | integer | attr1 | character varying | character_type | integer | character_ctrl | integer | default 0 c1 | integer | default 0 c2 | integer | default 0 c3 | integer | default 0 c4 | integer | default 0 c5 | integer | default 0 c6 | integer | default 0 c7 | integer | default 0 c8 | integer | default 0 c9 | integer | default 0 c10 | integer | default 0 c11 | integer | default 0 c12 | integer | default 0 c13 | integer | default 0 c14 | integer | default 0 c15 | integer | default 0 c16 | integer | default 0 c17 | integer | default 0 c18 | integer | default 0 c19 | integer | default 0 c0 | integer | default 0 custom_info | bytea | date_win | integer | default 0 week_win | integer | default 0 date_win_modify_time | timestamp without time zone | default now() week_win_modify_time | timestamp without time zone | default now() create_time | timestamp without time zone | default now() nick_name | character varying | str0 | character varying | str1 | character varying | str2 | character varying | str3 | character varying | str4 | character varying | status | integer | default 0 manual_status | integer | default 0 Indexes: \"pk_tbl_table\" PRIMARY KEY, btree (id) \"tbl_table_appid_key\" UNIQUE, btree (appid, skyid) \"character_appid\" btree (appid) \"character_create\" btree (create_time DESC) \"character_skyid\" btree (skyid) \"character_stength\" btree (strength_value DESC) \"idx_tbl_table_appid_status_manual\" btree (appid, status, manual_status) \"idx_tbl_table_c0_skyid\" btree (c0 DESC, skyid DESC) \"tbl_table_c0_idx\" btree (c0 DESC) \"tbl_table_c10_idx\" btree (c10 DESC) \"tbl_table_c11_idx\" btree (c11 DESC) \"tbl_table_c12_idx\" btree (c12 DESC) \"tbl_table_c13_idx\" btree (c13 DESC) \"tbl_table_c1_idx\" btree (c1 DESC) \"tbl_table_c2_idx\" btree (c2 DESC) \"tbl_table_c3_idx\" btree (c3 DESC) \"tbl_table_c4_idx\" btree (c4 DESC) \"tbl_table_c5_idx\" btree (c5 DESC) \"tbl_table_c6_idx\" btree (c6 DESC) \"tbl_table_c7_idx\" btree (c7 DESC) \"tbl_table_c8_idx\" btree (c8 DESC) \"tbl_table_c9_idx\" btree (c9 DESC) \"tbl_table_date_win_date_win_modify_time_idx\" btree (date_win DESC, date_win_modify_time) \"tbl_table_week_win_week_win_modify_time_idx\" btree (week_win DESC, week_win_modify_time) \"tbl_table_win_date_win_modify_time_idx\" btree (win DESC, date_win_modify_time) 创建组合索引1create index concurrently idx_tbl_table_c0_skyid on tbl_table using btree ( C0 desc, skyid desc); 优化后的PLAN1234567Limit (cost=25.59..51.17 rows=5 width=462) (actual time=0.069..0.090 rows=5 loops=1) -&gt; Index Scan using idx_tbl_table_c0_skyid on tbl_table (cost=0.00..161833.67 rows=31625 width=462) (actual time=0.025..0.087 rows=10 loops=1) Index Cond: ((c0 &gt; 0) AND (c0 &lt; 12)) Filter: ((appid = 324016) AND (status = 0) AND (manual_status = 0) AND (c8 = 1)) Total runtime: 0.146 ms (5 rows)Time: 1.199 ms 备注：创建索引后，这个查询只需要 1 ms, 速度提高了 1400 倍， 多么地神奇，仔细查看下这份 PLAN，这份PLAN先走索引”idx_tbl_table_c0_skyid” 将记录直接取出来， 然后根据过滤条件筛选，再 limit。 总结优化后，系统负载降到 5 左右，负载下降了 50%, 负载5还是有点高的，是因为还有其它 SQL比较费CPU，需要优化，这里不再详述。","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"PostgreSQL Time Zone Names","slug":"20110630134258","date":"2011-06-30T05:42:58.000Z","updated":"2018-09-04T01:33:51.147Z","comments":true,"path":"20110630134258.html","link":"","permalink":"https://postgres.fun/20110630134258.html","excerpt":"","text":"PostgreSQL 时间时区类型有 “timestamp with time zone” 和 “timestamp without time zone” , “time zone” 即为时区，这节简单描述下时区的概念，最后将给出一个全世界时区参照图，供参考。 了解时区创建一张测试表12345678skytf=&gt; create table test_42 (id integer, time_with timestamp with time zone, --带时区 time_without timestamp without time zone --不带时区 ); CREATE TABLEskytf=&gt; insert into test_42 values (1,now(),now()); INSERT 0 1 查询表数据12345skytf=&gt; select * from test_42; id | time_with | time_without ----+-------------------------------+---------------------------- 1 | 2011-06-30 13:28:12.340764+08 | 2011-06-30 13:28:12.340764 (1 row) 备注：可以看出，带时区的字段”time_with” 的内容显示了时区信息 “+08”。 查询其它时区时间举例2.1 查询当前时间12345skytf=&gt; select now(); now ------------------------------- 2011-06-30 13:17:01.867474+08 (1 row) 查询 china 时间12345skytf=&gt; select now() at time zone 'CCT'; timezone ---------------------------- 2011-06-30 13:17:03.493546 (1 row) 查询香港时间12345skytf=&gt; select now() at time zone 'HKT'; timezone ---------------------------- 2011-06-30 13:17:11.174238 (1 row) 查询法国时间1234skytf=&gt; select now() at time zone 'GFT'; timezone ---------------------------- 2011-06-30 02:36:19.137075 备注：这里用到了函数“ at time zone ”, 可以将时间转换成其它时区的时间。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: Transaction Isolation","slug":"20110629104331","date":"2011-06-29T02:43:31.000Z","updated":"2018-09-04T01:33:51.084Z","comments":true,"path":"20110629104331.html","link":"","permalink":"https://postgres.fun/20110629104331.html","excerpt":"","text":"数据库的隔离级别有四种，分别是 Read uncommitted ,Read committed Repeatable read,Serializable , PostgreSQL 数据库的默认事务隔离级别是 Read committed , 有系统参数可以更改数据库隔离级别。事务隔离级别的特点需要根据以下三种现象来判定： 事务隔离级别脏读 ( dirty read )脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。因为这个数据是还没有提交的数据，那么另外一个事务读 到的这个数据是脏数据，依据脏数据所做的操作可能是不正确的，简单的说，脏读即读到未提交的数据。 不可重复读 ( nonrepeatable read )不可重复读是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的数据可能是不一样的。这样就发生了在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。简单地说，不可重复读即指在一个事务中读到数据不一样，原因是其它会话已经更改了并提交了这些数据。 三幻影读 (phantom read )幻觉读是指当事务不是独立执行时发生的一种现象，例如 第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。 事务隔离级别如下: 图一: SQL Transaction Isolation Levels 事务隔离级别参数 default_transaction_isolation (enum) Each SQL transaction has an isolation level, which can be either “read uncommitted”, “read committed”, “repeatable read”, or “serializable”. This parameter controls the default isolation level of each new transaction. The default is “read committed”. 查看当前事务隔离级别12345skytf=&gt; show default_transaction_isolation; default_transaction_isolation ------------------------------- read committed (1 row) Read Committed 事务隔离级别举例“Read committed”是 PostgreSQL 的默认事务隔离级别，根据图一，”Read committed”可能发生不可重复读 (Nonrepeatable Read), 下面验证一下。 一张测试表，如下：1234567skytf=&gt; select * from test_41; id | name ----+------ 2 | b 3 | c 1 | a (3 rows) 事务一123456skytf=&gt; begin skytf=&gt; select * from test_41 where id=1; id | name ----+------ 1 | a (1 row) 事务二：修改这行数据12skytf=&gt; update test_41 set name='aaa' where id=1; UPDATE 1 再回到事务一12345skytf=&gt; select * from test_41 where id=1; id | name ----+------ 1 | aaa (1 row) 备注：发现在事务一中，两次读到的数据不一样了(红色部分)，这就是不可重复读。 REPEATABLE READ 事务隔离级别举例根据图一，”REPEATABLE READ “ 将不会出现“不可重复读”，下面通过实验验证一下； 表数据1234567skytf=&gt; select * From test_41; id | name ----+------ 2 | b 3 | c 1 | aaa (3 rows) 事务一123456789skytf=&gt; begin; BEGIN skytf=&gt; SET TRANSACTION ISOLATION LEVEL REPEATABLE READ ; SET skytf=&gt; select * from test_41 where id=2; id | name ----+------ 2 | b (1 row) 事务二：修改这行数据12skytf=&gt; update test_41 set name='bbb' where id=2; UPDATE 1 再回到事务一12345skytf=&gt; select * from test_41 where id=2; id | name ----+------ 2 | b (1 row) 备注: 将 PostgreSQL 事务隔离级别设置成 “ REPEATABLE READ” 后， “不可重复读”将不再出现。另外，这里是在会话中更改事务的隔离级别，只对当前会话有效，如果修改配置文件是的 default_transaction_isolation 参数，则会影响全局。 附：事务隔离级别设置语法 Name SET TRANSACTION ― set the characteristics of the current transaction SynopsisSET TRANSACTION transaction_mode [, …]SET SESSION CHARACTERISTICS AS TRANSACTION transaction_mode [, …]where transaction_mode is one of: ISOLATION LEVEL { SERIALIZABLE | REPEATABLE READ | READ COMMITTED | READ UNCOMMITTED } READ WRITE | READ ONLY 总结今天主要测试了下 “Read committed “ ,”Repeatable read “ 事务隔离级别下的”Nonrepeatable Read “ 特性，有兴趣的朋友可以去测试图二中的其它属性。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: Conditional Expressions","slug":"20110628132724","date":"2011-06-28T05:27:24.000Z","updated":"2018-09-04T01:33:51.021Z","comments":true,"path":"20110628132724.html","link":"","permalink":"https://postgres.fun/20110628132724.html","excerpt":"","text":"像其它数据库一样，PostgreSQL 也有条件表达式, 这里主要描述 PostgreSQL的条件表达式的用法。 CASE WHEN 语法 CASE WHEN condition THEN result [WHEN …] [ELSE result]END 备注：”case when” 的用法和 if/else 语句很相似，具体不解释了，看下面这个例子。 CASE When 用法举例123456789101112131415161718192021222324252627skytf=&gt; \\d test_41; Table \"skytf.test_41\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) |skytf=&gt; select * from test_41; id | name ----+------ 1 | a 2 | b 3 | c (3 rows)skytf=&gt; select a.name, skytf-&gt; case when a.id=1 then 'one' skytf-&gt; when a.id=2 then 'two' skytf-&gt; else 'other' skytf-&gt; end skytf-&gt; from test_41 a ; name | case ------+------- a | one b | two c | other (3 rows) COALESCE 函数 COALESCE(value [, …]) 如果第一个参数为非空，则COALESCE函数返回第一个参数值； 如果所有参数为空，则返回为空； COALESCE函数一般用于替换默认值为NULL的字段值; 如果第一个参数为空，则显示第二个参数的值，如果第一，二参数为空，则取第三参数的值，并按这规则判断。 COALESCE 函数用法举例子1234567891011121314151617181920212223242526272829skytf=&gt; select coalesce (1,0); coalesce ---------- 1 (1 row)skytf=&gt; select coalesce(null,1); coalesce ---------- 1 (1 row)skytf=&gt; select coalesce (null,null); coalesce ---------- (1 row)skytf=&gt; select coalesce(null,null,2); coalesce ---------- 2 (1 row)skytf=&gt; select coalesce(null,3,2); coalesce ---------- 3 (1 row) NULLIF 函数 NULLIF(value1, value2) 如果value1 等于 value2 ，则 NULLIF函数返回 null; 如果value1 值为 null, 则返回 null; 如果value2 值为 null, 则返回 value1 的值； 简单的说：如果 value1=value2 ,则返回 null,否则都返回 value1 的值。 NULLIF 函数用法举例1234567891011121314151617181920212223skytf=&gt; select nullif(1,1); nullif -------- (1 row)skytf=&gt; select nullif(null,1); nullif -------- (1 row)skytf=&gt; select nullif(1,null); nullif -------- 1 (1 row)skytf=&gt; select nullif(1,3); nullif -------- 1 (1 row)","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Linux: 主机空间监控脚本","slug":"20110623113037","date":"2011-06-23T03:30:37.000Z","updated":"2018-09-04T01:33:50.975Z","comments":true,"path":"20110623113037.html","link":"","permalink":"https://postgres.fun/20110623113037.html","excerpt":"","text":"今天写了个简单的主机空间监控脚本，适用于Linux 平台，以下为详细信息。 脚本介绍适用平台: Linux脚本功能：目录使用率达到 70%, 则自动发主机空间告警邮件。 脚本内容check_disk.sh 脚本内容如下：1234567891011121314151617#!/bin/bash export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib##declare variable v_email=\"[francs3@163i.com](mailto:francs3@163i.com)\" v_hostname=\"`hostname`\" v_email_file='/home/postgres/script/monitor/check_disk/mail_file.txt'df -Phv | awk '&#123;print $(NF-1)&#125;' | tr -d \"%\" | sed -n '2,$'p |while read v_used do if [ $&#123;v_used&#125; -gt 70 ]; then echo -e \"`date +%F %T`\" &gt; $&#123;v_email_file&#125; df -Phv &gt;&gt; $&#123;v_email_file&#125; cat $&#123;v_email_file&#125; | mutt -s \"Disk Space Alarm: $&#123;v_hostname&#125; \" $&#123;v_email&#125; exit fi done 备注：脚本比较简单，没有几行。 加入任务计划加入crontab任务计划，如下：12## Monitor Disk Space */5 * * * * /home/postgres/script/monitor/check_disk/check_disk.sh &gt; /home/postgres/script/monitor/check_disk/check_disk.log 2&gt;&amp;1 备注：加入任务计划，每五分钟执行一次。 总结这个脚本功能比较简单，仅做到表空间使用率达到一定程度发出告警邮件，其它功能可以根据需求再逐步完善。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"The Usage of Combining Queries","slug":"20110622142629","date":"2011-06-22T06:26:29.000Z","updated":"2018-09-04T01:33:50.912Z","comments":true,"path":"20110622142629.html","link":"","permalink":"https://postgres.fun/20110622142629.html","excerpt":"","text":"今天学习了下 PostgreSQL 有关结果集的合并方法, 主要包括 Uinon, Intersect，Except 三种方式, 以下详细介绍。 语法 query1 UNION [ALL] query2query1 INTERSECT [ALL] query2query1 EXCEPT [ALL] query2 UNION 表示返回结果集 query1 合并成 query2 ,如果不加 ALL 参数，则去重，否则不去。 INTERSECT 表示返回结果集 query1 和 query2 的交集。 EXCEPT 表示返回结果集在 query1 中但不在 query2, 即结果集 query1 减去 query2 的集合。 创建测试表1234567891011121314skytf=&gt; \\d test_41 Table \"skytf.test_41\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) |skytf=&gt; select * From test_41; id | name ----+------ 1 | a 2 | b 3 | c (3 rows) Union 操作1234567891011121314skytf=&gt; (select * From test_41) union ( select * from test_41); id | name ----+------ 3 | c 1 | a 2 | b (3 rows) skytf=&gt; (select * From test_41 where id=1) union ( select * from test_41 where id=2); id | name ----+------ 1 | a 2 | b (2 rows) union all 操作12345678910skytf=&gt; (select * From test_41) union all ( select * from test_41); id | name ----+------ 1 | a 2 | b 3 | c 1 | a 2 | b 3 | c (6 rows) Intersect 操作12345skytf=&gt; (select * From test_41) INTERSECT ( select * from test_41 where id=1); id | name ----+------ 1 | a (1 row) Except 操作123456skytf=&gt; (select * From test_41) except ( select * from test_41 where id=1); id | name ----+------ 3 | c 2 | b (2 rows) 注意事项使用 “Uinon, Intersect，Except” 操作需要 query1, query2 满足一定条件，即 query1 和 query2 返回同样的字段，并且返回的字段类型一致。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Db_link 问题一例 ( ERROR: password is required ) ","slug":"20110621134831","date":"2011-06-21T05:48:31.000Z","updated":"2018-09-04T01:33:50.865Z","comments":true,"path":"20110621134831.html","link":"","permalink":"https://postgres.fun/20110621134831.html","excerpt":"","text":"今天碰到个 PostgreSQL db_link 问题， 问题是这样的，数据库 mapfriends 需要访问数据库skytf 的一张表 test_41, 这两个数据库是在同一台主机上，在创建好了 view 后，查询报错，以下为详细信息。 创建视图mapfriends库创建视图，如下：12345CREATE OR REPLACE VIEW view_test_41 AS SELECT link_test_41.id FROM dblink('dbname=skytf host=localhost port=1921 user=skytf password=skytf' ::text, 'select * from test_41'::text) link_test_41( id integer,name character varying(32) ); 备注：上面创建视图完成。 查询视图报错1234mapfriends=&gt; select * From view_test_41; ERROR: password is required DETAIL: Non-superuser cannot connect if the server does not request a password. HINT: Target servers authentication method must be changed. 备注：刚开始看到这个报错觉得很奇怪，因为在上一步创建 view 语句里已经写了数据库的密码，而这里又提示非超级用户需要输入密码,接着查看 pg_hba.conf 文件，也没发现什么异常。于是google一把没发现啥有用信息，后来直接咨询德哥了，德哥了解了一下情况后，觉得可能是 pg_hba.conf 的配置问题； 查看 pg_hba.conf 文件12345678# TYPE DATABASE USER CIDR-ADDRESS METHOD# \"local\" is for Unix domain socket connections only local all all trust# IPv4 local connections: host all all 127.0.0.1/32 trust host all all 0.0.0.0/0 md5# IPv6 local connections: host all all ::1/128 trust 备注：这是老的 pg_hba.conf 文件，德哥发现下面这行有问题，即 host all all 127.0.0.1/32 trust， 建议认证方式改为 md5 试下。 修改后的 pg_hba.conf 文件123456789# TYPE DATABASE USER CIDR-ADDRESS METHOD# \"local\" is for Unix domain socket connections only local all all trust# IPv4 local connections: 增加一行 host skytf skytf 127.0.0.1/32 md5 host all all 127.0.0.1/32 trust host all all 0.0.0.0/0 md5# IPv6 local connections: host all all ::1/128 trust 这里增加一行 skytf 库的 md5 认证方式, 重新加载 pg_hba.conf 使其生效, 重新加载命令:1pg_ctl reload -D $PGDATA ) 再次查询视图1234567mapfriends=&gt; select * from view_test_41; id ---- 1 2 3 (3 rows) 备注：在修改 pg_hba.conf 文件后，果然可以查询了，看来德哥的思路是正确的，这次学习了。 查询 VIEW 定义12345678mapfriends=&gt; select * from pg_views where viewname='view_test_41'; schemaname | viewname | viewowner | definition ------------+--------------+------------+------------------------------------------------------------------------------------------- ------------------------------------------------------------------- mapfriends | view_test_41 | mapfriends | SELECT link_test_41.id FROM dblink('dbname=skytf host=localhost port=1921 user=skytf passw ord=skytf'::text, 'select * from test_41'::text) link_test_41(id integer, name character varying(32)); (1 row) 备注：上面SQL可以查出视图的定义，包括主机，用户名，密码信息，并且以明文形式，所以 db_link 是一种危险的行为，这很容易暴露生产环境的信息，所以不建议在生产环境上用 db_link。 总结之前查询视图报错是因为原来的 pg_hba.conf 设置了主机的认证方式为 trust, 而创建 db_link 语法里需要提供密码信息，这一点上是矛盾的，所以抛出之前的错误信息,解决方法只需要在将 trust 改为 md5 ，或者加一条db_link 到目标库 的 md5 认证方式即可。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"国外 PostgreSQL 用户会资料","slug":"20110620211418","date":"2011-06-20T13:14:18.000Z","updated":"2018-09-04T01:33:50.318Z","comments":true,"path":"20110620211418.html","link":"","permalink":"https://postgres.fun/20110620211418.html","excerpt":"","text":"2011 国外PG用户会资料 http://www.pgcon.org/2011/schedule/ 2010 国外PG用户会资料 http://www.pgcon.org/2010/schedule/ 2009 国外PG用户会资料 http://www.pgcon.org/2009/schedule/","categories":[{"name":"PG大会活动","slug":"PG大会活动","permalink":"https://postgres.fun/categories/PG大会活动/"}],"tags":[{"name":"PostgreSQL活动","slug":"PostgreSQL活动","permalink":"https://postgres.fun/tags/PostgreSQL活动/"}]},{"title":"The Parameter \"-c\" of Pg_restore","slug":"20110613170637","date":"2011-06-13T09:06:37.000Z","updated":"2018-09-04T01:33:50.271Z","comments":true,"path":"20110613170637.html","link":"","permalink":"https://postgres.fun/20110613170637.html","excerpt":"","text":"今天在bbs论坛上看到一个问题，有点意思，这里记录下；问题是这样的，想将 A库 的一部分表数据导入到 B库里, 而A库里的这部分表有些已经存在B库中，目标是在导入B库的过程中，先将B库已存在的表删除，再导入A库表。 后来查了下手册，发现 pg_resotre 有个 “-c” 参数可以达到这个目标，即在导入数据库对像之前先将已存在的对像删除掉。 pg_restore “-c” 选项1-c, --clean clean (drop) database objects before recreating 接下来做下实验，看看这个参数的效果： 创建测试表12345678910111213skytf=&gt; create table test_41 (id integer,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_41 values (1,'a'),(2,'b'),(3,'c'); INSERT 0 3skytf=&gt; select * from test_41; id | name ----+------ 1 | a 2 | b 3 | c (3 rows) 备份表1pg_dump -E UTF8 -Fc -t \"skytf.test_41\" skytf &gt; skytf.test_41.dmp 备份表后删除表数据12345678910skytf=&gt; delete from test_41 where id=3; DELETE 1skytf=&gt; delete from test_41 where id=2; DELETE 1skytf=&gt; select * from test_41; id | name ----+------ 1 | a (1 row) 使用 “-c” 选项还原表数据12345678[postgres@tf]$ pg_restore -U skytf -d skytf -c -v skytf.test_41.dmp pg_restore: connecting to database for restore pg_restore: dropping TABLE DATA test_41 pg_restore: dropping TABLE test_41 pg_restore: creating TABLE test_41 pg_restore: restoring data for table \"test_41\" pg_restore: setting owner and privileges for TABLE test_41 pg_restore: setting owner and privileges for TABLE DATA test_41 备注：从 pg_restore 日志看出，PG在导入表 test_41 之前，先将表 test_41 drop 掉,然后再创建表，再导表数据，表权限。 验证1234567skytf=&gt; select * From test_41; id | name ----+------ 1 | a 2 | b 3 | c (3 rows) 备注：在导数据之前 test_41 只有一条记录，导数据后，表 test_41 数据已经为三条了。 总结pg_restore 的 “-c” 参数在生产环境中很少用到，但在导库数据的某些场合提供了便利。","categories":[{"name":"PG备份与恢复","slug":"PG备份与恢复","permalink":"https://postgres.fun/categories/PG备份与恢复/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"数据库无法连接 psql FATAL: the database system is shutting down","slug":"20110610153653","date":"2011-06-10T07:36:53.000Z","updated":"2018-09-04T01:33:50.209Z","comments":true,"path":"20110610153653.html","link":"","permalink":"https://postgres.fun/20110610153653.html","excerpt":"","text":"今天开发人员求助，说是一台开发环境 PostgreSQL 库使用客户端连接不上， 在要得数据库主机 root 密码后，上去瞧瞧，问题还真奇怪。 查看主机进程1234567891011121314[root@localhost ~]# ps -ef | grep post postgre 4641 27639 0 2010 ? 00:00:00 postgres: community community 127.0.0.1(57739) idle postgre 8300 27639 0 Jan12 ? 00:00:00 postgres: gamehall gamehall 127.0.0.1(40177) idle in transaction postgre 8302 27639 0 Jan12 ? 00:00:00 postgres: gamehall gamehall 127.0.0.1(40179) idle in transaction postgre 8303 27639 0 Jan12 ? 00:00:00 postgres: gamehall gamehall 127.0.0.1(40180) idle in transaction postgre 8304 27639 0 Jan12 ? 00:00:00 postgres: gamehall gamehall 127.0.0.1(40181) idle in transaction root 17571 17467 0 10:07 pts/2 00:00:00 su - postgre postgre 17572 17571 0 10:07 pts/2 00:00:00 -bash root 21591 21557 0 14:42 pts/5 00:00:00 grep post postgre 24561 27639 0 Mar24 ? 00:00:00 postgres: gamehall gamehall 127.0.0.1(41383) idle postgre 27639 1 0 2010 ? 00:03:32 /opt/postgresql-9.0.1/bin/postgres -D /opt/postgresql-9.0.1/data postgre 27641 27639 0 2010 ? 00:00:00 postgres: writer process postgre 27644 27639 0 2010 ? 00:06:12 postgres: stats collector process [root@localhost ~]# su - postgres 备注：从上面可以看出，PostgreSQL 进程还在。 2 连接数据库报错1234567[postgre@localhost data]$ psql -h 127.0.0.1 psql: could not connect to server: Connection refused Is the server running on host \"127.0.0.1\" and accepting TCP/IP connections on port 5432? [postgre@localhost data]$ psql -p 1921 psql: FATAL: the database system is shutting down 备注：数据库连接不上，报 “the database system is shutting down” , 意思是数据库正在关闭， 而 PostgreSQL 进程明明都还在，再仔细观察了上面步骤的进程，发现有十几个“ 127.0.0.1(40181) idle in transaction” 难道是这些进程发生了异常，遗憾的是数据库日志参数没打开，无法查看日志。 正在分析的时候，开发人员告诉了个信息，说是这台机器在前段时间可能是断电了。汗，断电了？先不管什么原因，努力先把库恢复正常在说。 关闭数据库123[postgre@localhost data]$ pg_ctl stop -m fast pg_ctl: PID file \"/opt/postgresql-9.0.1/data/postmaster.pid\" does not exist Is server running? 备注：数据库无法关闭, 文件 postmaster.pid 不存在。 创建 postmaster.pid 文件参照其它库的 postmaster,pid 文件，构造了一个，创建文件 /opt/postgresql-9.0.1/data/postmaster.pid , 并增加以下内容:1227639 /opt/postgresql-9.0.1/data 备注: 第一行: “27639” 表示 PostgreSQL 的父进程，根据步骤一的红色标识可以看到； 第二行: “/opt/postgresql-9.0.1/data” 表示数据目录； 第三行：正常情况下，第三行还有两个字段，但具体信息不详，无法构造。 再次尝试关闭数据库再次尝试停数据库，数据库可以正常关闭123[postgre@localhost data]$ pg_ctl stop -m fast -D $PGDATA waiting for server to shut down.... done server stopped 启动数据库123456789101112131415[postgre@localhost data]$ pg_ctl start -D $PGDATA server starting[postgre@localhost data]$ ps -ef | grep post root 17571 17467 0 10:07 pts/2 00:00:00 su - postgre postgre 17572 17571 0 10:07 pts/2 00:00:00 -bash root 21595 21557 0 14:42 pts/5 00:00:00 su - postgre postgre 21596 21595 0 14:42 pts/5 00:00:00 -bash postgre 21933 1 6 14:54 pts/5 00:00:00 /opt/postgresql-9.0.1/bin/postgres -D /opt/postgresql-9.0.1/data postgre 21934 21933 0 14:54 ? 00:00:00 postgres: logger process postgre 21936 21933 0 14:54 ? 00:00:00 postgres: writer process postgre 21937 21933 0 14:54 ? 00:00:00 postgres: wal writer process postgre 21938 21933 0 14:54 ? 00:00:00 postgres: autovacuum launcher process postgre 21939 21933 0 14:54 ? 00:00:00 postgres: stats collector process postgre 21941 21596 0 14:54 pts/5 00:00:00 ps -ef postgre 21942 21596 0 14:54 pts/5 00:00:00 grep post 备注：数据库正常启动, 到这里数据库终于恢复正常了！ 总结 至今 postmaster.pid 文件丢失原因尚不明确，但这次通过构造 postmaster.pid 文件从而将数据库成功恢复！","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"PostgreSQL 的日期函数用法举例","slug":"20110603151659","date":"2011-06-03T07:16:59.000Z","updated":"2018-09-04T01:33:50.162Z","comments":true,"path":"20110603151659.html","link":"","permalink":"https://postgres.fun/20110603151659.html","excerpt":"","text":"最近偶有开发同事咨询 PostgreSQL 日期函数，对日期处理不太熟悉，今天详细看了下手册的日期函数，整理如下，供参考。 一 取当前日期/时间函数取当前时间12345678910skytf=&gt; select now(); now ------------------------------- 2011-06-03 14:45:43.633466+08 (1 row)skytf=&gt; select current_timestamp; now ------------------------------- 2011-06-03 14:46:58.768399+08 取当前时间的日期12345skytf=&gt; select current_date; date ------------ 2011-06-03 (1 row) 取当前具体时间 (除去日期)12345skytf=&gt; select current_time; timetz -------------------- 14:46:29.404942+08 (1 row) 二 日期的加减12345skytf=&gt; select now(); now ------------------------------- 2011-06-03 14:54:04.771193+08 (1 row) 表示三天后12345skytf=&gt; select now() + interval '3 day'; ?column? ------------------------------- 2011-06-06 14:54:06.119683+08 (1 row) 表示三天前12345skytf=&gt; select now() - interval '3 day'; ?column? ------------------------------- 2011-05-31 14:54:10.060558+08 (1 row) 表示1小时后12345skytf=&gt; select now() + interval '1 hour'; ?column? ------------------------------- 2011-06-03 15:55:24.600172+08 (1 row) 表示1小时前12345skytf=&gt; select now() - interval '1 hour'; ?column? ------------------------------- 2011-06-03 13:55:25.799537+08 (1 row) 表示10分钟后12345skytf=&gt; select now() + interval '10 minutes'; ?column? ------------------------------- 2011-06-03 15:06:23.363667+08 (1 row) 表示10分钟前1234skytf=&gt; select now() - interval '10 minutes'; ?column? ------------------------------- 2011-06-03 14:46:13.899526+08 三 取时间字段的部分值在开发过程中，经常要取日期的年，月，日，小时等值，PostgreSQL 提供一个非常便利的EXTRACT函数。 EXTRACT 函数解释EXTRACT(field FROM source): field 表示取的时间对象， source 表示取的日期来源，类型为 timestamp。 下面是一些例子，如下： 取年份12345skytf=&gt; select extract (year from now()); date_part ----------- 2011 (1 row) 取月份12345skytf=&gt; select extract (month from now()); date_part ----------- 6 (1 row) 取day1234567891011121314151617skytf=&gt; select extract(day from now()); date_part ----------- 3 (1 row)skytf=&gt; select extract(day from timestamp '2011-06-03'); date_part ----------- 3 (1 row)skytf=&gt; select timestamp '2011-06-03'; timestamp --------------------- 2011-06-03 00:00:00 (1 row) 取小时12345skytf=&gt; select extract (hour from now()); date_part ----------- 14 (1 row) 取分钟12345skytf=&gt; select extract (minute from now()); date_part ----------- 59 (1 row) 取秒12345skytf=&gt; select extract (second from now()); date_part ----------- 46.039333 (1 row) 取所在哪个星期12345skytf=&gt; select extract (week from now()); date_part ----------- 22 (1 row) 四 总结上面只是 PostgreSQL 日期函数的基本用法，希望这些对大家应用 PostgreSQL 起到一定作用。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Hot Standby ( ERROR: Canceling statement due to conflict with recovery ) ","slug":"20110530103706","date":"2011-05-30T02:37:06.000Z","updated":"2018-09-04T01:33:50.099Z","comments":true,"path":"20110530103706.html","link":"","permalink":"https://postgres.fun/20110530103706.html","excerpt":"","text":"今天开发人员跑来，说是在新搭建的 Hot Standby 环境下的 standby 节点执行查询时报错，报错信息如下, 数据库版本 9.0.4。 异常现象执行长时间查询时报错。123skytf=&gt; SELECT count(id) from skytf.tbl_info; ERROR: canceling statement due to conflict with recovery DETAIL: User query might have needed to see row versions that must be removed. 备注： 表 “skytf.tbl_info” 是个大表，光数据就有 12G，这个统计SQL 正常情况下需要2分钟左右完成, 但每次执行到一会儿是，抛出以上错误。根据错误信息，初步估计当在从库上执行查询时，与主库发生了冲突。 网上GOOGLE Long running queries on the standby are a bit tricky, because theymight need to see row versions that are already removed on the master. 意思是说，长时间SQL如果跑在 standby 节点上可以说是一个笑话，因为 standby 节点有可能需要读取主库上被 removed 的数据。 解决方法修改参数,设置成以下值1max_standby_streaming_delay = 300 s; 参数解释123max_standby_streaming_delay (integer) When Hot Standby is active, this parameter determines how long the standby server should wait before canceling standby queries that conflict with about-to-be-applied WAL entries, as described in Section 25.5.2. max_standby_streaming_delay applies when WAL data is being received via streaming replication. The default is 30 seconds. Units are milliseconds if not specified. A value of -1 allows the standby to wait forever for conflicting queries to complete. This parameter can only be set in the postgresql.conf file or on the server command line. Note that max_standby_streaming_delay is not the same as the maximum length of time a query can run before cancellation; rather it is the maximum total time allowed to apply WAL data once it has been received from the primary server. Thus, if one query has resulted in significant delay, subsequent conflicting queries will have much less grace time until the standby server has caught up again. 备注：上面的解释很好理解：当在 Standby 提供应用时，如果 Standby 节点上的 SQL 与接收主库日志发生冲突时，这个参数决定了从库等侍这个查询的时间，默认值为 30 s, 难怪，刚才的统计SQL，执行时间估计在二分钟左右，从而被 Standby 库主动 Cancel 了。也可以将这个参数设置成 -1. 表示 standby 节点永远等侍这个查询，这无疑是有风险的，如果这个查询不结束，那么从库一直处于与主库的中断状态，不会同步主库数据，而会一直等从库这个SQL执行完成, 这里将参数设置成 300s ，是经过了与开发人员的沟通后确定的一个值。 再次执行统计 SQL123456skytf=&gt; select count(*) from tbl_info; count ---------- 88123735 (1 row)Time: 131068.569 ms 备注：这回终于可以执行了，这个SQL花了 二分钟多，低于5分钟。 其它建议 Another option is to increase vacuum_defer_cleanup_age on the primary server, so that dead rows will not be cleaned up as quickly as they normally would be. This will allow more time for queries to execute before they are cancelled on the standby, without having to set a high max_standby_streaming_delay. However it is difficult to guarantee any specific execution-time window with this approach, since vacuum_defer_cleanup_age is measured in transactions executed on the primary server. 备注：上面这段话来自手册上的，也是针对从库与主库可能产生冲突时的建议方法，可以设置参数 vacuum_defer_cleanup_age, 由于这个参数是以事务数来确定的，在实际操作时很难操作，故不采设置这个参数的方法。 总结 PostgreSQL 的 Hot Standby 是个好东西，但用从库的时候也要注意，用得不好从库可能拒绝提供服务。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"PostgreSQL 9 的 Hot Standby 同步速度非常不错!","slug":"20110526172010","date":"2011-05-26T09:20:10.000Z","updated":"2018-09-04T01:33:50.037Z","comments":true,"path":"20110526172010.html","link":"","permalink":"https://postgres.fun/20110526172010.html","excerpt":"","text":"今天将一套单机 PostgreSQL ( 8.4.2) 的数据库成功迁移到 PostgreSQL ( 9.0.4 ) Hot Standby 环境中， 基本步骤如下； 迁移步骤 备份源库，二进制模式；( 在老库上操作 ) 在 Hot Standby 环境中创建用户；( on Primary ) 在 Hot Standby 环境中所有节点创建数据目录;( both Primary and Standby; Primary 节点和 Standby 节点创建相同数据目录，否则备会 dwon 掉; ) 在 Hot Standby 环境中创建表空间；( on Primary ) 在 Hot Standby 环境中创建新库; ( on Primary ) 将步骤一的文件导入到 Hot Standby 环境 的 Primary 库中。( on Primary ) 以上操作都还简单，脚本就不写出来了，这里第五步操作是向 Hot Standby 环境的 Primary 库里导入数据，这时 Standby 库在拼命地追， 初步估算了下数据的导入速度有 20MB/s 左右，而备库延时在毫秒级 , 这个同步速度非常可观，由此可以推测，对于以写为主的系统，通常是日志系统，PostgreSQL 9 版本的 Hot Standby 的数据同步完全可以胜任。","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"Copy 进程被 Kill 后数据库无法连接故障","slug":"20110524140547","date":"2011-05-24T06:05:47.000Z","updated":"2018-09-04T01:33:49.990Z","comments":true,"path":"20110524140547.html","link":"","permalink":"https://postgres.fun/20110524140547.html","excerpt":"","text":"昨天开发人员跑来，说是他的测试环境数据库无法连接，下面是详细过程。 故障现象登陆到数据库主机，执行 psql12[postgresql@test_db ~]$ psql psql: FATAL: the database system is in recovery mode 果然，连 psql 命令都不能执行，根据报错信息，知道数据库在做恢复。 排查过程查看 PostgreSQL 后台进程1234567[postgresql@test_db ~]$ ps -ef | grep post 501 26024 28829 4 17:34 ? 00:00:25 postgres: startup process recovering 00000001000001CA0000001F root 26476 25974 0 17:35 pts/0 00:00:00 su - postgresql 501 28829 1 0 Apr27 ? 00:00:12 /opt/postgresql-9.0.3/bin/postgres -D /opt/pg/data 501 28830 28829 0 Apr27 ? 00:01:01 postgres: logger process root 31526 31408 0 17:44 pts/1 00:00:00 su - postgresql 501 31656 31527 0 17:44 pts/1 00:00:00 grep post 备注：从上面输出，可以看到一个关键信息 “ recovering 00000001000001CA0000001F “ , 这说明数据库在做恢复，于是向开发人员了解一下情况，原来他正在执行一个 copy 操作，觉得太慢，后来在操作系统层面将 copy 进程 kill 了。原来如此。这就是PG 在做恢复的根本原因， 俺再一次和开发人员强调，PG的进程不能通过 kill 来杀。。。 试图关闭数据库123[postgresql@test_db ~]$ pg_ctl stop -m fast waiting for server to shut down............................................................... failed pg_ctl: server does not shut down 很不幸，数据库无法正常关闭。 查看PostgreSQL 后台进程1234567891011[postgresql@test_db ~]$ ps -ef | grep post 501 374 31527 0 17:46 pts/1 00:00:00 grep post 501 26024 28829 3 17:34 ? 00:00:25 postgres: startup process recovering 00000001000001CA0000001F root 26476 25974 0 17:35 pts/0 00:00:00 su - postgresql 501 28829 1 0 Apr27 ? 00:00:13 /opt/postgresql-9.0.3/bin/postgres -D /opt/pg/data 501 28830 28829 0 Apr27 ? 00:01:01 postgres: logger process root 31526 31408 0 17:44 pts/1 00:00:00 su - postgresql root 31952 31751 0 17:45 pts/3 00:00:00 su - postgresql [postgresql@test_db ~]$ psql psql: FATAL: the database system is shutting down 数据库还是老样子，不能连接。 查看 Csvlog12342011-05-23 17:48:53.006 CST,\"mylog\",\"skytflog\",1457,\"192.168.171.39:55019\",4dda2d85.5b1,1,\"\",2011-05-23 17:48:53 CST,,0,FATAL,57P03,\"the database system is shutting down\",,,,,,,,,\"\" 2011-05-23 17:48:53.007 CST,\"mylog\",\"skytflog\",1458,\"192.168.171.39:55020\",4dda2d85.5b2,1,\"\",2011-05-23 17:48:53 CST,,0,FATAL,57P03,\"the database system is shutting down\",,,,,,,,,\"\" 2011-05-23 17:48:53.008 CST,\"mylog\",\"skytflog\",1459,\"192.168.171.39:55021\",4dda2d85.5b3,1,\"\",2011-05-23 17:48:53 CST,,0,FATAL,57P03,\"the database system is shutting down\",,,,,,,,,\"\" 2011-05-23 17:48:53.298 CST,\"mylog\",\"skytflog\",1460,\"xxx.xxx.xxx.xx:48985\",4dda2d85.5b4,1,\"\",2011-05-23 17:48:53 CST,,0,FATAL,57P03,\"the database system is shutting down\",,,,,,,,,\"\" 备注：CSV日志里有大量the database system is shutting down ，意思是 PostgreSQL 正在关闭，俺在想，是否应该先等等，或者数据库就关闭了。 查看数据库 WAL123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778[postgresql@test_db pg_xlog]$ ls -lrt total 4920304 drwx------ 2 postgresql postgresql 4096 Feb 18 17:32 archive_status -rw------- 1 postgresql postgresql 67108864 May 23 15:55 00000001000001CA00000022 -rw------- 1 postgresql postgresql 67108864 May 23 16:02 00000001000001CA0000002A -rw------- 1 postgresql postgresql 67108864 May 23 16:05 00000001000001CA00000021 -rw------- 1 postgresql postgresql 67108864 May 23 16:06 00000001000001CA00000027 -rw------- 1 postgresql postgresql 67108864 May 23 16:08 00000001000001CA00000026 -rw------- 1 postgresql postgresql 67108864 May 23 16:16 00000001000001CA00000024 -rw------- 1 postgresql postgresql 67108864 May 23 16:16 00000001000001CA00000025 -rw------- 1 postgresql postgresql 67108864 May 23 16:20 00000001000001CA00000023 -rw------- 1 postgresql postgresql 67108864 May 23 16:22 00000001000001CA00000028 -rw------- 1 postgresql postgresql 67108864 May 23 16:23 00000001000001CA00000029 -rw------- 1 postgresql postgresql 67108864 May 23 16:24 00000001000001CA00000020 -rw------- 1 postgresql postgresql 67108864 May 23 16:25 00000001000001C90000001F -rw------- 1 postgresql postgresql 67108864 May 23 16:26 00000001000001C900000020 -rw------- 1 postgresql postgresql 67108864 May 23 16:27 00000001000001C900000021 -rw------- 1 postgresql postgresql 67108864 May 23 16:28 00000001000001C900000022 -rw------- 1 postgresql postgresql 67108864 May 23 16:29 00000001000001C900000023 -rw------- 1 postgresql postgresql 67108864 May 23 16:30 00000001000001C900000024 -rw------- 1 postgresql postgresql 67108864 May 23 16:31 00000001000001C900000025 -rw------- 1 postgresql postgresql 67108864 May 23 16:33 00000001000001C900000026 -rw------- 1 postgresql postgresql 67108864 May 23 16:34 00000001000001C900000027 -rw------- 1 postgresql postgresql 67108864 May 23 16:35 00000001000001C900000028 -rw------- 1 postgresql postgresql 67108864 May 23 16:36 00000001000001C900000029 -rw------- 1 postgresql postgresql 67108864 May 23 16:37 00000001000001C90000002A -rw------- 1 postgresql postgresql 67108864 May 23 16:42 00000001000001C90000002B -rw------- 1 postgresql postgresql 67108864 May 23 16:45 00000001000001C90000002C -rw------- 1 postgresql postgresql 67108864 May 23 16:46 00000001000001C90000002D -rw------- 1 postgresql postgresql 67108864 May 23 16:46 00000001000001C90000002E -rw------- 1 postgresql postgresql 67108864 May 23 16:47 00000001000001C90000002F -rw------- 1 postgresql postgresql 67108864 May 23 16:47 00000001000001C900000030 -rw------- 1 postgresql postgresql 67108864 May 23 16:47 00000001000001C900000031 -rw------- 1 postgresql postgresql 67108864 May 23 16:48 00000001000001C900000032 -rw------- 1 postgresql postgresql 67108864 May 23 16:48 00000001000001C900000033 -rw------- 1 postgresql postgresql 67108864 May 23 16:49 00000001000001C900000034 -rw------- 1 postgresql postgresql 67108864 May 23 16:49 00000001000001C900000035 -rw------- 1 postgresql postgresql 67108864 May 23 16:50 00000001000001C900000036 -rw------- 1 postgresql postgresql 67108864 May 23 16:51 00000001000001C900000037 -rw------- 1 postgresql postgresql 67108864 May 23 16:52 00000001000001C900000038 -rw------- 1 postgresql postgresql 67108864 May 23 16:53 00000001000001C900000039 -rw------- 1 postgresql postgresql 67108864 May 23 16:55 00000001000001C90000003A -rw------- 1 postgresql postgresql 67108864 May 23 16:56 00000001000001C90000003B -rw------- 1 postgresql postgresql 67108864 May 23 16:57 00000001000001C90000003C -rw------- 1 postgresql postgresql 67108864 May 23 16:58 00000001000001C90000003D -rw------- 1 postgresql postgresql 67108864 May 23 16:59 00000001000001C90000003E -rw------- 1 postgresql postgresql 67108864 May 23 17:00 00000001000001CA00000000 -rw------- 1 postgresql postgresql 67108864 May 23 17:01 00000001000001CA00000001 -rw------- 1 postgresql postgresql 67108864 May 23 17:02 00000001000001CA00000002 -rw------- 1 postgresql postgresql 67108864 May 23 17:03 00000001000001CA00000003 -rw------- 1 postgresql postgresql 67108864 May 23 17:05 00000001000001CA00000004 -rw------- 1 postgresql postgresql 67108864 May 23 17:06 00000001000001CA00000005 -rw------- 1 postgresql postgresql 67108864 May 23 17:07 00000001000001CA00000006 -rw------- 1 postgresql postgresql 67108864 May 23 17:08 00000001000001CA00000007 -rw------- 1 postgresql postgresql 67108864 May 23 17:15 00000001000001CA00000008 -rw------- 1 postgresql postgresql 67108864 May 23 17:16 00000001000001CA00000009 -rw------- 1 postgresql postgresql 67108864 May 23 17:16 00000001000001CA0000000A -rw------- 1 postgresql postgresql 67108864 May 23 17:17 00000001000001CA0000000B -rw------- 1 postgresql postgresql 67108864 May 23 17:17 00000001000001CA0000000C -rw------- 1 postgresql postgresql 67108864 May 23 17:17 00000001000001CA0000000D -rw------- 1 postgresql postgresql 67108864 May 23 17:18 00000001000001CA0000000E -rw------- 1 postgresql postgresql 67108864 May 23 17:18 00000001000001CA0000000F -rw------- 1 postgresql postgresql 67108864 May 23 17:19 00000001000001CA00000010 -rw------- 1 postgresql postgresql 67108864 May 23 17:19 00000001000001CA00000011 -rw------- 1 postgresql postgresql 67108864 May 23 17:20 00000001000001CA00000012 -rw------- 1 postgresql postgresql 67108864 May 23 17:21 00000001000001CA00000013 -rw------- 1 postgresql postgresql 67108864 May 23 17:22 00000001000001CA00000014 -rw------- 1 postgresql postgresql 67108864 May 23 17:23 00000001000001CA00000015 -rw------- 1 postgresql postgresql 67108864 May 23 17:24 00000001000001CA00000016 -rw------- 1 postgresql postgresql 67108864 May 23 17:25 00000001000001CA00000017 -rw------- 1 postgresql postgresql 67108864 May 23 17:26 00000001000001CA00000018 -rw------- 1 postgresql postgresql 67108864 May 23 17:27 00000001000001CA00000019 -rw------- 1 postgresql postgresql 67108864 May 23 17:29 00000001000001CA0000001A -rw------- 1 postgresql postgresql 67108864 May 23 17:30 00000001000001CA0000001B -rw------- 1 postgresql postgresql 67108864 May 23 17:31 00000001000001CA0000001C -rw------- 1 postgresql postgresql 67108864 May 23 17:32 00000001000001CA0000001D -rw------- 1 postgresql postgresql 67108864 May 23 17:33 00000001000001CA0000001E -rw------- 1 postgresql postgresql 67108864 May 23 17:34 00000001000001CA0000001F 备注：注意近一个文件是 “00000001000001CA0000001F” , 再根据 “ postgres: startup process recovering 00000001000001CA0000001F “ 说明，PG已经恢复到最后一个日志文件了，猜测再等等，应该就恢复完成。大概过了10分钟左右， 后来发现PostgreSQL 果然停了，正好应验了前面的猜测，有点险。接着，启动数据库。 启动数据库12[postgresql@test_db pg_log]$ pg_ctl start -D $PGDATA server starting 运气不错，数据库可以正常启动。 连接测试123456789101112131415161718192021222324252627[postgresql@test_db pg_log]$ ps -ef | grep post 501 5665 1 3 17:56 pts/3 00:00:00 /opt/postgresql-9.0.3/bin/postgres -D /opt/pg/data 501 5666 5665 1 17:56 ? 00:00:00 postgres: logger process 501 5672 5665 0 17:56 ? 00:00:00 postgres: writer process 501 5673 5665 0 17:56 ? 00:00:00 postgres: wal writer process 501 5674 5665 0 17:56 ? 00:00:00 postgres: autovacuum launcher process 501 5675 5665 0 17:56 ? 00:00:00 postgres: stats collector process 501 5679 5665 0 17:56 ? 00:00:00 postgres: mylog skytflog xxx.xxx.xxx.xx(43189) idle 501 5680 5665 0 17:56 ? 00:00:00 postgres: mylog skytflog xxx.xxx.xxx.xx(43190) idle 501 5681 5665 0 17:56 ? 00:00:00 postgres: mylog skytflog xxx.xxx.xxx.xx(43191) idle 501 5682 5665 0 17:56 ? 00:00:00 postgres: mylog skytflog xxx.xxx.xxx.xx(43192) idle 501 5689 31953 0 17:56 pts/3 00:00:00 grep post[postgresql@test_db pg_log]$ psql postgres postgresql psql (9.0.3) Type \"help\" for help.postgres=&gt; \\l List of databases Name | Owner | Encoding | Collation | Ctype | Access privileges -----------+--------+----------+-----------+-------+------------------- postgres | mylog | UTF8 | C | C | skytflog | skyppa | UTF8 | C | C | template0 | mylog | UTF8 | C | C | =c/mylog + | | | | | mylog=CTc/mylog template1 | mylog | UTF8 | C | C | =c/mylog + | | | | | mylog=CTc/mylog (4 rows) 汗，总算缓了口气，数据库正常启动，数据还在，再一次向开发人员强调，以后不要通过”kill “ 命令来杀 PostgreSQL 进程。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"How to change PostgreSQL database name ?","slug":"20110524100049","date":"2011-05-24T02:00:49.000Z","updated":"2018-09-04T01:33:49.927Z","comments":true,"path":"20110524100049.html","link":"","permalink":"https://postgres.fun/20110524100049.html","excerpt":"","text":"今天需要对一测试数据库进行更改数据库名, 操作过程中遇到一点小插曲，做下记录。 查看数据库列表123456789101112postgres=# \\l List of databases Name | Owner | Encoding | Collation | Ctype | Access privileges -----------+----------+----------+-----------+-------+----------------------- postgres | postgres | UTF8 | C | C | skytmp | skytmp | UTF8 | C | C | template0 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres test | test | UTF8 | C | C | (5 rows) 目标：将数据库 skytmp, 更名为 skytmp_test。 尝试对数据库 skytmp 更名123456789101112131415161718postgres=# \\h alter database; Command: ALTER DATABASE Description: change a database Syntax: ALTER DATABASE name [ [ WITH ] option [ ... ] ]where option can be: CONNECTION LIMIT connlimitALTER DATABASE name RENAME TO new_nameALTER DATABASE name OWNER TO new_ownerALTER DATABASE name SET TABLESPACE new_tablespaceALTER DATABASE name SET configuration_parameter &#123; TO | = &#125; &#123; value | DEFAULT &#125; ALTER DATABASE name SET configuration_parameter FROM CURRENT ALTER DATABASE name RESET configuration_parameter ALTER DATABASE name RESET ALLpostgres=# alter database skytmp rename to skytmp_test; ERROR: database \"skytmp\" is being accessed by other users DETAIL: There are 49 other session(s) using the database. 备注：当对数据库进行更名时， pg抛出了以上 Error, 根据 Error 内容, 很容易看懂意思是当前有 49 个 session 还连着数据库。 查看数据库应用进程查看 PostgreSQL 进程, 果然还有很多应用进程，如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[postgres@db-192-168-173-63](mailto:postgres@db-192-168-173-63)-&gt; ps -ef | grep post postgres 18942 20802 0 08:54 ? 00:00:00 postgres: skytmp skytmp xxxx.xxx.xx.xx(41538) idle postgres 18943 20802 0 08:54 ? 00:00:00 postgres: skytmp skytmp xxxx.xxx.xx.xx(41539) idle postgres 18944 20802 0 08:54 ? 00:00:00 postgres: skytmp skytmp xxxx.xxx.xx.xx(41540) idle postgres 18945 20802 0 08:54 ? 00:00:00 postgres: skytmp skytmp xxxx.xxx.xx.xx(41541) idle postgres 18946 20802 0 08:54 ? 00:00:00 postgres: skytmp skytmp xxxx.xxx.xx.xx(41542) idle postgres 18947 20802 0 08:54 ? 00:00:00 postgres: skytmp skytmp xxxx.xxx.xx.xx(41543) idle postgres 18948 20802 0 08:54 ? 00:00:00 postgres: skytmp skytmp xxxx.xxx.xx.xx(41544) idle postgres 18949 20802 0 08:54 ? 00:00:00 postgres: skytmp skytmp xxxx.xxx.xx.xx(41545) idle postgres 18950 20802 0 08:54 ? 00:00:00 postgres: skytmp skytmp xxxx.xxx.xx.xx(41546) idle postgres 18951 20802 0 08:54 ? 00:00:00 postgres: skytmp skytmp xxxx.xxx.xx.xx(41547) idle root 19088 19059 0 09:05 pts/1 00:00:00 su - postgres postgres 19089 19088 0 09:05 pts/1 00:00:00 -bash root 19209 19181 0 09:09 pts/2 00:00:00 su - postgres postgres 19210 19209 0 09:09 pts/2 00:00:00 -bash postgres 19273 19089 0 09:12 pts/1 00:00:00 psql -h 127.0.0.1 postgres 19318 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49122) idle postgres 19319 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49124) idle postgres 19320 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49125) idle postgres 19321 20802 0 09:16 ? 00:00:14 postgres: skytmp skytmp xxxx.xxx.xx.xx(49127) idle postgres 19322 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49128) idle postgres 19323 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49129) idle postgres 19324 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49130) idle postgres 19325 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49131) idle postgres 19326 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49132) idle postgres 19327 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49133) idle postgres 19328 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49134) idle postgres 19329 20802 0 09:16 ? 00:00:14 postgres: skytmp skytmp xxxx.xxx.xx.xx(49135) idle postgres 19330 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49136) idle postgres 19331 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49137) idle postgres 19332 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49138) idle postgres 19333 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49139) idle postgres 19334 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49140) idle postgres 19335 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49141) idle postgres 19336 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49142) idle postgres 19337 20802 0 09:16 ? 00:00:14 postgres: skytmp skytmp xxxx.xxx.xx.xx(49143) idle postgres 19338 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49144) idle postgres 19339 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49145) idle postgres 19340 20802 0 09:16 ? 00:00:14 postgres: skytmp skytmp xxxx.xxx.xx.xx(49146) idle postgres 19341 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49147) idle postgres 19342 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49148) idle postgres 19343 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49149) idle postgres 19344 20802 0 09:16 ? 00:00:15 postgres: skytmp skytmp xxxx.xxx.xx.xx(49150) idle postgres 19345 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49151) idle postgres 19346 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49152) idle postgres 19347 20802 0 09:16 ? 00:00:05 postgres: skytmp skytmp xxxx.xxx.xx.xx(49153) idle root 19428 19210 0 09:24 pts/2 00:00:00 su - postgres postgres 19429 19428 0 09:24 pts/2 00:00:00 -bash postgres 19829 20802 1 09:46 ? 00:00:01 postgres: postgres postgres 127.0.0.1(18376) idle postgres 19836 19429 0 09:48 pts/2 00:00:00 ps -ef postgres 19837 19429 0 09:48 pts/2 00:00:00 grep post postgres 20802 1 0 Mar24 ? 00:01:18 /opt/pgsql/bin/postgres -D /opt/pgdata/1921/pg_root postgres 20803 20802 0 Mar24 ? 00:00:00 postgres: logger process postgres 20805 20802 0 Mar24 ? 00:04:33 postgres: writer process postgres 20806 20802 0 Mar24 ? 00:03:11 postgres: wal writer process postgres 20807 20802 0 Mar24 ? 00:08:11 postgres: stats collector process postgres 23570 20802 0 May20 ? 00:00:27 postgres: skytmp skytmp xxxx.xxx.xx.xx(36165) idle postgres 23624 20802 0 May20 ? 00:00:17 postgres: skytmp skytmp xxxx.xxx.xx.xx(60897) idle postgres 23625 20802 0 May20 ? 00:00:24 postgres: skytmp skytmp xxxx.xxx.xx.xx(60898) idle postgres 23626 20802 0 May20 ? 00:00:23 postgres: skytmp skytmp xxxx.xxx.xx.xx(60899) idle postgres 23627 20802 0 May20 ? 00:00:30 postgres: skytmp skytmp xxxx.xxx.xx.xx(60900) idle postgres 23628 20802 0 May20 ? 00:00:14 postgres: skytmp skytmp xxxx.xxx.xx.xx(60901) idle postgres 23629 20802 0 May20 ? 00:00:11 postgres: skytmp skytmp xxxx.xxx.xx.xx(60902) idle postgres 23630 20802 0 May20 ? 00:00:19 postgres: skytmp skytmp xxxx.xxx.xx.xx(60903) idle 于是通知开发人员，停应用。 停应用后更改数据库名称123456789101112131415postgres=# alter database skytmp rename to skytmp_test; ALTER DATABASEpostgres=# \\l List of databases Name | Owner | Encoding | Collation | Ctype | Access privileges -------------+----------+----------+-----------+-------+----------------------- postgres | postgres | UTF8 | C | C | skytmp_test | skytmp | UTF8 | C | C | template0 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres test | test | UTF8 | C | C | (5 rows) 停应用后， 数据库更名成功。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"浅谈 PostgreSQL 类型转换","slug":"20110517102453","date":"2011-05-17T02:24:53.000Z","updated":"2018-12-04T00:26:06.554Z","comments":true,"path":"20110517102453.html","link":"","permalink":"https://postgres.fun/20110517102453.html","excerpt":"","text":"类似Oracle ，PostgreSQL也有强大的类型转换函数, 下面仅举两个类型转换例子。 例子12345postgres=# select 1/4; ?column? ---------- 0 (1 row) 在PG里如果想做除法并想保留小数，用上面的方法却行不通，因为”/“ 运算结果为取整，并且会截掉小数部分。 通过操作符类型转换常见的类型转换方法是通过操作符::进行类型转换，如下：12345postgres=# select round(1::numeric/4::numeric,2); round ------- 0.25 (1 row) 备注：类型转换后，就能保留小数部分了。1234francs=&gt; select oid,relname,reltuples from pg_class where oid='test_1'::regclass; oid | relname | reltuples-------+---------+-----------16416 | test_1 | 6 备注： ‘test_1’::regclass 这里将表名转换成表的 oid，其它用法参考本文属的附二。 通过 cast 函数类型转换12345postgres=# select round( cast ( 1 as numeric )/ cast( 4 as numeric),2); round ------- 0.25 (1 row) 关于 cast 函数的用法12345postgres=# SELECT substr(CAST (1234 AS text), 3,1); substr -------- 3 (1 row) 附一: PostgreSQL 类型转换函数 附二: Object Identifier Types","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"ERROR: Functions in index expression must be marked IMMUTABLE ","slug":"20110512164848","date":"2011-05-12T08:48:48.000Z","updated":"2018-09-04T01:33:49.818Z","comments":true,"path":"20110512164848.html","link":"","permalink":"https://postgres.fun/20110512164848.html","excerpt":"","text":"今天在创建函数索引时遇到报错，报错信息即为标题，弄了很久，百思不得其解，后来向德哥咨询了下，很快就搞定了，下面是详细信息。 1 表定义1234567skytf=&gt; \\d test_39; Table \"skytf.test_39\" Column | Type | Modifiers -------------+--------------------------+----------- skyid | integer | create_time | timestamp with time zone | name | character varying(32) | 现在需要在字段 “skyid”, “create_time” 上创建联合索引。 2 创建索引12skytf=&gt; create index CONCURRENTLY idx_test_skyid_ctime on test_39 using btree (skyid, to_char(create_time, 'YYYY-MM-DD') ); ERROR: functions in index expression must be marked IMMUTABLE 创建函数索引报错，”functions in index expression must be marked IMMUTABLE” ，意思为建立函数索引时 函数必须标识为 “IMMUTABLE”。 3 查看 to_char 函数12345678910111213skytf=&gt; \\df to_char(); List of functions Schema | Name | Result data type | Argument data types | Type ------------+---------+------------------+-----------------------------------+-------- pg_catalog | to_char | text | bigint, text | normal pg_catalog | to_char | text | double precision, text | normal pg_catalog | to_char | text | integer, text | normal pg_catalog | to_char | text | interval, text | normal pg_catalog | to_char | text | numeric, text | normal pg_catalog | to_char | text | real, text | normal pg_catalog | to_char | text | timestamp with time zone, text | normal pg_catalog | to_char | text | timestamp without time zone, text | normal (8 rows) 4 以 postgres 超级用户连接，修改 to_char 函数属性12skytf=# alter function to_char(timestamp with time zone, text) IMMUTABLE; ALTER FUNCTION 备注：由于表 test_39 上的列 create_time 类型为 “timestamp with time zone” , 所以修改函数时应该修改函数 to_char(timestamp with time zone, text)，为了安全己见，不要直接修改 to_char 函数，建议新建一个 IMMUTABLE 属性的 to_char_immutable 函数。 5 验证是否生效123456skytf=&gt; \\ef to_char(timestamp with time zone, text)CREATE OR REPLACE FUNCTION pg_catalog.to_char(timestamp with time zone, text) RETURNS text LANGUAGE internal IMMUTABLE STRICT AS $function$timestamptz_to_char$function$ 从“IMMUTABLE STRICT” 中可以看出，函数已经修改成 “IMMUTABLE”属性。 6 以 skytf 连接, 再次创建索引12skytf=&gt; create index CONCURRENTLY idx_test_skyid_ctime on test_39 using btree (skyid, to_char(create_time, 'YYYY-MM-DD') ); CREATE INDEX 备注：在修改函数 to_char(timestamp with time zone, text) 属性后，创建索引就成功了。 123456789skytf=&gt; \\d test_39 Table \"skytf.test_39\" Column | Type | Modifiers -------------+--------------------------+----------- skyid | integer | create_time | timestamp with time zone | name | character varying(32) | Indexes: \"idx_test_skyid_ctime\" btree (skyid, to_char(create_time, 'YYYY-MM-DD'::text)) 7 手册上关于 “IMMUTABLE” 属性解释 IMMUTABLE indicates that the function cannot modify the database and always returns the same result when given the same argument values; that is, it does not do database lookups or otherwise use information not directly present in its argument list. If this option is given, any call of the function with all-constant arguments can be immediately replaced with the function value. 8 总结函数的默认属性为 “VOLATILE”， 即可变的，在创建函数索引时，需要将引用函数的属性改为”IMMUTABLE”, 即稳定的，函数索引才能创建成功。也就是说，只有属性为稳定的函数，才能用来创建函数索引。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"关于 PostgreSQL 的 function 里不能小批量提交","slug":"20110511132438","date":"2011-05-11T05:24:38.000Z","updated":"2018-09-04T01:33:49.771Z","comments":true,"path":"20110511132438.html","link":"","permalink":"https://postgres.fun/20110511132438.html","excerpt":"","text":"近期有个生产库的大表需要全表更新一个字段，并且这张表为核心表,访问非常频繁, 考虑到一个 update 语句可能执行时间很长，会锁住其它更新会话，从而对业务产生较大影响，于是考虑到写个 function 来小批量提交，这样可以大大减轻对业务的影响。 但在这里遇到了问题，原本想每 10000 条更新提交一下，但 在 function 里输入 commit 时， 这个function 会立即提交并返回，后来想了会，有其它的办法实现批量提交。以下为详细刷新数据步骤。 目标表表结构此表为大表，且访问频繁，表结构如下：123456789101112131415161718192021222324252627282930 Table \"skytf.tmp_basic_info\" Column | Type | Modifiers --------------+--------------------------------+----------------------------------------- user_id | integer | not null account | character varying(32) | not null nickname | character varying(64) | comlum1 | smallint | default 1 comlum2 | character varying(20) | default '1991-01-01'::character varying comlum3 | character varying(20) | comlum4 | character varying(40) | comlum5 | character varying(20) | comlum6 | character varying(32) | default '中国'::character varying comlum7 | character varying(20) | comlum8 | character varying(16) | comlum9 | character varying(128) | comlum10 | integer | default 0 comlum11 | integer | default 0 comlum12 | timestamp(0) without time zone | comlum13 | integer | default 86 Indexes: \"tmp_basic_info_pkey\" PRIMARY KEY, btree (user_id) \"tbl_mpc_user_info_username_key\" UNIQUE, btree (account) \"tmp_basic_info_comlum2\" btree (comlum2) \"tmp_basic_info_comlum8_index\" btree (comlum8) \"tmp_basic_info_nickname_index\" btree (nickname) \"tmp_basic_info_comlum7_index\" btree (comlum7) \"tmp_basic_info_comlum12_idx\" btree (comlum12) \"tmp_basic_info_comlum1_index\" btree (comlum1, comlum8, comlum2, comlum6, comlum4) \"tmp_basic_info_comlum9\" btree (comlum9) \"tmp_basic_info_userid_index\" btree (user_id) 由于业务需要，需要将 nickname 的值全表更新成 account。 创建临时表1234create table tmp_nickname as select user_id,account,nickname from tmp_basic_info where nickname like '%蓝雪%'; create unique index idx_user_id on tmp_nickname using btree (user_id); alter table tmp_nickname add column flag char(1); update tmp_nickname set flag='N'; flag 为刷新标识字段，’N’ 表示为更新失败，’Y’ 表示更新成功。 创建数据刷新函数123456789101112131415161718192021222324252627282930CREATE OR REPLACE FUNCTION skytf.func_setnickname() RETURNS numeric AS $BODY$ DECLARE rec RECORD; i integer; BEGIN i:=0; FOR rec IN SELECT user_id,account FROM skytf.tmp_nickname where flag ='N' limit 10000 LOOP --记录不符合导入条件1的号码 PERFORM 1 FROM skytf.tmp_basic_info WHERE user_id= rec.user_id ; IF FOUND THEN UPDATE skytf.tmp_basic_info SET nickname=rec.account WHERE user_id = rec.user_id; update tmp_nickname set flag='Y' where user_id=rec.user_id; END IF; i:= i+1; RAISE NOTICE ' here is %', i; END LOOP; return 0; END; $BODY$ LANGUAGE 'plpgsql' VOLATILE COST 100; 批量执行函数脚本修改 batch_update.sql ,输入以下内容：12345678910select skytf.func_setnickname(); select skytf.func_setnickname(); select skytf.func_setnickname(); select skytf.func_setnickname(); select skytf.func_setnickname(); select skytf.func_setnickname(); select skytf.func_setnickname(); select skytf.func_setnickname(); select skytf.func_setnickname(); .......... 备注，函数 skytf.func_setnickname() 每更新10000条提交，这个文件输入多少行“select skytf.func_setnickname();” 可以根据数据量估算下就行。 执行脚本脚本自己组织下就行，例如：1psql -h ip_addr -d dbnmae -U rolename -fbatch_update.sql &gt;batch_update.out 总结虽然 一个 function 里不能实现批量提交，但是可以逻辑分为多个 function 串行执行，从而实现小批量提交！","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: Introduction of creating rule ","slug":"20110509113215","date":"2011-05-09T03:32:15.000Z","updated":"2018-09-04T01:33:49.708Z","comments":true,"path":"20110509113215.html","link":"","permalink":"https://postgres.fun/20110509113215.html","excerpt":"","text":"很早知道PostgreSQL有个创建规则功能，今天终于简单的做了下实验，简单地说，当向PostgreSQL 发出一条SQL时，可以创建一个规则，让PG去执行另外一条命令，举个例子，当创建一个表上的 update 规则时，可以让它什么都不做，当在一个表上创建一个 insert 规则时，也可以让它什么都不做，也可以创建一个查询规则，当查询表A时，可以让它去查询表B，这其实相当于实现了视图的功能，下面是两个测试。 创建 Insert 规则创建表并插入测试数据123456789101112skytf=&gt; create table test_35 (id integer ,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_35 values (1,'a'); INSERT 0 1skytf=&gt; insert into test_35 values (2,'b'); INSERT 0 1skytf=&gt; select * From test_35; id | name ----+------ 1 | a 2 | b (2 rows) 创建 insert 规则12345678910111213141516171819skytf=&gt; \\h create rule Command: CREATE RULE Description: define a new rewrite rule Syntax: CREATE [ OR REPLACE ] RULE name AS ON event TO table [ WHERE condition ] DO [ ALSO | INSTEAD ] &#123; NOTHING | command | ( command ; command ... ) &#125;skytf=&gt; create or replace rule r_ins_test_35 as on insert to test_35 do instead nothing;CREATE RULEskytf=&gt; \\d test_35 Table \"skytf.test_35\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Rules: r_ins_test_35 AS ON INSERT TO test_35 DO INSTEAD NOTHING 验证 Insert 规则12345678skytf=&gt; insert into test_35 values (3,'c'); INSERT 0 0skytf=&gt; select * From test_35; id | name ----+------ 1 | a 2 | b (2 rows) 说明：从上面可以看出，当在表 test_35 上创建了 insert 规则后，向此表上插入数据无效。 创建 Update 规则删除原来 insert 规则123456789101112skytf=&gt; \\d test_35; Table \"skytf.test_35\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Rules: r_ins_test_35 AS ON INSERT TO test_35 DO INSTEAD NOTHINGskytf=&gt; drop rule r_ins_test_35 on test_35; DROP RULE 创建 update 规则123456789101112skytf=&gt; create rule r_upd_test_35 as on update to test_35 do instead nothing; CREATE RULEskytf=&gt; \\d test_35 Table \"skytf.test_35\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Rules: r_upd_test_35 AS ON UPDATE TO test_35 DO INSTEAD NOTHING 验证 Update 规则12345678910111213141516skytf=&gt; select * from test_35; id | name ----+------ 1 | a 2 | b (2 rows)skytf=&gt; update test_35 set name='francs' where id=1; UPDATE 0skytf=&gt; select * From test_35; id | name ----+------ 1 | a 2 | b (2 rows) 说明：从上面可以看出，当在表 test_35 上创建了 update 规则以后，更新表 test_35 数据时无效。 附一: Create Rule 语法 NameCREATE RULE ― define a new rewrite ruleSynopsisCREATE [ OR REPLACE ] RULE name AS ON event TO table [ WHERE condition ] DO [ ALSO | INSTEAD ] { NOTHING | command | ( command ; command … ) } nameThe name of a rule to create. This must be distinct from the name of any other rule for the same table. Multiple rules on the same table and same event type are applied in alphabetical name order. eventThe event is one of SELECT, INSERT, UPDATE, or DELETE. tableThe name (optionally schema-qualified) of the table or view the rule applies to. conditionAny SQL conditional expression (returning boolean). The condition expression cannot refer to any tables except NEW and OLD, and cannot contain aggregate functions. INSTEADINSTEAD indicates that the commands should be executed instead of the original command. ALSOALSO indicates that the commands should be executed in addition to the original command.If neither ALSO nor INSTEAD is specified, ALSO is the default.commandThe command or commands that make up the rule action. Valid commands are SELECT, INSERT, UPDATE, DELETE, or NOTIFY. 附二: Drop Rule 语法 NameDROP RULE ― remove a rewrite rule SynopsisDROP RULE [ IF EXISTS ] name ON relation [ CASCADE | RESTRICT ]DescriptionDROP RULE drops a rewrite rule.ParametersIF EXISTSDo not throw an error if the rule does not exist. A notice is issued in this case. nameThe name of the rule to drop. relationThe name (optionally schema-qualified) of the table or view that the rule applies to. CASCADEAutomatically drop objects that depend on the rule. RESTRICTRefuse to drop the rule if any objects depend on it. This is the default. ExamplesTo drop the rewrite rule newrule:DROP RULE newrule ON mytable;","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Killed a query failed When using pg_termniate_backend","slug":"20110426185613","date":"2011-04-26T10:56:13.000Z","updated":"2018-09-04T01:33:49.646Z","comments":true,"path":"20110426185613.html","link":"","permalink":"https://postgres.fun/20110426185613.html","excerpt":"","text":"今天生产库上有个会话(Select 操作) 引起了我的注意，都运行两天了，还没结束，这个SQL关联查询两张表的数据，具体信息如下 数据库和 OS 版本PostgreSQL版本: 8.4.3OS: Red Hat Enterprise Linux Server release 4.5 查询数据库活动会话123456789101112131415db_skytf=# select procpid, usename,query_start,client_addr, client_port ,current_query ,waiting from pg_stat_activity where current_query !='&lt;IDLE&gt;' ; procpid | usename | query_start | client_addr | client_port | current_query | waiting ---------+----------+-------------------------------+---------------+-------------+------------------------------------ 15583 | postgres | 2011-04-26 15:35:48.909562+08 | | -1 | select procpid, usename,query_start,client_addr, client_port ,current_query ,waiting from pg_stat_activity where current_query !='&lt;IDLE&gt;' ; | f 1312 | skytf | 2011-04-24 13:59:49.318178+08 | 192.168.x.xxx | 34621 | SELECT | f : mrp.*,res.name as displayname : FROM : skytf.tbl_app_mrp as mrp,tbl_app_resource as res : where mrp.appid=res.appid and : mrp.status='1' and res.status='1' : order by id desc (2 rows) 备注：反复执行上述查询几次，正是会话 1312 一直在执行， 根据 query_start，可以看出这个SQL已经跑了两天了。下面看下相关表信息，并看下这个SQL的执行计划。 SQL 信息查看表大小12345678910111213db_skytf=&gt; \\dt+ tbl_app_mrp List of relations Schema | Name | Type | Owner | Size | Description ---------+-------------+-------+---------+--------+------------- skytf | tbl_app_mrp | table | skytf | 520 MB | (1 row)db_skytf=&gt; \\dt+ tbl_app_resource List of relations Schema | Name | Type | Owner | Size | Description ---------+------------------+-------+---------+-------+------------- skytf | tbl_app_resource | table | skytf | 24 kB | (1 row) 从上面看出表不大。 查看执行计划1234567891011121314151617181920212223242526272829303132db_skytf=&gt; explain SELECT db_skytf-&gt; mrp.*,res.name as displayname db_skytf-&gt; FROM skytf.tbl_app_mrp as mrp,tbl_app_resource as res db_skytf-&gt; where mrp.appid=res.appid and db_skytf-&gt; mrp.status='1' and res.status='1' ; QUERY PLAN ----------------------------------------------------------------------------------- Hash Join (cost=7.56..145797.09 rows=2839643 width=141) Hash Cond: (mrp.appid = res.appid) -&gt; Seq Scan on tbl_app_mrp mrp (cost=0.00..106744.44 rows=2839643 width=127) Filter: (status = '1'::bpchar) -&gt; Hash (cost=5.29..5.29 rows=182 width=18) -&gt; Seq Scan on tbl_app_resource res (cost=0.00..5.29 rows=182 width=18) Filter: (status = '1'::bpchar) (7 rows)db_skytf=&gt; explain analyze SELECT db_skytf-&gt; mrp.*,res.name as displayname db_skytf-&gt; FROM skytf.tbl_app_mrp as mrp,tbl_app_resource as res db_skytf-&gt; where mrp.appid=res.appid and db_skytf-&gt; mrp.status='1' and res.status='1' ; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------ Hash Join (cost=7.56..145797.09 rows=2839643 width=141) (actual time=0.270..3211.741 rows=2799308 loops=1) Hash Cond: (mrp.appid = res.appid) -&gt; Seq Scan on tbl_app_mrp mrp (cost=0.00..106744.44 rows=2839643 width=127) (actual time=0.043..1896.213 rows=2799308 loops=1) Filter: (status = '1'::bpchar) -&gt; Hash (cost=5.29..5.29 rows=182 width=18) (actual time=0.187..0.187 rows=184 loops=1) -&gt; Seq Scan on tbl_app_resource res (cost=0.00..5.29 rows=182 width=18) (actual time=0.017..0.112 rows=184 loops=1) Filter: (status = '1'::bpchar) Total runtime: 3455.818 ms (8 rows) 备注：从PLAN执行情况来看，执行时间还快的，3 秒钟左右就跑完了，但为什么 1312 的查询都跑了两天了，还没跑完，猜测进程可能僵死，于是决定将它Kill。 杀会话12345db_skytf=# select pg_cancel_backend(1312); pg_cancel_backend ------------------- t (1 row) 起初，使用 pg_cancel_backend, 返回结果为 true, 但发现 1312 会话依然还在下面接着使用 pg_terminate_backend看下效果。 12345db_skytf=# select pg_terminate_backend(1312); pg_terminate_backend ---------------------- t (1 row) 再次查询数据库活动的会话123456789101112131415db_skytf=# select procpid, usename,query_start,client_addr, client_port ,current_query ,waiting from pg_stat_activity where current_query !='&lt;IDLE&gt;' ; procpid | usename | query_start | client_addr | client_port | current_query | waiting ---------+----------+-------------------------------+---------------+-------------+------------------------------------------------- --------------------------------------------------------------------------------------------+--------- 15583 | postgres | 2011-04-26 15:35:48.909562+08 | | -1 | select procpid, usename,query_start,client_addr, client_port ,current_query ,waiting from pg_stat_activity where current_query !='&lt;IDLE&gt;' ; | f 1312 | skytf | 2011-04-24 13:59:49.318178+08 | 192.168.x.xxx | 34621 | SELECT | f : mrp.*,res.name as displayname : FROM : skytf.tbl_app_mrp as mrp,tbl_app_resource as res : where mrp.appid=res.appid and : mrp.status='1' and res.status='1' : order by id desc 使用 pg_terminate_backend 后，虽然结果也返回 t,但会话 1312 依然没被Kill, 好顽强的会话啊，要是这个库是 Oracle， 俺早使用 kill -9 结果它了，但这是 PostgreSQL 使用 kill -9 后果很严重。。。 从 OS 层面分析123[postgres@skytf-db](mailto:postgres@skytf-db)-&gt; ps -ef | grep 1312; postgres 1312 3949 0 Jan01 ? 00:41:55 postgres: skytf db_skytf 192.168.x.xxx(34621) SELECT postgres 15354 15105 0 15:16 pts/4 00:00:00 grep 1312 备注：怀疑 1312 进程可能已经僵死，于是让应用人员检查。 应用主机(192.168.x.xxx)查看进程12netstat -anp|grep 34621 tcp 3216507 0 ::ffff:192.168.x.xxx:34621 ::ffff:192.168.1.102:1921 ESTABLISHED 4187/java 备注：应用人员发现些进程无异常，后来建议将这进程杀掉，在通过应用人员将这个进程杀掉后， 1312 会话终于被干掉了。 总结有些SQL在通过 pg_cancel_backend 和 pg_terminate_backen 也未能 Kill 的情况下，建议联系应用人员 kill 对应的应用进程。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"pg_hba.conf 一种安全的配置策略","slug":"20110424150113","date":"2011-04-24T07:01:13.000Z","updated":"2018-09-04T01:33:49.599Z","comments":true,"path":"20110424150113.html","link":"","permalink":"https://postgres.fun/20110424150113.html","excerpt":"","text":"大家知道PostgreSQL在连接认证体系方面功能非常全面，因为它用到了一个 pg_hba.conf ( HBA stands for host-based authentication )文件。 pg_hba.conf 示例先来看 pg_hba.conf 文件的一个例子1234567# TYPE DATABASE USER CIDR-ADDRESS METHOD# \"local\" is for Unix domain socket connections only local all all trust# IPv4 local connections: host all all 127.0.0.1/32 md5# IPv6 local connections: host mydb query_man 192.168.1.55/32 md5 备注：为了显示方便，上面仅列出 pg_hba.conf文件的最后一部分。从上面看出 pg_hba.conf 文件是由一行行记录组成，每行记录包括认证类型，数据库名，用户名，认证IP，认证方式字段。就是这些记录控制着客户端的连接。 本文不准备详细介绍 pg_hba.conf 文件的配置方法，关于 pg_hba.conf 的详细概述可以参考手册 http://www.postgresql.org/docs/9.0/static/client-authentication.html , 本 pg_hba.conf 配置场景文仅讲述以下场景 pg_hba.conf 的配置方法。假设场景： 有一套生产库已经上线了，假设这个生产库的库名为 mydb, 生产用户为 skytf, 现在开发人员需要定期的去查看这个库里的某些表，这时考虑到生产库的安全，可以创建一个查询用户 query_man, 并授予一定的查询权限。这时开发人员可以在自己的电脑上通过 pgadmin等客户端工具连接数据库了，但此时有个问题，开发人员手里同时也有生产用户 skytf 的密码 ，因为项目上线时，DBA会所生产用户密码给开发人员，对应用进行配置，一般情况下都没收回这个权限，那么，理论上说，开发人员可以在本机使用生产帐号 skytf 连接生产库了( 不考虑网络情况下 ), 很明显，这是一种安全隐患，但是，PG里的 pg_hba.conf非常强大，可以利用它来配置一种比较严谨的认证方法。 修改后的 pg_hba.conf 文件123456789# TYPE DATABASE USER CIDR-ADDRESS METHOD# \"local\" is for Unix domain socket connections only local all all trust# IPv4 local connections: host all all 127.0.0.1/32 md5# IPv6 local connections: host mydb query_man 192.168.1.55/32 md5 host allall 192.168.1.55/32 reject host all all 0.0.0.0/0 md5 这里假设 “192.168.1.55” 是指开发人员的IP， 这只是举个例子，如果有多数开发人员，您也可以将它设置成一个网段。这里主要看以下两段, 第一段的意思是，主机 192.168.1.55 仅允许用 query_man 用户连接数据库 mydb, 第二段的意思是拒绝主机 192.168.1.55 以任何用户连接任何数据库。123# IPv6 local connections: host mydb query_man 192.168.1.55/32 md5 host all all 192.168.1.55/32 reject 讲到这里，需要讲解下 pg_hba.conf 认证流程，客户端在连接 PostgreSQL时，PostgreSQL 会从上到下读取 pg_hba.conf 文件，如果有匹配的记录且认证通过，则可以连接数据库，如果有匹配的记录且认证失败则访问被拒绝，而不再考虑 pg_hba.conf 接下来的记录。上面的配置有效地控制了开发人员通过本机以生产 用户连接生产库的情况, 同时只允许他们以查询用户连接生产库。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 跨大版本迁移 Bytea 字段内容显示不一致","slug":"20110422202034","date":"2011-04-22T12:20:34.000Z","updated":"2018-09-04T01:33:49.552Z","comments":true,"path":"20110422202034.html","link":"","permalink":"https://postgres.fun/20110422202034.html","excerpt":"","text":"今天在做数据库迁移时发现有张表的数据不对，这张表有个 bytea 字段，迁移后显示的内容不对，源库是 8.4.3 版本目标库是9.0.3 版本, 一时找不到原因，后来在德哥的指导下，只要在目标库设置一个 bytea_output 参数就行,例如可以在导入脚本里加上 set bytea_output = &#39;escape&#39;。 表结构12345678910111213cmp=&gt; \\d tbl_temp Table \"cmp.tbl_temp\" Column | Type | Modifiers ------------+-----------------------------+----------- txman | character varying(100) | mcc | character varying(10) | name | character varying(100) | filebytes | bytea | startprice | numeric | endprice | numeric | starttime | timestamp without time zone | endtime | timestamp without time zone | createtime | timestamp without time zone | PostgreSQL 9.0 手册介绍 bytea_output (enum) Sets the output format for values of type bytea. Valid values are hex (the default) and escape (the traditional PostgreSQL format). See Section 8.4 for more information. The bytea type always accepts both formats on input, regardless of this setting. 原因很明显了，由于在PostgreSQL老版本里(8.4以前) bytea_output 参数默认为 “escape” , 而 PostgreSQL 9.0 的这个参数被默认设置成了 “hex” ,所以在导入到 PostgreSQL9 版本里，bytea字段内容就不对了。在数据导完后，建议将 $PGDATA/postgresql.conf 的这个参数也改下，设置成 “escape” , 不然那个字段应用显示依然会有问题。","categories":[{"name":"PG备份与恢复","slug":"PG备份与恢复","permalink":"https://postgres.fun/categories/PG备份与恢复/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"GeenPlum 原理篇之二：物理数据分布","slug":"20110418144115","date":"2011-04-18T06:41:15.000Z","updated":"2018-09-04T01:33:49.490Z","comments":true,"path":"20110418144115.html","link":"","permalink":"https://postgres.fun/20110418144115.html","excerpt":"","text":"根据前面博文介绍，已经知道了 Master 节点不存放用户数据，用户数据而是按一定规则打散到各个子节点里，这篇主要介绍下子节点数据分布。 一 首先看一个模型以下模型中涉及到四张表(sale ,customer, vendor, product), 每个表的第一个字段为 Primary key其中 sale 表有三个 foreign key。 Figure: Sample Database Star Schema 在 Greenplum 中，所有表数据都按一定规则分割成不重叠的一部分，这些分割的部分分别位于各个节点, 其中 Master 仅存储系统表，所有 segments 节点存放用户数据，下图展示了 Greenplum 物理数据的分布原理。 二 Greenplum 数据分布策略在建表的语句里，提供distributed 属性来设置表数据分布策略，表数据主要有以下两种分布策略。 2.1 哈希( Hash )分布使用 Hash分布，表中的一个或多个字段可以设置成 distributed 键，相同的值被打散到同一个Segment 节点上。Hash分区是默认的分区方式， 如果不指定分区方式，那么表的的主键（如果存在主键） 或者表的第一个字段将被默认选择为 distributed 键。 2.2 随机( Random )分析使用 Random 分布方式，表中的数据将均匀地打散到各个子节点上，并且随着数据地进入顺序循环 地打散到各个子节点上，表上相同的值并不一定颁布在同一个节点上，官方认为选择 Hash 分区方式在性能上占有优势。 三 实验3.1创建 appendonly 测试表并插入数据123456warehouse=# create table test_3 (id integer , name varchar(32)) warehouse-# with ( appendonly=true ) warehouse-# distributed by (id); CREATE TABLEwarehouse=# insert into test_3 select generate_series(1,1000),'francs'; INSERT 0 1000 3.2 查看各个节点数据分布123456warehouse=# select get_ao_distribution('test_3'); get_ao_distribution --------------------- (0,501) (1,499) (2 rows) 3.3 查看当前GP配置123456789warehouse=# select * from gp_configuration; content | definedprimary | dbid | isprimary | valid | hostname | port | datadir ---------+----------------+------+-----------+-------+----------+-------+------------------------ -1 | t | 1 | t | t | gpmaster | 5432 | /opt/gp_data/gp-1 0 | t | 2 | t | t | gpnode1 | 50001 | /opt/gp_data/data/gp0 1 | t | 3 | t | t | gpnode2 | 50001 | /opt/gp_data/data/gp1 0 | f | 4 | f | t | gpnode2 | 60001 | /opt/gp_data/mdata/gp0 1 | f | 5 | f | t | gpnode1 | 60001 | /opt/gp_data/mdata/gp1 (5 rows) 从上面可以看到， content 为矩阵数据库标识， “-1” 表示 gpmaster 节点 , “0”表示第一个 Segment, “1”表示第二个 segment, 其中 Primary Segemnt 和 Mirror Segment 的值相同。根据 3.2结果来看，节点一 分布了501条数据，节点二分布了 499条数据 3.4 附:gp_configuration 中字段 content 解释 The ID for the portion of data on an instance. A primary segment instance and its mirror will have the same content ID.For a segment the value is from 0-N, where N is the number of segments in Greenplum Database.For the master, the value is -1. The combination of content and definedprimary is the PRIMARY KEY. 3.5 附: get_ao_distribution 函数解释 Function Return Type Description get_ao_distribution (oid,name) Set of (dbid, tuplecount) rows hows the distribution of rows of an append-only table across the array. Returns a set of rows, each of whichincludes a segment dbid and the numbetuples stored on the segment.","categories":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://postgres.fun/categories/GreenPlum/"}],"tags":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://postgres.fun/tags/GreenPlum/"}]},{"title":"GreenPlum 问题一例 ERROR: Interconnect timeout: Unable to complete setup of all connections within time limit.","slug":"20110418121800","date":"2011-04-18T04:18:58.000Z","updated":"2018-09-04T01:33:49.443Z","comments":true,"path":"20110418121800.html","link":"","permalink":"https://postgres.fun/20110418121800.html","excerpt":"","text":"虚拟机 GreenPlum 数据仓库搭建好以后，可以执行建库，建表，数据插入语句，就是不能执行”select”操作报以下ERROR，这个问题折腾了好几天，后来在德哥的帮忙下，终于解决了，原来自己在一个配置上疏忽了。 问题现象不能执行查询SQL，如下：1.1创建测试表，并插入数据12345678910111213warehouse=# create table test_1 (id integer , name varchar(32)) distributed by (id);CREATE TABLEwarehouse=# \\d test_1 Table \"public.test_1\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Distributed by: (id)warehouse=# insert into test_1 values (1,'francs');INSERT 0 1warehouse=# insert into test_1 values (2,'fpZhou');INSERT 0 1 1.2查询报错123warehouse=# select * From test_1;ERROR: Interconnect timeout: Unable to complete setup of all connections within time limit.DETAIL: Completed 0 of 2 incoming and 0 of 0 outgoing connections. gp_interconnect_setup_timeout = 20 seconds. 处理过程2.1检查Master节点，未发现明显异常 2.2检查Segment节点，发现以下ERROR12011-04-14 21:14:05.535064 CST,\"warehouse\",\"warehouse\",p20619,th-1208420672,\"192.168.1.50\",\"41829\",2011-04-14 21:08:26 CST,1279,con6,cmd16,seg0,slice1,,x1279,sx1,\"LOG\",\"58M01\",Interconnect could not connect to seg-1 127.0.0.1:61572 pid=9845; will retry. Connection refused (connect errno 111)\",,,,,,\"select * From test_1 从上面的信息来看“seg-1 127.0.0.1” , seg-1 指的是 master 节点，怎么解析成 “127.0.0.1”了, 其实这里我早应该发现，这是关键信息。 2.3尝试调整参数 gp_interconnect_setup_timeout 将参数 gp_interconnect_setup_timeout 调大到 120s 后，依然无效。 2.4 GP整体状态检查,未发现异常1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980[greenplum@gpmaster opt]$ gpstate -s20110414:20:12:43:gpstate:gpmaster:greenplum-[INFO]:-Obtaining GPDB array type, [Brief], please wait...20110414:20:13:01:gpstate:gpmaster:greenplum-[INFO]:-Obtaining GPDB array type, [Brief], please wait...20110414:20:13:20:gpstate:gpmaster:greenplum-[INFO]:-Spawning parallel processes batch [1], please wait.......20110414:20:13:40:gpstate:gpmaster:greenplum-[INFO]:-Waiting for parallel processes batch [1], please wait..................20110414:20:14:03:gpstate:gpmaster:greenplum-[INFO]:-Master Configuration &amp; Status20110414:20:14:03:gpstate:gpmaster:greenplum-[INFO]:------------------------------20110414:20:14:04:gpstate:gpmaster:greenplum-[INFO]:-Master host = gpmaster20110414:20:14:05:gpstate:gpmaster:greenplum-[INFO]:-Master postgres process ID = 2099220110414:20:14:06:gpstate:gpmaster:greenplum-[INFO]:-Master data directory = /opt/gp_data/gp-120110414:20:14:06:gpstate:gpmaster:greenplum-[INFO]:-Database name = template1 20110414:20:14:07:gpstate:gpmaster:greenplum-[INFO]:-Master port = 543220110414:20:14:07:gpstate:gpmaster:greenplum-[INFO]:-Master current role = dispatch20110414:20:14:08:gpstate:gpmaster:greenplum-[INFO]:-Greenplum array configuration type = Standard20110414:20:14:09:gpstate:gpmaster:greenplum-[INFO]:-Greenplum initsystem version = 2|3.3.6.1 build 120110414:20:14:10:gpstate:gpmaster:greenplum-[INFO]:-Greenplum current version = PostgreSQL 8.2.13 (Greenplum Database 3.3.6.1 build 1) on i686-pc-linux-gnu, compiled by GCC gcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-44) compiled on Apr 2 2010 16:39:4920110414:20:14:10:gpstate:gpmaster:greenplum-[INFO]:-Postgres version = 8.2.1320110414:20:14:11:gpstate:gpmaster:greenplum-[INFO]:-Greenplum fault mode = readonly20110414:20:14:11:gpstate:gpmaster:greenplum-[INFO]:-Greenplum mirroring status = on20110414:20:14:12:gpstate:gpmaster:greenplum-[INFO]:-Greenplum master standby = No master standby configured20110414:20:14:12:gpstate:gpmaster:greenplum-[INFO]:-Greenplum standby master state = 20110414:20:14:13:gpstate:gpmaster:greenplum-[INFO]:-Segment instance status20110414:20:14:13:gpstate:gpmaster:greenplum-[INFO]:------------------------------------------------20110414:20:14:14:gpstate:gpmaster:greenplum-[INFO]:-Parallel process exit status20110414:20:14:14:gpstate:gpmaster:greenplum-[INFO]:------------------------------------------------20110414:20:14:15:gpstate:gpmaster:greenplum-[INFO]:-Total processes marked as completed = 420110414:20:14:15:gpstate:gpmaster:greenplum-[INFO]:-Total processes marked as killed = 020110414:20:14:15:gpstate:gpmaster:greenplum-[INFO]:-Total processes marked as failed = 020110414:20:14:16:gpstate:gpmaster:greenplum-[INFO]:------------------------------------------------20110414:20:14:17:gpstate:gpmaster:greenplum-[INFO]:------------------------------------------------20110414:20:14:17:gpstate:gpmaster:greenplum-[INFO]:-Segment Instance Status Report20110414:20:14:17:gpstate:gpmaster:greenplum-[INFO]:------------------------------------------------20110414:20:14:18:gpstate:gpmaster:greenplum-[INFO]:------------------------------------------------20110414:20:14:18:gpstate:gpmaster:greenplum-[INFO]:-Segment instance hostname = gpnode120110414:20:14:19:gpstate:gpmaster:greenplum-[INFO]:-Segment instance port = 50001 20110414:20:14:19:gpstate:gpmaster:greenplum-[INFO]:-Segment instance datadir = /opt/gp_data/data/gp0 Type = Primary20110414:20:14:20:gpstate:gpmaster:greenplum-[INFO]:-Master reports status as = Valid20110414:20:14:20:gpstate:gpmaster:greenplum-[INFO]:-Total errors (remote) = 020110414:20:14:20:gpstate:gpmaster:greenplum-[INFO]:-File postmaster.pid = Found20110414:20:14:21:gpstate:gpmaster:greenplum-[INFO]:-PID from postmaster.pid file = 1885120110414:20:14:21:gpstate:gpmaster:greenplum-[INFO]:-Lock files in /tmp status = Found20110414:20:14:22:gpstate:gpmaster:greenplum-[INFO]:-Active PID = 1885120110414:20:14:22:gpstate:gpmaster:greenplum-[INFO]:-Instance status = Active20110414:20:14:23:gpstate:gpmaster:greenplum-[INFO]:------------------------------------------------20110414:20:14:23:gpstate:gpmaster:greenplum-[INFO]:-Segment instance hostname = gpnode120110414:20:14:24:gpstate:gpmaster:greenplum-[INFO]:-Segment instance port = 60001 20110414:20:14:24:gpstate:gpmaster:greenplum-[INFO]:-Segment instance datadir = /opt/gp_data/mdata/gp1 Type = Mirror20110414:20:14:24:gpstate:gpmaster:greenplum-[INFO]:-Master reports status as = Valid20110414:20:14:25:gpstate:gpmaster:greenplum-[INFO]:-Total errors (remote) = 020110414:20:14:25:gpstate:gpmaster:greenplum-[INFO]:-File postmaster.pid = Found20110414:20:14:26:gpstate:gpmaster:greenplum-[INFO]:-PID from postmaster.pid file = 1886520110414:20:14:26:gpstate:gpmaster:greenplum-[INFO]:-Lock files in /tmp status = Found20110414:20:14:26:gpstate:gpmaster:greenplum-[INFO]:-Active PID = 1886520110414:20:14:27:gpstate:gpmaster:greenplum-[INFO]:-Mirror status = Passive20110414:20:14:27:gpstate:gpmaster:greenplum-[INFO]:-Instance status = Valid20110414:20:14:28:gpstate:gpmaster:greenplum-[INFO]:------------------------------------------------20110414:20:14:29:gpstate:gpmaster:greenplum-[INFO]:-Segment instance hostname = gpnode220110414:20:14:29:gpstate:gpmaster:greenplum-[INFO]:-Segment instance port = 50001 20110414:20:14:29:gpstate:gpmaster:greenplum-[INFO]:-Segment instance datadir = /opt/gp_data/data/gp1 Type = Primary20110414:20:14:30:gpstate:gpmaster:greenplum-[INFO]:-Master reports status as = Valid20110414:20:14:30:gpstate:gpmaster:greenplum-[INFO]:-Total errors (remote) = 020110414:20:14:31:gpstate:gpmaster:greenplum-[INFO]:-File postmaster.pid = Found20110414:20:14:31:gpstate:gpmaster:greenplum-[INFO]:-PID from postmaster.pid file = 1714120110414:20:14:32:gpstate:gpmaster:greenplum-[INFO]:-Lock files in /tmp status = Found20110414:20:14:32:gpstate:gpmaster:greenplum-[INFO]:-Active PID = 1714120110414:20:14:32:gpstate:gpmaster:greenplum-[INFO]:-Instance status = Active20110414:20:14:33:gpstate:gpmaster:greenplum-[INFO]:------------------------------------------------20110414:20:14:33:gpstate:gpmaster:greenplum-[INFO]:-Segment instance hostname = gpnode220110414:20:14:34:gpstate:gpmaster:greenplum-[INFO]:-Segment instance port = 60001 20110414:20:14:34:gpstate:gpmaster:greenplum-[INFO]:-Segment instance datadir = /opt/gp_data/mdata/gp0 Type = Mirror20110414:20:14:35:gpstate:gpmaster:greenplum-[INFO]:-Master reports status as = Valid20110414:20:14:35:gpstate:gpmaster:greenplum-[INFO]:-Total errors (remote) = 020110414:20:14:36:gpstate:gpmaster:greenplum-[INFO]:-File postmaster.pid = Found20110414:20:14:36:gpstate:gpmaster:greenplum-[INFO]:-PID from postmaster.pid file = 1715320110414:20:14:36:gpstate:gpmaster:greenplum-[INFO]:-Lock files in /tmp status = Found20110414:20:14:37:gpstate:gpmaster:greenplum-[INFO]:-Active PID = 1715320110414:20:14:38:gpstate:gpmaster:greenplum-[INFO]:-Mirror status = Passive20110414:20:14:38:gpstate:gpmaster:greenplum-[INFO]:-Instance status = Valid 2.5网络检查,发现网络不好123456789101112131415161718192021[greenplum@gpmaster ~]$ gpchecknet -f all_host_file -d test//opt/greenplum-db/./bin/gpchecknet -f all_host_file -d test/--------------------- NETPERF TEST-------------------====================== RESULT====================Netperf bisection bandwidth testgpnode1 -&gt; gpnode2 = 6.090000gpnode2 -&gt; gpmaster = 7.730000gpnode2 -&gt; gpnode1 = 6.960000gpmaster -&gt; gpnode2 = 6.530000Summary:sum = 27.31 MB/secmin = 6.09 MB/secmax = 7.73 MB/secavg = 6.83 MB/secmedian = 6.96 MB/sec[Warning] connection between gpnode1 and gpnode2 is no good[Warning] connection between gpmaster and gpnode2 is no good 2.6虚拟机用的是 HOST-ONLY 网络连接，尝试将网络连接改成桥接，依然无效2.7后来实在搞不定，在德哥的帮助下，终于确定了以下问题2.8ping gpmaster 主机发现问题1234567[root@gpmaster ~]# ping gpmasterPING gpmaster (127.0.0.1) 56(84) bytes of data.64 bytes from gpmaster (127.0.0.1): icmp_seq=1 ttl=64 time=2.55 ms64 bytes from gpmaster (127.0.0.1): icmp_seq=2 ttl=64 time=0.073 ms64 bytes from gpmaster (127.0.0.1): icmp_seq=3 ttl=64 time=0.040 ms64 bytes from gpmaster (127.0.0.1): icmp_seq=4 ttl=64 time=0.049 ms64 bytes from gpmaster (127.0.0.1): icmp_seq=5 ttl=64 time=0.039 ms ping gpmaster 发现，gpmaster 解析为 127.0.0.1, 应该是 192.168.1.50 才对2.9 检查 /etc/hosts 2.9.1 原来的 /etc/hosts内容12345# that require network functionality will fail.127.0.0.1 gpmaster localhost.localdomain localhost192.168.1.50 gpmaster192.168.1.51 gpnode1192.168.1.52 gpnode2 发现，gpmaster 解析成 127.0.0.1 了，先读的第一行。 2.9.2修改后的 /etc/hosts123456# Do not remove the following line, or various programs# that require network functionality will fail.127.0.0.1 localhost.localdomain localhost192.168.1.50 gpmaster192.168.1.51 gpnode1192.168.1.52 gpnode2 2.9.3再次 ping gpmaster1234567[root@gpmaster ~]# ping gpmasterPING gpmaster (192.168.1.50) 56(84) bytes of data.64 bytes from gpmaster (192.168.1.50): icmp_seq=1 ttl=64 time=2.59 ms64 bytes from gpmaster (192.168.1.50): icmp_seq=2 ttl=64 time=0.066 ms64 bytes from gpmaster (192.168.1.50): icmp_seq=3 ttl=64 time=0.038 ms64 bytes from gpmaster (192.168.1.50): icmp_seq=4 ttl=64 time=0.051 ms64 bytes from gpmaster (192.168.1.50): icmp_seq=5 ttl=64 time=0.044 ms 这时，gpmaster 的IP解析为 “192.168.1.50”。 2.9.4退出 session 后，重新测试123456warehouse=# select * From test_1; id | name ----+-------- 1 | francs 2 | fpZhou(2 rows) 这次终于OK了，数据可以查询了。 总结这个问题折腾了好几天，结果问题发生在 /etc/hosts上，还是在高手帮忙的情况下解决的，应该好好反思下，OS层面的知识比较薄弱，对基本知识应该加强。 /etc/hosts 解析是从上到下解析的，当解析文件 /etc/hosts 时，第一次匹配成功则返回。","categories":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://postgres.fun/categories/GreenPlum/"}],"tags":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://postgres.fun/tags/GreenPlum/"}]},{"title":"GreenPlum 数据库安装","slug":"20110416144048","date":"2011-04-16T06:40:48.000Z","updated":"2018-09-04T01:33:49.380Z","comments":true,"path":"20110416144048.html","link":"","permalink":"https://postgres.fun/20110416144048.html","excerpt":"","text":"关于 GreenPlum 的介绍可以参考前一篇文章，这篇文章主要介绍 GreenPlum 3.6.1 详细安装步骤。 1 实验环境1.1 硬件环境一台 2G内存的联想 SL410K 双核笔记本, 并安装三台Linux虚拟机。 1.2虚拟机配置 节点 IP 内存 硬盘 gpmaster 192.168.1.50 400MB 15GB gpnode1 192.168.1.51 400MB 15GB gpnode2 192.168.1.52 400MB 15GB 1.3 本文中用到的术语master host : 主节点主机segment host : 子节点主机 2修改master, segments主机参数 (所有主机上执行)2.1 修改 /etc/sysctl.conf 添加以下12345678kernel.shmmax = 500000000 kernel.shmmni = 4096 kernel.shmall = 4000000000 kernel.sem = 250 64000 100 512 net.ipv4.tcp_tw_recycle=1 net.ipv4.tcp_max_syn_backlog=4096 net.core.netdev_max_backlog=10000 vm.overcommit_memory=2 2.2 修改 /etc/security/limits.conf ,添加以下1234* soft nofile 65536 * hard nofile 65536 * soft nproc 131072 * hard nproc 131072 2.3 修改 /etc/hosts ,添加以下123192.168.1.50 gpmaster 192.168.1.51 gpnode1 192.168.1.52 gpnode2 3 增加操作系统greenplum用户并创建数据目录（所有节点上执行）3.1增加 greenplum 组和用户123[root@gpmaster greenplum-db]# groupadd greenplum [root@gpmaster greenplum-db]# useradd -g greenplum greenplum [root@gpmaster greenplum-db]# passwd greenplum 3.2配置环境变量将文件 /opt/greenplum-db/greenplum_path.sh 写入 /home/greenplum/.bash_profilesource .bash_profile 3.3创建 segment 数据目录12mkdir -p /opt/gp_data/data chown -R greenplum:greenplum /opt/gp_data/data 3.4创建 sgement 镜像目录12mkdir -p /opt/gp_data/mdata chown -R greenplum:greenplum /opt/gp_data/mdata 说明：上面两个步骤也可以等配置好主机之前等效性用 gpssh 执行。 gpssh可以同时连接所有子节点主机并进行操作。 4 在Master节点上安装 Greenplum 软件4.1 下载介质在官网 http://gpn.greenplum.com/download.php 下载 4.2 解压1unzip greenplum-db-3[1].3.6.1-build-1-RHEL5-i386.zip 4.3 安装 Greenplum软件1# /bin/bash greenplum-db-3.3.6.1-build-1-RHEL5-i386.bin 提示“yes” 接受协议 和安装路径, 默认的是“(/usr/local/greenplum-db-3.3.6.0)” ，我这里选择的是 “/opt/greenplum-db-3.3.6.1” ,安装结束后会自动创建一个软链接,如 123[root@gpmaster opt]# ll /opt lrwxrwxrwx 1 root root 22 04-06 21:30 greenplum-db -&gt; ./greenplum-db-3.3.6.1 drwxr-xr-x 11 root root 4096 04-06 21:30 greenplum-db-3.3.6.1 5 在所有子节点主机上安装 greenplumn 软件5.1创建节点配置文件 1234vi /home/greenplum/all_host_file 添加以下内容 gpmaster gpnode1 gpnode2 5.2 执行文件greenplum_path.sh,设定环境变量1[root@gpmaster greenplum]#source /opt/greenplum-db/greenplum_path.sh 5.3以 root 用户交换密钥12345678910111213[root@gpmaster greenplum]# gpssh-exkeys -f /home/greenplum/all_host_file [STEP 1 of 5] create local ID and authorize on local host ... /root/.ssh/id_rsa file exists ... key generation skipped[STEP 2 of 5] keyscan all hosts and update known_hosts file[STEP 3 of 5] authorize current user on remote hosts ... send to gpnode1 Enter password for gpnode1: ... send to gpnode2[STEP 4 of 5] determine common authentication file content[STEP 5 of 5] copy authentication files to all remote hosts ... finished key exchange with gpnode1 ... finished key exchange with gpnode2[INFO] completed successfully 5.4以 greenplum 用户交换密钥12[root@gpmaster greenplum]# su - greenplum [greenplum@gpmaster ~]$ gpssh-exkeys -f /home/greenplum/all_host_file 5.5将主节点上 greenplum 软件复制到 所有segments 主机 ( 主节点上操作 )12345678910111213141 cd /opt tar cvf gp.tar greenplum-db-3.3.6.12 source /opt/greenplum-db/greenplum_path.sh3 生成 all_host_file文件，添加以下 gpmaster gpnode1 gpnode24 gpscp -f greenplum-db/all_host_file gp.tar =:/opt5 gpssh -f greenplum-db/all_host_file =&gt; cd /opt =&gt; tar xvf /opt/gp.tar =&gt; chown -R greenplum:greenplum greenplum-db; =&gt; chown -R greenplum:greenplum greenplum-db-3.3.6.1; =&gt; ln -s /opt/greenplum-db-3.3.6.1 /opt/greenplum-db 5.6同步系统时钟(如果配置了 ntpd服务)1gpssh -f /opt/greenplum-db/all_hosts_file ntpd 6 初始化 Greenplum (on Master )6.1 copy 配置文件模板1cp $GPHOME/docs/cli_help/gp_init_config_example /home/greenplum/gp_init_config 6.2修改配置文件 gp_init_config ，修改以下参数12345678910ARRAY_NAME=\"Greenplum\" MACHINE_LIST_FILE=/home/greenplum/multi_seg_host_file SEG_PREFIX=gp PORT_BASE=50000 declare -a DATA_DIRECTORY=(/opt/gp_data/data) MASTER_HOSTNAME=gpmaster MASTER_DIRECTORY=/opt/gp_data MASTER_PORT=5432 MIRROR_PORT_BASE=60000 declare -a MIRROR_DATA_DIRECTORY=(/opt/gp_data/mdata) 上面是偶配置的参数，比较重要的两个参数是 “DATA_DIRECTORY” 和 “MIRROR_DATA_DIRECTORY” ，DATA_DIRECTORY 是指 segment主机 primary 实例的数据目录，MIRROR_DATA_DIRECTORY是指 segment 主机 mirror 实例的数据目录，这样一份配置，当后面执行 gpinitsystem 时，会在每一台 segment主机上创建一个 primary 实例和 一个 mirror 实例。 6.3先检查下OS配置是否满足需求123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197[greenplum@gpmaster ~]$ gpcheckos -f all_host_file checking: postgres.md5 = 62406a6f410a5b324fcced3236150ee6 checking: postgres.version = postgres (Greenplum Database) 8.2.13 checking: sync.time between (2011-04-13 19:38:31.696946, 2011-04-13 19:38:51.696946) checking: platform.hostname checking: platform.memory checking: platform.memory = 352321536 checking: platform.system = linux or sunos checking: platform.system = linux checking: platform.release = 2.6.18-194.el5xen checking: sysctl.kernel.shmall = 4000000000 checking: sysctl.net.ipv4.tcp_max_syn_backlog = 4096 checking: sysctl.vm.overcommit_memory = 2 checking: sysctl.net.core.netdev_max_backlog = 10000 checking: ulimit.nofile &gt;= 65536 checking: sysctl.kernel.sem = 250 64000 100 512 checking: sysctl.kernel.shmmax &gt;= 500000000 checking: ulimit.nproc &gt;= 131072 checking: sysctl.kernel.shmmni &gt;= 4096 checking: sysctl.net.ipv4.ip_local_port_range = 1025 65535 checking: sysctl.net.ipv4.tcp_tw_recycle = 16.4初始化 greenplumn ( Master 节点上执行 ) [greenplum@gpmaster ~]$ gpinitsystem -c gp_init_config20110413:20:29:07:gpinitsystem:gpmaster:greenplum-[INFO]:-Checking configuration parameters, please wait... 20110413:20:29:09:gpinitsystem:gpmaster:greenplum-[INFO]:-Reading Greenplum configuration file gp_init_config 20110413:20:29:09:gpinitsystem:gpmaster:greenplum-[INFO]:-Locale has not been set in gp_init_config, will set to default value 20110413:20:29:11:gpinitsystem:gpmaster:greenplum-[INFO]:-Locale set to en_US.utf8 20110413:20:29:15:gpinitsystem:gpmaster:greenplum-[INFO]:-No DATABASE_NAME set, will exit following template1 updates 20110413:20:29:16:gpinitsystem:gpmaster:greenplum-[INFO]:-MASTER_MAX_CONNECT not set, will set to default value 25 20110413:20:29:18:gpinitsystem:gpmaster:greenplum-[INFO]:-Master IP address array = 192.168.122.1 20110413:20:29:18:gpinitsystem:gpmaster:greenplum-[INFO]:-Checking configuration parameters, Completed 20110413:20:29:19:gpinitsystem:gpmaster:greenplum-[INFO]:-Commencing multi-home checks, please wait... .. 20110413:20:29:24:gpinitsystem:gpmaster:greenplum-[INFO]:-Configuring build for standard array 20110413:20:29:24:gpinitsystem:gpmaster:greenplum-[INFO]:-Commencing multi-home checks, Completed 20110413:20:29:25:gpinitsystem:gpmaster:greenplum-[INFO]:-Building primary segment instance array, please wait... .. 20110413:20:29:34:gpinitsystem:gpmaster:greenplum-[INFO]:-Building group mirror array type , please wait... .. 20110413:20:29:45:gpinitsystem:gpmaster:greenplum-[INFO]:-Checking Master host 20110413:20:29:48:gpinitsystem:gpmaster:greenplum-[INFO]:-Checking new segment hosts, please wait... .... 20110413:20:30:24:gpinitsystem:gpmaster:greenplum-[INFO]:-Checking new segment hosts, Completed 20110413:20:30:26:gpinitsystem:gpmaster:greenplum-[INFO]:-Greenplum Database Creation Parameters 20110413:20:30:26:gpinitsystem:gpmaster:greenplum-[INFO]:--------------------------------------- 20110413:20:30:26:gpinitsystem:gpmaster:greenplum-[INFO]:-Master Configuration 20110413:20:30:27:gpinitsystem:gpmaster:greenplum-[INFO]:--------------------------------------- 20110413:20:30:27:gpinitsystem:gpmaster:greenplum-[INFO]:-Master instance name = Greenplum Database 20110413:20:30:27:gpinitsystem:gpmaster:greenplum-[INFO]:-Master hostname = gpmaster 20110413:20:30:28:gpinitsystem:gpmaster:greenplum-[INFO]:-Master port = 5432 20110413:20:30:28:gpinitsystem:gpmaster:greenplum-[INFO]:-Master instance dir = /opt/gp_data/gp-1 20110413:20:30:28:gpinitsystem:gpmaster:greenplum-[INFO]:-Master LOCALE = en_US.utf8 20110413:20:30:29:gpinitsystem:gpmaster:greenplum-[INFO]:-Greenplum segment prefix = gp 20110413:20:30:29:gpinitsystem:gpmaster:greenplum-[INFO]:-Master Database = 20110413:20:30:30:gpinitsystem:gpmaster:greenplum-[INFO]:-Master connections = 25 20110413:20:30:30:gpinitsystem:gpmaster:greenplum-[INFO]:-Master buffers = 128000kB 20110413:20:30:30:gpinitsystem:gpmaster:greenplum-[INFO]:-Segment connections = 125 20110413:20:30:31:gpinitsystem:gpmaster:greenplum-[INFO]:-Segment buffers = 128000kB 20110413:20:30:31:gpinitsystem:gpmaster:greenplum-[INFO]:-Checkpoint segments = 8 20110413:20:30:31:gpinitsystem:gpmaster:greenplum-[INFO]:-Encoding = UNICODE 20110413:20:30:32:gpinitsystem:gpmaster:greenplum-[INFO]:-Postgres param file = Off 20110413:20:30:32:gpinitsystem:gpmaster:greenplum-[INFO]:-Initdb to be used = /opt/greenplum-db/./bin/initdb 20110413:20:30:33:gpinitsystem:gpmaster:greenplum-[INFO]:-GP_LIBRARY_PATH is = /opt/greenplum-db/./lib 20110413:20:30:33:gpinitsystem:gpmaster:greenplum-[INFO]:-Ulimit check = Passed 20110413:20:30:33:gpinitsystem:gpmaster:greenplum-[INFO]:-Segment build type = Parallel 20110413:20:30:34:gpinitsystem:gpmaster:greenplum-[INFO]:-Array host connect type = Single hostname per node 20110413:20:30:34:gpinitsystem:gpmaster:greenplum-[INFO]:-Master IP address [1] = 192.168.122.1 20110413:20:30:34:gpinitsystem:gpmaster:greenplum-[INFO]:-Master IP address [2] = 192.168.1.50 20110413:20:30:35:gpinitsystem:gpmaster:greenplum-[INFO]:-Master only build = Off 20110413:20:30:35:gpinitsystem:gpmaster:greenplum-[INFO]:-Standby Master = Not Configured 20110413:20:30:35:gpinitsystem:gpmaster:greenplum-[INFO]:-Primary segment # = 1 20110413:20:30:36:gpinitsystem:gpmaster:greenplum-[INFO]:-Total Database segments = 2 20110413:20:30:36:gpinitsystem:gpmaster:greenplum-[INFO]:-Trusted shell = ssh 20110413:20:30:37:gpinitsystem:gpmaster:greenplum-[INFO]:-Number segment hosts = 2 20110413:20:30:37:gpinitsystem:gpmaster:greenplum-[INFO]:-Mirror port base = 60000 20110413:20:30:37:gpinitsystem:gpmaster:greenplum-[INFO]:-Mirror segment # = 1 20110413:20:30:38:gpinitsystem:gpmaster:greenplum-[INFO]:-Mirroring config = ON 20110413:20:30:38:gpinitsystem:gpmaster:greenplum-[INFO]:-Mirroring type = Group 20110413:20:30:38:gpinitsystem:gpmaster:greenplum-[INFO]:---------------------------------------- 20110413:20:30:39:gpinitsystem:gpmaster:greenplum-[INFO]:-Greenplum Primary Segment Configuration 20110413:20:30:39:gpinitsystem:gpmaster:greenplum-[INFO]:---------------------------------------- 20110413:20:30:40:gpinitsystem:gpmaster:greenplum-[INFO]:-gpnode1 /opt/gp_data/data/gp0 50001 2 0 20110413:20:30:40:gpinitsystem:gpmaster:greenplum-[INFO]:-gpnode2 /opt/gp_data/data/gp1 50001 3 1 20110413:20:30:40:gpinitsystem:gpmaster:greenplum-[INFO]:--------------------------------------- 20110413:20:30:41:gpinitsystem:gpmaster:greenplum-[INFO]:-Greenplum Mirror Segment Configuration 20110413:20:30:41:gpinitsystem:gpmaster:greenplum-[INFO]:--------------------------------------- 20110413:20:30:41:gpinitsystem:gpmaster:greenplum-[INFO]:-gpnode2 /opt/gp_data/mdata/gp0 60001 4 0 20110413:20:30:42:gpinitsystem:gpmaster:greenplum-[INFO]:-gpnode1 /opt/gp_data/mdata/gp1 60001 5 1 Continue with Greenplum creation Yy/Nn&gt; 20110413:20:31:04:gpinitsystem:gpmaster:greenplum-[INFO]:-Building the Master instance database, please wait... 20110413:20:31:34:gpinitsystem:gpmaster:greenplum-[INFO]:-Starting the Master in admin mode 20110413:20:32:08:gpinitsystem:gpmaster:greenplum-[INFO]:-Commencing parallel build of primary segment instances 20110413:20:32:08:gpinitsystem:gpmaster:greenplum-[INFO]:-Spawning parallel processes batch [1], please wait... .. 20110413:20:32:13:gpinitsystem:gpmaster:greenplum-[INFO]:-Waiting for parallel processes batch [1], please wait... ....................................................... 20110413:20:33:20:gpinitsystem:gpmaster:greenplum-[INFO]:------------------------------------------------ 20110413:20:33:21:gpinitsystem:gpmaster:greenplum-[INFO]:-Parallel process exit status 20110413:20:33:21:gpinitsystem:gpmaster:greenplum-[INFO]:------------------------------------------------ 20110413:20:33:21:gpinitsystem:gpmaster:greenplum-[INFO]:-Total processes marked as completed = 2 20110413:20:33:22:gpinitsystem:gpmaster:greenplum-[INFO]:-Total processes marked as killed = 0 20110413:20:33:22:gpinitsystem:gpmaster:greenplum-[INFO]:-Total processes marked as failed = 0 20110413:20:33:22:gpinitsystem:gpmaster:greenplum-[INFO]:------------------------------------------------ 20110413:20:33:24:gpinitsystem:gpmaster:greenplum-[INFO]:-Commencing parallel build of mirror segment instances 20110413:20:33:25:gpinitsystem:gpmaster:greenplum-[INFO]:-Spawning parallel processes batch [1], please wait... .. 20110413:20:33:30:gpinitsystem:gpmaster:greenplum-[INFO]:-Waiting for parallel processes batch [1], please wait... .................................................... 20110413:20:34:35:gpinitsystem:gpmaster:greenplum-[INFO]:------------------------------------------------ 20110413:20:34:35:gpinitsystem:gpmaster:greenplum-[INFO]:-Parallel process exit status 20110413:20:34:35:gpinitsystem:gpmaster:greenplum-[INFO]:------------------------------------------------ 20110413:20:34:36:gpinitsystem:gpmaster:greenplum-[INFO]:-Total processes marked as completed = 2 20110413:20:34:36:gpinitsystem:gpmaster:greenplum-[INFO]:-Total processes marked as killed = 0 20110413:20:34:36:gpinitsystem:gpmaster:greenplum-[INFO]:-Total processes marked as failed = 0 20110413:20:34:37:gpinitsystem:gpmaster:greenplum-[INFO]:------------------------------------------------ 20110413:20:34:38:gpinitsystem:gpmaster:greenplum-[INFO]:-Deleting distributed backout files 20110413:20:34:38:gpinitsystem:gpmaster:greenplum-[INFO]:-Removing back out file 20110413:20:34:39:gpinitsystem:gpmaster:greenplum-[INFO]:-No errors generated from parallel processes 20110413:20:34:39:gpinitsystem:gpmaster:greenplum-[INFO]:-Restarting the Greenplum instance in production mode 20110413:20:34:40:gpstop:gpmaster:greenplum-[INFO]:-Starting gpstop with args: '-a -i -d /opt/gp_data/gp-1' 20110413:20:34:40:gpstop:gpmaster:greenplum-[INFO]:-Gathering information and validating the environment... 20110413:20:34:41:gpstop:gpmaster:greenplum-[INFO]:-Obtaining Greenplum Master catalog information 20110413:20:34:41:gpstop:gpmaster:greenplum-[INFO]:-Obtaining Segment details from master... 20110413:20:34:41:gpstop:gpmaster:greenplum-[INFO]:-Greenplum Version: 'postgres (Greenplum Database) 3.3.6.1 build 1' 20110413:20:34:41:gpstop:gpmaster:greenplum-[INFO]:-There are 0 connections to the database 20110413:20:34:41:gpstop:gpmaster:greenplum-[INFO]:-Commencing Master instance shutdown with mode='immediate' 20110413:20:34:41:gpstop:gpmaster:greenplum-[INFO]:-Master host=gpmaster 20110413:20:34:41:gpstop:gpmaster:greenplum-[INFO]:-Commencing Master instance shutdown with mode=immediate 20110413:20:34:41:gpstop:gpmaster:greenplum-[INFO]:-Master segment instance directory=/opt/gp_data/gp-1 20110413:20:34:43:gpstop:gpmaster:greenplum-[INFO]:-No standby master host configured 20110413:20:34:47:gpstop:gpmaster:greenplum-[INFO]:-Commencing parallel segment instance shutdown, please wait... .......... 20110413:20:34:57:gpstop:gpmaster:greenplum-[INFO]:------------------------------------------------- 20110413:20:34:57:gpstop:gpmaster:greenplum-[INFO]:-Parallel process exit status 20110413:20:34:57:gpstop:gpmaster:greenplum-[INFO]:------------------------------------------------- 20110413:20:34:57:gpstop:gpmaster:greenplum-[INFO]:-Total processes marked as completed = 4 20110413:20:34:57:gpstop:gpmaster:greenplum-[INFO]:-Total processes marked as failed = 0 20110413:20:34:57:gpstop:gpmaster:greenplum-[INFO]:------------------------------------------------- 20110413:20:34:57:gpstop:gpmaster:greenplum-[INFO]:-Total instances marked invalid and bypassed = 0 20110413:20:34:57:gpstop:gpmaster:greenplum-[INFO]:-Successfully shutdown 4 of 4 segment instances 20110413:20:34:57:gpstop:gpmaster:greenplum-[INFO]:-Database successfully shutdown with no errors reported 20110413:20:34:59:gpstart:gpmaster:greenplum-[INFO]:-Starting gpstart with args: '-a -d /opt/gp_data/gp-1' 20110413:20:34:59:gpstart:gpmaster:greenplum-[INFO]:-Gathering information and validating the environment... 20110413:20:35:00:gpstart:gpmaster:greenplum-[INFO]:-local Greenplum Version: 'postgres (Greenplum Database) 3.3.6.1 build 1' 20110413:20:35:00:gpstart:gpmaster:greenplum-[INFO]:-Starting Master instance in admin mode 20110413:20:35:01:gpstart:gpmaster:greenplum-[INFO]:-Obtaining Greenplum Master catalog information 20110413:20:35:01:gpstart:gpmaster:greenplum-[INFO]:-Obtaining Segment details from master... 20110413:20:35:01:gpstart:gpmaster:greenplum-[INFO]:-Master Started... 20110413:20:35:01:gpstart:gpmaster:greenplum-[INFO]:-Shutting down master 20110413:20:35:10:gpstart:gpmaster:greenplum-[INFO]:-No standby master configured. skipping... 20110413:20:35:13:gpstart:gpmaster:greenplum-[INFO]:-Commencing parallel segment instance startup, please wait... ........... 20110413:20:35:24:gpstart:gpmaster:greenplum-[INFO]:-Process results... 20110413:20:35:24:gpstart:gpmaster:greenplum-[INFO]:----------------------------------------------------- 20110413:20:35:24:gpstart:gpmaster:greenplum-[INFO]:-Total processes marked as completed = 4 20110413:20:35:24:gpstart:gpmaster:greenplum-[INFO]:-Total processes marked as failed = 0 20110413:20:35:24:gpstart:gpmaster:greenplum-[INFO]:----------------------------------------------------- 20110413:20:35:24:gpstart:gpmaster:greenplum-[INFO]:-Total instances marked invalid and bypassed = 0 20110413:20:35:24:gpstart:gpmaster:greenplum-[INFO]:-Successfully started 4 of 4 segment instances 20110413:20:35:24:gpstart:gpmaster:greenplum-[INFO]:----------------------------------------------------- 20110413:20:35:24:gpstart:gpmaster:greenplum-[INFO]:-Starting Master instance gpmaster directory /opt/gp_data/gp-1 20110413:20:35:28:gpstart:gpmaster:greenplum-[INFO]:-Command pg_ctl reports Master gpmaster instance active 20110413:20:35:28:gpstart:gpmaster:greenplum-[INFO]:-Database successfully started with no errors reported 20110413:20:35:30:gpinitsystem:gpmaster:greenplum-[INFO]:-Completed restart of Greenplum instance in production mode psql: FATAL: building Gang: Primary and mirror both unavailable for segment 0 (i 1 j 2) (cdbgang.c:1332) 20110413:20:35:31:gpinitsystem:gpmaster:greenplum-[WARN]:-Issue with update Greenplum superuser password 20110413:20:35:33:gpinitsystem:gpmaster:greenplum-[INFO]:-Scanning utility log file for any warning messages 20110413:20:35:33:gpinitsystem:gpmaster:greenplum-[WARN]:-* 20110413:20:35:33:gpinitsystem:gpmaster:greenplum-[WARN]:-Scan of log file indicates that some warnings or errors 20110413:20:35:34:gpinitsystem:gpmaster:greenplum-[WARN]:-were generated during the array creation 20110413:20:35:34:gpinitsystem:gpmaster:greenplum-[INFO]:-Please review contents of log file 20110413:20:35:35:gpinitsystem:gpmaster:greenplum-[INFO]:-/home/greenplum/gpAdminLogs/gpinitsystem_20110413.log 20110413:20:35:35:gpinitsystem:gpmaster:greenplum-[INFO]:-To determine level of criticality 20110413:20:35:35:gpinitsystem:gpmaster:greenplum-[INFO]:-These messages could be from a previous run of the utility 20110413:20:35:36:gpinitsystem:gpmaster:greenplum-[INFO]:-that was called today! 20110413:20:35:36:gpinitsystem:gpmaster:greenplum-[WARN]:-* 20110413:20:35:37:gpinitsystem:gpmaster:greenplum-[INFO]:-Greenplum Database instance successfully created 20110413:20:35:37:gpinitsystem:gpmaster:greenplum-[INFO]:------------------------------------------------------- 20110413:20:35:38:gpinitsystem:gpmaster:greenplum-[INFO]:-To complete the environment configuration, please 20110413:20:35:38:gpinitsystem:gpmaster:greenplum-[INFO]:-update greenplum .bashrc file with the following 20110413:20:35:38:gpinitsystem:gpmaster:greenplum-[INFO]:-1. Ensure that the greenplum_path.sh file is sourced 20110413:20:35:39:gpinitsystem:gpmaster:greenplum-[INFO]:-2. Add \"export MASTER_DATA_DIRECTORY=/opt/gp_data/gp-1\" 20110413:20:35:39:gpinitsystem:gpmaster:greenplum-[INFO]:- to access the Greenplum scripts for this instance: 20110413:20:35:40:gpinitsystem:gpmaster:greenplum-[INFO]:- or, use -d /opt/gp_data/gp-1 option for the Greenplum scripts 20110413:20:35:40:gpinitsystem:gpmaster:greenplum-[INFO]:- Example gpstate -d /opt/gp_data/gp-1 20110413:20:35:40:gpinitsystem:gpmaster:greenplum-[INFO]:-Script log file = /home/greenplum/gpAdminLogs/gpinitsystem_20110413.log 20110413:20:35:41:gpinitsystem:gpmaster:greenplum-[INFO]:-To remove instance, run gpdeletesystem utility 20110413:20:35:41:gpinitsystem:gpmaster:greenplum-[INFO]:-To initialize a Standby Master Segment for this Greenplum instance 20110413:20:35:41:gpinitsystem:gpmaster:greenplum-[INFO]:-Review options for gpinitstandby 20110413:20:35:42:gpinitsystem:gpmaster:greenplum-[INFO]:------------------------------------------------------- 20110413:20:35:42:gpinitsystem:gpmaster:greenplum-[INFO]:-The Master /opt/gp_data/gp-1/pg_hba.conf post gpinitsystem 20110413:20:35:43:gpinitsystem:gpmaster:greenplum-[INFO]:-has been configured to allow all hosts within this new 20110413:20:35:43:gpinitsystem:gpmaster:greenplum-[INFO]:-array to intercommunicate. Any hosts external to this 20110413:20:35:43:gpinitsystem:gpmaster:greenplum-[INFO]:-new array must be explicitly added to this file 20110413:20:35:44:gpinitsystem:gpmaster:greenplum-[INFO]:-Refer to the Greenplum Admin support guide which is 20110413:20:35:44:gpinitsystem:gpmaster:greenplum-[INFO]:-located in the /opt/greenplum-db/./docs directory 20110413:20:35:44:gpinitsystem:gpmaster:greenplum-[INFO]:------------------------------------------------------- 根据输出文件信息，看到”-[INFO]:-Greenplum Database instance successfully created” 说明 Greenplum 安装成功。 6.4 修改 .bash_profile,增加一行,配置 Maser 节点环境变量12export MASTER_DATA_DIRECTORY=/opt/gp_data/gp-1 source .bash_profile 7 测试7.1连接测试123456789[greenplum@gpmaster ~]$ psql template1 template1=# l List of databases Name | Owner | Encoding -----------+-----------+---------- postgres | greenplum | UTF8 template0 | greenplum | UTF8 template1 | greenplum | UTF8 (3 rows) 7.2创建用户123template1=# CREATE ROLE warehouse LOGIN ENCRYPTED PASSWORD 'warehouse' template1-# superuser noinherit nocreatedb nocreaterole ; CREATE ROLE 7.3建库12345template1=# CREATE DATABASE warehouse template1-# WITH OWNER = warehouse template1-# TEMPLATE = template0 template1-# ENCODING = 'UTF8' ; CREATE DATABASE 8 常见问题8.1 Master 连接不上，报以下 Error12[greenplum@gpmaster ~]$ psql template1 psql: FATAL: building Gang: Primary and mirror both unavailable for segment 0 (i 1 j 2) (cdbgang.c:1332) 同时数据库日志信息 ( csv log )1234567892011-04-13 20:48:42.195865 CST,,,p13846,th1756048,,,,0,con3,,seg-1,,,,,\"LOG\",\"00000\",\"FTS: test connection failed, dbid 2 dbname postgres error message 'could not connect to server: No route to host Is the server running on host \"\"gpnode1\"\" and accepting TCP/IP connections on port 50001? ':136\",,,,,,,0,,,, 2011-04-13 20:48:42.196164 CST,,,p13846,th1756048,,,,0,con3,,seg-1,,,,,\"LOG\",\"00000\",\"Found a fault with a segment 0 segment-dbid 2\",,,,,,,0,,,, 2011-04-13 20:48:42.196727 CST,,,p13846,th14052240,,,,0,con3,,seg-1,,,,,\"LOG\",\"00000\",\"FTS: test connection failed, dbid 4 dbname postgres error message 'could not connect to server: No route to host Is the server running on host \"\"gpnode2\"\" and accepting TCP/IP connections on port 60001? ':136\",,,,,,,0,,,, 解决方法：从数据库日志可以看出Master无法连接 gpnode1 的 50001端口和 gpnode2 的 60001 端口，原因是防火墙问题端口权限没打开，只要开通数据库端口权限即可；","categories":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://postgres.fun/categories/GreenPlum/"}],"tags":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://postgres.fun/tags/GreenPlum/"}]},{"title":"GreenPlum 原理篇之一：体系结构简介","slug":"20110414163131","date":"2011-04-14T08:31:31.000Z","updated":"2018-09-04T01:33:49.318Z","comments":true,"path":"20110414163131.html","link":"","permalink":"https://postgres.fun/20110414163131.html","excerpt":"","text":"这段时间开始接触 Greenplum, 以前对 Greenplum 闻所未闻，首先很庆幸自己有这个机会了解，并认识它, 这篇文章主要对 Greenplum 做下简单的介绍。 1 Greenplum 简介GreenPlum数据库是基于 PostgreSQL 基础上开发，具有巨大的并行处理能力的数据仓库，MPP( massively parallel processing )是 Greenplum的主要特性， MPP是指服务器上拥有两个或者以上的处理节点，并且多个处理节点可以并行,协同的工作来完成一个计算， 这些处理节点拥有独自的内存,操作系统和硬盘, 处理节点可以理解成为一台物理主机。Greenplum 会分发 TB 及以上的数据到所有的子节点，并且当在Greenplum上执行查询时，所有的子节点能够利用各自的资源来并发地执行这个查询。 Greenplum 实际上是由一组 PostgreSQL 数据库组合而成的强大数据仓库， Greenplum基于PostgreSQL 8.2.14 开发, 并且在多数据情况下和 PostgreSQL 非常相似，以至于用户可以像是在使 用PostgreSQL 一样来使用Greenplum。 2 Greenplum 体系结构GreenPlum可以将数据按一定算法全部分发到所有 segment Host上，因而能够处理大量的数据请求。Greenplum的系统结构实际上是多台 PostgreSQL 数据库服务器组成的矩阵，Greenplum分两部分，Master 主机和 Segments 主机，master主机是Greenplum的入口，客户端，终端用户连接Master来执行 sql。当用户在 Master 节点上执行查询SQL时，MASTER会将 SQL,以及SQL PLAN分发到所有 segments 节点， segments 处理好后，将数据发回 Master 节点, 如下图。 2.1 Greenplum 的 Master 节点Master 节点是访问 Greenplum的入口, 用于处理客户端的连接，和 SQL命令请求，用户可以使用psql,或者应用程序接口来连接 Master, 例如 JDBC,ODBC接口。Master 节点仅存储 Greenplum的系统数据，而不存储用户数据，相反，用户数据存在所有的 Segments 节点上，Master 负责以下工作：处理客户端的连接请求，处理客户端发出的SQL命令，将SQL命令分发到所有Segments 节点，协调所有节点处理的结果数据，并将这些数据展现给终端用户。 2.2 Greenplum 的 Segment 节点在Greenplum中, Segments存诸用户的数据，并且负责处理Master 分发出的SQL请求任务，用户定义的表和索引按一定算法分发到所有的 segments节点，每个节点含有独立的一部分数据，用户不 需要直接和 segments 交互，相反，只需通过和 Master 节点交互。 一般地，建议一个CPU上跑一个活跃的 segments。 3 Greenplum的镜像备份功能3.1 关于 Segments节点的镜像搭建Greenplum时，为了保证系统的高可用性，可以给每个 Segment 搭建 Mirror Segment,当Primary Segment 不可用时， Mirror Segment可以failoer 接替工作。Mirror Segment(镜像节 点）通常和Primary Segment 分布在不同的主机上，详见下图。 3.2 关于 Master 节点的镜像搭建Greenplum时，也可以配置一个 Master 的镜像，称作 Master Standby, Standby Master通过WAL日志复制来同步数据，如果 Master 不可用，日志复制进程则会中断，并且 Standby Master 可以接替工作，同时，最近一次复制过来的WAL日志状态会变化来标记最近一次事务提交的时刻。 由于 Master 不存储用户数据，所以Master Standby 只需要同步 Master 系统表数据，这些数据不会非常频繁地改变，当 Master 节点数据变化时，这些变化会自动地应用到 Master Standby节点上。 从而使得 Master Standby 数据 始终和 Master 保持一致。 4总结在了解了以上 Greenplum 基础知识之后，应该对 Greenplum 有了大概的了解，后续还会有相关的文章介绍 Greenplum。","categories":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://postgres.fun/categories/GreenPlum/"}],"tags":[{"name":"GreenPlum","slug":"GreenPlum","permalink":"https://postgres.fun/tags/GreenPlum/"}]},{"title":"Linux 系统CPU、内存、负载、IO监控脚本","slug":"20110413134827","date":"2011-04-13T05:48:27.000Z","updated":"2018-09-04T01:33:49.255Z","comments":true,"path":"20110413134827.html","link":"","permalink":"https://postgres.fun/20110413134827.html","excerpt":"","text":"今天提供一个Linux 系统监控脚本，监控的指标有CPU，内存使用率，负载，IO等, 并且将主机的这些指标插入到数据库里，便于历史分析。 创建监控表1234567891011创建一张监控表，用于记录各项批指标信息 CREATE TABLE tbl_monitor ( id serial, hostname character varying(64),--主机名 add_time timestamp without time zone,--记录插入时间 cpu_useratio numeric(5,2),--cpu使用率 mem_useratio numeric(5,2), --内存使用率 load numeric(5,2), --负载(load)值 io_wa smallint, --系统IO等侍情况 constraint pk_tbl_monitor PRIMARY KEY (ID) ); SHELL 脚本CPU、内存、负载、IO指标监控脚本如下：1234567891011121314151617181920212223242526272829303132333435363738#!/bin/bash##### load system environment export PGHOME=/opt/pgsql export LD_LIBRARY_PATH=$PGHOME/lib:/lib64:/usr/lib64:/usr/local/lib64:/lib:/usr/lib:/usr/local/lib:$PGSQL_HOME/lib:$PROJ_HOME/lib:$GEOS_HOME/lib:$POSTGIS_HOME/lib export PATH=$PGHOME/bin:$PATH:.##### get cpu mem current information file_dir=\"/home/postgres/script/tf/monitor\" #/bin/bash##### get cpu mem current information file_dir=\"/home/postgres/script/tf/monitor\" vmstat 1 3 &gt; $&#123;file_dir&#125;/cpu_file.txt free -m &gt; $&#123;file_dir&#125;/mem_file.txt cpu_file=\"$&#123;file_dir&#125;/cpu_file.txt\" mem_file=\"$&#123;file_dir&#125;/mem_file.txt\"### declare variable v_hostname=\"`hostname`\" v_hostip=\"xxx.xxx.xxx.xxx\" v_time=\"`date +%F %T`\" v_cpuidle=`cat $&#123;cpu_file&#125; | sed -n '$'p | awk '&#123;print $15&#125;'` v_cpuuse=`echo \"scale=2; 100.00-$&#123;v_cpuidle&#125;\" | bc` v_memtotal=`cat $&#123;mem_file&#125; | sed -n '2'p | awk '&#123;print $2&#125;'` v_memused=`cat $&#123;mem_file&#125; | sed -n '2'p | awk '&#123;print $3&#125;'` v_memratio=`echo \"scale=2; $&#123;v_memused&#125;*100/$&#123;v_memtotal&#125;\" | bc` v_load=`uptime | awk '&#123;print $10&#125;'| tr -d \",\"` v_io=`cat $&#123;cpu_file&#125; | sed -n '$'p | awk '&#123;print $(NF-1) &#125;'` v_email=\"[francs.tan@sky-mobi.com](mailto:francs.tan@sky-mobi.com) [Francs3@163.com](mailto:Francs3@163.com)\"## cpu alarm if [ $&#123;v_cpuidle&#125; -lt 95 ]; then echo \"`date +%F %T` $&#123;v_hostip&#125;: CPU usage alarm ,please check ! \" | mutt -s \"CPU usage $&#123;v_cpuuse&#125;% , $&#123;v_hostname&#125; \" $&#123;v_email&#125; fi## insert data to monitor database psql -h 127.0.0.1 -p 1921 -U skytf -d skytf -c \" insert into skytf.tbl_monitor(hostname,add_time,cpu_useratio, mem_useratio , load ,io_wa ) values ( '$&#123;v_hostname&#125;', '$&#123;v_time&#125;', $&#123;v_cpuuse&#125;, $&#123;v_memratio&#125;, $&#123;v_load&#125;, $&#123;v_io&#125; );\"## remove temp file rm -f $&#123;cpu_file&#125; rm -f $&#123;mem_file&#125; 备注 这个脚本取当前LINUX操作系统的各项指标并录入到数据库里； 1. CPU 指标来源于 &quot;vmstat&quot; 输出的 id 字段； 2. memory 指标来源&quot; free -m&quot; 输出; 3. load 指标来源于 &quot; uptime &quot; 输出; 4. io 指标来源于 &quot;vmstat&quot; 输出的 wa 字段。 设置 Crontab 任务计划12###### Monitor cpu, memory, io, load */5 * * * * /home/postgres/script/tf/monitor/monitor.sh &gt;&gt; /home/postgres/script/tf/monitor/monitor.log 2&gt;&amp;1 取部分数据123456789skytf=&gt; select * From tbl_monitor limit 5; id | hostname | add_time | cpu_useratio | mem_useratio | load | io_wa ----+--------------+---------------------+--------------+--------------+------+------- 37 | 172_16_3_216 | 2011-04-13 09:55:03 | 14.00 | 99.42 | 1.47 | 7 38 | 172_16_3_216 | 2011-04-13 09:57:03 | 0.00 | 83.30 | 0.86 | 0 39 | 172_16_3_216 | 2011-04-13 09:59:03 | 0.00 | 83.26 | 0.11 | 0 40 | 172_16_3_216 | 2011-04-13 10:05:03 | 13.00 | 84.89 | 0.97 | 0 41 | 172_16_3_216 | 2011-04-13 10:10:03 | 13.00 | 85.05 | 1.02 | 0 (5 rows) 发现数据已经进去了，将OS层面数据记录到数据库里，从而当系统出现问题时，可以做为一个监控凭证。 总结这脚本尚处于测试阶段，后续如做生产用途还需完善。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"A very strange SQL of PostgreSQL","slug":"20110328182354","date":"2011-03-28T10:23:54.000Z","updated":"2018-09-04T01:33:49.193Z","comments":true,"path":"20110328182354.html","link":"","permalink":"https://postgres.fun/20110328182354.html","excerpt":"","text":"今天这篇贴子讲述的问题我以前也遇到过，只因发生在一测试环境下，所以没有深究，今天在一生产环境下也遇到了这个问题，所以应该重视了，这个问题困扰了我一阵子，事情是这样的，在生产库上有一个 SQL 非常奇怪，从SQL语句来看，很像是PostgreSQL SERVER 内部执行的，可事实却不是，下面看下今天的分析步骤。 查询数据库会话连接到数据库里查询，如下：12345678910postgres=# select procpid,datname,client_addr,client_port,current_query from pg_stat_activity where current_query !='&lt;IDLE&gt;'; 25858 | uims | XXX.XXX.XXX.XXX | 57791 | SELECT NULL AS TABLE_CAT, n.nspname AS TABLE_SCHEM, c.relname AS TABLE_NAME, CASE n.nspname ~ '^pg_' OR n.nspname = 'information_schema' WHEN true THEN CASE WHEN n.nspname = 'pg_catalog' OR n.nspname = 'info rmation_schema' THEN CASE c.relkind WHEN 'r' THEN 'SYSTEM TABLE' WHEN 'v' THEN 'SYSTEM VIEW' WHEN 'i' THEN 'SYSTEM INDEX' EL SE NULL END WHEN n.nspname = 'pg_toast' THEN CASE c.relkind WHEN 'r' THEN 'SYSTEM TOAST TABLE' WHEN 'i' THEN 'SYSTEM TOAST IN DEX' ELSE NULL END ELSE CASE c.relkind WHEN 'r' THEN 'TEMPORARY TABLE' WHEN 'i' THEN 'TEMPORARY INDEX' ELSE NULL END E ND WHEN false THEN CASE c.relkind WHEN 'r' THEN 'TABLE' WHEN 'i' THEN 'INDEX' WHEN 'S' THEN 'SEQUENCE' WHEN 'v' THEN 'VIEW' EL SE NULL END ELSE NULL END AS TABLE_TYPE, d.description AS REMARKS FROM pg_catalog.pg_namespace n, pg_catalog.pg_class c LEFT J OIN pg_catalog.pg_description d ON (c.oid = d.objoid AND d.objsubid = 0) LEFT JOIN pg_catalog.pg_class dc ON (d.classoid=dc.oid AND dc.relname='pg_class') 这个SQL看上去像 PostgreSQL 发出的系统维护SQL，但找不出是什么进程，后来根据 client_addr 咨询了下应用人员，回复说这是 APPLICATION 机器, 难道这个SQL是应用程序发出？ 格式化SQL格式化的SQL，如下：123456789101112131415161718192021222324252627282930313233343536373839404142SELECT NULL AS TABLE_CAT, n.nspname AS TABLE_SCHEM, c.relname AS TABLE_NAME, CASE n.nspname ~ '^pg_' OR n.nspname = 'information_schema' WHEN true THEN CASE WHEN n.nspname = 'pg_catalog' OR n.nspname = 'information_schema' THEN CASE c.relkind WHEN 'r' THEN 'SYSTEM TABLE' WHEN 'v' THEN 'SYSTEM VIEW' WHEN 'i' THEN 'SYSTEM INDEX' ELSE NULL END WHEN n.nspname = 'pg_toast' THEN CASE c.relkind WHEN 'r' THEN 'SYSTEM TOAST TABLE' WHEN 'i' THEN 'SYSTEM TOAST INDEX' ELSE NULL END ELSE CASE c.relkind WHEN 'r' THEN 'TEMPORARY TABLE' WHEN 'i' THEN 'TEMPORARY INDEX' ELSE NULL END END WHEN false THEN CASE c.relkind WHEN 'r' THEN 'TABLE' WHEN 'i' THEN 'INDEX' WHEN 'S' THEN 'SEQUENCE' WHEN 'v' THEN 'VIEW' ELSE NULL END ELSE NULL END AS TABLE_TYPE, d.description AS REMARKS FROM pg_catalog.pg_namespace n, pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_description d ON (c.oid = d.objoid AND d.objsubid = 0) LEFT JOIN pg_catalog.pg_class dc ON (d.classoid = dc.oid AND dc.relname='pg_class'); 执行此 SQL执行以上SQL,并取一条数据，如下：1234table_cat | table_schem | table_name | table_type | remarks -----------+-------------+-----------------------------------+--------------------+--------- | pg_toast | pg_foreign_data_wrapper_oid_index | SYSTEM TOAST INDEX | (1 row) 这个SQL执行很快，毫秒级，从查询结果来看，应该是查 TABLE 列表。 国外PostgreSQL论坛发贴求助实在找不出什么原因，后来在国外 PostgreSQL 社区论坛发贴，从上面的解释终于得到了答案。 回复如下: 后来在一国外论坛上发贴，上面回复说这个SQL是 PostgreSQL JDBC driver 发出的一个getTables()函数，原文如下 :It is part of the getTables() implementation in thepostgresql JDBC driver. 后来也咨询了下我们的开发人员，也解释说这是 PostgreSQL JDBC driver 发出的函数，用来获取TABLE列表，这个函数在连接进程空闲时会执行，也可以通过配置JDBC配置，禁止执行这函数。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Define multiple columns constraints","slug":"20110328173952","date":"2011-03-28T09:39:52.000Z","updated":"2018-09-04T01:33:49.146Z","comments":true,"path":"20110328173952.html","link":"","permalink":"https://postgres.fun/20110328173952.html","excerpt":"","text":"今天看到论坛上有篇贴子有点意思，这里记录下，问题是这样的，创建一张新表，并且需要对表上两个字段建立约束，约束的定义：columnA 或者 columnB 不能为空。以前在学习 oracle 时只能对某一张字段定义约束，今天才知道 PostgreSQL提供更强大的功能，可以对表上多个字段定义表级的约束，下面是测试步骤。 环境准备创建表12345skytf=&gt; create table test_28 ( skytf(&gt; id integer, skytf(&gt; name varchar(32), skytf(&gt; address varchar(128)); CREATE TABLE 查看表结构1234567skytf=&gt; \\d test_28 Table \"skytf.test_28\" Column | Type | Modifiers ---------+------------------------+----------- id | integer | name | character varying(32) | address | character varying(128) | 现在需要建立约束，约束定义：字段name或者 address 不能为空, 也就是这两个字段必须一个为非空。 创建约束1skytf=&gt; alter table test_28 add constraint con_test_28 check ( name is not null or address is not null);ALTER TABLE 验证约束123456789skytf=&gt; \\d test_28 Table \"skytf.test_28\" Column | Type | Modifiers ---------+------------------------+----------- id | integer | name | character varying(32) | address | character varying(128) | Check constraints: \"con_test_28\" CHECK (name IS NOT NULL OR address IS NOT NULL) 插入数据进行测试123456skytf=&gt; insert into test_28(id,name) values (1,'francs'); INSERT 0 1skytf=&gt; insert into test_28(id,address) values (1,'HZ'); INSERT 0 1skytf=&gt; insert into test_28(id,name,address) values (1,null,null); ERROR: new row for relation \"test_28\" violates check constraint \"con_test_28\" 备注：从上面测试步骤看出，约束正确,PostgreSQL真是功能强大啊。 文档解释 Column Check Constraints The SQL standard says that CHECK column constraints can only refer to the column they apply to; only CHECK table constraints can refer to multiple columns. PostgreSQL does not enforce this restriction; it treats column and table check constraints alike. Two ways to define constraints There are two ways to define constraints: table constraints and column constraints. A column constraint is defined as part of a column definition. A table constraint definition is not tied to a particular column, and it can encompass more than one column. Every column constraint can also be written as a table constraint; a column constraint is only a notational convenience for use when the constraint only affects one column.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"正则表达式之一: 用于查询过滤","slug":"20110322134848","date":"2011-03-22T05:48:48.000Z","updated":"2018-09-04T01:33:49.083Z","comments":true,"path":"20110322134848.html","link":"","permalink":"https://postgres.fun/20110322134848.html","excerpt":"","text":"今天有同事咨询有关数据检索方面的问题，是这样的，需要检索出一张表的字段值的内容全部是数字并且长度为11的记录， 开始想了想，长度为11这很好控制，用个 char_length 函数就行，但是要判断每一位都为数字，就不好判断了。好在PG里提供正则表达式，可以非常灵活地解决这个问题，以下为详细步骤。 测试场景一张测试表，里面有少量数据12345678910skytf=&gt; select * From test_27 order by id; id | name ----+------------- 1 | 123 2 | 123A 3 | 12AAA 4 | 123111111 5 | 1234 6 | 12311111111 (6 rows) 如何过滤出 name 字段长度为11，且 name 字段内容全为数字的记录呢？ 看下面这个查询，查询 name 字段长度为11位，并且内容全为数字的记录。12345skytf=&gt; select * From test_27 where name ~ '^[0-9]&#123;11&#125;'; id | name ----+------------- 6 | 12311111111 (1 row) 备注：上面检索条件中的大括号，表示检索范围。 正则表达式的基础知识正则表达式匹配操作符 正则表达式原子 正则表达式量词 正则表达式约束 上面只列出正则表达式的一些基础知识，正则表达式的检索能力相当强大，特别是对于类型为 Text 类型，并且存储的是XML数据时，正则表达式的优势将更能发挥作用。关于更详细的信息可以参考文档http://www.postgresql.org/docs/9.0/static/functions-matching.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL: 恢复删除的列","slug":"20110321175851","date":"2011-03-21T09:58:51.000Z","updated":"2018-09-04T01:33:49.036Z","comments":true,"path":"20110321175851.html","link":"","permalink":"https://postgres.fun/20110321175851.html","excerpt":"","text":"今天看了德哥的一篇日志，讲的是表上被 drop 的列还能恢复，很受启发，原文链接 http://blog.163.com/digoal@126/blog/static/163877040201112251058216/ ，根据德哥的BLOG，下面也来学习下。 PostgreSQL 在执行 Alter table table_name drop column 命令后，并没有在物理上删除这个列，而只是改下这个列的标志，可以通过修改 pg_attribute 属性进行恢复，下面是详细步骤： 先看 pg_attribute 几个关键的列 Name Type References Description attrelid oid pg_class.oid The table this column belongs to attname name The column name atttypid oid pg_type.oid The data type of this column attnum int2 The number of the column. Ordinary columns are numbered from 1 up. System columns, such as oid, have (arbitrary) negative numbers attisdropped bool This column has been dropped and is no longer valid. A dropped column is still physically present in the table, but is ignored by the parser and so cannot be accessed via SQL 测试场景一: 删除列后恢复创建测试表并插入数据1234skytf=&gt; create table test_26 (id integer,name varchar(32),machine varchar(32)); CREATE TABLEskytf=&gt; insert into test_26 select generate_series(1,10000),'francs','sky'; INSERT 0 10000 查看表结构1234567skytf=&gt; \\d test_26 Table \"skytf.test_26\" Column | Type | Modifiers ---------+-----------------------+----------- id | integer | name | character varying(32) | machine | character varying(32) | 查看一条测试数据12345skytf=&gt; select * from test_26 limit 1; id | name | machine ----+--------+--------- 1 | francs | sky (1 row) 查看表信息1234567891011skytf=&gt; select oid,relname from pg_class where relname='test_26'; oid | relname ----------+--------- 14280890 | test_26 (1 row)skytf=&gt; select attrelid,attname,atttypid,attisdropped ,attnum from pg_attribute where attrelid=14280890 and attnum=2; attrelid | attname | atttypid | attisdropped | attnum ----------+---------+----------+--------------+-------- 14280890 | name | 1043 | f | 2 (1 row) 删除列12skytf=&gt; alter table test_26 drop column name; ALTER TABLE 再次查看 pg_attribute 列信息12345skytf=&gt; select attrelid,attname,atttypid,attisdropped ,attnum from pg_attribute where attrelid=14280890 and attnum=2; attrelid | attname | atttypid | attisdropped | attnum ----------+------------------------------+----------+--------------+-------- 14280890 | ........pg.dropped.2........ | 0 | t | 2 (1 row) 查看表结构123456skytf=&gt; \\d test_26 Table \"skytf.test_26\" Column | Type | Modifiers ---------+-----------------------+----------- id | integer | machine | character varying(32) | 恢复删除的列1234567891011121314151617181920212223skytf=&gt; \\c skytf postgres You are now connected to database \"skytf\" as user \"postgres\". skytf=# update pg_attribute set attname='name' ,atttypid=1043,attisdropped='f' where attrelid=14280890 and attnum=2; UPDATE 1 skytf=# \\d skytf.test_26; Table \"skytf.test_26\" Column | Type | Modifiers ---------+-----------------------+----------- id | integer | name | character varying(32) | machine | character varying(32) |skytf=# select * From skytf.test_26 limit 5; id | name | machine ----+--------+--------- 1 | francs | sky 2 | francs | sky 3 | francs | sky 4 | francs | sky 5 | francs | sky (5 rows) 备注：通过修改系统表 pg_attribute 列 的 attname, atttypid, attisdropped 值, 列上数据可以恢复。 测试场景二: 删除列之后插入数据查看表信息12345678910111213141516171819skytf=&gt; select max(id) from test_26; max ------- 10000 (1 row)skytf=&gt; \\d test_26; Table \"skytf.test_26\" Column | Type | Modifiers ---------+-----------------------+----------- id | integer | name | character varying(32) | machine | character varying(32) |skytf=&gt; select * from test_26 limit 1; id | name | machine ----+--------+--------- 1 | francs | sky (1 row) 删除列12skytf=&gt; alter table test_26 drop column name; ALTER TABLE 查看数据12345skytf=&gt; select * from test_26 limit 1; id | machine ----+--------- 1 | sky (1 row) 插入一条数据12skytf=&gt; insert into test_26(id,machine) values (10001,'sky10001'); INSERT 0 1 恢复删除的列1234567891011121314skytf=&gt; \\c skytf postgres You are now connected to database \"skytf\" as user \"postgres\".skytf=# update pg_attribute set attname='name' ,atttypid=1043,attisdropped='f' where attrelid=14280890 and attnum=2; UPDATE 1skytf=# \\c skytf skytf You are now connected to database \"skytf\" as user \"skytf\".skytf=&gt; select * from test_26 where id=10001; id | name | machine -------+------+---------- 10001 | | sky10001 (1 row) 备注：对于删除的列恢复后，后面接着的 insert 记录的该列为空。测试场景三，删除列后，后面接着有数据 update 的场景 测试场景三: 删除列之后更新数据查看表信息12345678910111213skytf=&gt; \\d test_26 Table \"skytf.test_26\" Column | Type | Modifiers ---------+-----------------------+----------- id | integer | name | character varying(32) | machine | character varying(32) |skytf=&gt; select * From test_26 where id=1; id | name | machine ----+--------+--------- 1 | francs | sky (1 row) 删除列12skytf=&gt; alter table test_26 drop column name; ALTER TABLE 修改一条记录12345678skytf=&gt; update test_26 set machine ='sky_001' where id=1; UPDATE 1skytf=&gt; select * from test_26 where id=1; id | machine ----+--------- 1 | sky_001 (1 row) 恢复删除的列1234567891011skytf=&gt; \\c skytf postgres You are now connected to database \"skytf\" as user \"postgres\".skytf=# update pg_attribute set attname='name' ,atttypid=1043,attisdropped='f' where attrelid=14280890 and attnum=2; UPDATE 1skytf=# select * from skytf.test_26 where id=1; id | name | machine ----+------+--------- 1 | | sky_001 (1 row) 备注: 在删除列后，对于后面有 update 操作的行，则在列恢复后，被update的行的此列数据丢失。 测试场景四: 删除列之后 VACUUM FULL 表查看表信息1234567skytf=&gt; \\d test_26 Table \"skytf.test_26\" Column | Type | Modifiers ---------+-----------------------+----------- id | integer | name | character varying(32) | machine | character varying(32) | 查看表大小12345skytf=&gt; select pg_size_pretty(pg_relation_size('test_26')); pg_size_pretty ---------------- 440 kB (1 row) 删除列12skytf=&gt; alter table test_26 drop column name; ALTER TABLE 再次查看表大小，表大小没变化123456789skytf=&gt; select pg_size_pretty(pg_relation_size('test_26')); pg_size_pretty ---------------- 440 kB (1 row)vacuum full skytf=&gt; vacuum full verbose test_26; INFO: vacuuming \"skytf.test_26\" VACUUM 再次查看表大小，发现变小了12345skytf=&gt; select pg_size_pretty(pg_relation_size('test_26')); pg_size_pretty ---------------- 360 kB (1 row) 列恢复1234567891011121314151617181920skytf=&gt; \\c skytf postgres You are now connected to database \"skytf\" as user \"postgres\".skytf=# select attrelid,attname,atttypid,attisdropped ,attnum from pg_attribute where attrelid=14280890 and attnum=2; attrelid | attname | atttypid | attisdropped | attnum ----------+------------------------------+----------+--------------+-------- 14280890 | ........pg.dropped.2........ | 0 | t | 2 (1 row)skytf=# update pg_attribute set attname='name' ,atttypid=1043,attisdropped='f' where attrelid=14280890 and attnum=2; UPDATE 1skytf=# select attrelid,attname,atttypid,attisdropped ,attnum from pg_attribute where attrelid=14280890 and attnum=2; attrelid | attname | atttypid | attisdropped | attnum ----------+---------+----------+--------------+-------- 14280890 | name | 1043 | f | 2 (1 row)skytf=# \\c skytf skytf You are now connected to database \"skytf\" as user \"skytf\". 再次查看数据12345678910111213141516skytf=&gt; \\d test_26; Table \"skytf.test_26\" Column | Type | Modifiers ---------+-----------------------+----------- id | integer | name | character varying(32) | machine | character varying(32) |skytf=&gt; select * From test_26 limit 5; id | name | machine ----+------+--------- 2 | | sky 3 | | sky 4 | | sky 5 | | sky 6 | | sky (5 rows) 备注，在列被删除后，如果后面执行了 vacuum full 操作，被删除的列名能恢复，但列的数据丢失。 测试场景五: 删除列之后 VACUUM 表环境准备123456789101112131415161718skytf=&gt; update test_26 set name='francs'; UPDATE 10001skytf=&gt; select * From test_26 limit 5; id | name | machine ----+--------+--------- 2 | francs | sky 3 | francs | sky 4 | francs | sky 5 | francs | sky 6 | francs | sky (5 rows)查看表大小 skytf=&gt; select pg_size_pretty(pg_relation_size('test_26')); pg_size_pretty ---------------- 792 kB (1 row) 删除列12345678910111213141516171819skytf=&gt; alter table test_26 drop column name; ALTER TABLEskytf=&gt; select pg_size_pretty(pg_relation_size('test_26')); pg_size_pretty ---------------- 792 kB (1 row)VACUUM skytf=&gt; vacuum verbose test_26; INFO: vacuuming \"skytf.test_26\" INFO: \"test_26\": removed 0 row versions in 45 pages INFO: \"test_26\": found 0 removable, 10001 nonremovable row versions in 99 out of 99 pages DETAIL: 0 dead row versions cannot be removed yet. There were 0 unused item pointers. 0 pages are entirely empty. CPU 0.00s/0.00u sec elapsed 0.00 sec. VACUUM 再次查看表大小12345skytf=&gt; select pg_size_pretty(pg_relation_size('test_26')); pg_size_pretty ---------------- 792 kB (1 row) 恢复列12345skytf=&gt; \\c skytf postgres You are now connected to database \"skytf\" as user \"postgres\".skytf=# update pg_attribute set attname='name' ,atttypid=1043,attisdropped='f' where attrelid=14280890 and attnum=2; UPDATE 1 查看表数据123456skytf=# select * from skytf.test_26 limit 2; id | name | machine ----+--------+--------- 2 | francs | sky 3 | francs | sky (2 rows) 备注：删除列后，如果些表被 vacuum, 被 drop 的列依然能完全恢复。 总结 在删除列后，可以通过修改系统表pg_attribute 的 attname, atttypid, attisdropped值, 对删除的列进行恢复。 在删除列后，对于后面有 insert 操作的行，则在列恢复后，后面接着的 insert 记录的该列为空。 在删除列后，对于后面有 update 操作的行，则在列恢复后，后面接着的 update 的行的此列的数据为空。 在删除列后，如果后面执行了 vacuum full 操作，被删除的列名能恢复，但数据丢失。 在删除列后，如果此表被 vacuum, 被 drop 的列依然能完全恢复。","categories":[{"name":"PG备份与恢复","slug":"PG备份与恢复","permalink":"https://postgres.fun/categories/PG备份与恢复/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"Using pg_buffercache monitor the situation of Shared_buffers","slug":"20110318180817","date":"2011-03-18T10:08:17.000Z","updated":"2018-09-04T01:33:48.974Z","comments":true,"path":"20110318180817.html","link":"","permalink":"https://postgres.fun/20110318180817.html","excerpt":"","text":"PostgreSQL 不像Oracle 那样提供强大的共享内存监控view, 如果需要监控 PostgreSQL 的 shared_buffer 情况，需要单独安装 pg_buffercache, 以下是详细过程。 安装 pg_buffercache 扩展进入源码目录1cd /opt/soft_bak/postgresql-9.0.1/contrib/pg_buffercache 查看脚本 pg_buffercache.sql 内容123456789101112131415161718[postgres@pg_buffercache]$ cat pg_buffercache.sql /* $PostgreSQL: pgsql/contrib/pg_buffercache/pg_buffercache.sql.in,v 1.8 2008/08/14 12:56:41 heikki Exp $ */-- Adjust this setting to control where the objects get created. SET search_path = public;-- Register the function. CREATE OR REPLACE FUNCTION pg_buffercache_pages() RETURNS SETOF RECORD AS '$libdir/pg_buffercache', 'pg_buffercache_pages' LANGUAGE C;-- Create a view for convenient access. CREATE VIEW pg_buffercache AS SELECT P.* FROM pg_buffercache_pages() AS P (bufferid integer, relfilenode oid, reltablespace oid, reldatabase oid, relforknumber int2, relblocknumber int8, isdirty bool, usagecount int2); -- Don't want these to be available at public. REVOKE ALL ON FUNCTION pg_buffercache_pages() FROM PUBLIC; REVOKE ALL ON pg_buffercache FROM PUBLIC; 备注：根据脚本内容，只是创建了一个 view 安装 pg_buffercache123456[postgres@pg_buffercache]$ psql -h 127.0.0.1 -d skytf -f pg_buffercache.sql SET CREATE FUNCTION CREATE VIEW REVOKE REVOKE 由于 pg_buffercache 是系统VIEW，建议以 postgres 创建用户创建。 查看 pg_buffercache 结构，如下：123456789101112skytf=&gt; \\d pg_buffercache View \"public.pg_buffercache\" Column | Type | Modifiers ----------------+----------+----------- bufferid | integer | relfilenode | oid | reltablespace | oid | reldatabase | oid | relforknumber | smallint | relblocknumber | bigint | isdirty | boolean | usagecount | smallint | pg_buffercache 字段解释备注：主要字段 relfilenode,isdirty,usagecount, 如下： relfilenode： 是指表的文件id,与 pg_class.relfilenode 关联; isdirty: 标记 shared_buffers 里的块是否为脏，如果被修改的块还未被刷新到硬盘里，则标记为 t, 否则，标记为 f; usagecount: 指 shared_buffers 里的块被使用的次数。 查看部分数据1234567891011121314skytf=# select * from pg_buffercache limit 10; bufferid | relfilenode | reltablespace | reldatabase | relforknumber | relblocknumber | isdirty | usagecount ----------+-------------+---------------+-------------+---------------+----------------+---------+------------ 1 | 14208525 | 1663 | 14205898 | 0 | 78465 | f | 0 2 | 11867 | 1664 | 0 | 0 | 0 | f | 5 3 | 14208525 | 1663 | 14205898 | 0 | 78466 | f | 0 4 | 14208525 | 1663 | 14205898 | 0 | 78467 | f | 0 5 | 14208525 | 1663 | 14205898 | 0 | 78468 | f | 0 6 | 14208525 | 1663 | 14205898 | 0 | 78469 | f | 0 7 | 14208525 | 1663 | 14205898 | 0 | 78470 | f | 0 8 | 14208525 | 1663 | 14205898 | 0 | 78471 | f | 0 9 | 14208525 | 1663 | 14205898 | 0 | 78472 | f | 0 10 | 14208525 | 1663 | 14205898 | 0 | 78473 | f | 0 (10 rows) pg_buffercache 验证创建测试表，并插入数据1234skytf=&gt; create table test_24 (id integer, name varchar(32)); CREATE TABLEskytf=&gt; insert into test_24 select generate_series (1,10000),'francs'; INSERT 0 10000 表分析12skytf=&gt; analyze test_24; ANALYZE 查询统计信息12345678910skytf=&gt; select relpages,reltuples from pg_class where relname='test_24'; relpages | reltuples ----------+----------- 55 | 10000 (1 row)skytf=&gt; select oid,relfilenode,relname from pg_class where relname='test_24'; oid | relfilenode | relname ----------+-------------+--------- 14280826 | 14280826 | test_24 (1 row) 另开一 session ,以 postgres用户连接 skytf库12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667skytf=# select count(*) from pg_buffercache where relfilenode=14280826; count ------- 57 (1 row) skytf=# select * from pg_buffercache where relfilenode =14280826 order by relblocknumber; bufferid | relfilenode | reltablespace | reldatabase | relforknumber | relblocknumber | isdirty | usagecount ----------+-------------+---------------+-------------+---------------+----------------+---------+------------ 12021 | 14280826 | 14203070 | 14203071 | 1 | 0 | f | 5 11983 | 14280826 | 14203070 | 14203071 | 0 | 0 | t | 5 12026 | 14280826 | 14203070 | 14203071 | 0 | 1 | t | 5 11988 | 14280826 | 14203070 | 14203071 | 1 | 2 | f | 5 12027 | 14280826 | 14203070 | 14203071 | 0 | 2 | t | 5 12028 | 14280826 | 14203070 | 14203071 | 0 | 3 | t | 5 12057 | 14280826 | 14203070 | 14203071 | 0 | 4 | t | 5 12058 | 14280826 | 14203070 | 14203071 | 0 | 5 | t | 5 12071 | 14280826 | 14203070 | 14203071 | 0 | 6 | t | 5 12076 | 14280826 | 14203070 | 14203071 | 0 | 7 | t | 5 12105 | 14280826 | 14203070 | 14203071 | 0 | 8 | t | 5 12117 | 14280826 | 14203070 | 14203071 | 0 | 9 | t | 5 12120 | 14280826 | 14203070 | 14203071 | 0 | 10 | t | 5 12123 | 14280826 | 14203070 | 14203071 | 0 | 11 | t | 5 12133 | 14280826 | 14203070 | 14203071 | 0 | 12 | t | 5 12134 | 14280826 | 14203070 | 14203071 | 0 | 13 | t | 5 12136 | 14280826 | 14203070 | 14203071 | 0 | 14 | t | 5 12137 | 14280826 | 14203070 | 14203071 | 0 | 15 | t | 5 12138 | 14280826 | 14203070 | 14203071 | 0 | 16 | t | 5 12139 | 14280826 | 14203070 | 14203071 | 0 | 17 | t | 5 12141 | 14280826 | 14203070 | 14203071 | 0 | 18 | t | 5 12143 | 14280826 | 14203070 | 14203071 | 0 | 19 | t | 5 12144 | 14280826 | 14203070 | 14203071 | 0 | 20 | t | 5 12145 | 14280826 | 14203070 | 14203071 | 0 | 21 | t | 5 12147 | 14280826 | 14203070 | 14203071 | 0 | 22 | t | 5 12151 | 14280826 | 14203070 | 14203071 | 0 | 23 | t | 5 12152 | 14280826 | 14203070 | 14203071 | 0 | 24 | t | 5 12153 | 14280826 | 14203070 | 14203071 | 0 | 25 | t | 5 12155 | 14280826 | 14203070 | 14203071 | 0 | 26 | t | 5 12156 | 14280826 | 14203070 | 14203071 | 0 | 27 | t | 5 12160 | 14280826 | 14203070 | 14203071 | 0 | 28 | t | 5 12161 | 14280826 | 14203070 | 14203071 | 0 | 29 | t | 5 12162 | 14280826 | 14203070 | 14203071 | 0 | 30 | t | 5 12163 | 14280826 | 14203070 | 14203071 | 0 | 31 | t | 5 12164 | 14280826 | 14203070 | 14203071 | 0 | 32 | t | 5 12166 | 14280826 | 14203070 | 14203071 | 0 | 33 | t | 5 12167 | 14280826 | 14203070 | 14203071 | 0 | 34 | t | 5 12168 | 14280826 | 14203070 | 14203071 | 0 | 35 | t | 5 12169 | 14280826 | 14203070 | 14203071 | 0 | 36 | t | 5 12170 | 14280826 | 14203070 | 14203071 | 0 | 37 | t | 5 12172 | 14280826 | 14203070 | 14203071 | 0 | 38 | t | 5 12173 | 14280826 | 14203070 | 14203071 | 0 | 39 | t | 5 12174 | 14280826 | 14203070 | 14203071 | 0 | 40 | t | 5 12175 | 14280826 | 14203070 | 14203071 | 0 | 41 | t | 5 12176 | 14280826 | 14203070 | 14203071 | 0 | 42 | t | 5 12179 | 14280826 | 14203070 | 14203071 | 0 | 43 | t | 5 12181 | 14280826 | 14203070 | 14203071 | 0 | 44 | t | 5 12183 | 14280826 | 14203070 | 14203071 | 0 | 45 | t | 5 12184 | 14280826 | 14203070 | 14203071 | 0 | 46 | t | 5 12185 | 14280826 | 14203070 | 14203071 | 0 | 47 | t | 5 12186 | 14280826 | 14203070 | 14203071 | 0 | 48 | t | 5 12188 | 14280826 | 14203070 | 14203071 | 0 | 49 | t | 5 12189 | 14280826 | 14203070 | 14203071 | 0 | 50 | t | 5 12192 | 14280826 | 14203070 | 14203071 | 0 | 51 | t | 5 12193 | 14280826 | 14203070 | 14203071 | 0 | 52 | t | 5 12194 | 14280826 | 14203070 | 14203071 | 0 | 53 | t | 5 12195 | 14280826 | 14203070 | 14203071 | 0 | 54 | t | 5 (57 rows) 备注：pg_buffercache 每行记录的是一个 block 块信息。 pg_buffercahce 常用查询Top relations in the cache123456789101112131415161718192021222324SELECT c.relname, count(*) AS buffers FROM pg_class c INNER JOIN pg_buffercache b ON b.relfilenode=c.relfilenode INNER JOIN pg_database d ON (b.reldatabase=d.oid AND d.datname=current_database()) GROUP BY c.relname ORDER BY 2 DESC LIMIT 10; relname | buffers -------------------+--------- test_1 | 5411 postgres_log | 665 test_18 | 437 postgres_log_pkey | 142 pg_toast_2619 | 91 test_19 | 59 test_20 | 57 test_15 | 55 pg_statistic | 20 pg_operator | 13 (10 rows) 查数据表缓存占用 shared_buffers 百分比12345678910111213141516171819202122232425262728293031SELECT c.relname, pg_size_pretty(count(*) * 8192) as buffered, round(100.0 * count(*) / (SELECT setting FROM pg_settings WHERE name='shared_buffers')::integer,1) AS buffers_percent, round(100.0 * count(*) * 8192 / pg_relation_size(c.oid),1) AS percent_of_relation FROM pg_class c INNER JOIN pg_buffercache b ON b.relfilenode = c.relfilenode INNER JOIN pg_database d ON (b.reldatabase = d.oid AND d.datname = current_database()) GROUP BY c.oid,c.relname ORDER BY 3 DESC LIMIT 10; relname | buffered | buffers_percent | percent_of_relation ----------------------------------+----------+-----------------+--------------------- test_1 | 42 MB | 4.1 | 100.1 postgres_log | 5320 kB | 0.5 | 100.3 test_18 | 3496 kB | 0.3 | 100.9 pg_toast_2619 | 728 kB | 0.1 | 104.6 postgres_log_pkey | 1136 kB | 0.1 | 100.0 pg_depend | 80 kB | 0.0 | 23.3 pg_namespace_oid_index | 16 kB | 0.0 | 100.0 pg_operator_oid_index | 32 kB | 0.0 | 100.0 pg_statistic_relid_att_inh_index | 40 kB | 0.0 | 100.0 pg_constraint_oid_index | 16 kB | 0.0 | 100.0 (10 rows) 总结 Postgresql 没有提供系统VIEW来监控 shared_buffer 情况，需要单独安装 pg_buffercache 模块； 由于在查询 pg_buffercache 视图时会对系统存在一定影响，所以不建议频繁地查看 pg_buffercache 作为监控手段。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"How to estimate total number of WAL segments ?","slug":"20110318141508","date":"2011-03-18T06:15:08.000Z","updated":"2018-09-04T01:33:48.927Z","comments":true,"path":"20110318141508.html","link":"","permalink":"https://postgres.fun/20110318141508.html","excerpt":"","text":"和Oracle 一样， PostgreSQL也有着比较完善的日志写入流程，也是在将缓存的数据刷入到磁盘之前，先写日志, 这就是PostgreSQL WAL ( Write-Ahead Log )方式，中文为预写日志方式, 那么PG的日志数据是如何确定各数的呢？这个问题困扰了我很久，今天总算有了比较清明的理解，写出来，给有需要的人分享。 WAL 相关参数 checkpoint_segments (integer)Maximum number of log file segments between automatic WAL checkpoints (each segment is normally 16 megabytes). The default is three segments. Increasing this parameter can increase the amount of time needed for crash recovery. This parameter can only be set in the postgresql.conf file or on the server command line.checkpoint_timeout (integer)Maximum time between automatic WAL checkpoints, in seconds. The default is five minutes (5min). Increasing this parameter can increase the amount of time needed for crash recovery. This parameter can only be set in the postgresql.conf file or on the server command line. checkpoint_completion_target (floating point)Specifies the target of checkpoint completion, as a fraction of total time between checkpoints. The default is 0.5. This parameter can only be set in the postgresql.conf file or on the server command line. WAL 文件数估计方法通常地说，WAL segment 最大个数不超过如下：1( 2 + checkpoint_completion_target ) * checkpoint_segments + 1 在流复制环境下， WAL 最大数不超过如下：1wal_keep_segments + checkpoint_segments + 1 WAL 文件数验证主机 pg_xlog 日志数12[postgres@172_16_3_216 pg_root]$ ll $PGDATA/pg_xlog | wc -l 132 当前 checkpoint_segments 值12345postgres=# show checkpoint_segments; checkpoint_segments --------------------- 64 (1 row) 备注： 当前数据库的 checkpoint_segments 的值为 64, 而目前操作系统上 wal segments 为 131 ( 除去 pg_xlog 中的 archive_status), 上面的公式符合。 WAL Segments 占用空间计算由于 wal segments 日志个数会在一个范围变化，因此占用空间会随着 wal segments 个数变化，那么在设置 checkpoint_segments 参数时，需要考虑到PG SERVER 最多会产生多少个 wal segments 并且，并且最多会占用多少硬盘空间，下面是一个统计，前提条件: 每个 wal segments 为 16 M, 即系统的默认值。 如何设置 WAL Segments 大小?默认的 WAL segments 为 16 M, 这个参数在PG server 编译时指定，当执行 ./configure 时，指定 “–with-wal-segsize=target_value” 参数，即可设置。 总结 checkpoint_segments 值默认为 3, 这个值较小，建议设置成32以上，如果业务很繁忙，这个参数还应该调大，当然在恢复时也意味着恢复时间较长，这个需要综合考虑。 checkpoint_completion_target 默读值为 0.5,这个通常保持默认值即可。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL日志分析：将CSV日志导入到表中","slug":"20110315152859","date":"2011-03-15T07:28:59.000Z","updated":"2018-09-04T01:33:48.864Z","comments":true,"path":"20110315152859.html","link":"","permalink":"https://postgres.fun/20110315152859.html","excerpt":"","text":"像其它数据库一样，PostgreSQL也有自己的日志系统，postgresql 日志方面有非常全面的设置参数，这篇不准备仔细的介绍PG的日志参数，只介绍日志分析的一种方法，即可以将日志文件内容导入到数据库表里，便于分析日志。 CSV日志文件目录日志的目录可以通过参数 log_directory 来设置，下面是我的参数设置。1log_directory = '/var/applog/pg_log' 查看日志文件12345[postgres@pg_log]$ ll /var/applog/pg_log -rw------- 1 postgres postgres 4.8M Mar 14 23:57 postgresql-2011-03-14_000000.csv -rw------- 1 postgres postgres 0 Mar 14 00:00 postgresql-2011-03-14_000000.log -rw------- 1 postgres postgres 294K Mar 15 15:10 postgresql-2011-03-15_000000.csv -rw------- 1 postgres postgres 0 Mar 15 00:00 postgresql-2011-03-15_000000.log CSV日志文件内容122011-03-15 00:07:03.513 CST,\"wapportal\",\"wapportal_216\",4137,\"172.16.3.43:59356\",4d7e361f.1029,3,\"idle\",2011-03-14 23:37:03 CST,,0,LOG,00000,\"disconnection: session time: 0:30:00.086 user=wapportal database=wapportal_216 host=172.16.3.43 port=59356\",,,,,,,,,\"\" 2011-03-15 00:07:03.514 CST,,,5173,\"\",4d7e3d27.1435,1,\"\",2011-03-15 00:07:03 CST,,0,LOG,00000,\"connection received: host=172.16.3.43 port=51135\",,,,,,,,,\"\" 上面两条是 postgresql-2011-03-15_000000.csv 日志文件的部分内容 ,由于日志文件的可读性较差，于是可以通过下面方法将CSV日志导入到数据库表里。详细如下 将CSV日志导入数据库表调整 postgresql.conf 参数，如下：12log_destination = 'csvlog' logging_collector = on 这两个参数修改后，PG SERVER 需要重启。 创建日志记录表1234567891011121314151617181920212223242526272829CREATE TABLE postgres_log ( log_time timestamp(3) with time zone, user_name text, database_name text, process_id integer, connection_from text, session_id text, session_line_num bigint, command_tag text, session_start_time timestamp with time zone, virtual_transaction_id text, transaction_id bigint, error_severity text, sql_state_code text, message text, detail text, hint text, internal_query text, internal_query_pos integer, context text, query text, query_pos integer, location text, application_name text, PRIMARY KEY (session_id, session_line_num) );NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"postgres_log_pkey\" for table \"postgres_log\" CREATE TABLE; 备注：创建日志表 postgres_log 用来保存 CSV日志数据。 导入操作系统 csv 日志到表 postgres_log 表1234skytf=# copy skytf.postgres_log from '/var/applog/pg_log/postgresql-2011-03-14_000000.csv' with csv; COPY 26031skytf=# copy skytf.postgres_log from '/var/applog/pg_log/postgresql-2011-03-15_000000.csv' with csv; COPY 1297 备注：文件形式导入导出数据需要以超级用户 postgres 连接到目标库。 常用日志分析SQL1234567891011121314skytf=# select min(log_time),max(log_time) from skytf.postgres_log; min | max ----------------------------+---------------------------- 2011-03-14 14:04:07.275+08 | 2011-03-16 05:04:34.427+08 (1 row)skytf=&gt; select log_time,database_name,user_name,application_name,message from postgres_log where message like '%duration%'; log_time | database_name | user_name | application_name | mess age ----------------------------+---------------+-----------+------------------+-------------------------------------------------------- ------------------------------------------------------- 2011-03-15 00:23:38.957+08 | db_lbs | lbs | | duration: 1297.440 ms execute &lt;unnamed&gt;: SELECT cit yname,province,the_geom as the_geom FROM china_city ....... 为了显示方便，上面只取一条记录。 总结当数据库出现异常需要详细分析日志文件时，上面的方法提供了一个非常有效的方式，将数据库日志导入到表里，能够更准确，方便地分析数据库日志。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Autovacuum 参数解释","slug":"20110314162516","date":"2011-03-14T08:25:16.000Z","updated":"2018-09-04T01:33:48.817Z","comments":true,"path":"20110314162516.html","link":"","permalink":"https://postgres.fun/20110314162516.html","excerpt":"","text":"Autovacuum 是 PostgreSQL 里非常重要的一个服务端进程。它能够自动地执行，在一定条件下自动地对 dead tuples 进行清理并对表进行分析， 这篇主要是讲述下 autovacuum 的 一些参数的含义。 Autovaccum 相关参数 autovacuum (boolean)autovacuum参数控制 autovacuum 进程是否打开,默认为 “on” log_autovacuum_min_duration(integer)这个参数用来记录 autovacuum 的执行时间，当 autovaccum 的执行时间超过 log_autovacuum_min_duration参数设置时，则autovacuum信息记录到日志里，默认为 “-1”, 表示不记录。 autovacuum_max_workers (integer)指定同时运行的 最大的 autovacuum 进程，默认为3个。 autovacuum_naptime (integer)指定 autovacuum 进程运行的最小间隔，默认为 1 min。也就是说当前一个 autovacuum 进程运行完成后，第二个 autovacuum 进程至少在一分钟后才会运行。 autovacuum_vacuum_threshold (integer)autovacuum 进程进行vacuum 操作的阀值条件一，(指修改，删除的记录数。) autovacuum_analyze_threshold (integer)autovacuum 进程进行 analyze 操作的阀值条件一，(指插入，修改，删除的记录数。) autovacuum_vacuum_scale_factor (floating point)autovacuum因子, autovacuum 进程进行 vacuum 操作的阀值条件二,，默认为 0.2 ，autovacuum进程进行 vacuum 触发条件表上(update,delte 记录) &gt;= autovacuum_vacuum_scale_factor* reltuples(表上记录数) + autovacuum_vacuum_threshold autovacuum_analyze_scale_factor (floating point)autoanalyze 因子,autovacuum 进程进行 analyze 操作的阀值条件二,，默认为 0.1autovacuum进程进行 analyze 触发条件表上(insert,update,delte 记录) &gt;= autovacuum_analyze_scale_factor* reltuples(表上记录数) +autovacuum_analyze_threshold autovacuum_freeze_max_age (integer)指定表上事务的最大年龄，默认为2亿，达到这个阀值将触发 autovacuum进程，从而避免 wraparound。表上的事务年龄可以通过 pg_class.relfrozenxid 查询。 例如,查询表 test_1 的事务年龄 skytf=&gt; select relname,age(relfrozenxid) from pg_class where relname=&apos;test_1&apos;; relname | age ---------+---------- test_1 | 14208876 (1 row) autovacuum_vacuum_cost_delay (integer)当autovacuum进程即将执行时，对 vacuum 执行 cost 进行评估，如果超过 autovacuum_vacuum_cost_limit设置值时，则延迟，这个延迟的时间即为 autovacuum_vacuum_cost_delay。如果值为 -1, 表示使用vacuum_cost_delay 值，默认值为 20 ms autovacuum_vacuum_cost_limit (integer)这个值为 autovacuum 进程的评估阀值, 默认为 -1, 表示使用 “vacuum_cost_limit “ 值，如果在执行autovacuum 进程期间评估的 cost 超过 autovacuum_vacuum_cost_limit, 则 autovacuum 进程则会休眠。 附：官网解释 autovacuum (boolean)Controls whether the server should run the autovacuum launcher daemon. This is on by default; however, track_counts must also be turned on for autovacuum to work. This parameter can only be set in the postgresql.conf file or on the server command line.Note that even when this parameter is disabled, the system will launch autovacuum processes if necessary to prevent transaction ID wraparound. See Section 23.1.4, “Preventing Transaction ID Wraparound Failures” for more information. 2 log_autovacuum_min_duration (integer)Causes each action executed by autovacuum to be logged if it ran for at least the specified number of milliseconds. Setting this to zero logs all autovacuum actions. Minus-one (the default) disables logging autovacuum actions. For example, if you set this to 250ms then all automatic vacuums and analyzes that run 250ms or longer will be logged. Enabling this parameter can be helpful in tracking autovacuum activity. This setting can only be set in the postgresql.conf file or on the server command line. 3 autovacuum_max_workers (integer)Specifies the maximum number of autovacuum processes (other than the autovacuum launcher) which may be running at any one time. The default is three. This parameter can only be set in the postgresql.conf file or on the server command line. 4 autovacuum_naptime (integer)Specifies the minimum delay between autovacuum runs on any given database. In each round the daemon examines the database and issues VACUUM and ANALYZE commands as needed for tables in that database. The delay is measured in seconds, and the default is one minute (1m). This parameter can only be set in the postgresql.conf file or on the server command line. 5 autovacuum_vacuum_threshold (integer)Specifies the minimum number of updated or deleted tuples needed to trigger a VACUUM in any one table. The default is 50 tuples. This parameter can only be set in the postgresql.conf file or on the server command line. This setting can be overridden for individual tables by changing storage parameters. 6 autovacuum_analyze_threshold (integer)Specifies the minimum number of inserted, updated or deleted tuples needed to trigger an ANALYZE in any one table. The default is 50 tuples. This parameter can only be set in the postgresql.conf file or on the server command line. This setting can be overridden for individual tables by changing storage parameters. 7 autovacuum_vacuum_scale_factor (floating point)Specifies a fraction of the table size to add to autovacuum_vacuum_threshold when deciding whether to trigger a VACUUM. The default is 0.2 (20% of table size). This parameter can only be set in the postgresql.conf file or on the server command line. This setting can be overridden for individual tables by changing storage parameters. 8 autovacuum_analyze_scale_factor (floating point)Specifies a fraction of the table size to add to autovacuum_analyze_threshold when deciding whether to trigger an ANALYZE. The default is 0.1 (10% of table size). This parameter can only be set in the postgresql.conf file or on the server command line. This setting can be overridden for individual tables by changing storage parameters. 9 autovacuum_freeze_max_age (integer)Specifies the maximum age (in transactions) that a table is pg_class.relfrozenxid field can attain before a VACUUM operation is forced to prevent transaction ID wraparound within the table. Note that the system will launch autovacuum processes to prevent wraparound even when autovacuum is otherwise disabled. The default is 200 million transactions. This parameter can only be set at server start, but the setting can be reduced for individual tables by changing storage parameters. For more information see Section 23.1.4, “Preventing Transaction ID Wraparound Failures”. 10 autovacuum_vacuum_cost_delay (integer)Specifies the cost delay value that will be used in automatic VACUUM operations. If -1 is specified, the regular vacuum_cost_delay value will be used. The default value is 20 milliseconds. This parameter can only be set in the postgresql.conf file or on the server command line. This setting can be overridden for individual tables by changing storage parameters. 11 autovacuum_vacuum_cost_limit (integer)Specifies the cost limit value that will be used in automatic VACUUM operations. If -1 is specified (which is the default), the regular vacuum_cost_limit value will be used. Note that the value is distributed proportionally among the running autovacuum workers, if there is more than one, so that the sum of the limits of each worker never exceeds the limit on this variable. This parameter can only be set in the postgresql.conf file or on the server command line. This setting can be overridden for individual tables by changing storage parameters.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Autovacuum","slug":"Autovacuum","permalink":"https://postgres.fun/tags/Autovacuum/"}]},{"title":"表级隐含字段: Xmin 和 Xmax","slug":"20110310141842","date":"2011-03-10T06:18:42.000Z","updated":"2018-12-04T00:25:50.647Z","comments":true,"path":"20110310141842.html","link":"","permalink":"https://postgres.fun/20110310141842.html","excerpt":"","text":"PostgreSQL 里每张表都有隐含字段，确切地说应该是 system Columns , 本文主要讲述下其中两个 system columns,即为 xmin , xmax。 官网解释 xmin The identity (transaction ID) of the inserting transaction for this row version.(A row version is an individual state of a row; each update of a row creates a new rowversion for the same logical row.) xmax The identity (transaction ID) of the deleting transaction, or zero for an undeletedrow version. It is possible for this column to be nonzero in a visible row version. Thatusually indicates that the deleting transaction hasn it committed yet, or that an attempteddeletion was rolled back. 根据上面讲解，可以知道 xmin,xmax都指事务ID(transaction ID), 简单的说，xmin记录的是当数据插入( Insert )时的事务ID，xmax记录的是当行上的数据有变动（delete or update ）时的事务ID，下面看下详细的实验过程。 测试环境准备创建测试表并插入数据1234skytf=&gt; create table test_17 (id integer ,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_17 select generate_series(1,10),'a'; INSERT 0 10 查询表 test_17信息1234567891011121314skytf=&gt; select xmin,xmax,ctid,* from test_17; xmin | xmax | ctid | id | name ----------+------+--------+----+------ 14240187 | 0 | (0,1) | 1 | a 14240187 | 0 | (0,2) | 2 | a 14240187 | 0 | (0,3) | 3 | a 14240187 | 0 | (0,4) | 4 | a 14240187 | 0 | (0,5) | 5 | a 14240187 | 0 | (0,6) | 6 | a 14240187 | 0 | (0,7) | 7 | a 14240187 | 0 | (0,8) | 8 | a 14240187 | 0 | (0,9) | 9 | a 14240187 | 0 | (0,10) | 10 | a (10 rows) 这里看到, 表 test_17 里有十条数据，并且这10条数据的 xmin 都为 14240187 , xmax 值都为 0 14240187 即为事务ID，下面可以进一步地测试 测试场景一：Xmin开启一事务12345678910111213141516171819skytf=&gt; begin; BEGIN skytf=&gt; select txid_current(); txid_current -------------- 14240188 (1 row)skytf=&gt; select txid_current(); txid_current -------------- 14240188 (1 row)skytf=&gt; insert into test_17 values (11,'b'); INSERT 0 1skytf=&gt; end; COMMIT 上面事务提交后，查询 xmin12345skytf=&gt; select xmin,xmax,ctid,* from test_17 where id=11; xmin | xmax | ctid | id | name ----------+------+--------+----+------ 14240188 | 0 | (0,11) | 11 | b (1 row) 可以看到 14240188 即为开始 INSERT 数据时的事务ID。 测试场景二：Xmax查询表 test_17信息 123456789101112131415skytf=&gt; select xmin,xmax,ctid,* from test_17; xmin | xmax | ctid | id | name ----------+------+--------+----+------ 14240187 | 0 | (0,1) | 1 | a 14240187 | 0 | (0,2) | 2 | a 14240187 | 0 | (0,3) | 3 | a 14240187 | 0 | (0,4) | 4 | a 14240187 | 0 | (0,5) | 5 | a 14240187 | 0 | (0,6) | 6 | a 14240187 | 0 | (0,7) | 7 | a 14240187 | 0 | (0,8) | 8 | a 14240187 | 0 | (0,9) | 9 | a 14240187 | 0 | (0,10) | 10 | a 14240188 | 0 | (0,11) | 11 | b (11 rows) 大家可以看到, xmax 值为0，那什么情况下 xmax 值不为0呢，有几种情况，接下来看 情形一: deleting事务未提交会话一：开启一事务12345678910skytf=&gt; begin; BEGIN skytf=&gt; select txid_current(); txid_current -------------- 14240189 (1 row)skytf=&gt; delete from test_17 where id=11; DELETE 1 skytf=&gt; 会话一开启一个事务，并删除一条数据，并不 commit,此时开启另一会话。 另开一个会话查询123456789101112131415skytf=&gt; select xmin,xmax,ctid,* from test_17; xmin | xmax | ctid | id | name ----------+----------+--------+----+------ 14240187 | 0 | (0,1) | 1 | a 14240187 | 0 | (0,2) | 2 | a 14240187 | 0 | (0,3) | 3 | a 14240187 | 0 | (0,4) | 4 | a 14240187 | 0 | (0,5) | 5 | a 14240187 | 0 | (0,6) | 6 | a 14240187 | 0 | (0,7) | 7 | a 14240187 | 0 | (0,8) | 8 | a 14240187 | 0 | (0,9) | 9 | a 14240187 | 0 | (0,10) | 10 | a 14240188 | 14240189 | (0,11) | 11 | b (11 rows) 发现 id为 11的记录的 xmax 变为 14240189 了。有兴趣的可以测试下 updating 事务未提交的场景，和这个结果类似，这里不再详述。 情形二: deleting 事务 rollback开启事务1234567891011skytf=&gt; begin; BEGIN skytf=&gt; select txid_current(); txid_current -------------- 14240190 (1 row)skytf=&gt; delete from test_17 where id=10; DELETE 1 skytf=&gt; rollback; ROLLBACK 再次查询表信息1234567891011121314skytf=&gt; select xmin,xmax,ctid,* from test_17; xmin | xmax | ctid | id | name ----------+----------+--------+----+------ 14240187 | 0 | (0,1) | 1 | a 14240187 | 0 | (0,2) | 2 | a 14240187 | 0 | (0,3) | 3 | a 14240187 | 0 | (0,4) | 4 | a 14240187 | 0 | (0,5) | 5 | a 14240187 | 0 | (0,6) | 6 | a 14240187 | 0 | (0,7) | 7 | a 14240187 | 0 | (0,8) | 8 | a 14240187 | 0 | (0,9) | 9 | a 14240187 | 14240190 | (0,10) | 10 | a (10 rows) 发现 id为 10 的记录的 xmax 变为 14240190 了。 情况三: updating 事务 rollback开启事务1234567891011skytf=&gt; begin; BEGIN skytf=&gt; select txid_current(); txid_current -------------- 14240191 (1 row)skytf=&gt; update test_17 set name='aaa' where id=1; UPDATE 1skytf=&gt; rollback; ROLLBACK 再次查询表信息1234567891011121314skytf=&gt; select xmin,xmax,ctid,* from test_17; xmin | xmax | ctid | id | name ----------+----------+--------+----+------ 14240187 | 14240191 | (0,1) | 1 | a 14240187 | 0 | (0,2) | 2 | a 14240187 | 0 | (0,3) | 3 | a 14240187 | 0 | (0,4) | 4 | a 14240187 | 0 | (0,5) | 5 | a 14240187 | 0 | (0,6) | 6 | a 14240187 | 0 | (0,7) | 7 | a 14240187 | 0 | (0,8) | 8 | a 14240187 | 0 | (0,9) | 9 | a 14240187 | 14240190 | (0,10) | 10 | a (10 rows) 上面可以看到,id=1 的行的 xmax 值 变为 14240191。 总结 xmin,xmax 都指事务ID (transaction ID) xmin记录的是当数据插入( Insert )时的事务ID xmax记录的是当行上的数据有变动（delete or update ）时的事务ID,有两种情况 A: deleting/updating 事务未提交; B: deleting/updating 事务 rollback","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL Error Codes","slug":"20110307163115","date":"2011-03-07T08:31:15.000Z","updated":"2018-09-04T01:33:48.708Z","comments":true,"path":"20110307163115.html","link":"","permalink":"https://postgres.fun/20110307163115.html","excerpt":"","text":"PostgreSQL维护过程中，遇到 ERROR应该是非常正常的情况，目前国内PostgreSQL的中文资料比较少，在遇到问题时查找资料将是非常艰难的一件事情，幸运的是， 可以参考PG官网提供的错误代码,例如。 PostgreSQL Error Codes链接 http://www.postgresql.org/docs/9.0/static/errcodes-appendix.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"设置 Constraint_exclusion 避免扫描分区表所有分区","slug":"20110304142634","date":"2011-03-04T06:26:34.000Z","updated":"2018-09-04T01:33:48.645Z","comments":true,"path":"20110304142634.html","link":"","permalink":"https://postgres.fun/20110304142634.html","excerpt":"","text":"熟悉PostgreSQL的朋友应该知道，PostgreSQL 没有像 Oracle 一样智能的分区表，在PG里分区表是通过创建子表继承父表和设置插入，删除触发器实现的。 那么PG的分区表性能如何呢？ 举个简单的例子，如果查询表里的数据，PG会扫描所有分区。 尽管只查询某个分区的数据，PG 仍然会扫描所有分区，显然，这大大降低了查询性能，如何避免这种情况呢？还好PG里提供了参数 constraint_exclusion , 从而避免扫描所有分区，下面看个例子。 环境准备创建父表1234skytf=&gt; create table test (id integer, name varchar(32)); CREATE TABLEskytf=&gt; create index idx_test_id on test using btree(id); CREATE INDEX 创建三个子表123456789101112skytf=&gt; create table test_a (like test including constraints including defaults including indexes ) inherits (test ); NOTICE: merging column \"id\" with inherited definition NOTICE: merging column \"name\" with inherited definition CREATE TABLEskytf=&gt; create table test_b (like test including constraints including defaults including indexes ) inherits (test ); NOTICE: merging column \"id\" with inherited definition NOTICE: merging column \"name\" with inherited definition CREATE TABLEskytf=&gt; create table test_c (like test including constraints including defaults including indexes ) inherits (test ); NOTICE: merging column \"id\" with inherited definition NOTICE: merging column \"name\" with inherited definition CREATE TABLE 设置约束123456skytf=&gt; alter table test_a add constraint con_test_a check (id &gt;=1 and id &lt;=1000); ALTER TABLEskytf=&gt; alter table test_b add constraint con_test_b check (id &gt;=1001 and id &lt;=2000); ALTER TABLEskytf=&gt; alter table test_c add constraint con_test_c check (id &gt;=2001 and id &lt;=3000); ALTER TABLE 插入测试数据123456skytf=&gt; insert into test_a select generate_series(1,1000),'aaa'; INSERT 0 1000skytf=&gt; insert into test_b select generate_series(1001,2000),'bbb'; INSERT 0 1000skytf=&gt; insert into test_c select generate_series(2001,3000),'ccc'; INSERT 0 1000 查询数据1234567891011121314151617181920212223242526272829303132333435363738394041424344skytf=&gt; select * From test_a limit 10; id | name ----+------ 1 | aaa 2 | aaa 3 | aaa 4 | aaa 5 | aaa 6 | aaa 7 | aaa 8 | aaa 9 | aaa 10 | aaa (10 rows)skytf=&gt; select * From test_b limit 10; id | name ------+------ 1001 | bbb 1002 | bbb 1003 | bbb 1004 | bbb 1005 | bbb 1006 | bbb 1007 | bbb 1008 | bbb 1009 | bbb 1010 | bbb (10 rows)skytf=&gt; select * From test_c limit 10; id | name ------+------ 2001 | ccc 2002 | ccc 2003 | ccc 2004 | ccc 2005 | ccc 2006 | ccc 2007 | ccc 2008 | ccc 2009 | ccc 2010 | ccc (10 rows) 查看子表表结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445skytf=&gt; \\d test Table \"skytf.test\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Indexes: \"idx_test_id\" btree (id) Number of child tables: 3 (Use d+ to list them.)skytf=&gt; \\d test_a Table \"skytf.test_a\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Indexes: \"test_a_id_idx\" btree (id) Check constraints: \"con_test_a\" CHECK (id &gt;= 1 AND id &lt;= 1000) Inherits: testskytf=&gt; \\d test_b; Table \"skytf.test_b\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Indexes: \"test_b_id_idx\" btree (id) Check constraints: \"con_test_b\" CHECK (id &gt;= 1001 AND id &lt;= 2000) Inherits: testskytf=&gt; \\d test_c; Table \"skytf.test_c\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | name | character varying(32) | Indexes: \"test_c_id_idx\" btree (id) Check constraints: \"con_test_c\" CHECK (id &gt;= 2001 AND id &lt;= 3000) Inherits: test 表分析12345678skytf=&gt; analyze test; ANALYZE skytf=&gt; analyze test_a; ANALYZE skytf=&gt; analyze test_b; ANALYZE skytf=&gt; analyze test_c; ANALYZE 设置 constraint_exclusion=off12345678910111213141516171819202122skytf=&gt; show constraint_exclusion; constraint_exclusion ---------------------- off (1 row)skytf=&gt; explain select * From test where id=1; QUERY PLAN --------------------------------------------------------------------------------------------- Result (cost=2.28..20.60 rows=7 width=53) -&gt; Append (cost=2.28..20.60 rows=7 width=53) -&gt; Bitmap Heap Scan on test (cost=2.28..7.80 rows=4 width=86) Recheck Cond: (id = 1) -&gt; Bitmap Index Scan on idx_test_id (cost=0.00..2.28 rows=4 width=0) Index Cond: (id = 1) -&gt; Index Scan using test_a_id_idx on test_a test (cost=0.00..4.27 rows=1 width=8) Index Cond: (id = 1) -&gt; Index Scan using test_b_id_idx on test_b test (cost=0.00..4.27 rows=1 width=8) Index Cond: (id = 1) -&gt; Index Scan using test_c_id_idx on test_c test (cost=0.00..4.27 rows=1 width=8) Index Cond: (id = 1) (12 rows) 备注: 从上面看到，PLAN里扫描了 test_a,test_b,test_c 三张表，而 id=1 的记录落在 test_a 表，理论上只要扫描 test_a 分区就行了,接下来看 constraint_exclusion=partition的情况。 设置 constraint_exclusion=partition123456789101112131415161718192021skytf=&gt; set constraint_exclusion=partition; SETskytf=&gt; show constraint_exclusion; constraint_exclusion ---------------------- partition (1 row)skytf=&gt; explain select * From test where id=1; QUERY PLAN --------------------------------------------------------------------------------------------- Result (cost=2.28..12.07 rows=5 width=70) -&gt; Append (cost=2.28..12.07 rows=5 width=70) -&gt; Bitmap Heap Scan on test (cost=2.28..7.80 rows=4 width=86) Recheck Cond: (id = 1) -&gt; Bitmap Index Scan on idx_test_id (cost=0.00..2.28 rows=4 width=0) Index Cond: (id = 1) -&gt; Index Scan using test_a_id_idx on test_a test (cost=0.00..4.27 rows=1 width=8) Index Cond: (id = 1) (8 rows) 备注，将 constraint_exclusion 参数设置为 “partition” 后，PLAN只扫描指定分区， 官网解释 constraint_exclusion (enum) Controls the query planner iss use of table constraints to optimize queries. The allowedvalues of constraint_exclusion are on (examine constraints for all tables), off (never examineconstraints), and partition (examine constraints only for inheritance child tables and UNION ALLsubqueries). partition is the default setting. When this parameter allows it for a particular table, the planner compares query conditionswith the table is CHECK constraints, and omits scanning tables for which the conditions contradict the constraints. 总结constraint_exclusion 的含义是：当PG生产执行计划时是否考虑表上的约束，这个参数有三个选项 “off,on ,partition” ，默认参数为 off, 意思不使用表上的 constraint 来生成计划，如果设置成 on ,则对所有表生效，生成 PLAN 时会考虑表上的 constraint, 建议设置成 partition,只对分区表生效，从而避免扫描分区表所有分区。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"ERROR,22021,Invalid Byte Sequence For Encoding \"\"UTF8\"\"","slug":"20110302170752","date":"2011-03-02T09:07:52.000Z","updated":"2018-09-04T01:33:48.599Z","comments":true,"path":"20110302170752.html","link":"","permalink":"https://postgres.fun/20110302170752.html","excerpt":"","text":"最近发现一个生产库的日志频繁地抛出以下错误：1ERROR,22021,invalid byte sequence for encoding \"\"UTF8\"\" , 数据库日志122011-03-01 00:28:59.149 CST,\"skytf\",\"skytf\",27867,\"127.0.0.1:41149\",4d6bcd09.6cdb,1,\"idle in transaction\", 2011-03-01 00:27:53 CST,159/515139,108295573,ERROR,22021,\"invalid byte sequence for encoding \"\"UTF8\"\": 0xf09fe518\",,,,,,,,,\"\" 根据这个信息，猜测可能是字符集没有转换成 “utf8”，由于csv日志没有打出报错时相关的SQL语句，不好分析，于是联系开发人员，希望他们能够找出出错时间的SQL语句。开发人员非常配合，很快找到了相关SQL。 应用程序的日志12342011-02-28 14:54:26,059 /home/liweijing/skytf/gevent2/src/db.py(56): ERROR fail to batch write db. error 'ERROR: invalid byte sequence for encoding \"UTF8\": 0xf09fe518 ' in ' INSERT INTO table_log ( host, remoteip, ntime, reqdesc, httpcode, clength, refer, agent, hsman, hstype, imsi, fromcache, loadtime) VALUES ( '????????', '117.136.16.97', '2011-02-28 14:54:20 ', 'GET HTTP/1.1', 705, 0, '-', 'Sky-Wapproxy/v3.20(page,0-1-1251)', 'dewav', 'L968', '460008857083989', False, 6 ); ' 上面这段是开发人员贴出来的应用程序LOG，很容易发现，在向表 table_log 的字段插入数据时，host 字段的内容为乱码，即为 “????????” ，正是因为数据为乱码，才导致 PostgreSQL 频繁抛出以下错误：1ERROR,22021,invalid byte sequence for encoding \"\"UTF8\"\" 解决方法既然问题找出来了，解决起来就相对容易了，后来开发人员将字段 host 内容强制转换为 utf8 ，并且对服务端接收的数据进行检查，如果有非法字符，则丢弃。有了这些措施之后，数据库不再抛出类似ERROR。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"优化一例：创建最精简的索引","slug":"20110301181842","date":"2011-03-01T10:18:42.000Z","updated":"2018-09-04T01:33:48.536Z","comments":true,"path":"20110301181842.html","link":"","permalink":"https://postgres.fun/20110301181842.html","excerpt":"","text":"今天查看数据库 CSVLOG， 有个慢查询语句, 这个优化有点意思，记录下来，下面是详细的步骤。 1 数据库CSVLOG1234562011-03-01 17:24:49.442 CST,\"skytf\",\"skytf\",21500,\"192.168.164.37:39154\",4d1d3830.53fc,7950,\"SELECT\",2010-12-31 09:56:00 CST,16/3143301,0,LOG,00000,\"duration: 1701.007 ms execute &lt;unnamed&gt;: SELECT id, modifier, gmt_create as gmtCreate, gmt_modified as gmtModified, creator, is_deleted as isDeleted, mpxyzdatas, skyid, wap_token as wapToken, app_token as appToken, source, username, passwd, nickname, mpxyzdataf, mpxyzdatag, phone, phone2, mpxyzdatae, f FROM tmp_user WHERE is_deleted='n' AND mpxyzdatas=$1 order by id desc \",\"parameters: $1 = ')dzyDlyyLdyyfjzy)vzy'\",,,,,,,,\"\" 根据 duration: 1701.007 ，这个语句执行时间在1.7 s左右，而且数据库日志抛出大量这个慢SQL语句。 2 慢查询 SQL12345SELECT id, modifier, gmt_create as gmtCreate, gmt_modified as gmtModified, creator, is_deleted as isDeleted, mpxyzdatas, skyid, wap_token as wapToken, app_token as appToken, source, username, passwd, nickname, mpxyzdataf, mpxyzdatag, phone, phone2, mpxyzdatae, f FROM tmp_user WHERE is_deleted='n' AND mpxyzdatas=$1 order by id desc ; parameters: $1 = ')dzyL5zyzpzy)rzy!Pzy'\",,,,,,,,\" 3 老的执行计划12345678910111213skytf=&gt; explain analyze SELECT id, modifier, gmt_create as gmtCreate, gmt_modified as gmtModified, creator, is_deleted as isDeleted, skytf-&gt; mpxyzdatas, skyid, wap_token as wapToken, app_token as appToken, source, username, passwd, nickname, skytf-&gt; mpxyzdataf, mpxyzdatag, phone, phone2, mpxyzdatae, f FROM tmp_user WHERE is_deleted='n' AND skytf-&gt; mpxyzdatas=')dzyL5zyzpzy)rzy!Pzy' order by id desc ; QUERY PLAN --------------------------------------------------------------------------------------------------------------------- Sort (cost=139298.73..139298.74 rows=1 width=226) (actual time=1552.541..1552.542 rows=1 loops=1) Sort Key: id Sort Method: quicksort Memory: 25kB -&gt; Seq Scan on tmp_user (cost=0.00..139298.72 rows=1 width=226) (actual time=1352.316..1552.524 rows=1 loops=1) Filter: ((is_deleted = 'n'::bpchar) AND ((mpxyzdatas)::text = ')dzyL5zyzpzy)rzy!Pzy'::text))Total runtime: 1552.583 ms (6 rows)Time: 1566.089 ms 这个PLAN比较简单，很明了，”Seq Scan”，整个消耗 1.5s 4 表的相关信息1234567891011121314151617181920212223242526272829303132333435skytf=&gt; select pg_size_pretty(pg_relation_size('tmp_user')); pg_size_pretty ---------------- 573 MB (1 row)skytf=&gt; \\d tmp_user Table \"skytf.tmp_user\" Column | Type | Modifiers --------------+-----------------------------+------------------------------ id | integer | not null modifier | integer | gmt_create | timestamp without time zone | gmt_modified | timestamp without time zone | creator | integer | is_deleted | character(1) | not null default 'n'::bpchar mpxyzdatas | character varying | skyid | bigint | not null wap_token | character varying(100) | app_token | character varying(100) | source | character(10) | default 'applist'::bpchar username | character varying | passwd | character varying | nickname | character varying | mpxyzdataf | character varying | mpxyzdatag | character varying | phone | character varying(15) | mpxyzdatae | character varying | f | character varying | phone2 | character varying(15) | Indexes: \"tmp_user_new_pkey\" PRIMARY KEY, btree (id) \"unique_skyid1\" UNIQUE, btree (skyid) \"idx_mpxyzdatas\" btree (mpxyzdatas) \"index_gmt_create1\" btree (date(gmt_create)) 从上面信息可以看出，表tmp_user不太大，才 573 MB , 但查询条件 is_deleted 和 mpxyzdatas 都没有建索引, 根据直觉，像这种查询，建个联合索引 (is_deleted,mpxyzdatas ) 最合适不过了，先别着急，下面深入分析下。 5 查询列的 distinct 情况12345skytf=&gt; select tablename,attname,n_distinct from pg_stats where tablename='tmp_user' and attname in ('is_deleted','mpxyzdatas'); tablename | attname | n_distinct -----------+------------+------------ tmp_user | is_deleted | 1 tmp_user | mpxyzdatas | -1 pg_stats.n_distinct值为正数时，表时distinct实际值，当 n_distinct为 “ -1 “ 时，表示此列具有非常好的 distinct属性，即为 unique. 也就是说 表 tmp_user的列 is_deleted 只有一个 distinct 值, 而 mpxyzdatas 字段却为唯一。 6 设想几种创建索引的方法 create index idx_is_deleted_mpxyzdatas on skytf.tmp_user using btree (is_deleted,mpxyzdatas ); create index idx_is_mpxyzdatas_id on skytf.tmp_user using btree (mpxyzdatas, id desc ); create index idx_mpxyzdatas on skytf.tmp_user using btree (mpxyzdatas ); 现在权衡一下上面三种方法，方法一创建联合索引通常情况下适合这种QUERY 场合，但由于列 is_deleted 只有一个 distinct 值，所以这个字段上不适合建索引。于是排除方法一。 而方法二建了个(mpxyzdatas, id desc )索引，第二个方法主要是考虑到查询条件中按ID 进行降序排序，但由于 id 已经是 pk,并且排序代价不大，这可以从步骤3的 plan来查看，Sort (cost=139298.73..139298.74 rows=1 width=226) (actual time=1552.541..1552.542 rows=1 loops=1) 根据 cost=139298.73..139298或者 actual time=1552.541..1552.542 这个结点的起动代价，完成代价，起动时间完成时间可以非常明显地看出这个排序步骤代价非常小。而方法三，仅在列 mpxyzdatas 上创建索引，看上去不可行， 下面准备在测试库上测试下，接着把生产的这张表倒到测试环境下去验证一下，我的测试库主机，配置和生产的几乎相同,导表的步骤这里就省略了，通过 pg_dump备份表，pg_restore 还原表就行。 在测试环境上测试 7 先取部分数据，做测试用。1234567891011121314skytf=&gt; select mpxyzdatas from tmp_user limit 10; mpxyzdatas ---------------------- )dzyK5zy$vzyGdzyLNzy )dzyKBzyL1zy(~zy)dzy )dzyD5zyP1yy%xzyL9zy )dzyLlyy)5zz$NzyfFzy )dzyDlzyb#zybvzy)^zy )dzyD5yyD1zyC3zyP@zy )dzyH5yyK9zyy)zzz1yy )dzyL5yy~7zz(fzyL5yz )dzyD5yyKJzye3zyG7zy )dzyKlzy$jzyDtyy)Dzy (10 rows) 8 创建单列索引123skytf=&gt; create index idx_mpxyzdatas on skytf.tmp_user using btree (mpxyzdatas ); CREATE INDEX Time: 18191.498 ms 9 创建索引后的 plan123456789101112131415skytf=&gt; explain analyze SELECT id, modifier, gmt_create as gmtCreate, gmt_modified as gmtModified, creator, is_deleted as isDeleted, skytf-&gt; mpxyzdatas, skyid, wap_token as wapToken, app_token as appToken, source, username, passwd, nickname, skytf-&gt; mpxyzdataf, mpxyzdatag, phone, phone2, mpxyzdatae, f FROM tmp_user WHERE is_deleted='n' AND skytf-&gt; mpxyzdatas=')dzyDlzyb#zybvzy)^zy' order by id desc ; QUERY PLAN --------------------------------------------------------------------------------------------------------------------------------- Sort (cost=4.71..4.72 rows=1 width=226) (actual time=0.100..0.101 rows=1 loops=1) Sort Key: id Sort Method: quicksort Memory: 25kB -&gt; Index Scan using idx_mpxyzdatas on tmp_user (cost=0.00..4.70 rows=1 width=226) (actual time=0.088..0.090 rows=1 loops=1) Index Cond: ((mpxyzdatas)::text = ')dzyDlzyb#zybvzy)^zy'::text) Filter: (is_deleted = 'n'::bpchar) Total runtime: 0.131 ms (7 rows)Time: 0.459 ms 可以看出,plan 走了索引 idx_mpxyzdatas, 花费的 cost 大大降低，执行时间也仅为 0.459 ms。在测试环境上通过后，于是将方法三的索引加到生产环境下，慢查询消失。 总结 创建精简的索引对性能影响是很大的，一方面保证了查询的速度，另一方面降低了索引的维护成本，当表上有insert,update操作时，相比联合索引，维护代价更低。 引用一句德哥的话，在这节能俭排的年代，能够利用最小的代价实现最大的价值，何乐而不为呢。","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"全表扫描 Cost 代价计算方法","slug":"20110301163225","date":"2011-03-01T08:32:25.000Z","updated":"2018-09-04T01:33:48.473Z","comments":true,"path":"20110301163225.html","link":"","permalink":"https://postgres.fun/20110301163225.html","excerpt":"","text":"先看一份执行计划1234567skytf=&gt; explain (analyze on ,buffers on ) select * from test_15; QUERY PLAN ------------------------------------------------------------------------------------------------------------ Seq Scan on test_15 (cost=0.00..155.00 rows=10000 width=11) (actual time=0.009..6.160 rows=10000 loops=1) Buffers: shared hit=55 Total runtime: 11.662 ms (3 rows) 这是一个全表扫描的PLAN，cost=0.00..155.00, 然而，这个 cost=155 是如何计算得呢？这篇日志将针对全表扫描的前提下，讲解下 PLAN中 cost 的计算方法，在此之前，先看下与PLAN相关的系统参数。 执行计划相关参数seq_page_cost (floating point) Sets the planner is estimate of the cost of a disk page fetch that is part of a series of sequential fetches. The default is 1.0. 全表扫描时读取每个页面的代价值，默认为 1.0 cpu_tuple_cost (floating point) Sets the planner is estimate of the cost of processing each row during a query. The default is 0.01. 处理每行记录花费的代价，默认为 0.01 cpu_index_tuple_cost (floating point) Sets the planner is estimate of the cost of processing each index entry during an index scan. The default is 0.005. 每次索引查询进入索引处理的代价，默认为 0.005 cpu_operator_cost (floating point) Sets the planner is estimate of the cost of processing each operator or function executed during a query. The default is 0.0025. 执行计划 Cost 值验证删除原表 test_151skytf=&gt; drop table test_15 创建测试表 并插入数据1234skytf=&gt; create table test_15(id integer,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_15 select generate_series(1,10000),'francs'; INSERT 0 10000 表分析1234567skytf=&gt; analyze test_15; ANALYZEskytf=&gt; select pg_size_pretty(pg_relation_size('test_15')); pg_size_pretty ---------------- 440 kB (1 row) 查看下PLAN1234567skytf=&gt; explain (analyze on ,buffers on ) select * from test_15; QUERY PLAN ------------------------------------------------------------------------------------------------------------ Seq Scan on test_15 (cost=0.00..155.00 rows=10000 width=11) (actual time=0.009..6.160 rows=10000 loops=1) Buffers: shared hit=55 Total runtime: 11.662 ms (3 rows) 查询seq_page_cost，和 cpu_tuple_cost1234567891011121314skytf=&gt; SELECT skytf-&gt; relpages, skytf-&gt; current_setting('seq_page_cost') AS seq_page_cost, skytf-&gt; relpages * skytf-&gt; current_setting('seq_page_cost')::decimal AS page_cost, skytf-&gt; reltuples, skytf-&gt; current_setting('cpu_tuple_cost') AS cpu_tuple_cost, skytf-&gt; reltuples * skytf-&gt; current_setting('cpu_tuple_cost')::decimal AS tuple_cost skytf-&gt; FROM pg_class WHERE relname='test_15'; relpages | seq_page_cost | page_cost | reltuples | cpu_tuple_cost | tuple_cost ----------+---------------+-----------+-----------+----------------+------------ 55 | 1 | 55 | 10000 | 0.01 | 100 (1 row) 从上面可以看出，表 “test_15” 占用 55个 page, 共有 10000条记录，全表扫描的过程可以理解成 PG会扫描这张表上所有的页,并且处理页上的每条记录，所以COST可以根据以下公式来计算全表扫描 Cost= relpages seq_page_cost参数值 + reltuples cpu_tuple_cost参数值 所以上述 Plan的 cost =551 + 100000.01=155 附：SQL查询查询relpages,reltuples,和当前 seq_page_cost,cpu_tuple_cost 参数值。12345678SELECT relpages, current_setting('seq_page_cost') AS seq_page_cost, relpages * current_setting('seq_page_cost')::decimal AS page_cost, reltuples, current_setting('cpu_tuple_cost') AS cpu_tuple_cost, reltuples * current_setting('cpu_tuple_cost')::decimal AS tuple_cost FROM pg_class WHERE relname='test_15'; 总结以上实验只是针对 “Seqences Scan” 的方式计算 Cost, 其它扫描方式如索引扫描，表的其它连接方式的COST计算方法又不相同，这里不再实验了。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"浅谈 PostgreSQL 的柱状图","slug":"20110226204146","date":"2011-02-26T12:41:46.000Z","updated":"2018-09-04T01:33:48.427Z","comments":true,"path":"20110226204146.html","link":"","permalink":"https://postgres.fun/20110226204146.html","excerpt":"","text":"和Oracle 一样，PostgreSQL 的表也有柱状图, 用于保存表的列的统计信息。可以通过查询系统视图 pg_stats.histogram_bounds 来查看列的柱状图。 官网关于柱状图的描述Histogram_bounds A list of values that divide the column is values into groups of approximatelyequal population. The values in most_common_vals, if present, are omitted from thishistogram calculation. (This column is NULL if the column data type does not havea &lt; operator or if the most_common_vals list accounts for the entire population.) 上面这段英文对英文基础不好的人来说，可能有点费力;偶读了半天，也似懂非懂，但第一句话提供了重要信息, 说柱状图是指一系列的值将表的列值分成近似相同的组；这句话可能翻译得不太好，等下通过实验来了解一下，就明白了。histogram_bounds 的值受参数 default_statistics_target 控制。default_statistics_target 默认值为100，default_statistics_target参数值越大，那么PG搜集的列上的统计信息就越精确，当然在表做 Analyze 操作时花费的时间也稍长些。 参数 default_statistics_target 的描述 default_statistics_target (integer) Sets the default statistics target for table columns without a column-specifictarget set via ALTER TABLE SET STATISTICS. Larger values increase the time needed todo ANALYZE, butmight improve the quality of the planner is estimates. The default is For moreinformation on the use of statistics by the PostgreSQL query planner,refer to Section 14.2. 场景一: attstattarget 1000 时的场景pg_attribute.attstattarget 是指列上统计信息搜集指标，默认值为系统 default_statistics_target ，当attstattarget 值为 -1 时, 表示使用系统的 “default_statistics_target”值，关于这个参数详见官网文档。 查看默认参数值12345skytf=&gt; show default_statistics_target; default_statistics_target --------------------------- 1000 (1 row) 查看表 test_11 的表 id上的 attstattarget 参数1234567891011skytf=&gt; select oid,relname from pg_class where relname='test_11'; oid | relname ----------+--------- 14205208 | test_11 (1 row)skytf=&gt; select attrelid,attname,attstattarget from pg_attribute where attrelid=14205208 and attname=('id'); attrelid | attname | attstattarget ----------+---------+--------------- 14205208 | id | -1 (1 row) 备注：从上面看出，表 test_11 的列 id上的 attstattarget 参数为 -1 ，即使用了系统的参数 default_statistics_target 值 100. 创建测试表并测试数据1234skytf=&gt; create table test_11 (id integer ,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_11 select generate_series(1,1000),'francs'; INSERT 0 1000 表分析12skytf=&gt; analyze test_11; ANALYZE 查看表 test_11 列 id 上的柱状图1234567891011121314151617181920212223242526272829303132333435skytf=&gt; select histogram_bounds from pg_stats where tablename='test_11' and attname='id'; histogram_bounds-------------------------------------------------------------------- &#123;1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,4 7,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,9 1,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,12 6,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,15 9,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,19 2,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,22 5,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,25 8,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,29 1,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,32 4,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,35 7,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,39 0,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,42 3,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,45 6,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,48 9,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,52 2,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,55 5,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,58 8,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,62 1,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,65 4,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,68 7,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,72 0,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,75 3,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,78 6,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,81 9,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,85 2,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,88 5,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,91 8,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,95 1,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,98 4,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000&#125; (1 row) 备注：当default_statistics_target为 1000时，柱状图被分为 1000组。 场景二: attstattarget 100 时的场景更改表列上的attstattarget参数12skytf=&gt; alter table test_11 alter column id set STATISTICS 100; ALTER TABLE 验证下，attstattarget值已经改成100了12345skytf=&gt; select attrelid,attname,attstattarget from pg_attribute where attrelid=14205208 and attname=('id'); attrelid | attname | attstattarget ----------+---------+--------------- 14205208 | id | 100 (1 row) 表分析,当执行 analyze 命令后，看下结果12skytf=&gt; analyze test_11; ANALYZE 再次查看柱状图12345678910skytf=&gt; select histogram_bounds from pg_stats where tablename='test_11' and attname='id'; histogram_bounds ------------------------------------------------------------------------------------------------------------------------------------ ------------------------------------------------------------------------------------------------------------------------------------ &#123;1,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260,270,280,290,300,310,320,330,340,3 50,360,370,380,390,400,410,420,430,440,450,460,470,480,490,500,510,520,530,540,550,560,570,580,590,600,610,620,630,640,650,660,670,6 80,690,700,710,720,730,740,750,760,770,780,790,800,810,820,830,840,850,860,870,880,890,900,910,920,930,940,950,960,970,980,990,1000&#125; (1 row) 备注：表”test_11” 的列ID上的 histogram_bounds 已经被分成了100组。 场景三: attstattarget 10 时的场景更改表列上的attstattarget参数12skytf=&gt; alter table test_11 alter column id set STATISTICS 10; ALTER TABLE 验证下，attstattarget值已经改成10了12345skytf=&gt; select attrelid,attname,attstattarget from pg_attribute where attrelid=14205208 and attname=('id'); attrelid | attname | attstattarget ----------+---------+--------------- 14205208 | id | 10 (1 row) 表分析,当执行 analyze 命令后，看下结果12skytf=&gt; analyze test_11; ANALYZE 再次查看柱状图12345skytf=&gt; select histogram_bounds from pg_stats where tablename='test_11' and attname='id'; histogram_bounds ---------------------------------------------- &#123;1,100,200,300,400,500,600,700,800,900,1000&#125; (1 row) 备注：表”test_11” 的列ID上的 histogram_bounds 已经被分成了10 组。 结论 列上的柱状图信息可以通过 “alter table set STATISTICS “来更改，并通过视图 pg_attribute.attstattarget来查看列上的当前 statistics 值。 列上的柱状图被近似均匀的分割成 “pg_attribute.attstattarget” 组。即当 pg_attribute.attstattarget 值为 1000时，则列上的柱状图分为 1000组,当 pg_attribute.attstattarget 值为 100时， 则列上的柱状图分为 100组,当 pg_attribute.attstattarget 值为 10时， 则列上的柱状图分为 10组。 列上 STATISTICS 值越高，则柱状图分割的组越多，柱状图信息越详细 ,但做表分析时，所花的时间也稍长。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Using CONCURRENTLY without taking any locks when creating index","slug":"20110225152056","date":"2011-02-25T07:20:56.000Z","updated":"2018-09-04T01:33:48.364Z","comments":true,"path":"20110225152056.html","link":"","permalink":"https://postgres.fun/20110225152056.html","excerpt":"","text":"PostgreSQL 创建索引时通常会堵住 DML语句，如果给生产系统的大表加索引，可能加索引的过程会很长，那么在索引创建过程中，Application 的 insert,delete,update 语句会被 block，这对应用来说是非常沉痛的.幸运的是 PostgreSQL 在创建索引时，提供一个”CONCURRENTLY”选项，创建索引时， 使用这个选项则可以在线创建索引，同时又不会阻塞应用的 DML 语句，真是太棒了,下面这两段是介绍这个参数的,来自官网。 关于 CONCURRENTLY 选项 Creating an index can interfere with regular operation of a database. Normally PostgreSQLlocks the table to be indexed against writes and performs the entire index build with asingle scan of the table. Other transactions can still read the table, but if they try toinsert, update, or delete rows in the table they will block until the index build is finished.This could have a severe effect if the system is a live production database. Very large tablescan take many hours to be indexed, and even for smaller tables, an index build can lock outwriters for periods that are unacceptably long for a production system. PostgreSQL supports building indexes without locking out writes. This method is invoked byspecifying the CONCURRENTLY option of CREATE INDEX. When this option is used, PostgreSQLmust perform two scans of the table, and in addition it must wait for all existing transactionsthat could potentially use the index to terminate. Thus this method requires more total workthan a standard index build and takes significantly longer to complete. However, since it allowsnormal operations to continue while the index is built, this method is useful for adding newindexes in a production environment. Of course, the extra CPU and I/O load imposed by theindex creation might slow other operations. 创建索引语法12345678Command: CREATE INDEX Description: define a new index Syntax: CREATE [ UNIQUE ] INDEX [ CONCURRENTLY ] [ name ] ON table [ USING method ] ( &#123; column | ( expression ) &#125; [ opclass ] [ ASC | DESC ] [ NULLS &#123; FIRST | LAST &#125; ] [, ...] ) [ WITH ( storage_parameter = value [, ... ] ) ] [ TABLESPACE tablespace ] [ WHERE predicate ] 下面来做下测试，分别测试不带 “CONCURRENTLY” 参数和加上这个参数时的场景，具体步骤如下。 场景一：创建索引时不带”CONCURRENTLY”参数这里有张测试表，相关信息如下1234567891011121314151617181920212223242526skytf=&gt; select pg_size_pretty(pg_relation_size('my_table_201102')); pg_size_pretty ---------------- 1436 MB (1 row)skytf=&gt; \\d my_table_201102 Table \"skytf.my_table_201102\" Column | Type | Modifiers -------------+-----------------------------+----------- create_time | timestamp without time zone | not null log_time | timestamp without time zone | user_id | character varying(32) | action | character varying(128) | app_id | character varying(32) | result | numeric(10,0) | server_id | character varying(32) | arg1 | character varying(128) | arg2 | character varying(128) | arg3 | character varying(128) | arg4 | character varying(128) | username | character varying(64) | clientip | character varying(32) | Indexes: \"idx_my_table_201102_create_time\" btree (create_time) Inherits: my_table session一: 执行创建索引语句1skytf=&gt; create index idx_my_table_201102_action on my_table_201102 using btree (action); 备注：这张表有1G多, 这个SESION正在执行，还没跑会。 session二：向表中插入记录1skytf=&gt; insert into my_table_201102 (create_time,action) values (now(),'test'); 此时发现 session二正处于等侍状态，说明 insert 操作已经被 session一阻塞了。 场景二：创建索引时加上”CONCURRENTLY”参数先删除之前创建的索引12skytf=&gt; drop index idx_my_table_201102_action; DROP INDEX session一: 执行创建索引操作1skytf=&gt; create index CONCURRENTLY idx_my_table_201102_action on my_table_201102 using btree (action); 备注：表有1G多, 这个SESION正在执行，还没跑会 session二: 向这个表中插入记录12skytf=&gt; insert into my_table_201102 (create_time,action) values (now(),'test_b'); INSERT 0 1 备注，当session一还在创建索引的过程中，立即新开一个 session二向些表中插入记录，发现插入动作可以立即执行下去，说明创建索引时带参数 “CONCURRENTLY”不会阻塞 DML语句。(DELETE 操作就没测试了,有兴趣的朋友可以测试下。) 比较两种场景索引创建时间不带 “CONCURRENTLY” 参数123skytf=&gt; create index idx_my_table_201102_action on my_table_201102 using btree (action); CREATE INDEX Time: 46094.907 ms 带 “CONCURRENTLY” 参数123skytf=&gt; create index CONCURRENTLY idx_my_table_201102_action on my_table_201102 using btree (action); CREATE INDEX Time: 55521.221 ms 由于带”CONCURRENTLY” 创建索引时需要做更多的维护工作，所以耗时稍微长些。 Reindex 用法12345skytf=&gt; \\h reindex Command: REINDEX Description: rebuild indexes Syntax: REINDEX &#123; INDEX | TABLE | DATABASE | SYSTEM &#125; name [ FORCE ] 备注：但 Reindex 命令却不提供 “CONCURRENTLY” 参数。 总结 生产系统给表加索引时，建议使用”CONCURRENTLY”属性，对生产产生最小影响。 在使用”CONCURRENTLY”属性时，由于 PG需要做更多的内部操作，所以耗时稍微长些。 在使用”CONCURRENTLY”属性时，如果创建索引途中因为 “uniqueness violation” 或会话中断等原因创建失败，则这个索引需要删除重新创建。 Reindex 命令不提供 “CONCURRENTLY” 参数，生产系统在执行 Reindex 时还需谨慎。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"惊奇发现 PosgresSQL btree 索引可以存储空值","slug":"20110225121502","date":"2011-02-25T04:15:02.000Z","updated":"2018-09-04T01:33:48.317Z","comments":true,"path":"20110225121502.html","link":"","permalink":"https://postgres.fun/20110225121502.html","excerpt":"","text":"今天查阅了PG官网关于索引的知识，上面介绍到了 PG 的 btree 索引可以存储空值，觉得比较奇怪，因为之前了解到 Oracle btree 索引是不存储空值的，下面特做了以下测试，具体如下。 场景: 测试索引是否存储空值。创建测试表12skytf=&gt; create table test_7(id integer , name varchar(32)); CREATE TABLE 创建索引12skytf=&gt; create index idx_test_7_name on test_7 using btree (name); CREATE INDEX 查询索引初始大小12345skytf=&gt; select pg_size_pretty(pg_relation_size('idx_test_7_name')); pg_size_pretty ---------------- 8192 bytes (1 row) 插入测试数据1234567891011skytf=&gt; insert into test_7(id) select generate_series(1,100000); INSERT 0 100000skytf=&gt; select * from test_7 limit 5; id | name ----+------ 1 | 2 | 3 | 4 | 5 | (5 rows) 再次查看索引大小12345skytf=&gt; select pg_size_pretty(pg_relation_size('idx_test_7_name')); pg_size_pretty ---------------- 2768 kB (1 row) 说明：上面明显地看出，在向表 test_7 插入数据后，索引也随着增大了，说明索引能够存储空值。 再插入10条数据12skytf=&gt; insert into test_7 select generate_series(1,10),'francs'; INSERT 0 10 查看PLAN skytf=&gt; explain select count(*) from test_7 where name=&apos;francs&apos;; QUERY PLAN ------------------------------------------------------------------------------------ Aggregate (cost=4.28..4.29 rows=1 width=0) -&gt; Index Scan using idx_test_7_name on test_7 (cost=0.00..4.27 rows=1 width=0) Index Cond: ((name)::text = &apos;francs&apos;::text)(3 rows) skytf=&gt; explain select count(*) from test_7 where name is not null ; QUERY PLAN ------------------------------------------------------------------------------------ Aggregate (cost=4.28..4.29 rows=1 width=0) -&gt; Index Scan using idx_test_7_name on test_7 (cost=0.00..4.27 rows=1 width=0) Index Cond: (name IS NOT NULL) (3 rows) 从上面可以看出，空值也能走索引。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Hash 索引创建奇慢","slug":"20110224170924","date":"2011-02-24T09:09:24.000Z","updated":"2018-09-04T01:33:48.255Z","comments":true,"path":"20110224170924.html","link":"","permalink":"https://postgres.fun/20110224170924.html","excerpt":"","text":"今天学习了下 Hash 索引，在了解HASH索引的应用场合后，打算实践一下，但在创建过程中奇慢，有个测试表才 1.5G 大，在创建了两小时候还没结束，而且系统上也没有锁。于是猜测 HASH 索引创建应该比 btree 索引创建慢得多，下面是做的一些测试，用来比较不同数据量环境下创建 btree 索引和 HASH 索引的速度。 机器配置8 CPU 8G 内存OS Red Hat Enterprise Linux Server release 5.5 场景一：创建表test_a, 插入1000条数据12345678910skytf=&gt; create table test_a (id integer,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_a select generate_series(1,1000),'francs'; INSERT 0 1000skytf=&gt; create index idx_btree_test_a_name on test_a using btree (name); CREATE INDEX Time: 35.931 msskytf=&gt; create index idx_hash_test_a_name on test_a using hash (name); CREATE INDEX Time: 2.578 ms 在 1000条数据下，创建 btree索引花了 35.9 ms , 创建 hash索引花了2.6ms 场景二：创建表 test_b, 插入1万条数据12345678910skytf=&gt; create table test_b (id integer,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_a select generate_series(1,10000),'francs'; INSERT 0 10000skytf=&gt; create index idx_btree_test_b_name on test_b using btree (name); CREATE INDEX Time: 56.990 msskytf=&gt; create index idx_hash_test_b_name on test_b using hash (name); CREATE INDEX Time: 56.261 ms 在 1万 条数据下，创建 btree索引花了 56.9ms , 创建 hash索引花了 56.2 ms 场景三：创建表 test_c, 插入10万条数据12345678910skytf=&gt; create table test_c (id integer,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_a select generate_series(1,100000),'francs'; INSERT 0 100000skytf=&gt; create index idx_btree_test_c_name on test_c using btree (name); CREATE INDEX Time: 181.033 msskytf=&gt; create index idx_hash_test_c_name on test_c using hash (name); CREATE INDEX Time: 4643.801 ms 在 10万 条数据下，创建 btree索引花了 181.0 ms , 创建 hash索引花了 4643 ms 场景四：创建表 test_d, 插入100万条数据12345678910skytf=&gt; create table test_d (id integer,name varchar(32)); CREATE TABLEskytf=&gt; insert into test_d select generate_series(1,1000000),'francs'; INSERT 0 1000000skytf=&gt; create index idx_btree_test_d_name on test_d using btree (name); CREATE INDEX Time: 1909.001 msskytf=&gt; create index idx_hash_test_d_name on test_d using hash (name); CREATE INDEX Time: 591817.814 ms 在 100万 条数据下，创建 btree索引花了 1909.0 ms , 创建 hash索引花了 591817 ms 在这种情况下，创建HASH索引明显很慢，创建HASH索引的时间是创建btree 索引的近 31倍。 四种场景时间统计 表名 记录数 创建 Btree 索引耗时(ms) 创建 Hash 索引耗时 (ms) table_a 1000 35.9 2.6 table_b 1万 56.9 56.2 table_c 10万 181 4643 table_d 100万 1909 591817 总结 因为 hash 索引创建消耗的时间很长，可以预计当表达到 1G 以上，创建 HASH 索引将会变得非常艰难，保守估计，创建 hash 索引花费的时间至少是 创建btree 所花费的时间 20 倍以上。 尽管 Hash 索引适用于特定场合，但创建很慢，推测创建 HASH索引后，此索引维护代价(指删入操作)非常大，所以不建议使用到生产库中。 至于在特定场合 Hash 索引是否比 btree 索引高效，还有侍验证。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PG Hot Standby 从库宕机案例","slug":"20110223124951","date":"2011-02-23T04:49:51.000Z","updated":"2018-09-04T01:33:48.208Z","comments":true,"path":"20110223124951.html","link":"","permalink":"https://postgres.fun/20110223124951.html","excerpt":"","text":"今天在一台主机上创建数据库时犯了一个错误，导致 standby 库 PG Server 宕掉，具体如下。 背景今天业务需求，需要在原有主机上新增一个业务库，并分配新的表空间。已经有的主机上已经做了 PostgreSQL Hot standby 高可用。当时由于疏忽，忘记了重要的操作,下面是具体过程。 主库上创始建用户12CREATE ROLE mydb LOGIN ENCRYPTED PASSWORD '*' nosuperuser noinherit nocreatedb nocreaterole ; 主库上创建表空间12mkdir -p /database/pgdata/pg_tbs/tbs_mydb create tablespace tbs_mydb owner mydb LOCATION '/database/pgdata/pg_tbs/tbs_mydb'; 主库上创建数据库12345CREATE DATABASE mydb WITH OWNER = mydb TEMPLATE = template0 ENCODING = 'UTF8' TABLESPACE = tbs_mydb; 从库上验证在主库上创建好数据库后, 准备到从库上观察新建的库是否已经同步到备库。此时，发现PostgreSQL已经宕机了，当时还以为是谁停掉的呢，接下来查看日志。 从库CSV日志1232011-02-23 11:10:25.909 CST,,,6591,,4d35195a.19bf,12,,2011-01-18 12:38:50 CST,1/0,0,FATAL,58P01,\"directory \"\"/database/pgdata/pg_tbs/tbs_mydb\"\" does not exist\",,,,,\"xlog redo create ts: 3046144 \"\"/database/pgdata/pg_tbs/tbs_mydb\"\"\",,,,\"\" 2011-02-23 11:10:26.050 CST,,,6589,,4d351959.19bd,2,,2011-01-18 12:38:49 CST,,0,LOG,00000,\"startup process (PID 6591) exited with exit code 1\",,,,,,,,,\"\" 2011-02-23 11:10:26.050 CST,,,6589,,4d351959.19bd,3,,2011-01-18 12:38:49 CST,,0,LOG,00000,\"terminating any other active server processes\",,,,,,,,,\"\" 重要信息/database/pgdata/pg_tbs/tbs_mydb, 原来是自己在主库上创建表空间目录时，忘记在备机上创建相同的目录了,这么重要的一点居然忘记，汗，大意了, 想到库刚停不久，新建好这个目录，并启动备库，备库应该能赶得上主库。 解决方法在从库上创建目录 /database/pgdata/pg_tbs/tbs_mydb, 并启动 PG Server。 再次查看 CSV日志123456789101112132011-02-23 11:38:09.981 CST,,,15156,,4d648121.3b34,1,,2011-02-23 11:38:09 CST,,0,LOG,00000,\"database system was interrupted while in recovery at log time 2011-02-23 10:59:21 CST\",,\"If this has occurred more than once some data might be corrupted and you might need to choose an earlier recovery target.\",,,,,,,\"\" 2011-02-23 11:38:09.981 CST,,,15156,,4d648121.3b34,2,,2011-02-23 11:38:09 CST,,0,LOG,00000,\"could not open tablespace directory \"\"pg_tblspc/16386/PG_9.0_201004261\"\": No such file or directory\",,,,,,,,,\"\" 2011-02-23 11:38:09.981 CST,,,15156,,4d648121.3b34,3,,2011-02-23 11:38:09 CST,,0,LOG,00000,\"could not open tablespace directory \"\"pg_tblspc/16394/PG_9.0_201004261\"\": No such file or directory\",,,,,,,,,\"\" 2011-02-23 11:38:09.982 CST,,,15156,,4d648121.3b34,4,,2011-02-23 11:38:09 CST,,0,LOG,00000,\"could not open tablespace directory \"\"pg_tblspc/19507/PG_9.0_201004261\"\": No such file or directory\",,,,,,,,,\"\" 2011-02-23 11:38:09.982 CST,,,15156,,4d648121.3b34,5,,2011-02-23 11:38:09 CST,,0,LOG,00000,\"could not open tablespace directory \"\"pg_tblspc/16392/PG_9.0_201004261\"\": No such file or directory\",,,,,,,,,\"\" 2011-02-23 11:38:09.982 CST,,,15156,,4d648121.3b34,6,,2011-02-23 11:38:09 CST,,0,LOG,00000,\"could not open tablespace directory \"\"pg_tblspc/16396/PG_9.0_201004261\"\": No such file or directory\",,,,,,,,,\"\" 2011-02-23 11:38:09.982 CST,,,15156,,4d648121.3b34,7,,2011-02-23 11:38:09 CST,,0,LOG,00000,\"could not open tablespace directory \"\"pg_tblspc/17048/PG_9.0_201004261\"\": No such file or directory\",,,,,,,,,\"\" 2011-02-23 11:38:09.982 CST,,,15156,,4d648121.3b34,8,,2011-02-23 11:38:09 CST,,0,LOG,00000,\"entering standby mode\",,,,,,,,,\"\" 2011-02-23 11:38:10.054 CST,,,15156,,4d648121.3b34,9,,2011-02-23 11:38:09 CST,1/0,0,LOG,00000,\"redo starts at 60/ADC26678\",,,,,,,,,\"\" 2011-02-23 11:38:10.347 CST,,,15156,,4d648121.3b34,10,,2011-02-23 11:38:09 CST,1/0,0,LOG,00000,\"consistent recovery state reached at 60/AF50F140\",,,,,,,,,\"\" 2011-02-23 11:38:10.347 CST,,,15154,,4d648121.3b32,1,,2011-02-23 11:38:09 CST,,0,LOG,00000,\"database system is ready to accept read only connections\",,,,,,,,,\"\" 2011-02-23 11:38:10.387 CST,,,15156,,4d648121.3b34,11,,2011-02-23 11:38:09 CST,1/0,0,LOG,00000,\"invalid record length at 60/AF51E888\",,,,,,,,,\"\" 2011-02-23 11:38:10.393 CST,,,15160,,4d648122.3b38,1,,2011-02-23 11:38:10 CST,,0,LOG,00000,\"streaming replication successfully connected to primary\",,,,,,,,,\"\" 信息streaming replication successfully connected to primary说明库已经恢复正常。 这次还好是备库菪机，对业务没有影响，而且发现及时，如果发现晚了，从库可能要重做,好险。生产库上操作一定要谨慎，谨慎，再谨慎。 查看同步情况123456789postgres=# \\l List of databases Name | Owner | Encoding | Collation | Ctype | Access privileges --------------+--------------+----------+-----------+-------+----------------------------- postgres | postgres | UTF8 | C | C | template0 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | C | C | =c/postgres +mydb | mydb | UTF8 | C | C | 总结在PG主从模式环境下，新增业务库的方法如下 在主库上创建用户。 在主库主机上创建表空间目录。 在从库主机上创建与主库上相同表空间目录。( 非常重要 ) 在主库上创建表空间 在主库上创建数据库。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"Optimize a query by using Partial Index","slug":"20110222203042","date":"2011-02-22T12:30:42.000Z","updated":"2018-09-04T01:33:48.145Z","comments":true,"path":"20110222203042.html","link":"","permalink":"https://postgres.fun/20110222203042.html","excerpt":"","text":"今天通过查看数据库日志，有个核心生产库有条SQL执行在800 ms 以上,这引起了我的注意。 1 查看CSVLOG，有大量如下信息。186314,0,LOG,00000,\"duration: 819.900 ms execute &lt;unnamed&gt;: select count(recv_id) from skytf.my_invite_rec where recv_id=$1 and result=2\",\"parameters: $1 = '125662840'\",,,,,,,,\"\" 2 sql 语句1select count(recv_id) from skytf.my_invite_rec where recv_id=$1 and result=2 3 表结构1234567891011121314151617skytf=&gt; \\d my_invite_rec Table \"skytf.my_invite_rec\" Column | Type | Modifiers -------------+-----------------------------+-------------------- invite_id | numeric(12,0) | not null invite_time | timestamp without time zone | user_id | numeric(10,0) | not null recv_id | numeric(10,0) | not null invite_msg | character varying(512) | appinfo | character varying(32) | extinfo | character varying(128) | group_id | numeric(12,0) | result | numeric(10,0) | not null default 2 Indexes: \"pk_my_invite_rec\" PRIMARY KEY, btree (invite_id) \"idx_my_invite_rec_recvid\" btree (recv_id) \"idx_my_invite_rec_userid\" btree (user_id) 4 老的执行计划1234567891011121314skytf=&gt; explain analyze select count(recv_id) from skytf.my_invite_rec where recv_id=125662840 and result=2; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ --------------------- Aggregate (cost=42015.40..42015.41 rows=1 width=10) (actual time=739.196..739.196 rows=1 loops=1) -&gt; Bitmap Heap Scan on my_invite_rec (cost=5039.49..41753.20 rows=104879 width=10) (actual time=739.190..739.190 rows=0 loops=1) Recheck Cond: (recv_id = 125662840::numeric) Filter: (result = 2::numeric) -&gt; Bitmap Index Scan on idx_my_invite_rec_recvid (cost=0.00..5013.27 rows=329714 width=0) (actual time=213.024..213.024 rows=330385 loops=1) Index Cond: (recv_id = 125662840::numeric) Total runtime: 739.243 ms (7 rows)Time: 739.800 ms 从执行计划来看，PLAN正常。唯独 rows=330385 比较高. 5 通过查看数据库 csvlog 日志发现有两个 recv_id的出现的频率比较高1234[postgres@skytf1](mailto:postgres@skytf1)-&gt; cat postgresql-2011-02-20_000000.csv | grep 125662840 | wc -l 7749 [postgres@skytf1](mailto:postgres@skytf1)-&gt; cat postgresql-2011-02-20_000000.csv | grep 148161000 | wc -l 6060 6 随机取些recv_id,测试下这条SQL的时间1234567891011121314151617181920212223242526272829303132333435363738394041skytf=&gt; select recv_id from my_invite_rec where result=2 limit 10; recv_id ----------- 103103900 106460840 153989140 153099420 335932 132509748 110848636 101857296 107371328 135262800 (10 rows)Time: 0.556 ms skytf=&gt; select count(*) from my_invite_rec where recv_id=103103900; count ------- 5 (1 row) Time: 0.681 ms skytf=&gt; select count(*) from my_invite_rec where recv_id=107371328; count ------- 28 (1 row)Time: 0.668 msskytf=&gt; explain analyze select count(*) from my_invite_rec where recv_id=103103900; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ -------------- Aggregate (cost=95.78..95.79 rows=1 width=0) (actual time=0.058..0.058 rows=1 loops=1) -&gt; Index Scan using idx_my_invite_rec_recvid on my_invite_rec (cost=0.00..95.66 rows=48 width=0) (actual time=0.044..0.055 rows=5 loops=1) Index Cond: (recv_id = 103103900::numeric) Total runtime: 0.093 ms (4 rows)Time: 0.773 ms 从上面可以看出，SQL都在 1 ms 以下，非常的迅速啊，猜想可能与记录数有关。 7 猜想先前的两个recv_id的记录数比较多，查询如下12345678910111213skytf=&gt; select count(*) from my_invite_rec where recv_id=125662840; count -------- 332424 (1 row)Time: 804.972 ms skytf=&gt; select count(*) from my_invite_rec where recv_id=148161000; count -------- 416366 (1 row)Time: 850.757 ms 果然如此，是因为这两个 recv_id 对应的数据比较多，PG花了很长的时间寻找符合条件记录。 8 后来在德哥的指导下，创建联合索引12skytf=&gt; _create index idx_my_invite_rec_result on my_invite_rec (result,recv_id) where recv_id in (125662840,148161000);_CREATE INDEX Time: 4949.886 ms 9 再次查看PLAN1234567891011121314151617181920skytf=&gt; explain analyze select count(recv_id) from skytf.my_invite_rec where recv_id=125662840 and result=2; QUERY PLAN ------------------------------------------------------------------------------------------------------------------------------------ ------------ Aggregate (cost=39107.39..39107.40 rows=1 width=10) (actual time=0.032..0.032 rows=1 loops=1) -&gt; Bitmap Heap Scan on my_invite_rec (cost=1871.45..38847.11 rows=104111 width=10) (actual time=0.030..0.030 rows=0 loops=1) Recheck Cond: ((result = 2::numeric) AND (recv_id = 125662840::numeric)) -&gt; Bitmap Index Scan on idx_my_invite_rec_result (cost=0.00..1845.42 rows=104111 width=0) (actual time=0.028..0.028 rows=0 loops=1) Index Cond: ((result = 2::numeric) AND (recv_id = 125662840::numeric)) Total runtime: 0.079 ms (6 rows)Time: 1.075 msskytf=&gt; select count(recv_id) from skytf.my_invite_rec where recv_id=148161000 and result=2; count ------- 0 (1 row)Time: 0.625 ms 执行时间由 739 ms 优化成现在的 1 ms , 速度提高了近 739倍。 总结 联合索引在上述这种SQL语句下显得非常的有效。 查看执行计划，需要特别注意 PLAN中 rows的值，通常较大的 rows 值意味着查询较慢。","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"Optimize","slug":"Optimize","permalink":"https://postgres.fun/tags/Optimize/"}]},{"title":"脚本建库导致 Oracle 组件未安装故障案例","slug":"20110218203813","date":"2011-02-18T12:38:13.000Z","updated":"2018-09-04T01:33:48.098Z","comments":true,"path":"20110218203813.html","link":"","permalink":"https://postgres.fun/20110218203813.html","excerpt":"","text":"这篇记录也是以前在做数据库巡检时，客户数据库的一个故障，觉得不错，记录分享到博客里。 问题描述今天数据库应用人员在编译存过时,报以下错误 PACKAGE BODY TXCZ10.BILLELEMENT_TYPE 编译错误错误：1PLS-00201: 必须说明标识符 'XMLDOM.DOMDOCUMENT'行：5 应用影响 系统关键应用不可用。 背景 由于客户数据库有坏块，后来不得已帮客户重新创建了一个数据库，至于原库坏块的详细信息，详见我的上篇博文 记一次 ORA-600 [12700] 故障案例,由于使用 dbca 图形化界面创建数据库时遇到问题，网上找了半天也没有可行方法，后来通过脚本建库的。手工建库的脚本以下仅列出建库的SQL，和导入的SQL，其它信息省略 建库脚本12345678910111213141516171819202122232425CREATE DATABASE myschema CONTROLFILE REUSE USER SYS IDENTIFIED BY myschema155 USER SYSTEM IDENTIFIED BY myschema155 LOGFILE GROUP 1 ('/dev/vg01/rlvredolog11', '/dev/vg01/rlvredolog12') SIZE 512M, GROUP 2 ('/dev/vg01/rlvredolog21', '/dev/vg01/rlvredolog22') SIZE 512M, GROUP 3 ('/dev/vg01/rlvredolog31', '/dev/vg01/rlvredolog32') SIZE 512M, GROUP 4 ('/dev/vg01/rlvredolog41', '/dev/vg01/rlvredolog42') SIZE 512M, GROUP 5 ('/dev/vg01/rlvredolog51', '/dev/vg01/rlvredolog52') SIZE 512M, GROUP 6 ('/dev/vg01/rlvredolog61', '/dev/vg01/rlvredolog62') SIZE 512M MAXLOGFILES 6 MAXLOGMEMBERS 5 MAXLOGHISTORY 1 MAXDATAFILES 1000 MAXINSTANCES 1 CHARACTER SET UTF8 NATIONAL CHARACTER SET UTF8 DATAFILE '/dev/vg01/rmyschema_system_01.dbf' SIZE 3130M AUTOEXTEND OFF SYSAUX DATAFILE '/dev/vg01/rmyschema_sysaux_01.dbf' SIZE 3130M AUTOEXTEND OFF DEFAULT TEMPORARY TABLESPACE temp TEMPFILE '/dev/vg01/rmyschema_temp_01.dbf' SIZE 5140M AUTOEXTEND OFF UNDO TABLESPACE UNDOTBS1 DATAFILE '/dev/vg01/rmyschema_undo_01.dbf' SIZE 10240M AUTOEXTEND OFF; 导入数据字典123@?/rdbms/admin/catalog.sql; @?/rdbms/admin/catproc.sql; @?/rdbms/admin/pupbld.sql; 根据 ORA 报错信息，猜测可能是组件不正常。 查询新库和老库组件4.1 新库(myschema)组件Oracle9i CataLog Views 9.2.0.8.0Oracle9i Package and Type 9.2.0.8.0 4.2 原库(myschema)组件(老库还在的，新库建好后，暂时没删掉。)123456789101112131415161718192021222324252627282930313233343515:34:02 [SYS@myschema](mailto:SYS@myschema)&gt; select comp_name, status, substr(version,1,10) as version from dba_registry;COMP_NAME STATUS VERSION ------------------------------------------------------------------------------------------------------------------------------- ----------------------------------------------------------------------------------------------- ----------- ------------------- Oracle9i Catalog Views VALID 9.2.0.7.0 Oracle9i Packages and Types VALID 9.2.0.7.0 Oracle Workspace Manager VALID 9.2.0.1.0 JServer JAVA Virtual Machine VALID 9.2.0.7.0 Oracle XDK for Java VALID 9.2.0.9.0 Oracle9i Java Packages VALID 9.2.0.7.0 Oracle interMedia VALID 9.2.0.7.0 Spatial VALID 9.2.0.7.0 Oracle Text VALID 9.2.0.7.0 Oracle XML Database VALID 9.2.0.7.0 Oracle Ultra Search VALID 9.2.0.7.0 Oracle Data Mining VALID 9.2.0.7.0 OLAP Analytic Workspace UPGRADED 9.2.0.7.0 Oracle OLAP API UPGRADED 9.2.0.7.0 OLAP Catalog VALID 9.2.0.7.0 从上面可以清楚地看到，通过脚本手工创建的数据库少装了很多组件，现在考虑如何装组件 1 JServer JAVA Virtual Machine 2 Oracle XML Database 如果问题不能快速解决,都打算暂时迁回到老库去(将新库数据导入)。 解决步骤后来在老大的指导下，通过以下步骤解决停止实例。停listener启动1个实例。12345678910111213141516171819202122232425262728connect / as sysdba startup mount alter system set \"_system_trig_enabled\" = false scope=memory; alter database open;-- if jvm not installed,install jvm @?/javavm/install/initjvm.sql-- install XML (xdk for java) @?/xdk/admin/initxml.sql select comp_id,comp_name,version,status from dba_registry; SQL&gt; select comp_name, status, substr(version,1,10) as version 2 from dba_registry;COMP_NAME -------------------------------------------------------------------------------- STATUS VERSION ---------------------- ---------------------------------------- Oracle9i Catalog Views VALID 9.2.0.8.0Oracle9i Packages and Types VALID 9.2.0.8.0JServer JAVA Virtual Machine VALID 9.2.0.8.0COMP_NAME -------------------------------------------------------------------------------- STATUS VERSION ---------------------- ---------------------------------------- Oracle XDK for Java VALID 9.2.0.10.0 新装组件后,应用重新编译存过通过,然后通知应用人员进行了系统测试，测试正常。","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"记一次 ORA-600 [12700] 故障案例","slug":"20110218201904","date":"2011-02-18T12:19:04.000Z","updated":"2018-09-04T01:33:48.036Z","comments":true,"path":"20110218201904.html","link":"","permalink":"https://postgres.fun/20110218201904.html","excerpt":"","text":"这篇记录是以前做数据库巡检项目时遇到的，今天记录到博客上当作经验分享。 09年12月31日下午3：30左右，在对 myschema 用户做EXP导出时有ORA-00600报错，并会生成一跟踪报错文件，随后每次导出均有此错误产生，详细如下： Exp 脚本1234567891011121314151617181920212223242526272829303132exp [system/system@txczyy](mailto:system/system@txczyy) owner=myschema statistics=Y rows=N file='/arch/tf/1.dmp' log='/arch/tf/exp.log'exp.log . . exporting table WF_PASS . . exporting table WF_STATE . . exporting table WF_STATE_VALUE . . exporting table WF_TASK_EXECUTOR . . exporting table WF_TASK_GRANTER . . exporting table WF_TASK_TERM . . exporting table WF_TEMPLATE . . exporting table WF_VARIABLE . . exporting table WF_VARIABLE_VALUE . exporting synonyms . exporting view EXP-00008: ORACLE error 600 encountered ORA-00600: internal error code, arguments: [12700], [2], [4206885], [26], [0], [24], [], [] EXP-00056: ORACLE error 1403 encountered ORA-01403: no data found EXP-00000: Export terminated unsuccessfullyalert.log Wed Jan 6 22:02:01 2010 Errors in file /oracle/app/admin/txczyy/udump/txczyy_ora_917632.trc: ORA-00600: internal error code, arguments: [12700], [2], [4206885], [26], [0], [24], [], [] Wed Jan 6 22:02:02 2010 Errors in file /oracle/app/admin/txczyy/udump/txczyy_ora_1249630.trc: ORA-00600: 内部错误代码，参数: [12700], [2], [4206885], [26], [0], [24], [], [] Wed Jan 6 22:07:34 2010trace file ksedmp: internal or fatal error ORA-00600: internal error code, arguments: [12700], [2], [4206885], [26], [0], [24], [], [] Current SQL statement for this session: SELECT TOWNER, TNAME, NAME, LENGTH, PRECISION, SCALE, TYPE, ISNULL, CONNAME, COLID, INTCOLID, SEGCOLID, COMMENT$, DEFAULT$, DFLTLEN, ENABLED, DEFER, FLAGS, COLPROP, ADTNAME, ADTOWNER, CHARSETID, CHARSETFORM, FSPRECISION, LFPRECISION, CHARLEN, TFLAGS FROM SYS.EXU8COL WHERE TOBJID = :1 ORDER BY INTCOLID ----- Call Stack Trace ----- 1.开SR 3-1284405961 METALINK上分析trc发现在查询对象46907时出错,笔记如下1234567891011121314151617 Generic Note ------------ Hi,Trace file shows value of the bind variable as:bind 0: dty=2 mxl=22(22) mal=00 scl=00 pre=00 oacflg=00 oacfl2=0 size=24 offset=0 bfp=1104bb330 bln=22 avl=04 flg=05 value=46907 End of cursor dump * dump of cursor xsc=1104dbdd8 Action Plan ========= Please check the value of the following query:SELECT status, object_id, object_type, owner||'.'||object_name \"OWNER.OBJECT\" FROM dba_objects WHERE status != 'VALID' or object_id=46907 ORDER BY 4,2;Regards Anudeep GCS 2 既然ORACLE分析绑定变量46907有问题,而出错的sql里也有个绑定变量,所以尝试将46907代到以下sql执行试试 ,经查ID为46907的对象，是myschema用户下的视图： myschema.v_cp_av_data,执行以下sql(将TOBJID值改成46907 ,trace文件里出错的sql)1234SELECT TOWNER, TNAME, NAME, LENGTH, PRECISION, SCALE, TYPE, ISNULL, CONNAME, COLID, INTCOLID, SEGCOLID, COMMENT$, DEFAULT$, DFLTLEN, ENABLED, DEFER, FLAGS, COLPROP, ADTNAME, ADTOWNER, CHARSETID, CHARSETFORM, FSPRECISION, LFPRECISION, CHARLEN, TFLAGS FROM SYS.EXU8COL WHERE TOBJID = 46907 ORDER BY INTCOLID 查询此视图,抛出以下错误：1ORA-00600: 内部错误代码，参数: [12700], [2], [4206885], [26], [0], [24], [], [] 说明,问题就出现在这个视图上,当Oracle在查询SYS.EXU8COL时出错,发现SYS.exu8col是个视图 3.即然这张view有问题,于是尝试重建视图 myschema.v_cp_av_data，看是否有效果12345616:19:10 [SYS@txczyy](mailto:SYS@txczyy)&gt; drop view myschema.v_cp_av_data; drop view myschema.v_cp_av_data * 第 1 行出现错误: ORA-00604: 递归 SQL 层 1 出现错误 ORA-08102: 未找到索引关键字，obj# 46，dba 4249996 (2) 4.查看obj# 46对象为col$表的i_col2索引 5.重建view行不通,于是,打算重建view引用相关表上的索引 即TABLE:myschema.CP_VOUCHER,依然行不通 6.在经过以上操作行不通后,于是怀疑系统表col$有问题, 6.1 select * from col$ 正常12345678910116.2 31:30 [SYS@txczyy](mailto:SYS@txczyy)&gt; select obj#,col# from col$ where obj#=46907; OBJ# COL# ---------- ---------- 46907 1 46907 2 46907 3 46907 4 46907 5 46907 6 46907 7已选择7行。 6.3 select /*+ index(c i_col2) */ * from col$ c where obj#=46907;123ERROR: ORA-00600: internal error code, arguments: [12700], [2], [4206885], [26], [4249996], [25], [], [] 说明,查询走索引 i_col2时,ORACLE正好报这个600错误,问题就是col$表 和 i_col2索引的交叉引用有问题，12ORA-600 [12700]是索引项中的ROWID指向的数据没有， ORA-8102 是表中的某条数据没有对应的索引项。 7 尝试重建系系统表col$上的索引i_col212345SQL&gt; drop index i_col2; drop index i_col2 * ERROR at line 1: ORA-00701: object necessary for warmstarting database cannot be altered 8 metalink回复1234567891011 Generic Note ------------ Hi,I am Balaji and I Have reviewed the SR as per your request.Looking into the SR I understand that , the index I_COL2 which is created on the table COL$ is corrupted.Unfortunately , the object is a bootstrap segment , which cannot be modified using the sql scrpts.The best option here is to perform the restore / recovery from the last good backup .In case this is not possible then the only option way ahead is to perform an export of the entire database and import it.You can go for schema level export.Regards Balaji 由于故障根源为系统表col$的rowid 和其上的 i_col2索引标识的rowid不匹配，而引起“数据块的”损坏，要清出此“坏块”必须重建此索引，但由于此表为系统表，如对其做重建操作，很可能发生意想不到的事故，故推荐用以下方案来解决此问题。方案1.重新建立一数据库，然后将当前故障库中有用数据导入新建库中，而将此故障库保留并试图对索引做重建操作来解决。方案2.停机并将故障库数据完全拷贝至其它相应服务器恢复测试。 9 方案比较 方案1: 保守方案,风险小,速度快（目前数据量较小）; 历时短 方案2: 需要对Oracle系统表操作,首先在测试库上执行,风险评估可行方在生产库上执行,历时长; 10 最后采用方案一新建DB,并导入TXCZ10用户的数据,问题解决 附件一 view myschema.v_cp_av_data object_id=46907123create or replace view myschema.v_cp_av_data as select cp.org_code,cp.nd,cp.process_inst_id,cp.co_code, cp.org_money, cp.cur_money, cp.fund_code from cp_voucher cp where cp.is_sum_cp_voucher='0' and cp.control_type='1' and cp.cp_adjust_code='101' and cp.is_valid='1' and cp.a_status_code='2'; 附件二 关于 ORA-600 [12700] [a] [b] [c]12345678910111213141516171819202122232425262728293031323334353637383940414243ORA-00600: internal error code, arguments: [12700], [2], [4206885], [26]Note: For additional ORA-600 related information please read Note:146580.1PURPOSE: This article discusses the internal error \"ORA-600 [12700]\", what it means and possible actions. The information here is only applicable to the versions listed and is provided only for guidance. ERROR: ORA-600 [12700] [a] [b] [c] VERSIONS: versions 6.0 to 9.2 DESCRIPTION: Oracle is trying to access a row using its ROWID, which has been obtained from an index. A mismatch was found between the index rowid and the data block it is pointing to. The rowid points to a non-existent row in the data block. The corruption can be in data and/or index blocks. ORA-600 [12700] can also be reported due to a consistent read (CR) problem. The information dumped to the trace file varies greatly between releases:- in Oracle 7.3.x it is ORA-600 [12700][a][b] , where Arg [a] dba (Data Block Address) Arg [b] slot number (number of the row in the block pointed by the dba)- in Oracle 8.x and 9.x, it is ORA-600 [12700][a][b][c] , where Arg [a] dataobj# from sys.obj$ Arg [b] relative dba of the data block Arg [c] slot number of the row in the data block The arguments of the ORA-600 [12700] contains information obtained from the index we are using.FUNCTIONALITY: USER/ORACLE INTERFACE LAYERIMPACT: POSSIBLE CORRUPTIONSUGGESTIONS: Please refer to Note:155933.1 \"Resolving an ORA-600 [12700] error\" for additional help with diagnosing this problem. If the above note does not help, then please log the issue with Oracle Support Services. Known Issues: 查询块号123456789101112131415161718192021222324252627282930313233343536373839404142434445464715:29:39 [SYS@txczyy](mailto:SYS@txczyy)&gt; select obj#,name from sys.obj$ where dataobj#=2; OBJ# NAME ---------- ------------------------------ 4 TAB$ 2 C_OBJ# 19 IND$ 21 COL$ 20 ICOL$ 5 CLU$ 156 LOB$ 283 COLTYPE$ 286 SUBCOLTYPE$ 288 ATTRCOL$ 290 VIEWTRCOL$ OBJ# NAME ---------- ------------------------------ 295 TYPE_MISC$ 352 NTAB$ 359 LIBRARY$ 362 REFCON$ 365 OPQTYPE$ 367 ICOLDEP$15:31:00 [SYS@txczyy](mailto:SYS@txczyy)&gt; select dbms_utility.data_block_address_file(4206885),dbms_utility.data_block_address_block(4206885) from dual;DBMS_UTILITY.DATA_BLOCK_ADDRESS_FILE(4206885) DBMS_UTILITY.DATA_BLOCK_ADDRESS_BLOCK(4206885) --------------------------------------------- ---------------------------------------------- 1 1258119:31:30 [SYS@txczyy](mailto:SYS@txczyy)&gt; select obj#,col# from col$ where obj#=46907; OBJ# COL# ---------- ---------- 46907 1 46907 2 46907 3 46907 4 46907 5 46907 6 46907 7已选择7行。20:06:29 [SYS@txczyy](mailto:SYS@txczyy)&gt; select rowid,dbms_rowid.rowid_relative_fno(rowid),dbms_rowid.rowid_block_number(rowid) from col$ c where obj#=46907;ROWID DBMS_ROWID.ROWID_RELATIVE_FNO(ROWID) DBMS_ROWID.ROWID_BLOCK_NUMBER(ROWID) ------------------ ------------------------------------ ------------------------------------ AAAAACAABAAADElAAX 1 12581 AAAAACAABAAADElAAY 1 12581 AAAAACAABAAADElAAZ 1 12581 AAAAACAABAAADElAAo 1 12581 AAAAACAABAAADElAAp 1 12581 AAAAACAABAAADElAAq 1 12581 AAAAACAABAAADElAAr 1 12581 说明 block=12581发生坏块 查看数据行在哪个块1select depid,dept,dbms_rowid.rowid_block_number(rowid) blcokno from hw;","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"PostgreSQL 创建主键并设置自动递增的三种方法","slug":"20110217135035","date":"2011-02-17T05:50:35.000Z","updated":"2018-09-04T01:33:47.989Z","comments":true,"path":"20110217135035.html","link":"","permalink":"https://postgres.fun/20110217135035.html","excerpt":"","text":"Postgresql 有以下三种方法设置主键递增的方式，下面来看下相同点和不同点。 方法一123456789create table test_a ( id serial, name character varying(128), constraint pk_test_a_id primary key( id) );NOTICE: CREATE TABLE will create implicit sequence \"test_a_id_seq\" for serial column \"test_a.id\" NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"pk_test_a_id\" for table \"test_a\" CREATE TABLE 方法二12345678create table test_b ( id serial PRIMARY KEY, name character varying(128) );NOTICE: CREATE TABLE will create implicit sequence \"test_b_id_seq\" for serial column \"test_b.id\" NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_b_pkey\" for table \"test_b\" CREATE TABLE 方法三12345678910111213141516create table test_c ( id integer PRIMARY KEY, name character varying(128) ); NOTICE: CREATE TABLE / PRIMARY KEY will create implicit index \"test_c_pkey\" for table \"test_c\" CREATE TABLE CREATE SEQUENCE test_c_id_seq START WITH 1 INCREMENT BY 1 NO MINVALUE NO MAXVALUE CACHE 1; alter table test_c alter column id set default nextval('test_c_id_seq'); 很明显从上面可以看出，方法一和方法二只是写法不同，实质上主键都通过使用 serial 类型来实现的，使用serial类型，PG会自动创建一个序列给主键用，当插入表数据时如果不指定ID，则ID会默认使用序列的 NEXT值。方法三是先创建一张表，再创建一个序列，然后将表主键ID的默认值设置成这个序列的NEXT值。这种写法似乎更符合人们的思维习惯，也便于管理，如果系统遇到sequence 性能问题时，便于调整 sequence 属性； 比较三张表的表结构skytf=&gt; \\d test_a Table &quot;skytf.test_a&quot; Column | Type | Modifiers --------+------------------------+----------------------------------------------------- id | integer | not null default nextval(&apos;test_a_id_seq&apos;::regclass) name | character varying(128) | Indexes: &quot;pk_test_a_id&quot; PRIMARY KEY, btree (id) skytf=&gt; \\d test_b Table &quot;skytf.test_b&quot; Column | Type | Modifiers --------+------------------------+----------------------------------------------------- id | integer | not null default nextval(&apos;test_b_id_seq&apos;::regclass) name | character varying(128) | Indexes: &quot;test_b_pkey&quot; PRIMARY KEY, btree (id) skytf=&gt; \\d test_c Table &quot;skytf.test_c&quot; Column | Type | Modifiers --------+------------------------+----------------------------------------------------- id | integer | not null default nextval(&apos;test_c_id_seq&apos;::regclass) name | character varying(128) | Indexes: &quot;test_c_pkey&quot; PRIMARY KEY, btree (id) 从上面可以看出，三个表表结构一模一样， 三种方法如果要寻找差别，可能仅有以下一点，当 drop 表时，方法一和方法二会自动地将序列也 drop 掉,而方法三不会。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Autovacuum 进程引起 SWAP 使用率高","slug":"20110216134555","date":"2011-02-16T05:45:55.000Z","updated":"2018-09-04T01:33:47.926Z","comments":true,"path":"20110216134555.html","link":"","permalink":"https://postgres.fun/20110216134555.html","excerpt":"","text":"今天上午监控人员反映一台数据库主机 SWAP使用率达到 48%, 需要关注；通常情况下 SWAP 使用率都比较低的，只有当可用内存用完的时候进程才会去申请SWAP的内存空间, 下面是操作日志。 内存使用情况12345[postgres@logdb](mailto:postgres@logdb)-&gt; free -m total used free shared buffers cached Mem: 24104 24031 72 0 3 6411 -/+ buffers/cache: 17616 6487 Swap: 16386 8588 7797 Mem: 24G,几乎用尽; Swap: 16G,用了8.5G, 使用率 50%左右；那什么进程用了这么多内存呢？ 查找内存消耗 TOP 进程top命令查找消耗内存的进程1234PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 5821 postgres 14 -1 26.2g 16g 26m S 0.0 70.0 313:02.00 postgres: autovacuum launcher process 32161 postgres 14 -1 1473m 1.3g 1.3g S 0.3 5.4 558:48.77 postgres: community community 192.168.169.42(36625) idle 32167 postgres 14 -1 1473m 1.3g 1.3g S 0.0 5.4 559:08.57 postgres: community community 192.168.169.42(36638) idle 第一个进程引起了我的注意，是 PostgreSQL 的 “autovacuum” 进程，吃了16G的内存。 查看 Autovaccum 进程详细信息1234567891011121314151617181920212223242526postgres=# select query_start,current_query from pg_stat_activity where current_query !='&lt;IDLE&gt;'; query_start | current_query -------------------------------+---------------------------------------------------------------------------------------- 2011-02-14 09:37:41.495117+08 | autovacuum: VACUUM myschema.tbl_user_login_log_p201010 (to prevent wraparound) 2011-02-14 09:33:49.798097+08 | autovacuum: VACUUM myschema.tbl_download_stat (to prevent wraparound) 2011-02-14 08:53:50.657176+08 | autovacuum: VACUUM myschema.tbl_register_log (to prevent wraparound) 2011-02-14 10:12:21.757808+08 | select query_start,current_query from pg_stat_activity where current_query !='&lt;IDLE&gt;'; (4 rows)myschema=&gt; select pg_size_pretty(pg_relation_size('tbl_download_stat')); pg_size_pretty ---------------- 3550 MB (1 row)myschema=&gt; select pg_size_pretty(pg_relation_size('tbl_user_login_log_p201010')); pg_size_pretty ---------------- 5115 MB (1 row)myschema=&gt; select pg_size_pretty(pg_relation_size('tbl_register_log')); pg_size_pretty ---------------- 1763 MB (1 row) 发现有三张表在做 vacuum, 并且这三张表都有点大；根据query_start来看，有个VACUUM进程也做了有两小时了, 几G的表正常情况下也应该完成了，为什么还在跑呢？ 监控上午一直在监控，到了下午的时候发现 SWAP 空间使用率依然保持不变，但 autovaccum 进程依然还在跑;猜想 autovacuum 进程可能有异常，后来将上面现象请教德哥，德哥回复可能 OS SWAP 换页有异常，建议将 PostgreSQL 的 autovaccum 进程杀掉，它会自动重起；后来通过 pg_terminate_backend()将 vaccum 进程杀掉后，SWAP空间果然释放。 查看 SWAP 使用情况12345[postgres@logdb](mailto:postgres@logdb)-&gt; free -m total used free shared buffers cached Mem: 24104 24033 70 0 94 20118 -/+ buffers/cache: 3820 20283 Swap: 16386 53 16333 总结autovaccum 进程长时间未完成的原因尚不明确，今天将以上现象，经验记下，供参考。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL 扩 Varchar 字段长度的奇怪现象","slug":"20110215200654","date":"2011-02-15T12:06:54.000Z","updated":"2018-09-04T01:33:47.879Z","comments":true,"path":"20110215200654.html","link":"","permalink":"https://postgres.fun/20110215200654.html","excerpt":"","text":"今天测试了下，有张大表字段需要扩充长度，即将 character varying(128) 扩大到 character varying(256)，因为以前有Oracle 经验，类似的操作在Oracle 库里瞬间就能完成。因为只涉及到更改数据字典，不更改物理数据。下面来看下PG里的情况。 现象查看表大小12345wapreader_log=&gt; select pg_size_pretty(pg_relation_size('log_foot_mark')); pg_size_pretty ---------------- 5441 MB (1 row) 查看表结构12345678910111213141516171819wapreader_log=&gt; \\d log_foot_mark Table \"wapreader_log.log_foot_mark\" Column | Type | Modifiers -------------+-----------------------------+----------- id | integer | not null create_time | timestamp without time zone | sky_id | integer | url | character varying(1000) | refer_url | character varying(1000) | source | character varying(64) | users | character varying(64) | userm | character varying(64) | usert | character varying(64) | ip | character varying(32) | module | character varying(64) | resource_id | character varying(100) | user_agent | character varying(128) | Indexes: \"pk_log_footmark\" PRIMARY KEY, btree (id) 扩字段长度12345wapreader_log=&gt; timing Timing is on.wapreader_log=&gt; ALTER TABLE wapreader_log.log_foot_mark ALTER column user_agent TYPE character varying(256); ALTER TABLE Time: 603504.835 ms 通常加字段的DDL是很快的，瞬间就能完成，为什么这次经历了十分钟，觉得很奇怪。那为什么PG里扩字段长度会花很长时间呢？难道在更改物理数据？带着这个疑问，做了以下测试,等下看结果。 测试测试,创建一张表12wapreader_log=&gt; create table test_1 (id integer ,remark varchar(32)); CREATE TABLE 创建插入数据 function12345678910111213CREATE OR REPLACE FUNCTION wapreader_log.fun_ins_test_1() RETURNS integer LANGUAGE plpgsql AS $function$ DECLARE i INTEGER ; BEGIN for i in 1..100 loop insert into test_1 values (1,'a'); end loop; return 1; END; $function$ 插入 100 条数据12345wapreader_log=&gt; select wapreader_log.fun_ins_test_1(); fun_ins_test_1 ---------------- 1 (1 row) 查看数据，注意 ctid123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104wapreader_log=&gt; select ctid,* from test_1 ; ctid | id | remark ---------+----+-------- (0,1) | 1 | a (0,2) | 1 | a (0,3) | 1 | a (0,4) | 1 | a (0,5) | 1 | a (0,6) | 1 | a (0,7) | 1 | a (0,8) | 1 | a (0,9) | 1 | a (0,10) | 1 | a (0,11) | 1 | a (0,12) | 1 | a (0,13) | 1 | a (0,14) | 1 | a (0,15) | 1 | a (0,16) | 1 | a (0,17) | 1 | a (0,18) | 1 | a (0,19) | 1 | a (0,20) | 1 | a (0,21) | 1 | a (0,22) | 1 | a (0,23) | 1 | a (0,24) | 1 | a (0,25) | 1 | a (0,26) | 1 | a (0,27) | 1 | a (0,28) | 1 | a (0,29) | 1 | a (0,30) | 1 | a (0,31) | 1 | a (0,32) | 1 | a (0,33) | 1 | a (0,34) | 1 | a (0,35) | 1 | a (0,36) | 1 | a (0,37) | 1 | a (0,38) | 1 | a (0,39) | 1 | a (0,40) | 1 | a (0,41) | 1 | a (0,42) | 1 | a (0,43) | 1 | a (0,44) | 1 | a (0,45) | 1 | a (0,46) | 1 | a (0,47) | 1 | a (0,48) | 1 | a (0,49) | 1 | a (0,50) | 1 | a (0,51) | 1 | a (0,52) | 1 | a (0,53) | 1 | a (0,54) | 1 | a (0,55) | 1 | a (0,56) | 1 | a (0,57) | 1 | a (0,58) | 1 | a (0,59) | 1 | a (0,60) | 1 | a (0,61) | 1 | a (0,62) | 1 | a (0,63) | 1 | a (0,64) | 1 | a (0,65) | 1 | a (0,66) | 1 | a (0,67) | 1 | a (0,68) | 1 | a (0,69) | 1 | a (0,70) | 1 | a (0,71) | 1 | a (0,72) | 1 | a (0,73) | 1 | a (0,74) | 1 | a (0,75) | 1 | a (0,76) | 1 | a (0,77) | 1 | a (0,78) | 1 | a (0,79) | 1 | a (0,80) | 1 | a (0,81) | 1 | a (0,82) | 1 | a (0,83) | 1 | a (0,84) | 1 | a (0,85) | 1 | a (0,86) | 1 | a (0,87) | 1 | a (0,88) | 1 | a (0,89) | 1 | a (0,90) | 1 | a (0,91) | 1 | a (0,92) | 1 | a (0,93) | 1 | a (0,94) | 1 | a (0,95) | 1 | a (0,96) | 1 | a (0,97) | 1 | a (0,98) | 1 | a (0,99) | 1 | a (0,100) | 1 | a (100 rows) 查看表大小12345wapreader_log=&gt; select pg_size_pretty(pg_relation_size('test_1')); pg_size_pretty ---------------- 8192 bytes (1 row) 更改字段长度123456789wapreader_log=&gt; \\d test_1 Table \"wapreader_log.test_1\" Column | Type | Modifiers --------+-----------------------+----------- id | integer | remark | character varying(32) | wapreader_log=&gt; alter table test_1 alter column remark type character varying(256); ALTER TABLE 再次查看表的 ctid 和表大小123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110wapreader_log=&gt; select ctid,* from test_1; ctid | id | remark ---------+----+-------- (0,1) | 1 | a (0,2) | 1 | a (0,3) | 1 | a (0,4) | 1 | a (0,5) | 1 | a (0,6) | 1 | a (0,7) | 1 | a (0,8) | 1 | a (0,9) | 1 | a (0,10) | 1 | a (0,11) | 1 | a (0,12) | 1 | a (0,13) | 1 | a (0,14) | 1 | a (0,15) | 1 | a (0,16) | 1 | a (0,17) | 1 | a (0,18) | 1 | a (0,19) | 1 | a (0,20) | 1 | a (0,21) | 1 | a (0,22) | 1 | a (0,23) | 1 | a (0,24) | 1 | a (0,25) | 1 | a (0,26) | 1 | a (0,27) | 1 | a (0,28) | 1 | a (0,29) | 1 | a (0,30) | 1 | a (0,31) | 1 | a (0,32) | 1 | a (0,33) | 1 | a (0,34) | 1 | a (0,35) | 1 | a (0,36) | 1 | a (0,37) | 1 | a (0,38) | 1 | a (0,39) | 1 | a (0,40) | 1 | a (0,41) | 1 | a (0,42) | 1 | a (0,43) | 1 | a (0,44) | 1 | a (0,45) | 1 | a (0,46) | 1 | a (0,47) | 1 | a (0,48) | 1 | a (0,49) | 1 | a (0,50) | 1 | a (0,51) | 1 | a (0,52) | 1 | a (0,53) | 1 | a (0,54) | 1 | a (0,55) | 1 | a (0,56) | 1 | a (0,57) | 1 | a (0,58) | 1 | a (0,59) | 1 | a (0,60) | 1 | a (0,61) | 1 | a (0,62) | 1 | a (0,63) | 1 | a (0,64) | 1 | a (0,65) | 1 | a (0,66) | 1 | a (0,67) | 1 | a (0,68) | 1 | a (0,69) | 1 | a (0,70) | 1 | a (0,71) | 1 | a (0,72) | 1 | a (0,73) | 1 | a (0,74) | 1 | a (0,75) | 1 | a (0,76) | 1 | a (0,77) | 1 | a (0,78) | 1 | a (0,79) | 1 | a (0,80) | 1 | a (0,81) | 1 | a (0,82) | 1 | a (0,83) | 1 | a (0,84) | 1 | a (0,85) | 1 | a (0,86) | 1 | a (0,87) | 1 | a (0,88) | 1 | a (0,89) | 1 | a (0,90) | 1 | a (0,91) | 1 | a (0,92) | 1 | a (0,93) | 1 | a (0,94) | 1 | a (0,95) | 1 | a (0,96) | 1 | a (0,97) | 1 | a (0,98) | 1 | a (0,99) | 1 | a (0,100) | 1 | a (100 rows)wapreader_log=&gt; select pg_size_pretty(pg_relation_size('wapreader_log.test_1')); pg_size_pretty ---------------- 8192 bytes (1 row) 发现表大小，ctid 均无变化，说明扩字段长度操作没有更改物理数据, 上述猜想是不成立的。后来我把这个问题发到一国外论坛上，上面有个回复我觉得解释得比较好：原文如下，就不翻译了 When you alter a table, PostgreSQL has to make sure the old version doesn’t go away in some cases, to allow rolling back the change if the server crashes before it’s committed and/or written to disk. For those reasons, what it actually does here even on what seems to be a trivial change is write out a whole new copy of the table somewhere else first. When that’s finished, it then swaps over to the new one. Note that when this happens, you’ll need enough disk space to hold both copies as wellThere are some types of DDL changes that can be made without making a second copy of the table, but this is not one of them. For example, you can add a new column that defaults to NULL quickly. But adding a new column with a non-NULL default requires making a new copy instead. 总结针对PG在扩字段长度时会扫描全表且时间较长的问题，总结以下经验 尽可能熟悉应用，对于不确定长度的 Character Types ，建议不指定长度。 对于Character较长的字段可以采用 text类型。 Oracle 在给 varchar2 类型字段加长时，瞬间就能完成，在这点上，PG有着与Oracle 不同的方式；","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"2011年 francs 同志的春节健康表","slug":"20110127132329","date":"2011-01-27T05:23:29.000Z","updated":"2018-12-04T00:31:00.080Z","comments":true,"path":"20110127132329.html","link":"","permalink":"https://postgres.fun/20110127132329.html","excerpt":"","text":"春节感悟回家过年。。。","categories":[{"name":"生活点滴","slug":"生活点滴","permalink":"https://postgres.fun/categories/生活点滴/"}],"tags":[{"name":"生活点滴","slug":"生活点滴","permalink":"https://postgres.fun/tags/生活点滴/"}]},{"title":"Configure: error: readline library not found","slug":"20110126110423","date":"2011-01-26T03:04:23.000Z","updated":"2018-09-04T01:33:47.770Z","comments":true,"path":"20110126110423.html","link":"","permalink":"https://postgres.fun/20110126110423.html","excerpt":"","text":"今天一位同事在搭建 PostgreSQL 测试环境，安装过程中遇到一个问题，在执行 configure 过程中报错，如下：1configure: error: readline library not found 同事说已经在系统中安装 readline 包了，让我帮他看下。 环境信息OS: CentOS release 5.2 (Final)PG: PostgreSQL 9.0.0 Configre 测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980[root@HK81-107 postgresql-9.0.0]# ./configure checking build system type... i686-pc-linux-gnu checking host system type... i686-pc-linux-gnu checking which template to use... linux checking whether to build with 64-bit integer date/time support... yes checking whether NLS is wanted... no checking for default port number... 5432 checking for block size... 8kB checking for segment size... 1GB checking for WAL block size... 8kB checking for WAL segment size... 16MB checking for gcc... gcc checking for C compiler default output file name... a.out checking whether the C compiler works... yes checking whether we are cross compiling... no checking for suffix of executables... checking for suffix of object files... o checking whether we are using the GNU C compiler... yes checking whether gcc accepts -g... yes checking for gcc option to accept ISO C89... none needed checking if gcc supports -Wdeclaration-after-statement... yes checking if gcc supports -Wendif-labels... yes checking if gcc supports -fno-strict-aliasing... yes checking if gcc supports -fwrapv... yes checking whether the C compiler still works... yes checking how to run the C preprocessor... gcc -E checking allow thread-safe client libraries... yes checking whether to build with Tcl... no checking whether to build Perl modules... no checking whether to build Python modules... no checking whether to build with GSSAPI support... no checking whether to build with Kerberos 5 support... no checking whether to build with PAM support... no checking whether to build with LDAP support... no checking whether to build with Bonjour support... no checking whether to build with OpenSSL support... no checking for grep that handles long lines and -e... /bin/grep checking for egrep... /bin/grep -E checking for ld used by GCC... /usr/bin/ld checking if the linker (/usr/bin/ld) is GNU ld... yes checking for ranlib... ranlib checking for strip... strip checking whether it is possible to strip libraries... yes checking for ar... ar checking for tar... /bin/tar checking whether ln -s works... yes checking for gawk... gawk checking for a thread-safe mkdir -p... /bin/mkdir -p checking for bison... no configure: WARNING: * Without Bison you will not be able to build PostgreSQL from CVS nor * change any of the parser definition files. You can obtain Bison from * a GNU mirror site. (If you are using the official distribution of * PostgreSQL then you do not need to worry about this, because the Bison * output is pre-generated.) checking for flex... no configure: WARNING: * Without Flex you will not be able to build PostgreSQL from CVS nor * change any of the scanner definition files. You can obtain Flex from * a GNU mirror site. (If you are using the official distribution of * PostgreSQL then you do not need to worry about this because the Flex * output is pre-generated.) checking for perl... /usr/bin/perl configure: using perl 5.8.8 checking for main in -lm... yes checking for library containing setproctitle... no checking for library containing dlopen... -ldl checking for library containing socket... none required checking for library containing shl_load... no checking for library containing getopt_long... none required checking for library containing crypt... -lcrypt checking for library containing fdatasync... none required checking for library containing gethostbyname_r... none required checking for library containing shmget... none required checking for -lreadline... no checking for -ledit... no configure: error: readline library not found If you have readline already installed, see config.log for details on the failure. It is possible the compiler isnt looking in the proper directory. Use --without-readline to disable readline support. 根据提示，应该是没有安装 readline包。 检查是否安装 Readline 包12[root@HK81-107 postgresql-9.0.0]# rpm -qa | grep readline readline-5.1-3.el5 说明系统已经安装了 readline包。 YUM 搜索相关的 Readline 包123456789[root@HK81-107 postgresql-9.0.0]# yum search readline lftp.i386 : A sophisticated file transfer program lftp.i386 : A sophisticated file transfer program php-readline.i386 : Standard PHP module provides readline library support lftp.i386 : A sophisticated file transfer program readline.i386 : A library for editing typed command lines. compat-readline43.i386 : The readline 4.3 library for compatibility with older software. readline-devel.i386 : Files needed to develop programs which use the readline library. readline.i386 : A library for editing typed command lines. 根据提示，有一个包引起了我的注意 “readline-devel”, 猜想可能与这个包有关。 安装 Readline-Devel 包12345678910111213141516171819202122232425262728293031323334353637[root@HK81-107 postgresql-9.0.0]# yum -y install -y readline-devel Setting up Install Process Parsing package install arguments Resolving Dependencies --&gt; Running transaction check ---&gt; Package readline-devel.i386 0:5.1-3.el5 set to be updated --&gt; Processing Dependency: libtermcap-devel for package: readline-devel --&gt; Running transaction check ---&gt; Package libtermcap-devel.i386 0:2.0.8-46.1 set to be updated --&gt; Finished Dependency ResolutionDependencies Resolved============================================================================= Package Arch Version Repository Size ============================================================================= Installing: readline-devel i386 5.1-3.el5 base 146 k Installing for dependencies: libtermcap-devel i386 2.0.8-46.1 base 56 kTransaction Summary ============================================================================= Install 2 Package(s) Update 0 Package(s) Remove 0 Package(s)Total download size: 201 k Downloading Packages: (1/2): libtermcap-devel-2 100% |=========================| 56 kB 00:00 (2/2): readline-devel-5.1 100% |=========================| 146 kB 00:00 Running rpm_check_debug Running Transaction Test Finished Transaction Test Transaction Test Succeeded Running Transaction Installing: libtermcap-devel ######################### [1/2] Installing: readline-devel ######################### [2/2]Installed: readline-devel.i386 0:5.1-3.el5 Dependency Installed: libtermcap-devel.i386 0:2.0.8-46.1 Complete! 再次执行 configure 成功。 关于 Readline 的解释 without-readlinePrevents use of the Readline library (and libedit as well). This option disables command-lineediting and history in psql, so it is not recommended. 说明： 根据步骤2 执行 configure 时报错提示，可以加上 “–without-readline” 从而避开这个ERROR，但Postgresql官方不推荐这么做，所以还是安装吧。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"DataGuard 之四：Using Real-Time Apply (Contuine DataGuard 之一 ) ","slug":"20110112235847","date":"2011-01-12T15:58:47.000Z","updated":"2018-09-04T01:33:47.708Z","comments":true,"path":"20110112235847.html","link":"","permalink":"https://postgres.fun/20110112235847.html","excerpt":"","text":"本文接着前面做的DataGuard 实验, 前面搭建的Physical Standby 不能做到实时同步主库数据，当主库Online Redo 归档后，才会将这个归档日志传到备机并Applied 到备库，这个延迟时间较长，如果主库业务不繁忙，可能好几天才产生一个归档文件，这时备库数据和主库数据相差就好几天了，今天看了下Oracle DataGuard官方文档，Oracle 提供实时应用日志的方式，以下摘自官网，就不翻译了。 Using Real-Time Apply to Apply Redo Data ImmediatelyIf the real-time apply feature is enabled, log apply services can apply redo data as it is received,without waiting for the current standby redo log file to be archived. This results in faster switchoverand failover times because the standby redo log files have been applied already to the standby database by the timethe failover or switchover begins. Figure 6-1 shows a Data Guard configuration with a local destination and a standby destination.As the remote file server (RFS) process writes the redo data to standby redo log files on the standby database,log apply services can recover redo from standby redo log files as they are being filled. 下面是在已经搭建Physical Standby 的基础上进行， 1 停 Standby REDO 应用进程 (On standby )12SQL&gt; alter database recover managed standby database cancel; Database altered. 2 增加 STANDBY redo 日志 (Both on standby and Primary)12345678910111213141516171819ALTER DATABASE ADD STANDBY LOGFILE thread 1 GROUP 4 ('/oradata/MANUA/stadnbylog/stdlog01.rdo') SIZE 50M reuse; ALTER DATABASE ADD STANDBY LOGFILE thread 1 GROUP 5 ('/oradata/MANUA/stadnbylog/stdlog02.rdo') SIZE 50M reuse; ALTER DATABASE ADD STANDBY LOGFILE thread 1 GROUP 6 ('/oradata/MANUA/stadnbylog/stdlog03.rdo') SIZE 50M reuse; ALTER DATABASE ADD STANDBY LOGFILE thread 1 GROUP 7 ('/oradata/MANUA/stadnbylog/stdlog04.rdo') SIZE 50M reuse; ALTER DATABASE ADD STANDBY LOGFILE thread 1 GROUP 8 ('/oradata/MANUA/stadnbylog/stdlog05.rdo') SIZE 50M reuse; ALTER DATABASE ADD STANDBY LOGFILE thread 1 GROUP 9 ('/oradata/MANUA/stadnbylog/stdlog06.rdo') SIZE 50M reuse; ALTER DATABASE ADD STANDBY LOGFILE thread 1 GROUP 10 ('/oradata/MANUA/stadnbylog/stdlog7.rdo') SIZE 50M reuse;SQL&gt; SELECT GROUP#,THREAD#,SEQUENCE#,ARCHIVED,STATUS FROM V$STANDBY_LOG; GROUP# THREAD# SEQUENCE# ARC STATUS ---------- ---------- ---------- --- ---------- 4 1 14 YES ACTIVE 5 1 0 YES UNASSIGNED 6 1 0 YES UNASSIGNED 7 1 0 YES UNASSIGNED 8 1 0 YES UNASSIGNED 9 1 0 YES UNASSIGNED 10 1 0 YES UNASSIGNED7 rows selected. 3 启动数据库到实时恢复管理模式(On Standby)12SQL&gt; alter database recover managed standby database using current logfile disconnect from session;Database altered. 4 主库创建一张测试表12345678910111213SQL&gt; create table test_1 (id integer,name varchar2(32));Table created.SQL&gt; insert into test_1 values (1,'francs');1 row created.SQL&gt; insert into test_1 values(2,'tf');1 row created.SQL&gt; commit;Commit complete.SQL&gt; select * from test_1; ID NAME ---------- -------------------------------- 1 francs 2 tf 注意，这步操作后，不执行日志切换操作 “Alter system switch log file”, 并接着观察日志是否进来。 5 备库上初次验证12345678SQL&gt; alter database recover managed standby database cancel;Database altered.SQL&gt; alter database open read only;Database altered.SQL&gt; select * from test_1; ID NAME ---------- --------------------------------------------------2 rows selected. 说明：第下次验证发现数据没有传过来，表是已经创建上了，猜想可能有延迟，再等等。 6 过了大概五分钟后，备库上再次验证12345678910SQL&gt; alter database recover managed standby database cancel;Database altered.SQL&gt; alter database open read only;Database altered.SQL&gt; select * from test_1; ID NAME ---------- -------------------------------------------------- 1 francs 2 tf2 rows selected. 发现数据已经同步了。 总结： 在备库上创建 standby log ，可以实现主备机实时数据同步。 这种实时数据同步的方法比归档日志传递的方法高效得多，但也有延迟，这个延迟应该在五分钟以内。","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"PostgreSQL: Setting up streaming log replication (Hot Standby ) ","slug":"20110108200738","date":"2011-01-08T12:07:38.000Z","updated":"2018-09-04T01:33:47.661Z","comments":true,"path":"20110108200738.html","link":"","permalink":"https://postgres.fun/20110108200738.html","excerpt":"","text":"Postgresql 9.0 的一个主要新特性是可以实施流复制，这有点像ORACLE 里的DataGuard(Physial Standby) 但是这种方式比Oracle的DataGuard更为安全，更为高效，因为从库同步主库是实时的，几乎没有时间差。 而Oracle的 DataGuard的从库接收并应用主库的日志的延迟，本人测试了下，大概有几分钟，具体延时决定于主库的业务繁忙程度。 下面是流复制实验的详细步骤： 环境信息 PG版本: PostgreSQL 9.0beta3 OS版本: Red Hat Enterprise Linux Server release 5.5 硬件环境： WINDOWS XP上安装两台虚拟机 Master信息 IP: 192.168.1.25 Standby信息 IP: 192.168.1.26 安装 PostgreSQL由于主库已经安装 PostgreSQL 软件 ，主库上安装PG的步骤这里就不介绍了 配置从库主机参数3.1 设置 /etc/sysctl.conf,增加以下内容1234567891011121314kernel.shmmni = 4096 kernel.sem = 501000 6412800000 501000 12800 fs.file-max = 767246 net.ipv4.ip_local_port_range = 1024 65000 net.core.rmem_default = 1048576 net.core.rmem_max = 1048576 net.core.wmem_default = 262144 net.core.wmem_max = 262144 net.ipv4.tcp_tw_recycle=1 net.ipv4.tcp_max_syn_backlog=4096 net.core.netdev_max_backlog=10000 vm.overcommit_memory=0 net.ipv4.ip_conntrack_max=655360sysctl -p 生效 3.2 设置/etc/security/limits.conf 增加以下内容12345678* soft nofile 131072 * hard nofile 131072 * soft nproc 131072 * hard nproc 131072 * soft core unlimited * hard core unlimited * soft memlock 50000000 * hard memlock 50000000 3.3 设置 /etc/pam.d/login ，增加以下内容1session required pam_limits.so 主库上创建超级用户4.1 创建用户12345CREATE USER repuser SUPERUSER LOGIN CONNECTION LIMIT 2 ENCRYPTED PASSWORD 'repuser'; 4.2 设置 master 库 pg_hba.conf1host replication repuser 192.168.1.26/16 md5 说明：超级用户 repuser 是用来从库上读取库主库(Master)的 WAL stream,并且在4。2中设置 权限，只允许主机 192.168.1.26(Standby 节点)以 md5 加密方式访问。 设置日志参数设置postgresql.conf以下参数，记录连接信息 ( Both Master and Standby 库)1log_connections = on 说明：”log_connections” 参数用来记录数据库连接信息,打开这个开关，从而在接下来的CSV日志中能更好的观察Master库和 Standby 库情况。 设置主库 postgresql.conf12345max_wal_senders = 1 --WAL STREAM 日志发送进程数 wal_level = hot_standby --主库设置成 hot_standby ，从库才能以READ-ONLY模式打开 archive_mode = on archive_command = 'cd .' wal_keep_segments = 64 说明，关键参数”max_wal_senders” 是指 wal 发送进程数, 我这里只有一台从库，所以设置为1，如果有多台从库，则应该设置成从库个数，因为在Master库上，每台从库需要一个 WAL日志发送进程向从库发送WAL日志流。 这一参数官网的介绍。 max_wal_senders (integer)Specifies the maximum number of concurrent connections from standby servers (i.e., the maximum numberof simultaneously running WAL sender processes). The default is zero. This parameter can only be setat server start. wal_level must be set to archive or hot_standby to allow connections from standby servers. 主库全备7.1 tart the backup (On Master )1select pg_start_backup('base backup for log streaming'); 7.2 COPY 数据文件1tar czvf pg_root.tar.gz pg_root --exclude=pg_root/pg_xlog 由于 $PGDATA/pg_xlog 不是必须的，这里排除了这个目录,节省时间。 7.3 将数据文件COPY到standby 主机并解压1scp pgdata.tar.gz pgb:/database 7.4 数据COPY完后,结束备份 Stop the backup (On Master)1select pg_stop_backup(), current_timestamp; 说明：建议主库和从库配置信息一致，包括硬件信息，目录结构，主机配置等。 修改从库 postgresql.conf1hot_standby = on --从库上可以执行只读操作 设置从库 recovery.conf1234recovery_target_timeline = 'latest' standby_mode = 'on' --标记PG为STANDBY SERVER primary_conninfo = 'host=192.168.1.25 port=1921 user=repuser password=repuser' trigger_file = '/tmp/postgresql.trigger.1921' 说明：关键参数“primary_conninfo (string)” ,这里配置了hostname,port,username ,password, 关于这个参数的更多解释可以参考官网.其中更多关于连接的参数可以配置，这里不说明了 ,详见 http://www.postgresql.org/docs/9.0/static/libpq-connect.html 删除从库文件,并创建 pg_xlog目录12$ rm -f $PGDATA/postmaster.pid $ mkdir -p $PGDATA/pg_xlog 开启从库11.1 观察CSVLOG123452011-01-08 17:22:49.757 CST,,,24243,,4d282ce9.5eb3,2,,2011-01-08 17:22:49 CST,,0,LOG,00000,\"entering standby mode\",,,,,,,,,\"\" 2011-01-08 17:22:49.887 CST,,,24244,,4d282ce9.5eb4,1,,2011-01-08 17:22:49 CST,,0,LOG,00000,\"streaming replication successfully connected to primary\",,,,,,,,,\"\" 2011-01-08 17:22:52.677 CST,,,24243,,4d282ce9.5eb3,3,,2011-01-08 17:22:49 CST,1/0,0,LOG,00000,\"redo starts at 1/94000020\",,,,,,,,,\"\" 2011-01-08 17:22:52.696 CST,,,24243,,4d282ce9.5eb3,4,,2011-01-08 17:22:49 CST,1/0,0,LOG,00000,\"consistent recovery state reached at 1/98000000\",,,,,,,,,\"\" 2011-01-08 17:22:52.805 CST,,,24241,,4d282ce8.5eb1,4,,2011-01-08 17:22:48 CST,,0,LOG,00000,\"database system is ready to accept read only connections\",,,,,,,,,\"\" 日志中 “streaming replication successfully connected to primary”,”database system is ready to accept read only connections” 这些信息说明流复制已经成功，从库正准备接收主库的WAL-STREAM。 11.2 主库观察WAL-Sender 进程12[postgres@pg1 pg_root]$ ps -ef | grep post postgres 27225 27166 0 17:22 ? 00:00:05 postgres: wal sender process repuser 192.168.1.26(59836) streaming 1/9801E000 说明：将输出结果省略部分，可以看到 “ wal sender process repuser”进程 11.3 在从库上观察 WAL-接收进程12[postgres@pgb pg_log]$ ps -ef | grep post postgres 24244 24241 0 17:22 ? 00:00:04 postgres: wal receiver process streaming 1/9801DF00 说明：同样省略部分输出结果，可以看到“ wal receiver process ” 进程。 复制测试12.1 主库上创建用户123postgres=# CREATE ROLE browser LOGIN ENCRYPTED PASSWORD 'browser' postgres-# nosuperuser noinherit nocreatedb nocreaterole CONNECTION LIMIT 200; CREATE ROLE 从库上验证12345postgres=# \\du List of roles Role name | Attributes | Member of -----------+-----------------------------------+----------- browser | No inheritance +| &#123;&#125; 说明：果然，在从库上就立刻创建了新用户 ‘browser’ 12.2 主库上创建表空间主库上创建表空间目录1mkdir -p /database/pgdata/pg_tbs/tbs_browser 从库上也执行 mkdir -p /database/pgdata/pg_tbs/tbs_browser (On Sandby) 主库上创建表空间12postgres=# create tablespace tbs_browser owner skytf LOCATION '/database/pgdata/pg_tbs/tbs_browser'; CREATE TABLESPACE 在从库上验证12345678postgres=# \\db List of tablespaces Name | Owner | Location -------------+----------+------------------------------------- pg_default | postgres | pg_global | postgres | tbs_browser | skytf | /database/pgdata/pg_tbs/tbs_browser tbs_mydb | skytf | /database/pgdata/pg_tbs/tbs_mydb 表空间”tbs_browser” 也立刻创建过来了 12.3 主库上创建数据库123456postgres=# CREATE DATABASE browser postgres-# WITH OWNER = skytf postgres-# TEMPLATE = template0 postgres-# ENCODING = 'UTF8' postgres-# TABLESPACE = tbs_browser; CREATE DATABASE 从库上验证123456789101112postgres=# \\l List of databases Name | Owner | Encoding | Collation | Ctype | Access privileges -----------+----------+----------+-----------+-------+----------------------- browser | skytf | UTF8 | C | C | mydb | skytf | UTF8 | C | C | postgres | postgres | UTF8 | C | C | =Tc/postgres + | | | | | postgres=CTc/postgres template0 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres template1 | postgres | UTF8 | C | C | =c/postgres + | | | | | postgres=CTc/postgres 从库上数据库 “browser” 也立刻有了，几乎没有延时。 12.4 同时观察CSV日志，从日志上看，基本没有延迟12342011-01-08 17:28:59.335 CST,\"postgres\",\"postgres\",24274,\"[local]\",4d282e5b.5ed2,2,\"authentication\",2011-01-08 17:28:59 CST,2/3,0,LOG,00000,\"connection authorized: user=postgres database=postgres\",,,,,,,,,\"\"12.5 在从库上建表 mydb=&gt; create table table3(id integer); ERROR: cannot execute CREATE TABLE in a read-only transaction 说明：从库是以只读形式打开，只能执行读操作，不能写。 监控流复制1234567891011121314151617CREATE OR REPLACE VIEW pg_stat_replication AS SELECT S.procpid, S.usesysid, U.rolname AS usename, S.application_name, S.client_addr, S.client_port, S.backend_start FROM pg_stat_get_activity(NULL) AS S, pg_authid U WHERE S.usesysid = U.oid AND S.datid = 0; postgres=# select * from pg_stat_replication ; procpid | usesysid | usename | application_name | client_addr | client_port | backend_start ---------+----------+---------+------------------+--------------+-------------+------------------------------- 27225 | 64949 | repuser | | 192.168.1.26 | 59836 | 2011-01-08 17:22:05.480584+08 (1 row) 总结以上就是搭建 streaming(又称Hot Standby)的详细过程，这是一个令人兴奋的学习过程，因为PG的HOT STANDBY 提供的数据及时性和可靠性丝毫不比ORACLE的DataGuard逊色，相反，本人还觉得比在这方面比Oracle更给力，谢谢开源的人们提供这么优秀的数据库。 参考 http://www.postgresql.org/docs/9.0/static/high-availability.html","categories":[{"name":"PG高可用性","slug":"PG高可用性","permalink":"https://postgres.fun/categories/PG高可用性/"}],"tags":[{"name":"PostgreSQL Replication","slug":"PostgreSQL-Replication","permalink":"https://postgres.fun/tags/PostgreSQL-Replication/"}]},{"title":"PostgreSQL 几种禁止应用访问的方法","slug":"20110106104625","date":"2011-01-06T02:46:25.000Z","updated":"2018-09-04T01:33:47.598Z","comments":true,"path":"20110106104625.html","link":"","permalink":"https://postgres.fun/20110106104625.html","excerpt":"","text":"在数据库维护时，有时因为特殊的数据库维护，在维护期间，需要禁止应用访问，下面总结下常见的禁止应用访问的方法。 方法一：停应用最常见的方法也就是停应用，将应用程序停掉，这种方法也有不足，当应用服务器很多，有十几台甚至几十台时，一台台停显然太慢了，下面有几种方法是从数据库服务器考虑的。 方法二：设置pg_hba.conf设置 $PGDATA/pg_hba.conf ，对外所有IP不允许连接。 在$PGDATA/pg_hba中加一条12# IPv6 local connections: host all all 0.0.0.0/0 reject 同时将允许的行注释掉1# host all all 0.0.0.0/0 md5 设置好后，重新载入 pg_hba，只需要以下命令12[postgres@pg1 pg_root]$ pg_ctl reload -D $PGDATA server signaled 这样，就不允许所有的IP连接数据库了，当然，对已经连上数据库的会话不受影响，如有需要,可以通过”pg_teminate_backend” 结束已经连接的会话 (PG 8.4或者以下版本)。 方法三：修改数据库连接数修改数据库 mydb 的连接数为212postgres=# alter database mydb connection limit 2; ALTER DATABASE 已经2个连接的情况下，再次连接12[postgres@pg1 ~]$ psql mydb skytf psql: FATAL: too many connections for database \"mydb\" 方法四：修改应用帐号密码修改应用帐号的密码，数据库维护操作完成后再修改回来。修改密码语法12postgres=# alter role dwetl with encrypted password '111111'; ALTER ROLE 方法五：设置OS层防火墙通过OS层面设置防火墙访问策略，由于这个设置需要ROOT权限，这里暂时不讨论。 总结上面介绍了五种方法，为了省事，本人经常采用方法4, 同时推荐用方法2和3, 具体采用哪种方法也要根据具体情况选择。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL：官网介绍","slug":"20101230134456","date":"2010-12-30T05:44:56.000Z","updated":"2018-09-04T01:33:47.551Z","comments":true,"path":"20101230134456.html","link":"","permalink":"https://postgres.fun/20101230134456.html","excerpt":"","text":"关于 PostgreSQL 的官网介绍，以前只是稍微浏览了下，今天稍微空些，认真的看了一遍，偶比较关心的是关于PG的一些限制，在文中已用蓝色标识，全文链接为 http://www.postgresql.org/about/ PostgreSQL is a powerful, open source object-relational database system. It has more than 15 years of active development and a proven architecture that has earned it a strong reputation for reliability, data integrity, and correctness. It runs on all major operating systems, including Linux, UNIX (AIX, BSD, HP-UX, SGI IRIX, Mac OS X, Solaris, Tru64), and Windows. It is fully ACID compliant, has full support for foreign keys, joins, views, triggers, and stored procedures (in multiple languages). It includes most SQL:2008 data types, including INTEGER, NUMERIC, BOOLEAN, CHAR, VARCHAR, DATE, INTERVAL, and TIMESTAMP. It also supports storage of binary large objects, including pictures, sounds, or video. It has native programming interfaces for C/C++, Java, .Net, Perl, Python, Ruby, Tcl, ODBC, among others, and exceptional documentation.An enterprise class database, PostgreSQL boasts sophisticated features such as Multi-Version Concurrency Control (MVCC), point in time recovery, tablespaces, asynchronous replication, nested transactions (savepoints), online/hot backups, a sophisticated query planner/optimizer, and write ahead logging for fault tolerance. It supports international character sets, multibyte character encodings, Unicode, and it is locale-aware for sorting, case-sensitivity, and formatting. It is highly scalable both in the sheer quantity of data it can manage and in the number of concurrent users it can accommodate. There are active PostgreSQL systems in production environments that manage in excess of 4 terabytes of data. Some general PostgreSQL limits are included in the table below. Limit ValueMaximum Database Size UnlimitedMaximum Table Size 32 TBMaximum Row Size 1.6 TBMaximum Field Size 1 GBMaximum Rows per Table UnlimitedMaximum Columns per Table 250 - 1600 depending on column typesMaximum Indexes per Table Unlimited PostgreSQL has won praise from its users and industry recognition, including the Linux New Media Award for Best Database System and five time winner of the The Linux Journal Editors’ Choice Award for best DBMS.Featureful and Standards CompliantPostgreSQL prides itself in standards compliance. Its SQL implementation strongly conforms to the ANSI-SQL:2008 standard. It has full support for subqueries (including subselects in the FROM clause), read-committed and serializable transaction isolation levels. And while PostgreSQL has a fully relational system catalog which itself supports multiple schemas per database, its catalog is also accessible through the Information Schema as defined in the SQL standard.Data integrity features include (compound) primary keys, foreign keys with restricting and cascading updates/deletes, check constraints, unique constraints, and not null constraints.It also has a host of extensions and advanced features. Among the conveniences are auto-increment columns through sequences, and LIMIT/OFFSET allowing the return of partial result sets. PostgreSQL supports compound, unique, partial, and functional indexes which can use any of its B-tree, R-tree, hash, or GiST storage methods.GiST (Generalized Search Tree) indexing is an advanced system which brings together a wide array of different sorting and searching algorithms including B-tree, B+-tree, R-tree, partial sum trees, ranked B+-trees and many others. It also provides an interface which allows both the creation of custom data types as well as extensible query methods with which to search them. Thus, GiST offers the flexibility to specify what you store, how you store it, and the ability to define new ways to search through it — ways that far exceed those offered by standard B-tree, R-tree and other generalized search algorithms.GiST serves as a foundation for many public projects that use PostgreSQL such as OpenFTS and PostGIS. OpenFTS (Open Source Full Text Search engine) provides online indexing of data and relevance ranking for database searching. PostGIS is a project which adds support for geographic objects in PostgreSQL, allowing it to be used as a spatial database for geographic information systems (GIS), much like ESRI’s SDE or Oracle’s Spatial extension.Other advanced features include table inheritance, a rules systems, and database events. Table inheritance puts an object oriented slant on table creation, allowing database designers to derive new tables from other tables, treating them as base classes. Even better, PostgreSQL supports both single and multiple inheritance in this manner.The rules system, also called the query rewrite system, allows the database designer to create rules which identify specific operations for a given table or view, and dynamically transform them into alternate operations when they are processed.The events system is an interprocess communication system in which messages and events can be transmitted between clients using the LISTEN and NOTIFY commands, allowing both simple peer to peer communication and advanced coordination on database events. Since notifications can be issued from triggers and stored procedures, PostgreSQL clients can monitor database events such as table updates, inserts, or deletes as they happen.Highly CustomizablePostgreSQL runs stored procedures in more than a dozen programming languages, including Java, Perl, Python, Ruby, Tcl, C/C++, and its own PL/pgSQL, which is similar to Oracle’s PL/SQL. Included with its standard function library are hundreds of built-in functions that range from basic math and string operations to cryptography and Oracle compatibility. Triggers and stored procedures can be written in C and loaded into the database as a library, allowing great flexibility in extending its capabilities. Similarly, PostgreSQL includes a framework that allows developers to define and create their own custom data types along with supporting functions and operators that define their behavior. As a result, a host of advanced data types have been created that range from geometric and spatial primitives to network addresses to even ISBN/ISSN (International Standard Book Number/International Standard Serial Number) data types, all of which can be optionally added to the system.Just as there are many procedure languages supported by PostgreSQL, there are also many library interfaces as well, allowing various languages both compiled and interpreted to interface with PostgreSQL. There are interfaces for Java (JDBC), ODBC, Perl, Python, Ruby, C, C++, PHP, Lisp, Scheme, and Qt just to name a few.Best of all, PostgreSQL’s source code is available under a liberal open source license: the PostgreSQL License. This license gives you the freedom to use, modify and distribute PostgreSQL in any form you like, open or closed source. Any modifications, enhancements, or changes you make are yours to do with as you please. As such, PostgreSQL is not only a powerful database system capable of running the enterprise, it is a development platform upon which to develop in-house, web, or commercial software products that require a capable RDBMS.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 9.0 的 Grant 操作得到增强","slug":"20101229195336","date":"2010-12-29T11:53:36.000Z","updated":"2018-09-04T01:33:47.504Z","comments":true,"path":"20101229195336.html","link":"","permalink":"https://postgres.fun/20101229195336.html","excerpt":"","text":"PostgreSQL 9.0 在 grant 命令上得到了增强。它提供一条 grant命令，能够将某个schema下的全部表赋给用户, 这在工作中带来了便利。来自PG9.0官网文档,语法如下12345GRANT &#123; &#123; SELECT | INSERT | UPDATE | DELETE | TRUNCATE | REFERENCES | TRIGGER &#125; [,...] | ALL [ PRIVILEGES ] &#125; ON &#123; [ TABLE ] table_name [, ...] | ALL TABLES IN SCHEMA schema_name [, ...] &#125; TO &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ WITH GRANT OPTION ] 其中 “ALL TABLES IN SCHEMA schema_name” 就为上面的这种用法。今天正好有项目有查询需求，需要新增一查询帐号，能够查询生产库上所有的业务表。以下是详细步骤 开通数据库连接权限12skytf=#grant connect on database skytf to select_csl; GRANT 开通SCHEMA使用权限数据库 skytf下业务SCHEMA的使用权限1grant select on all tables in schema skytf to select_csl; 开通帐号对表的只读权限给查询帐号 select_csl赋予 schemal skytf 下所有表的只读权限12skytf=# grant select on all tables in schema skytf to select_csl; GRANT 检查权限是否已加1234567891011skytf=&gt; select grantor,grantee,table_schema,table_name from information_schema.table_privileges where grantee='select_csl' and table_schema='skytf' order by table_name ; grantor | grantee | table_schema | table_name ---------+------------+--------------+----------------------------- skytf | select_csl | skytf | skytf_user skytf | select_csl | skytf | albumcheck_log skytf | select_csl | skytf | building_guest_log .. .. 省略 (15 rows) 上面部分结果已经省略，从结果可以看出 帐号select_csl 获得了skytf下所有的表的查询权限。 官网解释 There is also an option to grant privileges on all objects of the same type within one or more schemas. This functionality is currently supported only for tables, sequences, and functions(but note that ALL TABLES is considered to include views). 上面说 tables,sequences,functions 支持这种用法。 更多用法12345678910GRANT &#123; &#123; USAGE | SELECT | UPDATE &#125; [,...] | ALL [ PRIVILEGES ] &#125; ON &#123; SEQUENCE sequence_name [, ...] | ALL SEQUENCES IN SCHEMA schema_name [, ...] &#125; TO &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ WITH GRANT OPTION ] GRANT &#123; EXECUTE | ALL [ PRIVILEGES ] &#125; ON &#123; FUNCTION function_name ( [ [ argmode ] [ arg_name ] arg_type [, ...] ] ) [, ...] | ALL FUNCTIONS IN SCHEMA schema_name [, ...] &#125; TO &#123; [ GROUP ] role_name | PUBLIC &#125; [, ...] [ WITH GRANT OPTION ] 总结PostgrSQL 提供的 ALL ... IN SCHEMA schema_name 用法使给DBA在日常工作中只需要一条命令就能给指定用户授权，打破了传统的每张表单独赋权的模式。 上面只演示 tables 的授权方法，有兴趣的朋友可能对 functions 和 sequences 做下类似的实验。这里不再详细描述。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"普通表改为分区表","slug":"20101227203837","date":"2010-12-27T12:38:37.000Z","updated":"2018-09-04T01:33:47.442Z","comments":true,"path":"20101227203837.html","link":"","permalink":"https://postgres.fun/20101227203837.html","excerpt":"","text":"生产库有张表数据量已经很大了，比较可行的方法是将它改成分区表，经分析，打算用交换分区的方法实施，在正式在生产系统上操作之前，先在测试环境上测试一把，以下是详细步骤。 创建普通表12345678910111213141516171819202122232425262728293031323334353637CREATE TABLE OCP.TABLE_DATE1 ( OUT_MSG CLOB, FIRST_TIME DATE, STATUS VARCHAR2(1), DELETED VARCHAR2(3)) TABLESPACE OCP_TF LOB (OUT_MSG) STORE AS ( TABLESPACE LOB_DATA ENABLE STORAGE IN ROW CHUNK 8192 RETENTION NOCACHE INDEX ( TABLESPACE LOB_DATA STORAGE ( INITIAL 64K NEXT 1 MINEXTENTS 1 MAXEXTENTS UNLIMITED PCTINCREASE 0 BUFFER_POOL DEFAULT )) STORAGE ( INITIAL 64K MINEXTENTS 1 MAXEXTENTS UNLIMITED PCTINCREASE 0 BUFFER_POOL DEFAULT ) ) NOCACHE NOPARALLEL MONITORING;create index idx_first_time on table_date1(first_time);create index idx_status on table_date1(status); 创建分区表12345678910create table TABLE_DATE1_NEW ( OUT_MSG CLOB, FIRST_TIME DATE, STATUS VARCHAR2(1), DELETED VARCHAR2(3) ) PARTITION BY RANGE (FIRST_TIME) (PARTITION P1 VALUES LESS THAN (TO_DATE('2009-11-26', 'YYYY-MM-DD')), PARTITION P2 VALUES LESS THAN (MAXVALUE)); 建索LOCAL引12create index idx_first_time_NEW on table_date1_NEW(first_time) local ;create index idx_status_NEW on table_date1_NEW(status) local ; 注意: 如果要交换分区，分区表的索引必须为local类型，且索引列及顺序要与交换的表一致。否则报 ORA-14098: ALTER TABLE EXCHANGE PARTITION 中的表索引不匹配也做了个实验,不加local参数,交换分区后,索引虽然没分区(全局的),但是状态为 UNUSABLE 交换分区12345678910111213141516171819ALTER TABLE TABLE_DATE1_NEW EXCHANGE PARTITION P1 WITH TABLE TABLE_DATE1 INCLUDING INDEXES ;19:45:30 [SYS@tf](mailto:SYS@tf)&gt; select count(*) from ocp.table_date1 ; COUNT(*) ---------- 835586已选择 1 行。 19:45:40 [SYS@tf](mailto:SYS@tf)&gt; @tablesize 输入 table_name 的值: TABLE_DATE1OWNER SEGMENT_NAME SEGMENT_TYPE size(M) ------------ ----------------------------------- -------------------- ---------- OCP SYS_IL0000057740C00001$$ LOBINDEX 1 IDX_FIRST_TIME INDEX 18 TABLE_DATE1 TABLE 57 ---------- sum 76 OCP_TEST TABLE_DATE1 TABLE 1 ---------- sum 1 查看TABLE_DATE1_NEW表和索引大小12345678910111213141516171819:45:47 [SYS@tf](mailto:SYS@tf)&gt; select segment_name,partition_name,bytes/1024/1024 M 19:45:48 2 From dba_segments where segment_name in ('SYS_IL0000057744C00001$$','IDX_FIRST_TIME_NEW');SEGMENT_NAME PARTITION_NAME M ----------------------------------- ------------------------------ ---------- SYS_IL0000057744C00001$$ SYS_IL_P63 1 SYS_IL0000057744C00001$$ SYS_IL_P64 1 IDX_FIRST_TIME_NEW P1 1 IDX_FIRST_TIME_NEW P2 1已选择4行。再次查看TABLE_DATE1_NEW表和索引大小 19:47:35 [SYS@tf](mailto:SYS@tf)&gt; select segment_name,partition_name,bytes/1024/1024 M 19:49:46 2 From dba_segments where segment_name in ('SYS_IL0000057744C00001$$','IDX_FIRST_TIME_NEW');SEGMENT_NAME PARTITION_NAME M ----------------------------------- ------------------------------ ---------- SYS_IL0000057744C00001$$ SYS_IL_P63 1 SYS_IL0000057744C00001$$ SYS_IL_P64 1 IDX_FIRST_TIME_NEW P1 18 IDX_FIRST_TIME_NEW P2 1 注意 : IDX_FIRST_TIME_NEW 由原来1M变为现在18M1234alter table TABLE_DATE1 rename to TABLE_DATE1_OLD;alter table TABLE_DATE1_NEW rename to TABLE_DATE1;drop table table_date1_old;(删除原表)alter index IDX_FIRST_TIME_NEW rename to IDX_FIRST_TIME;(索引RENAME) 到这里,普通表转化成分区表成功完成 增加分区12345678910111213141516171819202122232425262728293031alter table TABLE_DATE1 drop partition P2; (删除上界分区)alter table ocp.TABLE_DATE1 add partition TABLE_DATE1_2009_11 values less than (TO_DATE(' 2009-12-01 00:00:00', 'SYYYY-MM-DD HH24:MI:SS', 'NLS_CALENDAR=GREGORIAN')) tablespace OCP_TF pctfree 10 initrans 1 maxtrans 255 storage ( initial 1M next 1M minextents 1 maxextents unlimited pctincrease 0 ); alter table ocp.TABLE_DATE1 add partition TABLE_DATE1_2009_12 values less than (TO_DATE(' 2010-01-01 00:00:00', 'SYYYY-MM-DD HH24:MI:SS', 'NLS_CALENDAR=GREGORIAN')) tablespace OCP_TF pctfree 10 initrans 1 maxtrans 255 storage ( initial 1M next 1M minextents 1 maxextents unlimited pctincrease 0 ); 分区分析12EXECUTE DBMS_STATS.GATHER_TABLE_STATS(ownname=&gt;'OCP',tabname=&gt;'TABLE_DATE1',partname=&gt;'TABLE_DATE1_2009_11',estimate_percent=&gt;10,granularity=&gt;'PARTITION',CASCADE=&gt;TRUE); EXECUTE DBMS_STATS.GATHER_TABLE_STATS(ownname=&gt;'OCP',tabname=&gt;'TABLE_DATE1',partname=&gt;'TABLE_DATE1_2009_12',estimate_percent=&gt;10,granularity=&gt;'PARTITION',CASCADE=&gt;TRUE); 附录 EXCHANGE PARTITION 选项 WITH TABLE tableSpecify the table with which the partition or subpartition will be exchanged.INCLUDING INDEXESSpecify INCLUDING INDEXES if you want local index partitions or subpartitions to be exchanged with the corresponding table index (for a nonpartitioned table) or local indexes (for a hash-partitioned table).EXCLUDING INDEXESSpecify EXCLUDING INDEXES if you want all index partitions or subpartitions corresponding to the partition and all the regular indexes and index partitions on the exchanged table to be marked UNUSABLE.WITH VALIDATIONSpecify WITH VALIDATION if you want Oracle Database to return an error if any rows in the exchanged table do not map into partitions or subpartitions being exchanged.WITHOUT VALIDATIONSpecify WITHOUT VALIDATION if you do not want Oracle Database to check the proper mapping of rows in the exchanged table.","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"Oracle 10g 手工建库","slug":"20101227203151","date":"2010-12-27T12:31:51.000Z","updated":"2018-09-04T01:33:47.395Z","comments":true,"path":"20101227203151.html","link":"","permalink":"https://postgres.fun/20101227203151.html","excerpt":"","text":"以前在听播布客小布老师视频的时候，学习了手工建库, oacle 创建数据库一般都是使用dbca ，通过脚本建库给自己在创建数据库上多了一种选择。 手工建库步骤 Choose a unique instance and database name Choose a database character set Set a operating system variable Create the initialization parameter file Start the instance in NOMOUNT stage Create and Excetue the CREATE DATABASE commond Run scripts to generate the data dictionary and accomplish post-creation setps Create addtitional tablespaces as needed 1 ) CHOOSE A UNIQUE SID AND DATABASE CHARACTER 和相关目录1234*.db_name='MANUA' CHARACTER SET ZHS16GBK NATIONAL CHARACTER SET AL16UTF16mkdir /oradata/MANUALmkdir $ORACLE_BASE/admin/MANUAcd MANUA mkdir adump bdump cdump dpdump pfile udump 2 ) 用orapwd命令创建密码文件1orapwd file=$ORACLE_HOME/dbs/orapwMANUA password=MANUA20 entries=10 注意:需要创建密码文件,否则只能通过操作系统论证,远程Oracle客户端不能连接. 3) 准备环境变量 以下五个为常用123456exprt ORACLE_BASE=/app/oracleexport ORACLE_HOME=$ORACLE_BASE/product/10gexport ORACLE_SID=MANUAexport ORA_NLS33=$ORACLE_HOME/ocommon/nls/admin/dataexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/libexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:$LD_LIBRARY_PATH 4) 准备初始化参数123456789101112131415161718192021[oracle@standby dbs]$ cat initMANUA.ora*.audit_file_dest='/app/oracle/admin/MANUA/adump'*.background_dump_dest='/app/oracle/admin/MANUA/bdump'#*.compatible='10.2.0.3.0'*.control_files='/oradata/MANUA/control01.ctl','/oradata/MANUA/control02.ctl','/oradata/MANUA/control03.ctl'*.core_dump_dest='/app/oracle/admin/MANUA/cdump'*.db_block_size=8192*.db_domain=''*.db_file_multiblock_read_count=16*.db_name='MANUA'*.dispatchers='(PROTOCOL=TCP) (SERVICE=MANUAXDB)'*.job_queue_processes=10*.open_cursors=300*.pga_aggregate_target=103936000*.processes=150*.remote_login_passwordfile='EXCLUSIVE'*.sga_max_size=251225472*.sga_target=221225472*.undo_management='AUTO'*.undo_tablespace='UNDOTBS1'*.user_dump_dest='/app/oracle/admin/MANUA/udump' 5) 创建数据库如果是WINDOWS,用以下命令创建或者删除服务123456789101112131415161718192021222324252627282930oradim -NEW -SID monitor1oradim -DELETE -SID OSSStartup nomount;--create database sqlCREATE DATABASE CMTT81CONTROLFILE REUSEUSER SYS IDENTIFIED BY CMTT81155USER SYSTEM IDENTIFIED BY CMTT81155LOGFILE GROUP 1 ('/dev/vg01/rlvredolog11', '/dev/vg01/rlvredolog12') SIZE 512M, GROUP 2 ('/dev/vg01/rlvredolog21', '/dev/vg01/rlvredolog22') SIZE 512M, GROUP 3 ('/dev/vg01/rlvredolog31', '/dev/vg01/rlvredolog32') SIZE 512M, GROUP 4 ('/dev/vg01/rlvredolog41', '/dev/vg01/rlvredolog42') SIZE 512M, GROUP 5 ('/dev/vg01/rlvredolog51', '/dev/vg01/rlvredolog52') SIZE 512M, GROUP 6 ('/dev/vg01/rlvredolog61', '/dev/vg01/rlvredolog62') SIZE 512MMAXLOGFILES 6MAXLOGMEMBERS 5MAXLOGHISTORY 1MAXDATAFILES 1000MAXINSTANCES 1CHARACTER SET UTF8NATIONAL CHARACTER SET UTF8DATAFILE '/dev/vg01/rCMTT81_system_01.dbf' SIZE 3130M AUTOEXTEND OFFSYSAUX DATAFILE '/dev/vg01/rCMTT81_sysaux_01.dbf' SIZE 3130M AUTOEXTEND OFFDEFAULT TEMPORARY TABLESPACE temp TEMPFILE '/dev/vg01/rCMTT81_temp_01.dbf' SIZE 5140M AUTOEXTEND OFFUNDO TABLESPACE UNDOTBS1 DATAFILE '/dev/vg01/rCMTT81_undo_01.dbf' SIZE 10240M AUTOEXTEND OFF; 6) 导入数据字典123@?/rdbms/admin/catalog.sql;@?/rdbms/admin/catproc.sql;@?/sqlplus/admin/pupbld.sql; --system 用户执行 附一：查自动扩展1234567col file_name format a50;alter database datafile 1 autoextend off;alter database datafile 2 autoextend off;alter database datafile 3 autoextend off;alter database tempfile 1 autoextend off;select file_name,file_id,autoextensible ,bytes/(1024*1024*1024)G from dba_data_files where autoextensible='YES';select file_name,file_id,autoextensible,bytes/(1024*1024*1024)G From dba_temp_files; 附二：手工建库DBCA找不到解决方法vi /etc/oratab加入你手工创建的数据:形如：1MANUA :/app/oracle/product/10g:N 然后在运行dbca就能看到了！","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"MySQL：二进制安装","slug":"20101227202529","date":"2010-12-27T12:25:29.000Z","updated":"2018-09-04T01:33:47.332Z","comments":true,"path":"20101227202529.html","link":"","permalink":"https://postgres.fun/20101227202529.html","excerpt":"","text":"Linux 上 Mysql 二进制安装 介质下载http://downloads.mysql.com/archives/mysql-5.5/mysql-5.5.0-m2-linux-i686-glibc23.tar.gz 安装简版12345678shell&gt; groupadd mysql shell&gt; useradd -r -g mysql mysql shell&gt; cd /usr/local shell&gt; tar zxvf /path/to/mysql-VERSION-OS.tar.gz shell&gt; ln -s full-path-to-mysql-VERSION-OS mysql shell&gt; cd mysql shell&gt; chown -R mysql . shell&gt; chgrp -R mysql . 库初始化123456shell&gt; scripts/mysql_install_db --user=mysql shell&gt; chown -R root . shell&gt; chown -R mysql dataCOPY配置文件 # Next command is optional shell&gt; cp support-files/my-medium.cnf /etc/my.cnf 开启MySQL服务123shell&gt; bin/mysqld_safe --user=mysql &amp;# Next command is optional shell&gt; cp support-files/mysql.server /etc/init.d/mysql.server 环境变量# .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH=$PATH:$HOME/bin export PATH export LANG=en_US.utf8 export MYSQLHOME=/opt/mysql export DATE=`date +&quot;%Y%m%d%H%M&quot;` export PATH=$PGHOME/bin:$PATH:. alias rm=&apos;rm -i&apos; alias ll=&apos;ls -lh&apos; 总结：以上就是第一次在LINUX上安装 mysql的过程，感觉步骤还是比较简洁的。","categories":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/categories/MySQL/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://postgres.fun/tags/MySQL/"}]},{"title":"Oracle HWM 实验","slug":"20101226105649","date":"2010-12-26T02:56:49.000Z","updated":"2018-09-04T01:33:47.285Z","comments":true,"path":"20101226105649.html","link":"","permalink":"https://postgres.fun/20101226105649.html","excerpt":"","text":"基本概念关于Oracle数据库概念性的知识Oracle高水位标志:high-water mark 可以用以下几点对high-water mark进行理解 指一个表中曾经被用过的最后一个块2 .假如有数据被插入表，high-water mark 就移到到被使用的最后一个块 假如有数据被删除，high-water mark的位置不会变 high-water mark被储存在表的段头(segment header of the table) 当对表执行全表扫描时，oracle server 被所有的块直到high-water mark 原理 实验证明假如有数据被删除，high-water mark的位置不会降低 创建测试表123414:13:48 [OCP@tf](mailto:OCP@tf)&gt; create table n (id number, remark char(100));表已创建。已用时间: 00: 00: 00.09 14:13:51 [OCP@tf](mailto:OCP@tf)&gt; set timi off; 插入测试数据1234567891011121314:14:47 [OCP@tf](mailto:OCP@tf)&gt; edit 已写入 file afiedt.buf 1 begin 2 for i in 1..1000000 loop 3 insert into n values ( i ,'bbk'); 4 end loop; 5 commit; 6* end; 14:15:03 [OCP@tf](mailto:OCP@tf)&gt; /PL/SQL 过程已成功完成。14:17:46 [OCP@tf](mailto:OCP@tf)&gt; 14:17:46 [OCP@tf](mailto:OCP@tf)&gt; set timi on; 14:17:50 [OCP@tf](mailto:OCP@tf)&gt; set autotrace on; 查看统计信息12345678910111213141516171819202122232425262728293031323314:17:56 [OCP@tf](mailto:OCP@tf)&gt; select count(*) from n; COUNT(*) ---------- 1000000已用时间: 00: 00: 08.78执行计划 ---------------------------------------------------------- Plan hash value: 1185471427------------------------------------------------------------------- | Id | Operation | Name | Rows | Cost (%CPU)| Time | ------------------------------------------------------------------- | 0 | SELECT STATEMENT | | 1 | 3427 (2)| 00:00:42 | | 1 | SORT AGGREGATE | | 1 | | | | 2 | TABLE ACCESS FULL| N | 989K| 3427 (2)| 00:00:42 | -------------------------------------------------------------------Note ----- - dynamic sampling used for this statement 统计信息 ---------------------------------------------------------- 29 recursive calls 1 db block gets 31038 consistent gets 8493 physical reads 1106036 redo size 408 bytes sent via SQL*Net to client 385 bytes received via SQL*Net from client 2 SQL*Net roundtrips to/from client 0 sorts (memory) 0 sorts (disk) 1 rows processed 发现 physical reads 8493 ,下面进行表分析123456789101112131415161718192021222324252614:18:37 [OCP@tf](mailto:OCP@tf)&gt; edit 已写入 file afiedt.buf 1 begin 2 DBMS_STATS.GATHER_TABLE_STATS(OWNNAME=&gt; 'OCP', TABNAME=&gt; 'N', PARTNAME=&gt; NULL); 3* end; 14:18:52 [OCP@tf](mailto:OCP@tf)&gt; /PL/SQL 过程已成功完成。已用时间: 00: 00: 12.82 14:19:06 [OCP@tf](mailto:OCP@tf)&gt; select owner,table_name, num_rows,blocks from dba_tables where table_name='N';OWNER TABLE_NAME NUM_ROWS BLOCKS ------------------------------ ------------------------------ ---------- ---------- OCP N 998179 15458 14:19:11 [OCP@tf](mailto:OCP@tf)&gt; set autotrace off; 14:19:18 [OCP@tf](mailto:OCP@tf)&gt; select owner,table_name, num_rows,blocks from dba_tables where table_name='N';OWNER TABLE_NAME NUM_ROWS BLOCKS ------------------------------ ------------------------------ ---------- ---------- OCP N 998179 1545814:19:35 [OCP@tf](mailto:OCP@tf)&gt; col segment_name format a30; 14:19:55 [OCP@tf](mailto:OCP@tf)&gt; select owner,segment_name,bytes/1024/1024M from dba_segments where segment_name='N';OWNER SEGMENT_NAME M ------------------------------ ------------------------------ ---------- OCP N 12214:19:57 [OCP@tf](mailto:OCP@tf)&gt; delete from n; 已删除1000000行。1214:23:12 [commit\"&gt;OCP@tf&gt;commit](mailto:OCP@tf);提交完成。 DELETE数据后再次查看执行计划1234567891011121314151617181920212223242526272829303114:23:12 [OCP@tf](mailto:OCP@tf)&gt; 14:23:12 [OCP@tf](mailto:OCP@tf)&gt; set autotrace on; 14:23:24 [OCP@tf](mailto:OCP@tf)&gt; select count(*) from n; COUNT(*) ---------- 0已用时间: 00: 00: 03.42执行计划 ---------------------------------------------------------- Plan hash value: 1185471427------------------------------------------------------------------- | Id | Operation | Name | Rows | Cost (%CPU)| Time | ------------------------------------------------------------------- | 0 | SELECT STATEMENT | | 1 | 3428 (2)| 00:00:42 | | 1 | SORT AGGREGATE | | 1 | | | | 2 | TABLE ACCESS FULL| N | 998K| 3428 (2)| 00:00:42 | ------------------------------------------------------------------- 统计信息 ---------------------------------------------------------- 1 recursive calls 0 db block gets 15473 consistent gets 11988 physical reads 0 redo size 407 bytes sent via SQL*Net to client 385 bytes received via SQL*Net from client 2 SQL*Net roundtrips to/from client 0 sorts (memory) 0 sorts (disk) 1 rows processed physical reads 为11988 ,不但不减少,而且增多 再次查看统计信息1234567891014:23:30 [OCP@tf](mailto:OCP@tf)&gt; select owner,table_name, num_rows,blocks from dba_tables where table_name='N';OWNER TABLE_NAME NUM_ROWS BLOCKS ------------------------------ ------------------------------ ---------- ---------- OCP N 998179 1545814:24:04 [OCP@tf](mailto:OCP@tf)&gt; set autotrace off; 14:24:09 [OCP@tf](mailto:OCP@tf)&gt; select owner,table_name, num_rows,blocks from dba_tables where table_name='N';OWNER TABLE_NAME NUM_ROWS BLOCKS ------------------------------ ------------------------------ ---------- ---------- OCP N 998179 15458已用时间: 00: 00: 00.01 表分析一下12345678910111214:24:12 [OCP@tf](mailto:OCP@tf)&gt; begin 14:24:23 2 DBMS_STATS.GATHER_TABLE_STATS(OWNNAME=&gt; 'OCP', TABNAME=&gt; 'N', PARTNAME=&gt; NULL); 14:24:25 3 end; 14:24:26 4 /PL/SQL 过程已成功完成。已用时间: 00: 00: 08.31 14:24:36 [OCP@tf](mailto:OCP@tf)&gt; select owner,table_name, num_rows,blocks from dba_tables where table_name='N';OWNER TABLE_NAME NUM_ROWS BLOCKS ------------------------------ ------------------------------ ---------- ---------- OCP N 0 15458已用时间: 00: 00: 00.01 14:24:38 [OCP@tf](mailto:OCP@tf)&gt; 发现数据delete后,占用的BLOCKS没有释放,也就是高水位并没有降下去 说明一: DBA_TABLES字段empty_blocks 显示HWM以上有多少块,即没有被使用的块 说明二 : HWM以下的块有些被使用,有些是空块,以下SQL查询123456select blocks \"Actual_blocks_uses_below_HWM\", round( d.avg_row_len*d.num_rows/1024/8 ,0)\"Blocks_need_below_HWM\" , blocks - round( d.avg_row_len*d.num_rows/1024/8 ,0) \"Wasted_blocks_below_HWM\", tablespace_name from dba_tables d where owner='OCP' and table_name='TEST_1' 说明表OCP.TEST_1中HWM以下共用285个块,其中HWM以下有76个空块,实际有209块有数据 降低HWM方法 1 导出,并导入此表 2使用ALTER TABLE…MOVE…命令重建表 dba_extents123408:12:04 [OCP@tf](mailto:OCP@tf)&gt; select sum(blocks) from dba_extents where segment_name='TEST_1' and owner='OCP';SUM(BLOCKS) ----------- 384 这里是384个BLOCK,多出的部分是段头;","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"DataGuard之三：常用查询","slug":"20101226104407","date":"2010-12-26T02:44:07.000Z","updated":"2018-09-04T01:33:47.223Z","comments":true,"path":"20101226104407.html","link":"","permalink":"https://postgres.fun/20101226104407.html","excerpt":"","text":"Physical Standby 搭建完后，需要定期的检查主库和备库状态，以下是我整理的一些常见查询SQL。 在主库(Primary)查询查询主库日志归档情况12SELECT name, SEQUENCE#, FIRST_TIME ,REGISTRAR,APPLIED ,NEXT_TIME,status FROM V$ARCHIVED_LOG ORDER BY SEQUENCE# desc ; 查询primary/standby库属性1select name,open_mode,database_role,protection_mode,SWITCHOVER_STATUS From v$database; 查看 db alert信息1SELECT message FROM V$DATAGUARD_STATUS; 查看log历史1select * From V$LOG_HISTORY ; 查看归档是否丢失1SELECT THREAD#, LOW_SEQUENCE#, HIGH_SEQUENCE# FROM V$ARCHIVE_GAP; 在备库(Standby)查询检查日志应用进程是否开了 MRP0123456789101112select pid,process,status from v$managed_standby;SQL&gt; select pid,process,status from v$managed_standby; PID PROCESS STATUS ---------- --------- ------------ 21589 ARCH CONNECTED 21591 ARCH CONNECTED 21593 ARCH CONNECTED 21595 RFS IDLE 21597 RFS IDLE 21604 RFS IDLE 9809 MRP0 WAIT_FOR_LOGMRP0 即为归档应用进程。 检查备库日志应用情况123456SELECT name, SEQUENCE#, FIRST_TIME ,REGISTRAR,APPLIED ,NEXT_TIME,status FROM V$ARCHIVED_LOG where applied='YES' and REGISTRAR='RFS' and name is not null ORDER BY SEQUENCE# ; ----------------------------查看dg状态----------------------------------- select * From V$DATAGUARD_CONFIGselect * From V$DATAGUARD_STATUS","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"通过 FUNCTION 创建大量子表","slug":"20101224145433","date":"2010-12-24T06:54:33.000Z","updated":"2018-09-04T01:33:47.176Z","comments":true,"path":"20101224145433.html","link":"","permalink":"https://postgres.fun/20101224145433.html","excerpt":"","text":"在PG里，用到分区表的时候需要创建大量的子表，今天有个项目，有个分区表已经很大了，一个月的数据有300G左右，目前这张分区表是以月分区，现在需要将它改为以日分区了, 下面是通过创建一个 FUNCTION 来创建日表。 创建父表1234567891011121314CREATE TABLE tbl_tmp ( id bigint DEFAULT nextval('public.tbl_tmp_id_seq'::regclass), host character varying(50), remoteip character varying(50), ntime timestamp without time zone, reqdesc character varying(2000), httpcode character varying(10), clength integer, refer character varying(2000), agent character varying(50), hsman character varying, hstype character varying(50), imsi character varying(50) ); 创建建表函数1234567891011121314151617CREATE OR REPLACE FUNCTION skytf.create_table() RETURNS void LANGUAGE plpgsql AS $function$ declarev_date char(8); v_tablename varchar(64);begin for i in 0..1500 loopv_date :=to_char(current_date + i,'YYYYMMDD'); v_tablename := 'tbl_tmp_' || v_date;execute 'create table ' || v_tablename || ' (like tbl_tmp including constraints including indexes including defaults) inherits (tbl_tmp)'; execute 'grant select on skytf.' || v_tablename || ' to dwetl';end loop; end $function$; 说明：上面function有一段是 ‘grant…’ ，这段代码在创建表的同时也把权限给加上。 执行建表函数1skytf=&gt; select create_table(); 验证1234567891011121314skytf=&gt; \\d List of relations Schema | Name | Type | Owner ---------+-----------------------------------+----------+--------- skytf | tbl_tmp_201012 | table | skytf skytf | tbl_tmp_20101224 | table | skytf skytf | tbl_tmp_20101225 | table | skytf skytf | tbl_tmp_20101226 | table | skytf skytf | tbl_tmp_20101227 | table | skytf skytf | tbl_tmp_20101228 | table | skytf skytf | tbl_tmp_20101229 | table | skytf skytf | tbl_tmp_20101230 | table | skytf.... 以上省略了大部分输出结果，说明子表已创建。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"RHEL5: SCP,SSH 不输入密码的方法","slug":"20101222094145","date":"2010-12-22T01:41:45.000Z","updated":"2018-09-04T01:33:47.113Z","comments":true,"path":"20101222094145.html","link":"","permalink":"https://postgres.fun/20101222094145.html","excerpt":"","text":"有时候Linux 主机之间传文件需要输密码，或者SSH登陆输需要密码，比较麻烦，可以通过以下方法配置。 12345678910111213141516171. Setup a new directory \".ssh\" under the home folder: $ mkdir .ssh2. Set the priority of \".ssh\" as drwxr-xr-x: $ chmod 755 .ssh3. Generating public key of the namenode for passphraseless SSH $ ssh-keygen -t rsa $ ssh-keygen -t dsa and press three \"Enter\"s4. Add the public key SSH to authorized key list: If \"~/.ssh/authorized_keys\" does not exit: $ cp ~/.ssh/id_rsa.pub ~/.ssh/authorized_keys Else appending: $ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys $ cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys 经以上配置后，scp文件到目标主机或者 ssh 登陆 不需要输入密码了。不过，如果不是特殊需要，最好不要这么做，有风险。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"DataGuard之二：归档日志维护","slug":"20101221133750","date":"2010-12-21T05:37:50.000Z","updated":"2018-09-04T01:33:47.067Z","comments":true,"path":"20101221133750.html","link":"","permalink":"https://postgres.fun/20101221133750.html","excerpt":"","text":"最近给生产项目上了 DataGuard, 前两天一直考虑如何清除 stadnby和 primary 库上的归档,同时必须保证所清理的归档已经应用到备库上。网上查了好多资料也没有比较好的方法，后来咨询了 David.guo, 他建议先在备库上删除已被应用的归档日志，然后在到 primary 库上删除这个日志。 听到这个想法，我顿时大悟，之前我的想法是通过 rman去批量删除，然后又不好判断哪些归档。 既然有了思路, 做起来只是写 Shell脚本了，写Shell脚本之前，月几个地方需要注意下 如何判断备机上哪些日志被应用?可以通过查询 v$archived_log的字段 applied 字段来判断，同时加上 REGISTRAR 和 name的限制条件， REGISTRAR=’RFS’ 表示 是 Remote File Server process 进程，别外加上 name is not null 是因为，通过rman crosscheck 并删除 expired的 archivelog 后 name 字段会被置空， –sql 如下123456SELECT name, SEQUENCE#, FIRST_TIME, REGISTRAR, APPLIED, NEXT_TIME, status FROM V$ARCHIVED_LOG where applied = 'YES' and REGISTRAR = 'RFS' and name is not null ORDER BY SEQUENCE#; 在备机删除了已归档的archivelog后，如何在远端 primary 库同时删除这个日志?这个问题在网上查了些资料，可以通过配置公钥和私钥，配置之后，scp文件，ssh 到远程主机执行命令都可以不用输入密码，具体原理可以到网上查查，这两点克服后，就可以写脚本了。 脚本 Clear_arch_orasid.sh12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788------- 功能：删除备机被 applied 的归档日志，同时也到主库上删除这个归档日志。 #!/bin/bash #Author Francs.tan #Date 2010-12-20################################################ #Set Result Flag #0-Successful #1-No Archived log that have been applied by standby database #2-ERROR:Excute file clear_primary.sh #3-ERROR: File clear_primary.sh delete error#initialize variable RESULT_FLAG=0 file_num='0'export ORACLE_HOME=/opt/oracle/product/10.2.0/db_1 export PATH=$&#123;ORACLE_HOME&#125;/bin:$&#123;PATH&#125; export ORACLE_SID=orasid export script_dir=/home/oracle/script/clear_arch export log_dir=/home/oracle/script/clear_arch/log DATE=`date +%Y%m%d`## Select the earliest archived log file that have been applied by standby database sqlplus -s \"/as sysdba\"&lt;&lt;EOF set echo off set head off set linesize 200 set feedback off;col name format a50; col REGISTRAR format a10; col APPLIED format a10;spool $&#123;script_dir&#125;/archivelog_list.txtselect 'file_num=' || name from (SELECT nvl(name, '0') as name FROM V$ARCHIVED_LOG where applied = 'YES' and REGISTRAR = 'RFS' and name is not null ORDER BY SEQUENCE#) where rownum = 1;spool off; exit EOF##Delete earliest archived log file that applied by standby database control_file=$&#123;script_dir&#125;/archivelog_list.txt . $&#123;control_file&#125;if [ $&#123;file_num&#125; == '0' ] then echo \"WARNING: No Archived log that have been applied by standby database !\" | tee -a $&#123;log_file&#125; RESULT_FLAG=1 echo \"Result Flag:$&#123;RESULT_FLAG&#125;\" | tee -a $&#123;log_file&#125; exit $&#123;RESULT_FLAG&#125; filog_file=$&#123;log_dir&#125;/clear_log_$&#123;DATE&#125;.log echo \" begin rm archivelog file = $&#123;file_num&#125; * \" | tee -a $&#123;log_file&#125; rm -f $&#123;file_num&#125;##Crosscheck archivelog echo \"* begin Crosscheck archivelog * \" | tee -a $&#123;log_file&#125; . $&#123;script_dir&#125;/check_arc.sh &gt;&gt; $&#123;log_file&#125;## Delete primary archived log echo \"* Delete primary archived log * \" | tee -a $&#123;log_file&#125; echo \"rm -f $&#123;file_num&#125;\" &gt; $&#123;script_dir&#125;/clear_primary.sh scp $&#123;script_dir&#125;/clear_primary.sh [oracle@xxx.xxx.xxx.xx:$&#123;script_dir](mailto:oracle@xxx.xxx.xxx.xx:$&#123;script_dir)&#125; ssh [oracle@xxx.xxx.xxx.xx](mailto:oracle@xxx.xxx.xxx.xx) chmod u+x $&#123;script_dir&#125;/clear_primary.sh ssh [oracle@xxx.xxx.xxx.xx](mailto:oracle@xxx.xxx.xxx.xx) $&#123;script_dir&#125;/clear_primary.shif [ $? -ne 0 ] ####如果前一条命令执行成功,则返回非0,否则为0 then echo \"*\" | tee -a $&#123;log_file&#125; echo \"ERROR:Excute file clear_primary.sh ,please check it! \" | tee -a $&#123;log_file&#125; echo \"*\" | tee -a $&#123;log_file&#125; RESULT_FLAG=2 echo \"Result Flag:$&#123;RESULT_FLAG&#125;\" | tee -a $&#123;log_file&#125; exit $&#123;RESULT_FLAG&#125; fi##rm temp files echo \"* Remove temp files * \" | tee -a $&#123;log_file&#125; rm -f $&#123;control_file&#125; rm -f $&#123;script_dir&#125;/clear_primary.sh ssh [oracle@xxx.xxx.xxx.xx](mailto:oracle@xxx.xxx.xxx.xx) rm -f $&#123;script_dir&#125;/clear_primary.shif [ $? -ne 0 ] ####如果前一条命令执行成功,则返回非0,否则为0 then echo \"*\" | tee -a $&#123;log_file&#125; echo \"ERROR: File clear_primary.sh delete error ,please check it! \" | tee -a $&#123;log_file&#125; echo \"*\" | tee -a $&#123;log_file&#125; RESULT_FLAG=3 echo \"Result Flag:$&#123;RESULT_FLAG&#125;\" | tee -a $&#123;log_file&#125; exit $&#123;RESULT_FLAG&#125; fiecho \" Remove archivelog of Primary and Standby database successfully ! *\" | tee -a $&#123;log_file&#125; exit #end 脚本 Check_arch.sh123456789---- 功能：在备库上较验归档日志，并删除过期的归档日志。 #!/bin/bashexport ORACLE_HOME=/opt/oracle/product/10.2.0/db_1 export PATH=$&#123;ORACLE_HOME&#125;/bin:$&#123;PATH&#125;rman target / &lt;&lt;EOF crosscheck archivelog all; delete noprompt expired archivelog all; exit;EOF 然后放到 crontab 里 ##Clear archivelog that have been applied by standby database 05 5 * * * /home/oracle/script/clear_arch/clear_arc_orasid.sh &gt;&gt; /home/oracle/script/clear_arch/log/clear.log 2&gt;&amp;1 总结通过操作系统删除已归档的日志虽然并不建议，当是在DG环境下，通过操作系统删除归档日志后然后通过RMAN crosscheck 后其实也没多在关系。最主要的是这个方法能定期清理归档日志，同时又能保证清理的归档日志已经被应用到了备库,后期只要在备库上再加个归档日志定期较验并删除已经过期的归档日志脚本即可。以上就是DG 环境下归档日志清理方法全部脚本，这些脚本得益于 david.guo的建议。","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"DataGuard之一：Oracle10g dg搭建","slug":"20101216134228","date":"2010-12-16T05:42:28.000Z","updated":"2018-09-04T01:33:47.004Z","comments":true,"path":"20101216134228.html","link":"","permalink":"https://postgres.fun/20101216134228.html","excerpt":"","text":"最近准备给一个生产项目上oracle10g Dataguard, 两台上机上均装好oracle 10204软件，并在一台上已经建库。OS: Red Hat Enterprise 5DB: Oracle 10204 这篇日志不讲述 DataGuard 的原理，只是oracle 10g DataGuard 搭建的详细过程。 1 将主库设为LOGGING模式,并生成初始化参数文件和，standby 控制文件1SQL&gt; alter database force logging 将主库设为归档模式1234SQL&gt; SHUTDOWN IMMEDIATE; SQL&gt; STARTUP MOUNT; SQL&gt; ALTER DATABASE ARCHIVELOG; SQL&gt; ALTER DATABASE OPEN; 创建初始化参数文件12SQL&gt; create pfile='/oradata/initbc_ocp5.ora' from spfile;File created. 创建standby控制文件,在主库上执行12SQL&gt; ALTER DATABASE CREATE STANDBY CONTROLFILE AS '/home/oracle/script/control01.ctl';Database altered. 2 关闭主库,复制数据文件,redo和 standby 控制文件到备机的相应目录12tar cvf skycard.tar /oradata/skycard -- ftp控制文件(standby controfile),数据文件(skycard.tar)到从库相应目录 生成三份控制文件12cp control01.ctl control02.ctl cp control01.ctl control03.ctl 3 修改主库的初始化参数文件,增加以下内容在主库$ORACLE_HOME/dbs/initskycard.ora,增加以下内容12345678910*.DB_UNIQUE_NAME='PRIMARY_1' *.log_archive_config='DG_CONFIG=(PRIMARY_1,STANDBY_1)' *.log_archive_dest_1='LOCATION=/opt/oracle/archive/skycard/ VALID_FOR=(ALL_LOGFILES,ALL_ROLES) DB_UNIQUE_NAME=PRIMARY_1' *.log_archive_dest_2='SERVICE=STANDBY_1 LGWR ASYNC VALID_FOR=(ONLINE_LOGFILES,PRIMARY_ROLE) DB_UNIQUE_NAME=STANDBY_1' *.LOG_ARCHIVE_DEST_STATE_1=ENABLE *.LOG_ARCHIVE_DEST_STATE_2=ENABLE *.FAL_SERVER=STANDBY_1 *.FAL_CLIENT=PRIMARY_1 *.standby_file_management='AUTO' *.LOG_ARCHIVE_MAX_PROCESSES=3 4 修改从库$ORACLE_HOME/dbs/initskycard.ora参数，增加以下内容12345678910*.DB_UNIQUE_NAME='STANDBY_1' *.log_archive_config='DG_CONFIG=(PRIMARY_1,STANDBY_1)' *.log_archive_dest_1='LOCATION=/opt/oracle/archive/skycard/ VALID_FOR=(ALL_LOGFILES,ALL_ROLES) DB_UNIQUE_NAME=STANDBY_1' *.log_archive_dest_2='SERVICE=PRIMARY_1 LGWR ASYNC VALID_FOR=(ONLINE_LOGFILES,PRIMARY_ROLE) DB_UNIQUE_NAME=PRIMARY_1' *.LOG_ARCHIVE_DEST_STATE_1=ENABLE *.LOG_ARCHIVE_DEST_STATE_2=ENABLE *.FAL_SERVER=PRIMARY_1 *.FAL_CLIENT=STANDBY_1 *.standby_file_management='AUTO' *.LOG_ARCHIVE_MAX_PROCESSES=3 5 配置主库和备库上的 tnsnames.ora ,并测试连通性12345678910111213141516171819202122PRIMARY_1= (DESCRIPTION= (ADDRESS= (PROTOCOL=TCP) (HOST=134.109.171.72) (PORT=1521) ) (CONNECT_DATA= (SERVICE_NAME=skycard) ) )STANDBY_1= (DESCRIPTION= (ADDRESS= (PROTOCOL=TCP) (HOST=134.109.171.73) (PORT=1521) ) (CONNECT_DATA= (SERVICE_NAME=skycard) ) ) 6 主库的 listen.ora12345678910111213141516171819202122# listener.ora Network Configuration File: /opt/oracle/product/10.2.0/db_1/network/admin/listener.ora # Generated by Oracle configuration tools.SID_LIST_LISTENER = (SID_LIST = (SID_DESC = (SID_NAME = PLSExtProc) (ORACLE_HOME = /opt/oracle/product/10.2.0/db_1) #(PROGRAM = extproc) ) (SID_DESC = (GLOBAL_DBNAME = skycard) (ORACLE_HOME = /opt/oracle/product/10.2.0/db_1) (SID_NAME = skycard) ) )LISTENER = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = db-134-109-171-72.test-db.com.hz.sandun )(PORT = 1521)) (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC0)) ) ) 7 从库的 listen.ora12345678910111213141516171819202122# listener.ora Network Configuration File: /opt/oracle/product/10.2.0/db_1/network/admin/listener.ora # Generated by Oracle configuration tools.SID_LIST_LISTENER = (SID_LIST = (SID_DESC = (SID_NAME = PLSExtProc) (ORACLE_HOME = /opt/oracle/product/10.2.0/db_1) #(PROGRAM = extproc) ) (SID_DESC = (GLOBAL_DBNAME = skycard) (ORACLE_HOME = /opt/oracle/product/10.2.0/db_1) (SID_NAME = skycard) ) )LISTENER = (DESCRIPTION_LIST = (DESCRIPTION = (ADDRESS = (PROTOCOL = TCP)(HOST = db-134-109-171-73.test-db.com.hz.sandun )(PORT = 1521)) (ADDRESS = (PROTOCOL = IPC)(KEY = EXTPROC0)) ) ) 8 在从库上执行,创建密码文件1$ORACLE_HOME/dbs/orapwd file=orapwskycard password=12dfdf entries=10 9 创建相应的目录123cd $ORACLE_BASE/admin mkdir skycard mkdir adump bdump cdump dpdump pfile scripts udump 10 启动从库到mount状态12345678SQL&gt; startup nomount pfile='/opt/oracle/product/10.2.0/db_1/dbs/initskycard.ora'; ORACLE instance started.Total System Global Area 3355443200 bytes Fixed Size 2087640 bytes Variable Size 671089960 bytes Database Buffers 2667577344 bytes Redo BuffersSQL&gt; ALTER DATABASE MOUNT STANDBY DATABASE; 初始化log apply服务1SQL&gt; ALTER DATABASE RECOVER MANAGED STANDBY DATABASE DISCONNECT FROM SESSION; 11 打开主库并测试archive log 是否传送主库上执行123456789SQL&gt; startup; ORACLE instance started.Total System Global Area 251658240 bytes Fixed Size 1218820 bytes Variable Size 96470780 bytes Database Buffers 150994944 bytes Redo Buffers 2973696 bytes Database mounted. Database opened. 主库上执行12SQL&gt; ALTER SYSTEM SWITCH LOGFILE;Database altered. 到备库主机上查看归档文件是否传来1234cd /opt/oracle/archive/skycard/ [oracle@db-134-109-171-73](mailto:oracle@db-134-109-171-73)-&gt; ll -rw-r----- 1 oracle oinstall 196K Dec 16 09:42 1_20_737391288.dbf -rw-r----- 1 oracle oinstall 120K Dec 16 09:45 1_21_737391288.dbf 说明日志文件已经传送 12 切换测试(停主库,启备库) 主库上操作1alter database commit to switchover to physical standby; 说明：如果此时有连接着的会话，这个命令会报错，需要加个”with session shutdown” 属性12345678910alter database commit to switchover to physical standby with session shutdown;shutdown immediate;startup nomount; alter database mount standby database;SQL&gt; select name,open_mode,PROTECTION_MODE,DATABASE_ROLE from v$database;NAME OPEN_MODE PROTECTION_MODE DATABASE_ROLE --------- ---------- -------------------- ---------------- SKYCARD MOUNTED MAXIMUM PERFORMANCE PHYSICAL STANDBYSQL&gt; ALTER DATABASE RECOVER MANAGED STANDBY DATABASE DISCONNECT FROM SESSION;Database altered. 在备库上操作123456789101112131415161718192021222324252627SQL&gt; select name,open_mode,PROTECTION_MODE,DATABASE_ROLE from v$database;NAME OPEN_MODE PROTECTION_MODE DATABASE_ROLE --------- ---------- -------------------- ---------------- SKYCARD READ ONLY MAXIMUM PERFORMANCE PHYSICAL STANDBYSQL&gt; alter database commit to switchover to primary;Database altered.SQL&gt; select name,open_mode,PROTECTION_MODE,DATABASE_ROLE from v$database;NAME OPEN_MODE PROTECTION_MODE DATABASE_ROLE --------- ---------- -------------------- ---------------- SKYCARD MOUNTED MAXIMUM PERFORMANCE PRIMARYSQL&gt; shutdown immediate;SQL&gt; startup nomount pfile='/opt/oracle/product/10.2.0/db_1/dbs/initskycard.ora'; ORACLE instance started.Total System Global Area 3355443200 bytes Fixed Size 2087640 bytes Variable Size 671089960 bytes Database Buffers 2667577344 bytes Redo Buffers 14688256 bytesSQL&gt; alter database mount;Database altered.SQL&gt; alter database open;Database altered. SQL&gt; select name,open_mode,PROTECTION_MODE,DATABASE_ROLE from v$database;NAME OPEN_MODE PROTECTION_MODE DATABASE_ROLE --------- ---------- -------------------- ---------------- SKYCARD READ WRITE MAXIMUM PERFORMANCE PRIMARY 完成切换。 13 测试一:数据测试主库上创建表12345678910111213141516171819202122232425SQL&gt; create table test_tf (id integer ,remark varchar2(32));Table created.SQL&gt; insert into test_tf values (1,'tf');1 row created.SQL&gt; commit;Commit complete.SQL&gt; select * from test_tf; ID REMARK ---------- -------------------------------- 1 tfSQL&gt; alter system switch logfile;System altered.--从库上验证 SQL&gt; ALTER DATABASE RECOVER MANAGED STANDBY DATABASE CANCEL;Database altered.SQL&gt; alter database open read only;Database altered.SQL&gt; select name,open_mode,PROTECTION_MODE,DATABASE_ROLE from v$database;NAME OPEN_MODE PROTECTION_MODE DATABASE_ROLE --------- ---------- -------------------- ---------------- SKYCARD READ ONLY MAXIMUM PERFORMANCE PHYSICAL STANDBYSQL&gt; select * from test_tf; ID REMARK ---------- -------------------------------- 1 tf 数据已经过来，测试成功。 测试二，在主库上新建表空间1234567891011121314SQL&gt; select name from v$tablespace;NAME ------------------------------ SYSTEM UNDOTBS1 SYSAUX USERS TEMP TS_SKYCARD TS_TEST_02SQL&gt; create tablespace ts_test_03 datafile '/opt/oradata/skycard/ts_skyts_test_03.dbf' size 20M autoextend off;Tablespace created.SQL&gt; alter system switch logfile;System altered. 备库日志情况1234567891011Thu Dec 16 10:38:29 2010 RFS[1]: Archived Log: '/opt/oracle/archive/skycard/1_33_737391288.dbf' Primary database is in MAXIMUM PERFORMANCE mode Thu Dec 16 10:38:34 2010 Media Recovery Log /opt/oracle/archive/skycard/1_33_737391288.dbf WARNING: File being created with same name as in Primary Existing file may be overwritten Recovery created file /opt/oradata/skycard/ts_skyts_test_03.dbf Successfully added datafile 7 to media recovery Datafile #7: '/opt/oradata/skycard/ts_skyts_test_03.dbf' Media Recovery Waiting for thread 1 sequence 34 (in transit) 到备库上验证一下123456789101112SQL&gt; select name from v$tablespace;NAME ------------------------------ SYSTEM UNDOTBS1 SYSAUX USERS TEMP TS_SKYCARD TS_TEST_02 TS_TEST_038 rows selected. 表空间 “TS_TEST_03” 已经同步过来了，成功。 14 再切回到主节点在当前的主节点（原来的备节点进行）12345678910111213141516171819202122232425262728293031323334SQL&gt; select name,open_mode,PROTECTION_MODE,DATABASE_ROLE from v$database;NAME OPEN_MODE PROTECTION_MODE DATABASE_ROLE --------- ---------- -------------------- ---------------- SKYCARD READ WRITE MAXIMUM PERFORMANCE PRIMARYSQL&gt; SQL&gt; select name,open_mode,database_role from v$database;NAME OPEN_MODE DATABASE_ROLE --------- ---------- ---------------- SKYCARD READ WRITE PRIMARYSQL&gt; alter database commit to switchover to physical standby;Database altered. SQL&gt; shutdown immediate; ORA-01507: database not mounted[oracle@db-134-109-171-73](mailto:oracle@db-134-109-171-73)-&gt; sqlplus \" / as sysdba\"SQL&gt; startup nomount pfile='/opt/oracle/product/10.2.0/db_1/dbs/initskycard.ora'; ORACLE instance started.Total System Global Area 3355443200 bytes Fixed Size 2087640 bytes Variable Size 671089960 bytes Database Buffers 2667577344 bytes Redo Buffers 14688256 bytes SQL&gt; alter database mount standby database;Database altered.SQL&gt; select name,open_mode ,database_role from v$database;NAME OPEN_MODE DATABASE_ROLE --------- ---------- ---------------- SKYCARD MOUNTED PHYSICAL STANDBYSQL&gt; alter database recover managed standby database disconnect from session;Database altered.SQL&gt; select name,open_mode ,database_role from v$database;NAME OPEN_MODE DATABASE_ROLE --------- ---------- ---------------- SKYCARD MOUNTED PHYSICAL STANDBY 在备节点（原先的主节点上进行）12345678910111213141516171819202122SQL&gt; select open_mode ,database_role from v$database;OPEN_MODE DATABASE_ROLE ---------- ---------------- MOUNTED PHYSICAL STANDBYSQL&gt; alter database commit to switchover to primary;Database altered.SQL&gt; shutdown immediate; ORA-01507: database not mounted[oracle@db-134-109-171-72](mailto:oracle@db-134-109-171-72)-&gt; sqlplus \" / as sysdba\" SQL&gt; startup; ORACLE instance started.Total System Global Area 3355443200 bytes Fixed Size 2087640 bytes Variable Size 671089960 bytes Database Buffers 2667577344 bytes Redo Buffers 14688256 bytes Database mounted. Database opened.SQL&gt; select open_mode ,database_role from v$database;OPEN_MODE DATABASE_ROLE ---------- ---------------- READ WRITE PRIMARY 到这里，完成主库和备库之间自由切换。 15 附：主库从库开启顺序。开启顺序: 先将从库启动到Mount状态,然后再开主库关闭顺序: 先关闭主库,再关闭从库 16 附上一张LGWR ASYNC原理图，来自官网,这也是这次实验应用的DATAGUARD 模式说明： When the LGWR and ASYNC attributes are specified, the log writer process writes to the local online redo log file,while the network server (LNSn) processes (one for each destination) asynchronously transmit the redo to remote destinations.The LGWR process continues processing the next request without waiting for the LNS network I/O to complete.If redo transport services transmit redo data to multiple remote destinations, the LNSn processes (one for each destination)initiate the network I/O to all of the destinations in parallel.When an online redo log file fills up, a log switch occurs and an archiver process archives the log file locally, as usual.ARC0 process: ARC0 process archives the local online redo log to the local destination (LOG_ARCHIVE_DEST_1)。 ARC1 process: transmits redo from the local archived redo log files (instead of the online redo log files) to the remote standby destination (LOG_ARCHIVE_DEST_2). RFS:the remote file server process receives redo data over the network from the LGWR process and writes the redo data to the standby redo log files MRP: Redo Apply process apply the redo to the standby database 再次附上主库参数12345678910*.DB_UNIQUE_NAME='PRIMARY_1' *.log_archive_config='DG_CONFIG=(PRIMARY_1,STANDBY_1)' *.log_archive_dest_1='LOCATION=/opt/oracle/archive/skycard/ VALID_FOR=(ALL_LOGFILES,ALL_ROLES) DB_UNIQUE_NAME=PRIMARY_1' *.log_archive_dest_2='SERVICE=STANDBY_1 LGWR ASYNC VALID_FOR=(ONLINE_LOGFILES,PRIMARY_ROLE) DB_UNIQUE_NAME=STANDBY_1' *.LOG_ARCHIVE_DEST_STATE_1=ENABLE *.LOG_ARCHIVE_DEST_STATE_2=ENABLE *.FAL_SERVER=STANDBY_1 *.FAL_CLIENT=PRIMARY_1 *.standby_file_management='AUTO' *.LOG_ARCHIVE_MAX_PROCESSES=3","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"Something About Tuple ID","slug":"20101210100526","date":"2010-12-10T02:05:26.000Z","updated":"2018-09-04T01:33:46.942Z","comments":true,"path":"20101210100526.html","link":"","permalink":"https://postgres.fun/20101210100526.html","excerpt":"","text":"Tid 扫描(tuple ID scan) )很少用到，表的每一行记录都有一个唯一标记，这个标记名为 tuple ID, 当我们查询记录时，也可以将行的 tuple ID 标记查出来,例如以下 123456mydb=&gt; select ctid,* from test_id limit 2; ctid | id | name -------+----+------ (0,1) | 1 | a (0,2) | 2 | b (2 rows) ctid 是一个很特殊的字段，它是自动生成的，ctid由两部分组成，第一部分是指 block的编号，第二部分是指这个BLOCK上的记录编号。 创建测试表1create table test_8 (id integer,remark varchar(32) ); 插入10000条数据12345678910111213141516171819CREATE OR REPLACE FUNCTION skytf.fun_ins_test_1() RETURNS integer LANGUAGE plpgsql AS $function$ DECLARE i INTEGER ; BEGIN for i in 1..10000 loop insert into test_8 values (1,'a'); end loop; return 1; END; $function$ mydb=&gt; select skytf.fun_ins_test_1(); fun_ins_test_1 ---------------- 1 (1 row) 表分析并查询统计信息1234567891011121314mydb=&gt; analyze test_8; ANALYZEmydb=&gt; select relname,relpages,reltuples from pg_class where relname='test_8'; relname | relpages | reltuples ---------+----------+----------- test_8 | 45 | 10000 (1 row) mydb=&gt; select 10000/45; ?column? ---------- 222 (1 row) 上面可以看出表test_8 占用了 45 个PAGE,每个pages含有约 222 条记录。查看最后300条记录的ctid123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286mydb=&gt; select ctid,id from test_8 offset 9700; ctid | id ----------+---- (43,1) | 1 (43,2) | 1 (43,3) | 1 (43,4) | 1 (43,5) | 1 (43,6) | 1 (43,7) | 1 (43,8) | 1 (43,9) | 1 (43,10) | 1 (43,11) | 1 (43,12) | 1 (43,13) | 1 (43,14) | 1 (43,15) | 1 (43,16) | 1 (43,17) | 1 (43,18) | 1 (43,19) | 1 (43,20) | 1 (43,21) | 1 (43,22) | 1 (43,23) | 1 (43,24) | 1 (43,25) | 1 (43,26) | 1 (43,27) | 1 (43,28) | 1 (43,29) | 1 (43,30) | 1 (43,31) | 1 (43,32) | 1 (43,33) | 1 (43,34) | 1 (43,35) | 1 (43,36) | 1 (43,37) | 1 (43,38) | 1 (43,39) | 1 (43,40) | 1 (43,41) | 1 (43,42) | 1 (43,43) | 1 (43,44) | 1 (43,45) | 1 (43,46) | 1 (43,47) | 1 (43,48) | 1 (43,49) | 1 (43,50) | 1 (43,51) | 1 (43,52) | 1 (43,53) | 1 (43,54) | 1 (43,55) | 1 (43,56) | 1 (43,57) | 1 (43,58) | 1 (43,59) | 1 (43,60) | 1 (43,61) | 1 (43,62) | 1 (43,63) | 1 (43,64) | 1 (43,65) | 1 (43,66) | 1 (43,67) | 1 (43,68) | 1 (43,69) | 1 (43,70) | 1 (43,71) | 1 (43,72) | 1 (43,73) | 1 (43,74) | 1 (43,75) | 1 (43,76) | 1 (43,77) | 1 (43,78) | 1 (43,79) | 1 (43,80) | 1 (43,81) | 1 (43,82) | 1 (43,83) | 1 (43,84) | 1 (43,85) | 1 (43,86) | 1 (43,87) | 1 (43,88) | 1 (43,89) | 1 (43,90) | 1 (43,91) | 1 (43,92) | 1 (43,93) | 1 (43,94) | 1 (43,95) | 1 (43,96) | 1 (43,97) | 1 (43,98) | 1 (43,99) | 1 (43,100) | 1 (43,101) | 1 (43,102) | 1 (43,103) | 1 (43,104) | 1 (43,105) | 1 (43,106) | 1 (43,107) | 1 (43,108) | 1 (43,109) | 1 (43,110) | 1 (43,111) | 1 (43,112) | 1 (43,113) | 1 (43,114) | 1 (43,115) | 1 (43,116) | 1 (43,117) | 1 (43,118) | 1 (43,119) | 1 (43,120) | 1 (43,121) | 1 (43,122) | 1 (43,123) | 1 (43,124) | 1 (43,125) | 1 (43,126) | 1 (43,127) | 1 (43,128) | 1 (43,129) | 1 (43,130) | 1 (43,131) | 1 (43,132) | 1 (43,133) | 1 (43,134) | 1 (43,135) | 1 (43,136) | 1 (43,137) | 1 (43,138) | 1 (43,139) | 1 (43,140) | 1 (43,141) | 1 (43,142) | 1 (43,143) | 1 (43,144) | 1 (43,145) | 1 (43,146) | 1 (43,147) | 1 (43,148) | 1 (43,149) | 1 (43,150) | 1 (43,151) | 1 (43,152) | 1 (43,153) | 1 (43,154) | 1 (43,155) | 1 (43,156) | 1 (43,157) | 1 (43,158) | 1 (43,159) | 1 (43,160) | 1 (43,161) | 1 (43,162) | 1 (43,163) | 1 (43,164) | 1 (43,165) | 1 (43,166) | 1 (43,167) | 1 (43,168) | 1 (43,169) | 1 (43,170) | 1 (43,171) | 1 (43,172) | 1 (43,173) | 1 (43,174) | 1 (43,175) | 1 (43,176) | 1 (43,177) | 1 (43,178) | 1 (43,179) | 1 (43,180) | 1 (43,181) | 1 (43,182) | 1 (43,183) | 1 (43,184) | 1 (43,185) | 1 (43,186) | 1 (43,187) | 1 (43,188) | 1 (43,189) | 1 (43,190) | 1 (43,191) | 1 (43,192) | 1 (43,193) | 1 (43,194) | 1 (43,195) | 1 (43,196) | 1 (43,197) | 1 (43,198) | 1 (43,199) | 1 (43,200) | 1 (43,201) | 1 (43,202) | 1 (43,203) | 1 (43,204) | 1 (43,205) | 1 (43,206) | 1 (43,207) | 1 (43,208) | 1 (43,209) | 1 (43,210) | 1 (43,211) | 1 (43,212) | 1 (43,213) | 1 (43,214) | 1 (43,215) | 1 (43,216) | 1 (43,217) | 1 (43,218) | 1 (43,219) | 1 (43,220) | 1 (43,221) | 1 (43,222) | 1 (43,223) | 1 (43,224) | 1 (43,225) | 1 (43,226) | 1 (44,1) | 1 (44,2) | 1 (44,3) | 1 (44,4) | 1 (44,5) | 1 (44,6) | 1 (44,7) | 1 (44,8) | 1 (44,9) | 1 (44,10) | 1 (44,11) | 1 (44,12) | 1 (44,13) | 1 (44,14) | 1 (44,15) | 1 (44,16) | 1 (44,17) | 1 (44,18) | 1 (44,19) | 1 (44,20) | 1 (44,21) | 1 (44,22) | 1 (44,23) | 1 (44,24) | 1 (44,25) | 1 (44,26) | 1 (44,27) | 1 (44,28) | 1 (44,29) | 1 (44,30) | 1 (44,31) | 1 (44,32) | 1 (44,33) | 1 (44,34) | 1 (44,35) | 1 (44,36) | 1 (44,37) | 1 (44,38) | 1 (44,39) | 1 (44,40) | 1 (44,41) | 1 (44,42) | 1 (44,43) | 1 (44,44) | 1 (44,45) | 1 (44,46) | 1 (44,47) | 1 (44,48) | 1 (44,49) | 1 (44,50) | 1 (44,51) | 1 (44,52) | 1 (44,53) | 1 (44,54) | 1 (44,55) | 1 (44,56) | 1 (300 rows) 到这里，大家可以清楚的看到TID的含义，TID的第一列为BLOCK的编号，第二列为这个BLOCK上的记录编号，并注意表 TEST_8的最后一条记录的TID为 (44,56),现在我们接着做个实验，更新全表的 remark字段，看下会发生什么情况。 更新记录并查看表占用PAGE12345678910mydb=&gt; update test_8 set remark='b'; UPDATE 10000mydb=&gt; analyze test_8; ANALYZEmydb=&gt; select relname,relpages,reltuples from pg_class where relname='test_8'; relname | relpages | reltuples ---------+----------+----------- test_8 | 89 | 10000 大家看到，test_8 占用的 pages数由原来的44，上升到现在的89, 刚好翻了一倍 再次查询TID1234567891011121314151617181920212223242526272829303132333435mydb=&gt; select ctid,id from test_8 order by ctid limit 250 ; ctid | id ----------+---- (44,57) | 1 (44,58) | 1 (44,59) | 1 (44,60) | 1 (44,61) | 1 (44,62) | 1 (44,63) | 1 (44,64) | 1 (44,65) | 1 (44,66) | 1 (44,67) | 1 (44,68) | 1 (44,69) | 1 (44,70) | 1 (44,71) | 1 (44,72) | 1 (44,73) | 1 (44,74) | 1 (44,75) | 1 (44,76) | 1 (44,77) | 1 (44,78) | 1 (44,79) | 1 (44,80) | 1 (44,81) | 1 (44,82) | 1 (44,83) | 1 (44,84) | 1 (44,85) | 1 (44,86) | 1 (44,87) | 1 ... 上面省略了部分结果集, 注意第一条 (44,57) , 以及上一个步骤的最后一条(44,56), 惊奇的发现，update后，PG保留了原有数据，而是在物理存储上顺序接着往下写下新的数据。 TID 查询的执行计划1234567mydb=&gt; explain analyze select ctid, id from test_8 where ctid ='(44,87)'; QUERY PLAN ------------------------------------------------------------------------------------------------- Tid Scan on test_8 (cost=0.00..4.01 rows=1 width=10) (actual time=0.147..0.158 rows=1 loops=1) TID Cond: (ctid = '(44,87)'::tid) Total runtime: 0.272 ms (3 rows) 总结TID是每张表的隐含列，它指定了记录行在存储上的物理位置，TID SCAN 能很快的检索数据，因为它根据物理文件的bloack号和记录编号可以直接读取数据。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"MVCC","slug":"MVCC","permalink":"https://postgres.fun/tags/MVCC/"}]},{"title":"关于PG MVCC原理的一点思考","slug":"20101209133156","date":"2010-12-09T05:31:56.000Z","updated":"2018-09-04T01:33:46.895Z","comments":true,"path":"20101209133156.html","link":"","permalink":"https://postgres.fun/20101209133156.html","excerpt":"","text":"今天早上骑电屏车上班时突然想到了一个问题，昨天看了些PG的有关MVCC的资料，PG在处理 UPDATE，DELETE 语句时，不会立即删除掉旧的记录，而是将旧的记录保存一段时间，直到VACCUM之后才会真正删除旧的记录，我在想，当DELETE语句时PG保留原有的记录这很好理解，但是当UPDATE时，PG是保留UPDATE的整行记录，还是仅保留那条记录被UPDATE的字段，带着这个疑问，做了以下测试。 创建测试表创建测试表并插入100万数据1234mydb=&gt; create table test_7 (id integer, remark varchar(32)); CREATE TABLEmydb=&gt; insert into test_7 select generate_series(1,1000000),'aaaaaa'; INSERT 0 1000000 查看表大小和占用PAGE查看表大小12345mydb=&gt; select pg_size_pretty(pg_relation_size('test_7')); pg_size_pretty ---------------- 38 MB (1 row) 这里看出表 test_7 为 38 M。 查看表的page数12345678910111213141516171819202122232425262728293031323334353637383940414243mydb=&gt; \\d pg_class Table \"pg_catalog.pg_class\" Column | Type | Modifiers -----------------+-----------+----------- relname | name | not null relnamespace | oid | not null reltype | oid | not null reloftype | oid | not null relowner | oid | not null relam | oid | not null relfilenode | oid | not null reltablespace | oid | not null relpages | integer | not null reltuples | real | not null reltoastrelid | oid | not null reltoastidxid | oid | not null relhasindex | boolean | not null relisshared | boolean | not null relistemp | boolean | not null relkind | \"char\" | not null relnatts | smallint | not null relchecks | smallint | not null relhasoids | boolean | not null relhaspkey | boolean | not null relhasexclusion | boolean | not null relhasrules | boolean | not null relhastriggers | boolean | not null relhassubclass | boolean | not null relfrozenxid | xid | not null relacl | aclitem[] | reloptions | text[] | Indexes: \"pg_class_oid_index\" UNIQUE, btree (oid) \"pg_class_relname_nsp_index\" UNIQUE, btree (relname, relnamespace)mydb=&gt; analyze test_7; ANALYZEmydb=&gt; select relname,relpages,reltuples from pg_class where relname='test_7'; relname | relpages | reltuples ---------+----------+----------- test_7 |4902 | 1e+06 (1 row) 这里可以看出表 test_7 有4902 page。测试UPDATE, 并再次查看表大小和占用 page数 UPDATE表1234567891011mydb=&gt; update test_7 set remark='bbbbbb'; UPDATE 1000000mydb=&gt; select relname,relpages,reltuples from pg_class where relname='test_7'; relname | relpages | reltuples ---------+----------+----------- test_7 | 4902 | 1e+06 (1 row)mydb=&gt; analyze test_7; ANALYZE 再次查看表大小和占用PAGE1234567891011mydb=&gt; select relname,relpages,reltuples from pg_class where relname='test_7'; relname | relpages | reltuples ---------+----------+----------- test_7 | 9804 | 1e+06 (1 row)mydb=&gt; select pg_size_pretty(pg_relation_size('test_7')); pg_size_pretty ---------------- 77 MB (1 row) 从上面看出，表的PAGE数为9804，刚好增加了一倍，这也说明，PG处理UPDATE语句时，也是保留了原有记录的一份,只是在存储上标记为不可读而已,同时也说明,保留的是原有的记录整行,而不只是更新的字段值。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"MVCC","slug":"MVCC","permalink":"https://postgres.fun/tags/MVCC/"}]},{"title":"PG里的层次查询方法","slug":"20101208135721","date":"2010-12-08T05:57:21.000Z","updated":"2018-09-04T01:33:46.832Z","comments":true,"path":"20101208135721.html","link":"","permalink":"https://postgres.fun/20101208135721.html","excerpt":"","text":"今天开发人员咨询，他们项目有张表，表中的记录有父子关系，根据父记录ID，如何查询所有子节点的记录, PG里是否有方法可以实现, 听到这，我还是第一次碰到PG的这种问题，想到 Oracle 是有方法实现的，查询语法如下：123select * from aclorgtreeinfo start with orgid = '06e0d0bb1389a196011389b31abd0002' connect by prior orgid = parentorgid; 后来查了些文档发现PG里可以利用递归查询实现，具体用法可以参考，http://www.postgresql.org/docs/8.4/static/queries-with.html, 以下是具体测试过程。创建测试表，并插入测试记录，如下： 创建测试表1234567891011121314create table emp (empid integer,empparid integer,empname varchar(32)); insert into emp values (001,nul,'CEO'); insert into emp values (002,001,'CTO'); insert into emp values (003,001,'CFO'); insert into emp values (004,001,'OOO'); insert into emp values (005,002,'MANAGER');mydb=&gt; select * from emp; empid | empparid | empname -------+----------+--------- 2 | 1 | CTO 3 | 1 | CFO 4 | 1 | OOO 5 | 2 | MANAGER 查询empid=2下的所有记录1234567891011WITH RECURSIVE r AS ( SELECT * FROM emp WHERE empid = 2 union ALL SELECT emp.* FROM emp, r WHERE emp.empparid = r.empid ) SELECT * FROM r ORDER BY empid;empid | empparid | empname -------+----------+--------- 2 | 1 | CTO 5 | 2 | MANAGER (2 rows) 从结果看出，果然功能实现了，”WITH RECURSIVE”有个特点，它不立即将执行结果输出，而是需要再通过 SELECT 命令将结果输出到终端。 最后将以上方法告诉开发人员，开发人员开心死了 ^_^","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL:几个日志相关的参数","slug":"20101207170034","date":"2010-12-07T09:00:34.000Z","updated":"2018-09-04T01:33:46.785Z","comments":true,"path":"20101207170034.html","link":"","permalink":"https://postgres.fun/20101207170034.html","excerpt":"","text":"今天在检查数据库时，发现一个奇怪的现象,发现今天的csvlog文件有很多个，差不多半小时就会新生成一个,正常情况下是每天生成一个CSV文件，具体信息如下: 123456789101112131415161718192021222324252627282930313233343536-rw------- 1 postgres postgres 743K Nov 28 23:59 postgresql-2010-11-28_000000.csv -rw------- 1 postgres postgres 4.8K Nov 28 23:59 postgresql-2010-11-28_000000.log -rw------- 1 postgres postgres 787K Nov 29 23:59 postgresql-2010-11-29_000000.csv -rw------- 1 postgres postgres 5.7K Nov 29 23:47 postgresql-2010-11-29_000000.log -rw------- 1 postgres postgres 898K Nov 30 23:58 postgresql-2010-11-30_000000.csv -rw------- 1 postgres postgres 3.6K Nov 30 23:49 postgresql-2010-11-30_000000.log -rw------- 1 postgres postgres 11M Dec 1 20:22 postgresql-2010-12-01_000000.csv -rw------- 1 postgres postgres 16K Dec 1 23:56 postgresql-2010-12-01_000000.log -rw------- 1 postgres postgres 11M Dec 1 21:40 postgresql-2010-12-01_202238.csv -rw------- 1 postgres postgres 11M Dec 1 22:34 postgresql-2010-12-01_214022.csv -rw------- 1 postgres postgres 11M Dec 1 23:23 postgresql-2010-12-01_223425.csv -rw------- 1 postgres postgres 7.4M Dec 1 23:59 postgresql-2010-12-01_232323.csv -rw------- 1 postgres postgres 11M Dec 2 00:52 postgresql-2010-12-02_000000.csv -rw------- 1 postgres postgres 2.9K Dec 2 23:49 postgresql-2010-12-02_000000.log -rw------- 1 postgres postgres 11M Dec 2 01:44 postgresql-2010-12-02_005256.csv -rw------- 1 postgres postgres 11M Dec 2 02:34 postgresql-2010-12-02_014403.csv -rw------- 1 postgres postgres 11M Dec 2 03:28 postgresql-2010-12-02_023406.csv -rw------- 1 postgres postgres 11M Dec 2 04:23 postgresql-2010-12-02_032824.csv -rw------- 1 postgres postgres 11M Dec 2 05:20 postgresql-2010-12-02_042349.csv -rw------- 1 postgres postgres 11M Dec 2 06:17 postgresql-2010-12-02_052024.csv -rw------- 1 postgres postgres 11M Dec 2 07:22 postgresql-2010-12-02_061739.csv -rw------- 1 postgres postgres 11M Dec 2 08:35 postgresql-2010-12-02_072208.csv -rw------- 1 postgres postgres 11M Dec 2 09:47 postgresql-2010-12-02_083552.csv -rw------- 1 postgres postgres 11M Dec 2 11:02 postgresql-2010-12-02_094759.csv -rw------- 1 postgres postgres 11M Dec 2 12:07 postgresql-2010-12-02_110215.csv -rw------- 1 postgres postgres 11M Dec 2 13:13 postgresql-2010-12-02_120707.csv -rw------- 1 postgres postgres 11M Dec 2 14:13 postgresql-2010-12-02_131335.csv -rw------- 1 postgres postgres 11M Dec 2 15:21 postgresql-2010-12-02_141323.csv -rw------- 1 postgres postgres 11M Dec 2 17:00 postgresql-2010-12-02_152102.csv -rw------- 1 postgres postgres 11M Dec 2 18:45 postgresql-2010-12-02_170029.csv -rw------- 1 postgres postgres 11M Dec 2 20:10 postgresql-2010-12-02_184512.csv -rw------- 1 postgres postgres 11M Dec 2 21:06 postgresql-2010-12-02_201010.csv -rw------- 1 postgres postgres 11M Dec 2 21:50 postgresql-2010-12-02_210647.csv -rw------- 1 postgres postgres 11M Dec 2 22:31 postgresql-2010-12-02_215027.csv -rw------- 1 postgres postgres 11M Dec 2 23:12 postgresql-2010-12-02_223159.csv -rw------- 1 postgres postgres 11M Dec 2 23:49 postgresql-2010-12-02_231223.csv 发现从 2010-12-01月起开始出现这种现象，并且每个CSV日志文件大小均为11M。接着查看配置文件 postgresql.conf 几个日志参数：1234567# - Where to Log -log_destination = 'csvlog' logging_collector = on log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' log_truncate_on_rotation = on log_rotation_age = 1d log_rotation_size = 10MB 由于 log_rotation_age 设置为1天，正常情况下是每天新生成一个 csv 日志，原来 log_rotation_size=10M, CSV日志文件大小超过10M，就会新生成一个，关于log_rotation_size 参数，文档上是这样写的，如下： log_rotation_size (integer)When logging_collector is enabled, this parameter determines the maximum size of an individual log file.After this many kilobytes have been emitted into a log file, a new log file will be created.Set to zero to disable size-based creation of new log files. This parameter can only be set in the postgresql.conffile or on the server command line.log_rotation_age (integer)When logging_collector is enabled, this parameter determines the maximum lifetime of an individual log file.After this many minutes have elapsed, a new log file will be created. Set to zero to disable time-basedcreation of new log files. This parameter can only be set in the postgresql.conf file or on the server command line. 其它的LOG相关参数可以参照文档。平常看文档时，大概能明白几个日志参数的含义，但不是很清楚，结合实际维护过程中，才能更准确的理解参数的含义。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Using Only when truncating parent table","slug":"20101202123725","date":"2010-12-02T04:37:25.000Z","updated":"2018-09-04T01:33:46.723Z","comments":true,"path":"20101202123725.html","link":"","permalink":"https://postgres.fun/20101202123725.html","excerpt":"","text":"在PG里，通常会对大表做分区，建成月表形式，通常月表都是继承父表，在维护时，有个重要的属性需要时刻记牢， 比如 truncate 父表时，需要加 only 属性, 否则，一个 truncate下去，本来是想清空父表的数据，结果把所有子表的数据也干掉了，此时为时已晚，下面做了个简单的实验，验证了下。 创建父表12mydb=&gt; create table table_fenqu (id integer, addtime timestamp without time zone); CREATE TABLE 创建三张子表1234567891011121314mydb=&gt; create table table_fenqu_201009( id integer, addtime timestamp without time zone) inherits (table_fenqu); NOTICE: merging column \"id\" with inherited definition NOTICE: merging column \"addtime\" with inherited definition CREATE TABLEmydb=&gt; create table table_fenqu_201010( id integer, addtime timestamp without time zone) inherits (table_fenqu); NOTICE: merging column \"id\" with inherited definition NOTICE: merging column \"addtime\" with inherited definition CREATE TABLEmydb=&gt; create table table_fenqu_201011( id integer, addtime timestamp without time zone) inherits (table_fenqu); NOTICE: merging column \"id\" with inherited definition NOTICE: merging column \"addtime\" with inherited definition CREATE TABLE 查看字段属性1234567mydb=&gt; \\d table_fenqu Table \"skytf.table_fenqu\" Column | Type | Modifiers ---------+-----------------------------+----------- id | integer | addtime | timestamp without time zone | Number of child tables: 3 (Use d+ to list them.) 插入测试数据12345678mydb=&gt; insert into table_fenqu_201009 (id,addtime) values (1,'2010-09-01 00:00:00'); INSERT 0 1 mydb=&gt; insert into table_fenqu_201010 (id,addtime) values (1,'2010-10-01 00:00:00'); INSERT 0 1 mydb=&gt; insert into table_fenqu_201011 (id,addtime) values (1,'2010-11-01 00:00:00'); INSERT 0 1 mydb=&gt; insert into table_fenqu_201011 (id,addtime) values (1,'2010-11-02 00:00:00'); INSERT 0 1 查看父表数据12345mydb=&gt; select count(*) from table_fenqu; count ------- 4 (1 row) 查看子表数据1234567891011mydb=&gt; select count(*) from table_fenqu_201011; count ------- 2 (1 row)mydb=&gt; select count(*) from table_fenqu_201009; count ------- 1 (1 row) 不加 Only 属性 Truncate 父表不加Only属性 Truncate父表数据，看下有什么结果 ，如下：1234567891011121314mydb=&gt; truncate table table_fenqu; TRUNCATE TABLEmydb=&gt; select count(*) from table_fenqu; count ------- 0 (1 row)mydb=&gt; select count(*) from only table_fenqu_201009; count ------- 0 (1 row) 这里说明，不加only时,所有数据,包括子表都被清空了. 重新插入测试数据，验证加’only’的情况123456789101112131415161718192021222324252627282930mydb=&gt; insert into table_fenqu_201009 (id,addtime) values (1,'2010-09-01 00:00:00'); INSERT 0 1 mydb=&gt; insert into table_fenqu_201010 (id,addtime) values (1,'2010-10-01 00:00:00'); INSERT 0 1 mydb=&gt; insert into table_fenqu_201011 (id,addtime) values (1,'2010-11-01 00:00:00'); INSERT 0 1 mydb=&gt; insert into table_fenqu_201011 (id,addtime) values (1,'2010-11-02 00:00:00'); INSERT 0 1 mydb=&gt; select count(*) from table_fenqu; count ------- 4 (1 row)mydb=&gt; insert into table_fenqu (id,addtime) values (1,'2010-11-02 00:00:00'); INSERT 0 1 mydb=&gt; select count(*) from table_fenqu; count ------- 5 (1 row)mydb=&gt; truncate table only table_fenqu; TRUNCATE TABLE mydb=&gt; select count(*) from table_fenqu; count ------- 4 (1 row) 这里可以看出，加only时，只清空父表的一条数据,而子表的数据保留着。12345mydb=&gt; select count(*) from only table_fenqu; count ------- 0 (1 row) Truncate 语法官方文档 NameTRUNCATE ― empty a table or set of tablesSynopsisTRUNCATE [ TABLE ] [ ONLY ] name [, … ] [ RESTART IDENTITY | CONTINUE IDENTITY ] [ CASCADE | RESTRICT ] nameThe name (optionally schema-qualified) of a table to be truncated. If ONLY is specified, only that table is truncated.If ONLY is not specified, the table and all its descendant tables (if any) are truncated.","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"记录一次操作失误: DBA不应该急着干活","slug":"20101127100646","date":"2010-11-27T02:06:46.000Z","updated":"2018-09-04T01:33:46.676Z","comments":true,"path":"20101127100646.html","link":"","permalink":"https://postgres.fun/20101127100646.html","excerpt":"","text":"昨天是周五，是我上周中最忙的一天，昨天在处理开发人员提过的sir时，稍微操作失误了下sir中要求新建一个用户，并且导入三个脚本，第一个脚本是创建新表和序列，第二个脚本是创建procedure和function,第三个脚本是往新建的表中插入数据，在新建完表空间和用户后，我把脚本copy到数据库主机执行，执行的时候忘记以这个新建的用户登陆，结果把这些表，序列,procedure 和function都建到 sys下面去了…. 解决方法虽然很简单，只要把这些建到sys下的对象删掉即可，但从中也给了我很大的提示。 DBA应该时刻保持头脑清醒，再忙的时候，事情再多被催的时候，别人催，自己可不能催自己一件一件来，大胆心细，要不然，越急忙处理越容易出问题, 相比情急之下的误操作给数据库带来灾难，还不如宁愿被项目组催催,这也要求DBA需要很好的心理素质。 平常开发人员提供的sir当中的建表语句，DML语句一般都不会带上schema前辍，这时应该主动和开发人员交流清楚，建到哪个schema下。 如果是批量新建表，存过，序列的语句，如果开发人员测试后是正常的，则可以放到主机上以脚本批量导入，如果是老表的DDL语句，上面这种方法则可能带来给数据库带来灾难，例如给一张大表加个字段，并新建个索引，这时一般要在业务不繁忙的时候进行，因为建索引，加字段时会堵住一大批更新此表的应用SQL，给业务带来影响,而且最好是不要批量执行。","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"使用 Mutt 定时发邮件","slug":"20101125174823","date":"2010-11-25T09:48:23.000Z","updated":"2018-09-04T01:33:46.629Z","comments":true,"path":"20101125174823.html","link":"","permalink":"https://postgres.fun/20101125174823.html","excerpt":"","text":"今天学习了使用mutt自动发邮件，觉得 mutt发邮件非常便捷, 下面是一个简单的shell脚本，通过mutt 和 crontab 定时发邮件,今天发邮件过程中遇到一个问题，就是邮件内容中含有中文，经mutt发出后，通过foxmail接收邮件后，发现中文变成乱码，而在linux主机上查看文件中的中文能正常显示后来在网上查了下资料，有两个地方需要设置下字符集，第下个是设置 NLS_LANG 环境变量，第二个是配置mutt配置文件 .muttrc 设置字符集，问题解决。 Shell脚本12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/sh DD=`date +%Y%m%d%H`export NLS_LANG=AMERICAN_AMERICA.UTF8 export ORACLE_HOME=/opt/oracle/product/10.2.0/db_1 export PATH=$ORACLE_HOME/bin:$PATH export ORACLE_SID=devdb export LOGFILE=/home/oracle/script/tf/devdb.logif [ $# != 1 ]; then echo Usage: ./1.sh arg1 echo Examples: echo 1. To run s_org_ext_x.sql, enter: echo 'oad.sh s_org_ext &gt;s_org_ext_x.out' exit 0 ficd /home/oracle/script/tfsqlplus \"/as sysdba\" &lt;&lt; EOF set time on set echo on set linesize 150 col sum(prize_num) format 99999999 col PRIZE_UNIT format a30 col PHONE format a20 col award_time format a30 spool $&#123;LOGFILE&#125; @$1 spool off exit EOF#EMAIL . /home/oracle/.muttrc cat $&#123;LOGFILE&#125; | mutt -s \"devdb_again\" mailaddr1 mailaddr2 mailaddr3award.sql 统计所有奖品中奖数量---------- SELECT SUM(prize_num),prize_unit FROM devdb.tbl_award GROUP BY prize_unit;--统计所有实物中奖---------- SELECT ar.sky_id ,ar.phone , ar.lottery_prize_id ,ar.prize_num , ar.prize_unit , ar.award_time FROM devdb.tbl_award ar , devdb.tbl_prize lp WHERE ar.lottery_prize_id = lp.lottery_prize_id AND lp.prize_type = 1 ; 设置配置文件设置文件/home/posgres/.muttrc 以下变量，如下：12set charset = \"utf-8\" set locale = \"zh_CN.UTF-8\" 加入任务计划15,35 09-23 * * * /home/oracle/script/tf/1.sh /home/oracle/script/tf/award.sql &gt;/dev/null 2&gt;&amp;1","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"一个 Pg_dump SHELL 脚本","slug":"20101123164931","date":"2010-11-23T08:49:31.000Z","updated":"2018-09-04T01:33:46.566Z","comments":true,"path":"20101123164931.html","link":"","permalink":"https://postgres.fun/20101123164931.html","excerpt":"","text":"最近有个处于开发阶段的项目需要DBA介入，希望DBA对这个项目的数据库进行备份。要到主机帐号后，先对数据库进行一翻熟悉，发现上面有虽然有9个库，但是数据量都不大，除了一个库有5G左右，其它的都在1G以下，因为目前业务还没完全上线，等以后业务起来后数据量应该会大些, 由于数据库不是很大，并且和项目发员沟通后，只需要一周备一次，于是决定用pg_dump方式备份,以下是详细内容。 备份脚本 dump_db.sh 内容12345678910111213141516171819202122232425# The folling is database dump shell #!/bin/bashexport PGHOME=/usr/local/pgsql export PGPORT=5432 export PGDATA=/usr/local/pgsql/data export PATH=$PGHOME/bin:$PATHCUR_DATE=`date +%Y%m%d`#Loading config-file CONFIG_FILE=\"/appt/pgbackup/db.config\" #Set config file . $&#123;CONFIG_FILE&#125;#Data directory SAVE_BASE_DIR=\"/appt/pgbackup/dump_dir\" DAT_FILE_DIR=\"$&#123;SAVE_BASE_DIR&#125;/$&#123;CUR_DATE&#125;\" if [ -d $&#123;DAT_FILE_DIR&#125; ] then : else mkdir -p $&#123;DAT_FILE_DIR&#125; fi# The reall backup process ! echo \"`date +%F %T` begin backup db \"for db_name in $&#123;db_name[@]&#125; do pg_dump -E UTF8 -Fc $&#123;db_name&#125; -U postgres -v &gt; $&#123;DAT_FILE_DIR&#125;/$&#123;db_name&#125;_$&#123;CUR_DATE&#125;.dmp doneecho \"`date +% F%T` end backup db \" db.config 脚本内容123456789db_name[1]=db_name1 db_name[2]=db_name2 db_name[3]=db_name3 db_name[4]=db_name4 db_name[5]=db_name5 db_name[6]=db_name6 db_name[7]=db_name7 db_name[8]=db_name8 db_name[9]=db_name9 将备份脚本写入任务计划每周三零晨四点备份12###Backup maopao db every week### 0 4 * * 3 /appt/pgbackup/dump_db.sh &gt;&gt;/appt/pgbackup/log/dump.err 2&gt;&amp;1 后期由于没有备份服务器，备份集暂时保存在数据库本机，等有了机器后，需要完善下备份脚本，并且根据需求将备份集清理脚本也加上。","categories":[{"name":"PG备份与恢复","slug":"PG备份与恢复","permalink":"https://postgres.fun/categories/PG备份与恢复/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"一个灵活的统计SQL","slug":"20101119174955","date":"2010-11-19T09:49:55.000Z","updated":"2018-09-04T01:33:46.519Z","comments":true,"path":"20101119174955.html","link":"","permalink":"https://postgres.fun/20101119174955.html","excerpt":"","text":"今天测试人员提出一个需求，需要统一张日志表今天上午9点到12点之间每隔五分钟的统计总值，和速率，以下SQL根据小时，分钟分组比较灵活，而且速度也还快，记录下。 12345678910111213141516171819202122232425262728temphall=&gt; select to_char(act_time,'yyyy-mm-dd hh24 '),min (act_time) begin_time, max (act_time) end_time , temphall-&gt; sum(down_bytes), sum(down_time), sum(down_bytes)/sum(down_time) temphall-&gt; from tmp_table temphall-&gt; where act_time &gt; '2010-11-19 00:00:00' temphall-&gt; and date_part('hour',act_time) in ('09','10','11') temphall-&gt; group by to_char(act_time,'yyyy-mm-dd hh24 ') , floor(to_number( to_char(act_time,'mi') ,'99')/5) temphall-&gt; order by 1,2; to_char | begin_time | end_time | sum | sum | ?column? ----------------+----------------------------+----------------------------+-----------+--------+---------- 2010-11-19 09 | 2010-11-19 09:00:00.150474 | 2010-11-19 09:04:59.919062 | XXXXXXXXX | XXXXXX | XXXX 2010-11-19 09 | 2010-11-19 09:05:00.012315 | 2010-11-19 09:09:59.9162 | XXXXXXXXX | XXXXXX | XXXX ....后面省略 ``` 日期分组如下所示，每五分钟的数据在一个组。 ``` 1/5 0 2/5 0 3/5 0 4/5 0 5/5 1 6/5 1 7/5 1 8/5 1 9/5 1 10/5 2 11/5 2 ....后面省略 扩展一下，按天统计每小时的统计量，这个相对上面的就简单些了，也顺便记录下1234567891011temphall=&gt; select to_char(act_time,'yyyy-mm-dd '), date_part('hour',act_time),sum(down_bytes), sum(down_time),sum(down_bytes)/sum(down_time) temphall-&gt; from tmp_table temphall-&gt; where act_time &gt; '2010-11-17 00:00:00' temphall-&gt; group by to_char(act_time,'yyyy-mm-dd '), date_part('hour',act_time) temphall-&gt; order by 1,2 temphall-&gt; ; to_char | date_part | sum | sum | ?column? -------------+-----------+------------+---------+---------- 2010-11-17 | 0 | XXXXXXXXXX | XXXXXXX | XXXX 2010-11-17 | 1 | XXXXXXXXXX | XXXXXX | XXXX 2010-11-17 | 2 | XXXXXXXXX | XXXXXX | XXXX","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostGis 问题一例","slug":"20101119150557","date":"2010-11-19T07:05:57.000Z","updated":"2018-09-04T01:33:46.473Z","comments":true,"path":"20101119150557.html","link":"","permalink":"https://postgres.fun/20101119150557.html","excerpt":"","text":"安装环境今天准备在 PostgreSQL 9.0 版本基础上搭建一套postgis数据库环境，安装过程中遇到一点小问题，后来还是解决了。 安装过程环境信息OS : Red Hat Enterprise 5PostgreSQL: 9.0.1PostGIS: 1.5.1proj: proj-4.7.0geos: geos-3.2.2 proj,geos 的安装已经完成，这里不再记录，下面是安装postgis过程， config 通过12345678910111213141516171819./configure --prefix=/usr/local/pg_tool/postgis --with-pgconfig=/opt/pgsql/bin/pg_config --with-projdir=/usr/local/pg_tool/proj --with-geosconfig=/usr/local/pg_tool/geos/bin/geos-config PostGIS is now configured for x86_64-unknown-linux-gnu-------------- Compiler Info ------------- C compiler: gcc -g -O2 C++ compiler: g++ -g -O2-------------- Dependencies -------------- GEOS config: /usr/local/pg_tool/geos/bin/geos-config GEOS version: 3.2.2 PostgreSQL config: /opt/pgsql/bin/pg_config PostgreSQL version: PostgreSQL 9.0.1 PROJ4 version: 47 Libxml2 config: /usr/bin/xml2-config Libxml2 version: 2.6.26 PostGIS debug level: 0-------- Documentation Generation -------- xsltproc: /usr/bin/xsltproc xsl style sheets: dblatex: convert: 编译通过123456789101112131415161718192021222324[root@172_16_3_216 postgis-1.5.1]# make make -C liblwgeom make[1]: Entering directory `/opt/soft_bak/postgis-1.5.1/liblwgeom' make[1]: Nothing to be done for `all'. make[1]: Leaving directory `/opt/soft_bak/postgis-1.5.1/liblwgeom' make -C postgis make[1]: Entering directory `/opt/soft_bak/postgis-1.5.1/postgis' Makefile.pgxs:17: warning: overriding commands for target `install' /opt/pgsql/lib/pgxs/src/makefiles/pgxs.mk:99: warning: ignoring old commands for target `install' Makefile.pgxs:63: warning: overriding commands for target `installdirs' /opt/pgsql/lib/pgxs/src/makefiles/pgxs.mk:147: warning: ignoring old commands for target `installdirs' Makefile.pgxs:82: warning: overriding commands for target `uninstall' /opt/pgsql/lib/pgxs/src/makefiles/pgxs.mk:171: warning: ignoring old commands for target `uninstall' make[1]: Nothing to be done for `all'. make[1]: Leaving directory `/opt/soft_bak/postgis-1.5.1/postgis' make -C loader make[1]: Entering directory `/opt/soft_bak/postgis-1.5.1/loader' make[1]: Nothing to be done for `all'. make[1]: Leaving directory `/opt/soft_bak/postgis-1.5.1/loader' make -C utils make[1]: Entering directory `/opt/soft_bak/postgis-1.5.1/utils' chmod +x postgis_restore.pl create_undef.pl postgis_proc_upgrade.pl profile_intersects.pl test_estimation.pl test_joinestimation.pl make[1]: Leaving directory `/opt/soft_bak/postgis-1.5.1/utils' PostGIS was built successfully. Ready to install. Make Check 不成功make check报CUnit没安装 [root@172_16_3_216 postgis-1.5.1]# make check make -C liblwgeom make[1]: Entering directory `/opt/soft_bak/postgis-1.5.1/liblwgeom&apos; make[1]: Nothing to be done for `all&apos;. make[1]: Leaving directory `/opt/soft_bak/postgis-1.5.1/liblwgeom&apos; make -C postgis make[1]: Entering directory `/opt/soft_bak/postgis-1.5.1/postgis&apos; Makefile.pgxs:17: warning: overriding commands for target `install&apos; /opt/pgsql/lib/pgxs/src/makefiles/pgxs.mk:99: warning: ignoring old commands for target `install&apos; Makefile.pgxs:63: warning: overriding commands for target `installdirs&apos; /opt/pgsql/lib/pgxs/src/makefiles/pgxs.mk:147: warning: ignoring old commands for target `installdirs&apos; Makefile.pgxs:82: warning: overriding commands for target `uninstall&apos; /opt/pgsql/lib/pgxs/src/makefiles/pgxs.mk:171: warning: ignoring old commands for target `uninstall&apos; make[1]: Nothing to be done for `all&apos;. make[1]: Leaving directory `/opt/soft_bak/postgis-1.5.1/postgis&apos; make -C liblwgeom/cunit check make[1]: Entering directory `/opt/soft_bak/postgis-1.5.1/liblwgeom/cunit&apos; WARNING: configure was unable to find CUnit which is required for unit testing. In order to enable unit testing, you must install CUnit and then re-run configure. make[1]: Leaving directory `/opt/soft_bak/postgis-1.5.1/liblwgeom/cunit&apos; make -C regress check make[1]: Entering directory `/opt/soft_bak/postgis-1.5.1/regress&apos; make -C ../postgis REGRESS=1 DESTDIR=/opt/soft_bak/postgis-1.5.1/regress/00-regress-install install make[2]: Entering directory `/opt/soft_bak/postgis-1.5.1/postgis&apos; Makefile.pgxs:17: warning: overriding commands for target `install&apos; /opt/pgsql/lib/pgxs/src/makefiles/pgxs.mk:99: warning: ignoring old commands for target `install&apos; Makefile.pgxs:63: warning: overriding commands for target `installdirs&apos; /opt/pgsql/lib/pgxs/src/makefiles/pgxs.mk:147: warning: ignoring old commands for target `installdirs&apos; Makefile.pgxs:82: warning: overriding commands for target `uninstall&apos; /opt/pgsql/lib/pgxs/src/makefiles/pgxs.mk:171: warning: ignoring old commands for target `uninstall&apos; /bin/mkdir -p &apos;/opt/soft_bak/postgis-1.5.1/regress/00-regress-install/lib&apos; &apos;/opt/soft_bak/postgis-1.5.1/regress/00-regress-install/share/contrib/postgis-1.5&apos; make[2]: /opt/soft_bak/postgis-1.5.1/regress/00-regress-install/share/contrib/postgis-1.5: Command not found make[2]: * [installdirs] Error 127 make[2]: Leaving directory `/opt/soft_bak/postgis-1.5.1/postgis&apos; make[1]: * [check] Error 2 make[1]: Leaving directory `/opt/soft_bak/postgis-1.5.1/regress&apos; make: * [check] Error 2 注意红色字体部分,出现一个 WARNING, WARNING: configure was unable to find CUnit which is required for unit testing. In order to enable unit testing, you must install CUnit and then re-run configure. 从上面信息来看，应该是没装CUnit,后来在网上找到 CUnit 官网,以下是关于CUnit的介绍。 CUnit is a lightweight system for writing, administering, and running unit tests in C. It provides C programmers a basic testing functionality with a flexible variety of user interfaces. 解决方法后来猜测 PostGis 可能与新版本的PostgreSQL不兼容，于是到PostGIS官网，发现有最新版本，1.5.2,下载最新版本后，重新编译安装，成功。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PostGIS","slug":"PostGIS","permalink":"https://postgres.fun/tags/PostGIS/"}]},{"title":"年底了， 给分区表扩分区","slug":"20101029100418","date":"2010-10-29T02:04:18.000Z","updated":"2018-09-04T01:33:46.410Z","comments":true,"path":"20101029100418.html","link":"","permalink":"https://postgres.fun/20101029100418.html","excerpt":"","text":"年底了，这边Postgresql的很多库都建了分区表，需要扩分区，这次计划将所有PG库的所有分区表的分扩扩到201207，扩分区是一件比较琐碎而又重要的事情， 琐碎是因为扩分区需要编辑大量脚本，重要是因为如果一个分区月份搞错，或者是权限未加，那么这张表很有可能写不进数据，也不能访问，所以在整个扩分区的过程中需要仔细检查每一个脚本，核对月份，月份范围，以前触发器脚本等。这边有很多分区表，扩分区比较费时间，有空的时候才愿意去做，在年底前，计划每天扩个两张分区表一直到扩完为止。 PostgreSQL的分区表创建较Oracle分区表复杂些，需要借助创建继承表和触发器的模式实现, 具体创建方法可以参照以下实验, 供参考。 创建父表123456789101112131415161718CREATE TABLE fenqu_table ( id integer NOT NULL, mcc character varying(5), lac character varying(8), mnc character(2), cellid character varying(8), signal integer, antennaheight integer, cellpower integer, addtime timestamp(0) without time zone DEFAULT now(), cellidfrom character varying(200), address character varying(200), mark integer DEFAULT 100, fail_times integer DEFAULT 0, success_times integer DEFAULT 0, skyid integer DEFAULT 0, mapaddress character varying(200) ); 创建子表1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162CREATE TABLE fenqu_table_p200905 ( id integer, mcc character varying(5), lac character varying(8), mnc character(2), cellid character varying(8), signal integer, antennaheight integer, cellpower integer, addtime timestamp(0) without time zone, cellidfrom character varying(200), address character varying(200), mark integer, fail_times integer, success_times integer, skyid integer, mapaddress character varying(200), CONSTRAINT fenqu_table_p200905_addtime_check CHECK (((addtime &gt;= '2009-05-01 00:00:00'::timestamp without time zone) AND (addtime &lt; '2009-06-01 00:00:00'::timestamp without time zone))) ) INHERITS (fenqu_table);CREATE TABLE fenqu_table_p200906 ( id integer, mcc character varying(5), lac character varying(8), mnc character(2), cellid character varying(8), signal integer, antennaheight integer, cellpower integer, addtime timestamp(0) without time zone, cellidfrom character varying(200), address character varying(200), mark integer, fail_times integer, success_times integer, skyid integer, mapaddress character varying(200), CONSTRAINT fenqu_table_p200906_addtime_check CHECK (((addtime &gt;= '2009-06-01 00:00:00'::timestamp without time zone) AND (addtime &lt; '2009-07-01 00:00:00'::timestamp without time zone))) ) INHERITS (fenqu_table);CREATE TABLE fenqu_table_p200907 ( id integer, mcc character varying(5), lac character varying(8), mnc character(2), cellid character varying(8), signal integer, antennaheight integer, cellpower integer, addtime timestamp(0) without time zone, cellidfrom character varying(200), address character varying(200), mark integer, fail_times integer, success_times integer, skyid integer, mapaddress character varying(200), CONSTRAINT fenqu_table_p200907_addtime_check CHECK (((addtime &gt;= '2009-07-01 00:00:00'::timestamp without time zone) AND (addtime &lt; '2009-08-01 00:00:00'::timestamp without time zone))) ) INHERITS (fenqu_table); 创建数据删除触发器函数1234567891011121314151617CREATE OR REPLACE FUNCTION public.fenqu_table_delete_trigger() RETURNS trigger LANGUAGE plpgsql AS $function$ BEGIN IF ( OLD.addtime&gt;='2009-05-01' and OLD.addtime&lt;'2009-06-01' ) THEN DELETE FROM fenqu_table_p200905 where cretime=OLD.cretime; ELSIF ( OLD.addtime&gt;='2009-06-01' and OLD.addtime&lt;'2009-07-01' ) THEN DELETE FROM fenqu_table_p200906 where cretime=OLD.cretime; ELSIF ( OLD.addtime&gt;='2009-07-01' and OLD.addtime&lt;'2009-08-01' ) THEN DELETE FROM fenqu_table_p200907 where cretime=OLD.cretime; ELSE RAISE EXCEPTION 'addtime out of range. Fix the fenqu_table_delete_trigger() function!'; END IF; RETURN NULL; END; $function$ 创建数据插入触发器函数1234567891011121314151617CREATE OR REPLACE FUNCTION public.fenqu_table_insert_trigger() RETURNS trigger LANGUAGE plpgsql AS $function$ BEGIN IF ( NEW.addtime&gt;='2009-05-01' and NEW.addtime&lt;'2009-06-01' ) THEN INSERT INTO fenqu_table_p200905 VALUES (NEW.*); ELSIF ( NEW.addtime&gt;='2009-06-01' and NEW.addtime&lt;'2009-07-01' ) THEN INSERT INTO fenqu_table_p200906 VALUES (NEW.*); ELSIF ( NEW.addtime&gt;='2009-07-01' and NEW.addtime&lt;'2009-08-01' ) THEN INSERT INTO fenqu_table_p200907 VALUES (NEW.*); ELSE RAISE EXCEPTION 'addtime out of range. Fix the fenqu_table_insert_trigger() function!'; END IF; RETURN NULL; END; $function$ 创建数据插入、删除触发器12create trigger delete_fenqu_table_trigger BEFORE DELETE ON fenqu_table FOR EACH ROW EXECUTE PROCEDURE fenqu_table_delete_trigger(); create trigger insert_fenqu_table_trigger BEFORE INSERT ON fenqu_table FOR EACH ROW EXECUTE PROCEDURE fenqu_table_insert_trigger(); create sequence1234create sequence fenqu_table_id_seq INCREMENT by 1 MINVALUE 1 start with 1; 分区表测试1234567891011121314151617181920mydb=&gt; insert into fenqu_table (id,addtime) values (nextval('fenqu_table_id_seq'),'2009-05-01 00:00:00'); INSERT 0 0mydb=&gt; select count(*) from fenqu_table; count ------- 1 (1 row)mydb=&gt; select count(*) from only fenqu_table; count ------- 0 (1 row)mydb=&gt; select count(*) from fenqu_table_p200905; count ------- 1 (1 row)","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"Partition Table","slug":"Partition-Table","permalink":"https://postgres.fun/tags/Partition-Table/"}]},{"title":"PostgreSQL 索引坏块一例","slug":"20101009140507","date":"2010-10-09T06:05:07.000Z","updated":"2018-09-04T01:33:46.363Z","comments":true,"path":"20101009140507.html","link":"","permalink":"https://postgres.fun/20101009140507.html","excerpt":"","text":"今天应用反应有张表查询报错，报错信息如下12back=# select max(create_time) from public.tbl_index_table where create_time&gt;='2010-10-08'; ERROR: could not read block 41381 of relation 16779/24769/24938: read only 0 of 8192 bytes 看到这个错误信息，首先想到的是表 tbl_index_table 上有坏块，估计需要表重建下。 查看执行计划1234567891011121314151617181920212223242526272829back=# \\d tbl_index_table; Table \"public.tbl_index_table\" Column | Type | Modifiers ----------------+-----------------------------+------------------------ total | integer | logined | integer | logining | integer | http | integer | rawtcp | integer | create_time | timestamp without time zone | not null default now() logincountdesc | character varying | logincountaddr | character varying | not null Indexes: \"tbl_index_table_pkey\" PRIMARY KEY, btree (create_time, logincountaddr) \"index_tbl_index_table_create_time\" btree (create_time) back=# select max(create_time) from public.tbl_index_table where create_time&gt;='2010-10-08'; ERROR: could not read block 41381 of relation 16779/24769/24938: read only 0 of 8192 bytes back=# explain select max(create_time) from public.tbl_index_table where create_time&gt;='2010-10-08'; QUERY PLAN ----------------------------------------------------------------------------------------------------------------------------- Result (cost=0.04..0.05 rows=1 width=0) InitPlan -&gt; Limit (cost=0.00..0.04 rows=1 width=8) -&gt; Index Scan Backward using index_tbl_index_table_create_time on tbl_index_table (cost=0.00..66.28 rows=1507 width=8) Index Cond: (create_time &gt;= '2010-10-08 00:00:00'::timestamp without time zone) Filter: (create_time IS NOT NULL) (6 rows) 发现上面的查询走的索引 index_tbl_index_table_create_time,猜测索引可能有问题。 根据报错信息,从relation后面的数字分析1234567891011121314151617back=# select oid,relname from pg_class where oid=24938; oid | relname -------+----------------------------------------- 24938 | index_tbl_index_table_create_time (1 row)Time: 0.596 ms back=# select oid,relname from pg_class where oid=24769; oid | relname -----+--------- (0 rows)Time: 0.369 ms back=# select oid,relname from pg_class where oid=16779; oid | relname -----+--------- (0 rows) 发现 24938正好是表上的索引 index_tbl_index_table_create_time。 查看索引状态12345back=# select * from pg_index where indexrelid=24938; indexrelid | indrelid | indnatts | indisunique | indisprimary | indisclustered | indisvalid | indcheckxmin | indisready | indkey | indclass | indoption | indexprs | indpred ------------+----------+----------+-------------+--------------+----------------+------------+--------------+------------+--------+----------+-----------+----------+--------- 24938 | 24823 | 1 | f | f | f | t | f | t | 6 | 10053 | 0 | | (1 row) indisvalid=t 表示索引处于可用状态。 尝试重建索引12345678910111213141516back=# select current_query from pg_stat_activity; current_query --------------------------------------------- &lt;IDLE&gt; &lt;IDLE&gt; select current_query from pg_stat_activity; &lt;IDLE&gt; &lt;IDLE&gt; (22 rows)back=# timing Timing is on.back=# reindex index index_tbl_index_table_create_time; REINDEX Time: 107796.232 ms 索引重建后，查询恢复正常123456789101112back=# select max(create_time) from public.tbl_index_table where create_time&gt;='2010-10-08'; max ----- (1 row)Time: 73.600 msback=# select pg_size_pretty(pg_relation_size('index_tbl_index_table_create_time')); pg_size_pretty ---------------- 327 MB (1 row) 总结网上查了下，说是 Postgresql 的bug 2197, 但从上面的处理过程来看，应该是索引上有坏块，索引重建后，查询恢复正常。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"Oracle 用户密码含有特殊字符时 Exp 的使用","slug":"20101008191751","date":"2010-10-08T11:17:51.000Z","updated":"2018-09-04T01:33:46.301Z","comments":true,"path":"20101008191751.html","link":"","permalink":"https://postgres.fun/20101008191751.html","excerpt":"","text":"今天通过exp迁移Oracle 数据，由于密码含有很有很多特殊字符，弄了好久，都没成功， 后来在网上找到方法。 Exp用法Linux 下(密码用一对双引号, 整体userid用对单引号括住)1exp 'skytf/\"?`&#125;:Q*H\"@skytf'\"&lt;akJWI-q-@#&lt;&gt;?`&#125;:Q*H\"@skytf' DIRECT=Y buffer=52428800 FEEDBACK=10000 statistics=none file=skytf.dmp log=exp_skytf.log windows下(密码前后分别用三个双引号括住):1exp skytf/\"\"[\"#$adf`%6^dfom\"\"\"@skytf](mailto:\"#$adf) buffer=52428800 file=skytf.dmp log=exp_skytf.log Exp Query 参数的用法。12Linux: exp scott/tiger tables=emp query=\"where job='SALESMAN' and sal&lt;1600\" Windows exp&gt;exp userid=tkyte/tkyte tables=t query=\"\"\"where object_id &lt; 5000\"\"\" 在windows中，需要在WHERE语句的两端使用三个双引号 其它123419:13:20 [SYS@skytf](mailto:SYS@skytf)&gt; alter user skytf identified by \"#$adf`%6^dfom\"; 用户已更改。19:14:29 [SYS@skytf](mailto:SYS@skytf)&gt; conn skytf/\"#$adf`%6^dfom\"@skytf 已连接。","categories":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/categories/Oracle/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"https://postgres.fun/tags/Oracle/"}]},{"title":"不要使用kill -9 杀 PostgreSQL 进程","slug":"20101008174858","date":"2010-10-08T09:48:58.000Z","updated":"2018-09-04T01:33:46.254Z","comments":true,"path":"20101008174858.html","link":"","permalink":"https://postgres.fun/20101008174858.html","excerpt":"","text":"今天应用反映数据库很慢，有些SQL执行一天都没执行完，数据库版本为 8.3。 检查数据库，发现大量会话在更新同一张表 tbl_tmp, 产生大量行锁申请等侍。等于等侍状态的SQL (被堵住的SQL)。 查找等待状态的SQL123456789101112131415anpstat=# select datname,procpid,query_start,current_query,waiting,client_addr from pg_stat_activity where waiting='t'; datname | procpid |query_start |current_query | waiting | client_addr ----------------------------------------------------------------------------------------------------------------------+---------+------------- sanpstat | 14044 | 2010-10-07 16:46:40.386904+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 sanpstat | 27839 | 2010-10-07 16:49:18.022499+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 sanpstat | 3539 | 2010-10-07 20:28:13.212075+08 |update tbl_tmp setfeedback =$1 where seq=$2 | t | 127.0.0.1 sanpstat | 3894 | 2010-10-07 20:53:43.466517+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 sanpstat | 10130 | 2010-10-08 09:37:51.253871+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 sanpstat | 9083 | 2010-10-08 08:08:59.221976+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 sanpstat | 10038 | 2010-10-08 09:25:55.434459+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 sanpstat | 10241 | 2010-10-08 09:51:13.990492+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 | t | 127.0.0.1 sanpstat | 11147 | 2010-10-08 11:11:21.574665+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 sanpstat | 11168 | 2010-10-08 11:17:17.158246+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 sanpstat | 11926 | 2010-10-08 11:54:10.704641+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 sanpstat | 11959 | 2010-10-08 11:56:07.021072+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | t | 127.0.0.1 查找正在执行的语句12345678910111213141516sanpstat=# select datname,procpid,query_start,current_query,waiting,client_addr from pg_stat_activity where waiting='f' sanpstat-# and current_query like '%update tbl_tmp %'; datname | procpid |query_start |current_query | waiting |client_addr ----------+---------+-------------------------------+-----------------------------------------------------------------------------------------------------------------------------------+---------+--------------- sanpstat | 2012 | 2010-10-07 16:52:55.8228+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | f | 127.0.0.1 sanpstat | 14157 | 2010-10-07 16:41:37.308062+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | f | 127.0.0.1 sanpstat | 10177 | 2010-10-08 09:43:43.297872+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | f | 127.0.0.1 sanpstat | 2043 | 2010-10-07 19:25:46.931806+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | f | 127.0.0.1 sanpstat | 3298 | 2010-10-07 19:37:53.307125+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | f | 127.0.0.1 sanpstat | 3322 | 2010-10-07 19:40:39.373079+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | f | 127.0.0.1 sanpstat | 4114 | 2010-10-08 09:25:18.701269+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | f | 127.0.0.1 sanpstat | 9082 | 2010-10-08 09:18:16.153882+08 |update tbl_tmp set show=1 , gmt_modified=now(), modifier=$1 where seq=$2 | f | 127.0.0.1 sanpstat | 10042 | 2010-10-08 09:32:55.260732+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | f | 127.0.0.1 sanpstat | 10278 | 2010-10-08 11:08:55.334562+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | f | 127.0.0.1 sanpstat | 11810 | 2010-10-08 11:46:24.147652+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | f | 127.0.0.1 sanpstat | 11964 | 2010-10-08 11:58:32.831916+08 |update tbl_tmp set responsed=true ,feedback =$1, gmt_modified=now(), modifier=$2 where seq=$3 | 使用pg_cancel_backend()杀会话12345sanpstat=# select pg_cancel_backend(2012); pg_cancel_backend ------------------- t (1 row) 用 pg_cancel_backend()杀进程，虽然显示为 ‘t’ ，但进程还在，文档上解释 Cancel a backend&#39;s current query,猜想pg_cancel_backend()只能 kill select 语句，而不能kill update语句，后来实验也证实了这一点，有兴趣的朋友可以自己做实验去验证下。12345678910111213kill -9 杀进程 kill -9 2012 kill -9 14157 kill -9 10177 kill -9 2043 kill -9 3298 kill -9 3322 kill -9 4114 kill -9 9082 kill -9 10042 kill -9 10278 kill -9 11810 kill -9 11964 这里将正在执行的update tbl_tmp 表的会话kill 掉. 数据库无法连接12[enterprisedb@sanp-rich-db1 ~]$ psql -h 192.168.3.27 -p 1921 -d postgres -U rmt_db_bak psql: FATAL:the database system is in recovery mode 备注：进程kill -9 以后，发现数据库无法连接，遭了，还好是个测试库。 数据库处于恢复模式12345678-bash-3.2$ ps -ef | grep post postgres 4627 1 0 Apr12 ? 00:02:24 /home/postgres/bin/postgres -D /mnt/data postgres 4629 4627 0 Apr12 ? 00:00:04 postgres: logger process root 14195 14168 0 15:37 pts/7 00:00:00 su - postgres postgres 14196 14195 0 15:37 pts/7 00:00:00 -bash postgres 14332 4627 44 15:41 ? 00:00:18 postgres: startup process recovering 00000001000000DA0000006E postgres 21495 14196 0 15:41 pts/7 00:00:00 ps -ef postgres 21496 14196 0 15:41 pts/7 00:00:00 grep post 发现Postgresql 处于 recovery 状态，奇怪，Postgresql 恢复过程中是不允许连接的，在这点上，Oracle似乎要友好些 结论 postgresql 8.3.3的版本 只提供系统函数 pg_cancel_backend(pid int) 杀进程，但是，这个函数只能 kill Select 查询，而updae,delete DML不生效,感兴趣的朋友可以自己做实验验证下； postgresql 8.4 以后版本提供 函数 pg_terminate_backend(pid int), 这个函数功能比前者强大，可以kill 各种DML(SELECT,UPDATE,DELETE,DROP)操作; 对于Postgresql ，最好不要用kill -9 去杀用户进程，因为如果是一个很大的UPDATE,kill后，Postgresql需要很长的时间做Recovery,而在这个RECOVERY过程中，数据库是不可用的，在这点上，Oracle 要友好些，在数据库恢复的过程中数据库依然可用,不太需要因Kill -9 用户进程而造成数据库不可用的情况。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL 锁浅析","slug":"20100921154343","date":"2010-09-21T07:43:43.000Z","updated":"2018-09-04T01:33:46.191Z","comments":true,"path":"20100921154343.html","link":"","permalink":"https://postgres.fun/20100921154343.html","excerpt":"","text":"一、概述此文档主要对Postgresql 锁机制进行分析，在讲解的过程中结合实验，理解Postgresql的锁机制。 二、表级锁类型表级锁类型分为八种，以下对各种表级锁类型进行简单介绍下, 锁的冲突模式可以参考3.1的图一:表级锁冲突模式。 ACCESS SHARE“ACCESS SHARE”锁模式只与“ACCESS EXCLUSIVE” 锁模式冲突;查询命令（Select command）将会在它查询的表上获取”Access Shared” 锁,一般地，任何一个对表上的只读查询操作都将获取这种类型的锁。 ROW SHARE“Row Share” 锁模式与”Exclusive’和”Access Exclusive”锁模式冲突;”Select for update”和”Select for share”命令将获得这种类型锁，并且所有被引用但没有 FOR UPDATE 的表上会加上”Access shared locks”锁。 ROW EXCLUSIVE“Row exclusive” 与 “Share,Shared roexclusive,Exclusive,Access exclusive”模式冲突;“Update,Delete,Insert”命令会在目标表上获得这种类型的锁，并且在其它被引用的表上加上”Access shared”锁,一般地，更改表数据的命令都将在这张表上获得”Row exclusive”锁。 SHARE UPDATE EXCLUSIVE”Share update exclusive,Share,Share row ,exclusive,exclusive,Access exclusive”模式冲突，这种模式保护一张表不被并发的模式更改和VACUUM;“Vacuum(without full), Analyze ”和 “Create index concurrently”命令会获得这种类型锁。 SHARE与“Row exclusive,Shared update exclusive,Share row exclusive ,Exclusive,Access exclusive”锁模式冲突,这种模式保护一张表数据不被并发的更改;“Create index”命令会获得这种锁模式。 SHARE ROW EXCLUSIVE与“Row exclusive,Share update exclusive,Shared,Shared row exclusive,Exclusive,Access Exclusive”锁模式冲突;任何Postgresql 命令不会自动获得这种锁。 EXCLUSIVE与” ROW SHARE, ROW EXCLUSIVE, SHARE UPDATE EXCLUSIVE, SHARE, SHARE ROW EXCLUSIVE, EXCLUSIVE, ACCESS EXCLUSIVE”模式冲突,这种索模式仅能与Access Share 模式并发,换句话说，只有读操作可以和持有”EXCLUSIVE”锁的事务并行；任何Postgresql 命令不会自动获得这种类型的锁； ACCESS EXCLUSIVE与所有模式锁冲突(ACCESS SHARE, ROW SHARE, ROW EXCLUSIVE, SHARE UPDATE EXCLUSIVE, SHARE, SHARE ROW EXCLUSIVE, EXCLUSIVE, and ACCESS EXCLUSIVE),这种模式保证了当前只有一个事务访问这张表; “ALTER TABLE, DROP TABLE, TRUNCATE, REINDEX, CLUSTER, VACUUM FULL” 命令会获得这种类型锁，在Lock table 命令中，如果没有申明其它模式，它也是缺省模式。 三、表级锁冲突模式3.1 Conflicting lock modes 图一 表级锁冲突模式 备注：上图是 PostgreSQL 表级锁的各种冲突模式对照表，红色的‘X’表示冲突项, 在章节四中会对其中典型的锁模式进行模似演示。 四、实验在这一章节中将会对图一中比较典型的锁冲突进行模似演练，了解这些在 PostgreSQL DBA的日常维护工作中很有帮助，同时也能减少人为故障的发生。 4.1 Access exclusive 锁与Access share锁冲突在日常维护中，大家应该执行过’ALTER TABLE’更改表结构的DDL，例如加字段，更改字段数据类型等，根据章节二的理论，在执行’ALTER TABLE’命令时将申请一个Access exclusive锁, 根据图一，大家知道Access exclusive 锁和所有的锁模式都冲突，那么，它将会’Select’命令冲突，因为Select 加的是Access share锁，那么真的会与‘SELECT‘命令冲突吗，接下来给大家演示下: 创建一张测试表 test_2 并插入测试数据123456789101112mydb=&gt; create table test_2 (id integer,name varchar(32));CREATE TABLEmydb=&gt; insert into test_2 values (1,'franc');INSERT 0 1mydb=&gt; insert into test_2 values (2,'tan');INSERT 0 1mydb=&gt; select * from test_2;id | name----+------- 1 | franc 2 | tan(2 rows) 会话一 查询表数据 ( 这里获得Access Shared 锁)1234567mydb=&gt; begin;BEGINmydb=&gt; select * from test_2 where id=1;id | name----+------- 1 | franc(1 row) 注意：这里begin开始事务，没有提交； 会话二 更改表结构 (这里申请 Access Exclusive锁 )12mydb=&gt; alter table test_2 add column sex char(1);发现，命令一直等侍，执行不下去; 会话三 查询状态1234567891011121314151617mydb=# select oid,relname from pg_class where relname='test_2'; oid | relname-------+---------33802 | test_2mydb=# select locktype,database,relation,pid,mode from pg_locks where relation='33802';locktype | database | relation | pid | mode----------+----------+----------+-------+---------------------relation | 16466 | 33802 | 18577 | AccessShareLockrelation | 16466 | 33802 | 18654 | AccessExclusiveLockmydb=# select datname,procpid,usename,current_query from pg_stat_activity where procpid in (18577,18654);datname | procpid | usename | current_query---------+---------+---------+--------------------------------------------mydb | 18577 | skytf | &lt;IDLE&gt; in transactionmydb | 18654 | skytf | alter table test_2 add column sex char(1);(2 rows) 这里可以看出会话一(pid=18577) 获取的是 “AccessShareLock”锁,会话二(pid=18654 ) 获取的是 “AccessExclusiveLock”锁。 再次回到会话一，执行’end’结束事务后会发生什么结果,注意，此时会话二还处于等侍状态12mydb=&gt; end;COMMIT 回到会话二发现 原来处于等侍状态的’ALTER TABLE’命令执行成功123456789mydb=&gt; alter table test_2 add column sex char(1);ALTER TABLEmydb=&gt; \\d test_2 Table \"skytf.test_2\"Column | Type | Modifiers--------+-----------------------+-----------id | integer |name | character varying(32) |sex | character(1) | 回到会话三，锁已经释放1234567891011mydb=# select locktype,database,relation,pid,mode from pg_locks where relation='33802';locktype | database | relation | pid | mode----------+----------+----------+-----+------(0 rows)mydb=# select datname,procpid,usename,client_addr,current_query from pg_stat_activity where procpid in (18577,18654);datname | procpid | usename | client_addr | current_query---------+---------+---------+-------------+---------------mydb | 18577 | skytf | | &lt;IDLE&gt;mydb | 18654 | skytf | | &lt;IDLE&gt;(2 rows) 实验说明：这个实验说明了 ‘ALTER TABLE’命令与’SELECT’命令会产生冲突，证实了开始的结论，即”Access exclusive”锁模式与申请”Access shared”锁模式的’SELECT’命令相冲突。 4.2 Share 锁与 Row Exclusive 锁冲突在数据库的维护过程中，创建索引也是经常做的工作，别小看创建索引，如果是一个很繁忙的系统，索引不一定能创建得上，可能会发生等侍, 严重时造成系统故障；根据章节二的理论，’Create Index’ 命令需要获取Share 锁模式。根据图一，”Share” 锁和”Row Exclusive”锁冲突，下面来验证一下：根据图三可以看出，share锁模式和多种锁模式冲突，有可能会问我，为什么单独讲share锁和Row Exclusive冲突呢？因为” Update,Delete,Insert”命令获取的是Row Exclusive 操作，而这种操作在生产过程中非常频繁；这个实验正是模似生产维护过程。 会话一， 向 test_2 上插入一条数据1234567891011mydb=&gt; select * from test_2;id | name | sex----+-------+----- 1 | franc | 2 | tan |(2 rows)mydb=&gt; begin;BEGINmydb=&gt; insert into test_2 values (3,'fpzhou');INSERT 0 1mydb=&gt; 说明： 这个Insert 操作放在一个事务里，注意此时事务尚未提交。 会话二，在表test_2上创建索引12345678mydb=&gt; \\d test_2; Table \"skytf.test_2\"Column | Type | Modifiers--------+-----------------------+-----------id | integer |name | character varying(32) |sex | character(1) |mydb=&gt; create unique index idx_test_2_id on test_2 (id); 说明： 创建索引命令发生等侍 会话三，查询状态123456789101112mydb=# select locktype,database,relation,pid,mode from pg_locks where relation='33802';locktype | database | relation | pid | mode----------+----------+----------+-------+------------------relation | 16466 | 33802 | 18577 | RowExclusiveLockrelation | 16466 | 33802 | 18654 | ShareLock(2 rows)mydb=# select datname,procpid,usename,client_addr,current_query from pg_stat_activity where procpid in (18577,18654);datname | procpid | usename | client_addr | current_query---------+---------+---------+-------------+----------------------------------------------------mydb | 18577 | skytf | | &lt;IDLE&gt; in transactionmydb | 18654 | skytf | | create unique index idx_test_2_id on test_2 (id); 说明： 这里可以看出”Insert into”(procpid=18577) 命令获取”RowExclusiveLock”,而”Create Index”(procpid=18654)操作获取的是”Sharelock”, 并且创建索引操作发了等侍，因为这两种锁模式是冲突的。 回到会话一，提交事务,看看会发生什么,注意，此时创建索引的会话二仍处于等侍状态12mydb=&gt; end;COMMIT 回到会话二，发现创建索引命令成功,等侍消失12mydb=&gt; create unique index idx_test_2_id on test_2 (id);CREATE INDEX 实验结论： 上述实验说明 “Create index “操作和”Insert”操作冲突;也就是 “Share”锁和”RowExclusive”锁冲突。 在生产库上应该避免在业务高峰期执行新建索引操作，因为如果在张大表上新建索引，消耗时间较长，在这个过程中会阻塞业务的DML操作。 4.3 SHARE UPDATE EXCLUSIVE 与自身冲突根据章节二，大家知道 VACUUM(Without full), Analyze 和 Create index (Concurently)操作会申请获得”Shared update Exclusive 锁”。根据图一，”Shared update Exclusive 锁”与本身也是会冲突的，下面实验验证一下: 会话一，分析表test_212345678910111213mydb=&gt; select * from test_2;id | name | sex----+--------+----- 1 | franc | 2 | tan | 3 | fpzhou |(3 rows)mydb=&gt;mydb=&gt;mydb=&gt; begin;BEGINmydb=&gt; analyze test_2;ANALYZE 注意： 表分析放在一个事务里，此时并没有提交； 会话二 对表 test_2 做 vacuum12345678910mydb=&gt; \\d test_2; Table \"skytf.test_2\"Column | Type | Modifiers--------+-----------------------+-----------id | integer |name | character varying(32) |sex | character(1) |Indexes: \"idx_test_2_id\" UNIQUE, btree (id)mydb=&gt; vacuum test_2; 注意： 当对表 test_2 执行 vacuum操作时，操作等侍， 会话三，观察系统哪里锁住了12345678[postgres@pg1 ~]$ psql -d mydbpsql (9.0beta3)Type \"help\" for help.mydb=# select datname,procpid,waiting,current_query from pg_stat_activity where waiting='t';datname | procpid | waiting | current_query---------+---------+---------+----------------mydb | 20625 | t | vacuum test_2;(1 row) 这里说明会话 vacuum test_2 在等侍123456789101112mydb=# select oid,relname from pg_class where relname='test_2'; oid | relname-------+---------33802 | test_2(1 row)mydb=# select locktype,database,relation,pid,mode from pg_locks where relation='33802';locktype | database | relation | pid | mode----------+----------+----------+-------+--------------------------relation | 16466 | 33802 | 20625 | ShareUpdateExclusiveLockrelation | 16466 | 33802 | 20553 | ShareUpdateExclusiveLock(2 rows) 说明: 这里可以看出 ‘Analyze’操作 (pid=20553) 和’Vacuum’操作 (pid=20625)都是加的”ShareUpdateExclusiveLock”。123456mydb=# select datname,procpid,waiting,current_query from pg_stat_activity where procpid in (20625,20553);datname | procpid | waiting | current_query---------+---------+---------+-----------------------mydb | 20553 | f | &lt;IDLE&gt; in transactionmydb | 20625 | t | vacuum test_2;(2 rows) 说明： 结束上面查询可以看出会话20625在等侍会话20553,也就是说”vacuum test_2” 被事务堵住了， 再次回到会话一，提交会话,注意此时会话二处于等侍姿态；12mydb=&gt; end;COMMIT 再次回到会话二，发现 vacuum命令执行下去了，等侍消失。123456mydb=&gt; vacuum test_2;VACUUMmydb=&gt; select datname,procpid,waiting,current_query from pg_stat_activity where waiting='t';datname | procpid | waiting | current_query---------+---------+---------+---------------(0 rows) 实验结论： Analyze 和 Vacuum 操作都会申请获得 “ShareUpdateExclusiveLock”。 ShareUpdateExclusiveLoc与ShareUpdateExclusiveLock是冲突的。 五、参考 http://www.postgresql.org/docs/9.4/static/explicit-locking.html","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 恢复一例","slug":"20100918104907","date":"2010-09-18T02:49:07.000Z","updated":"2018-09-04T01:33:46.144Z","comments":true,"path":"20100918104907.html","link":"","permalink":"https://postgres.fun/20100918104907.html","excerpt":"","text":"今天在将pg_dump压缩过的dump文件，通过pg_restore 导入到测试库时，中途异常中断，造成测试库宕机，而且之后数据无法启动。 数据库宕机的 Csvlog123456782010-09-17 17:28:03.943 CST,\"mydb\",\"mydb\",23936,\"192.168.1.25:58855\",4c9334a3.5d80,1,\"/opt/pgsql/bin/postgres\",2010-09-17 17:28:03 CST,,0,FATAL,57P03,\"the database system is in recovery mode\",,,,,,,, 2010-09-17 17:28:03.944 CST,\"mydb\",\"mydb\",23937,\"192.168.1.25:58856\",4c9334a3.5d81,1,\"/opt/pgsql/bin/postgres\",2010-09-17 17:28:03 CST,,0,FATAL,57P03,\"the database system is in recovery mode\",,,,,,,, 2010-09-17 17:28:03.954 CST,\"mydb\",\"mydb\",23938,\"192.168.1.25:58857\",4c9334a3.5d82,1,\"/opt/pgsql/bin/postgres\",2010-09-17 17:28:03 CST,,0,FATAL,57P03,\"the database system is in recovery mode\",,,,,,,, 2010-09-17 17:28:03.955 CST,\"mydb\",\"mydb\",23939,\"192.168.1.25:58858\",4c9334a3.5d83,1,\"/opt/pgsql/bin/postgres\",2010-09-17 17:28:03 CST,,0,FATAL,57P03,\"the database system is in recovery mode\",,,,,,,, 2010-09-17 17:28:03.956 CST,\"mydb\",\"mydb\",23940,\"192.168.1.25:58859\",4c9334a3.5d84,1,\"/opt/pgsql/bin/postgres\",2010-09-17 17:28:03 CST,,0,FATAL,57P03,\"the database system is in recovery mode\",,,,,,,, 2010-09-17 17:28:04.032 CST,,,32240,,4c932f7d.7df0,9,,2010-09-17 17:06:05 CST,,0,FATAL,53100,\"could not write to file \"\"pg_xlog/xlogtemp.32240\"\": No space left on device\",,,,,,,, 2010-09-17 17:28:04.098 CST,,,946,,4c9321d4.3b2,5,,2010-09-17 16:07:48 CST,,0,LOG,00000,\"startup process (PID 32240) exited with exit code 1\",,,,,,,, 2010-09-17 17:28:04.098 CST,,,946,,4c9321d4.3b2,6,,2010-09-17 16:07:48 CST,,0,LOG,00000,\"aborting startup due to startup process failure\",,,,,,,, 从数据库down机前的 csvlog日志来看，down机前数据库SERVER 处于 recovery mode , 说明pg_restore时 SERVER此时有异常，之后尝试重新起动Server,命令如下 12[postgres@PG1 bin]$ pg_ctl -D $PGDATA start server starting 虽然显示’server starting’, 但数据库并没有真正起来，因为这时数据库根本无法连接,接着查看 csvlog 数据库启动异常时 Csvlog12345672010-09-17 17:36:36.660 CST,\"mydb\",\"mydb\",24245,\"192.168.169.42:42566\",4c9336a4.5eb5,1,\"/opt/pgsql/bin/postgres\",2010-09-17 17:36:36 CST,,0,FATAL,57P03,\"the database system is starting up\",,,,,,,, 2010-09-17 17:36:36.662 CST,\"mydb\",\"mydb\",24247,\"192.168.169.42:42567\",4c9336a4.5eb7,1,\"/opt/pgsql/bin/postgres\",2010-09-17 17:36:36 CST,,0,FATAL,57P03,\"the database system is starting up\",,,,,,,, 2010-09-17 17:36:36.663 CST,\"mydb\",\"mydb\",24246,\"192.168.1.25:33223\",4c9336a4.5eb6,1,\"/opt/pgsql/bin/postgres\",2010-09-17 17:36:36 CST,,0,FATAL,57P03,\"the database system is starting up\",,,,,,,, 2010-09-17 17:36:36.663 CST,\"mydb\",\"mydb\",24248,\"192.168.169.42:42568\",4c9336a4.5eb8,1,\"/opt/pgsql/bin/postgres\",2010-09-17 17:36:36 CST,,0,FATAL,57P03,\"the database system is starting up\",,,,,,,, 2010-09-17 17:36:36.665 CST,\"mydb\",\"mydb\",24249,\"192.168.169.42:42569\",4c9336a4.5eb9,1,\"/opt/pgsql/bin/postgres\",2010-09-17 17:36:36 CST,,0,FATAL,57P03,\"the database system is starting up\",,,,,,,, 2010-09-17 17:36:36.666 CST,,,24163,,4c9336a3.5e63,1,,2010-09-17 17:36:35 CST,,0,LOG,00000,\"startup process (PID 24165) exited with exit code 1\",,,,,,,, 2010-09-17 17:36:36.666 CST,,,24163,,4c9336a3.5e63,2,,2010-09-17 17:36:35 CST,,0,LOG,00000,\"aborting startup due to startup process failure\",,,,,,,, 从上面日志来看，可以看出Server 数据库正在启动（the database system is starting up”）, 但是到后面就异常中上了; 而且也没有多余的信息，由于事情紧迫，随即向师傅请教，师傅说数据库需要恢复，可以恢复到具体的时间点。以下是解决过程。 解决过程pg_controldata 查看PG SERVER 详细信息123456789101112131415161718192021222324252627[postgres@PG1 bin]$ pg_controldata $PGDATA pg_control version number: 843 Catalog version number: 200904091 Database system identifier: 5509641090052341117 Database cluster state: shut down pg_control last modified: Fri 17 Sep 2010 05:28:03 PM CST Latest checkpoint location: 137/BFFFF68 Prior checkpoint location: 131/9D90C818 Latest checkpoint's REDO location: 137/BFFFF68 Latest checkpoint's TimeLineID: 1 Latest checkpoint's NextXID: 0/64282 Latest checkpoint's NextOID: 215491390 Latest checkpoint's NextMultiXactId: 1 Latest checkpoint's NextMultiOffset: 0 Time of latest checkpoint: Fri 17 Sep 2010 05:26:05 PM CST Minimum recovery ending location: 0/0 Maximum data alignment: 8 Database block size: 8192 Blocks per segment of large relation: 1048576 WAL block size: 65536 Bytes per WAL segment: 67108864 Maximum length of identifiers: 64 Maximum columns in an index: 32 Maximum size of a TOAST chunk: 1996 Date/time type storage: 64-bit integers Float4 argument passing: by value Float8 argument passing: by value 重要的信息：Latest checkpoint&#39;s NextXID: 0/64282, Latest checkpoint&#39;s NextXID 是指最近一次安全的checkpoints的下一个事务ID，我们可以将数据库恢复到这一时刻。 通过pg_resetxlog 将数据库恢复到事务 64282 时刻123456789101112131415161718[postgres@PG1 bin]$ pg_resetxlog --help pg_resetxlog resets the PostgreSQL transaction log.Usage: pg_resetxlog [OPTION]... DATADIROptions: -e XIDEPOCH set next transaction ID epoch -f force update to be done -l TLI,FILE,SEG force minimum WAL starting location for new transaction log -m XID set next multitransaction ID -n no update, just show extracted control values (for testing) -o OID set next OID -O OFFSET set next multitransaction offset -x XID set next transaction ID --help show this help, then exit --version output version information, then exitReport bugs to &lt;[pgsql-bugs@postgresql.org](mailto:pgsql-bugs@postgresql.org)&gt;.[postgres@PG1 bin]$ pg_resetxlog -x 64282 $PGDATA Transaction log reset 再次启动 PG SERVER，正常,此时数据库已恢复12345678[postgres@PG1 bin]$ pg_ctl -D $PGDATA start server starting [postgres@PG1 bin]$ ps -ef | grep post postgres 25297 1 14 18:08 pts/1 00:00:00 /opt/pgsql/bin/postgres -D /opt/pgdata/pg_root postgres 25298 25297 0 18:08 ? 00:00:00 postgres: logger process postgres 25829 25297 0 18:08 ? 00:00:00 postgres: writer process postgres 25830 25297 0 18:08 ? 00:00:00 postgres: wal writer process postgres 25831 25297 0 18:08 ? 00:00:00 postgres: stats collector process 到了这里，数据库PG SERVER 终于可以启来了，这里顺便说一下，之前的 pg_restore 中途异常是因为脚本中加了 -j 参数，同时跑多个 pg_restore 线程， 造成 pg_restore 子线程连接丢失，pg_restore 脚本中去掉 -j 参数时，数据库顺利导入。 Pg_controldata 官网文档介绍 Namepg_controldata ― display control information of a PostgreSQL database clusterSynopsispg_controldata [datadir]Descriptionpg_controldata prints information initialized during initdb, such as the catalog version. It also shows information about write-ahead logging and checkpoint processing. This information is cluster-wide, and not specific to any one database.This utility can only be run by the user who initialized the cluster because it requires read access to the data directory. You can specify the data directory on the command line, or use the environment variable PGDATA.EnvironmentPGDATADefault data directory location Pg_resetxlog 官网文档介绍 pg_resetxlog ― reset the write-ahead log and other control information of a PostgreSQL database cluster","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"PostgreSQL Lock 一例","slug":"20100908165944","date":"2010-09-08T08:59:44.000Z","updated":"2018-09-04T01:33:46.066Z","comments":true,"path":"20100908165944.html","link":"","permalink":"https://postgres.fun/20100908165944.html","excerpt":"","text":"今天开发人员发来SIR，需要给生产库上一张表加字段,SQL语句如下，表名用 table_test 代替 ALTER TABLE table_test ADD COLUMN column_a integer NOT NULL DEFAULT 0;当时偶查了一下这张表的记录数，才3000多条，这么小的表，接着查了下 pg_stata_activity，发现没有多少活动的会话，于是开始执行上面SQL，执行了大概10S之后，发现还没执行完，觉得奇怪，这么小的表，应该很快才对，估计是被HOLD住了, 详细操作过程如下。 表结构12345678910111213141516171819202122232425262728293031323334testtf=# \\d pg_class; Table \"pg_catalog.pg_class\" Column | Type | Modifiers -----------------+-----------+----------- relname | name | not null relnamespace | oid | not null reltype | oid | not null reloftype | oid | not null relowner | oid | not null relam | oid | not null relfilenode | oid | not null reltablespace | oid | not null relpages | integer | not null reltuples | real | not null reltoastrelid | oid | not null reltoastidxid | oid | not null relhasindex | boolean | not null relisshared | boolean | not null relistemp | boolean | not null relkind | \"char\" | not null relnatts | smallint | not null relchecks | smallint | not null relhasoids | boolean | not null relhaspkey | boolean | not null relhasexclusion | boolean | not null relhasrules | boolean | not null relhastriggers | boolean | not null relhassubclass | boolean | not null relfrozenxid | xid | not null relacl | aclitem[] | reloptions | text[] | Indexes: \"pg_class_oid_index\" UNIQUE, btree (oid) \"pg_class_relname_nsp_index\" UNIQUE, btree (relname, relnamespace) 先查出表table_test 的oid123456789101112131415161718192021222324testtf=# select oid,relname from pg_class where relname='table_test'; oid | relname -------+--------- 16678 | table_test (1 row)testtf=# \\d pg_locks; View \"pg_catalog.pg_locks\" Column | Type | Modifiers --------------------+----------+----------- locktype | text | database | oid | relation | oid | page | integer | tuple | smallint | virtualxid | text | transactionid | xid | classid | oid | objid | oid | objsubid | smallint | virtualtransaction | text | pid | integer | mode | text | granted | boolean | 查询表table_test上持有的锁12345678910111213141516171819202122232425262728293031testtf=# select locktype,database,pid,relation ,mode from pg_locks where relation=16678; locktype | database | pid | relation | mode ----------+----------+-------+----------+--------------------- relation | 16400 | 9905 | 16678 | AccessShareLock relation | 16400 | 9902 | 16678 | AccessShareLock relation | 16400 | 9127 | 16678 | AccessShareLock relation | 16400 | 9909 | 16678 | AccessShareLock relation | 16400 | 6781 | 16678 | AccessShareLock relation | 16400 | 9910 | 16678 | AccessShareLock relation | 16400 | 9903 | 16678 | AccessShareLock relation | 16400 | 6779 | 16678 | AccessShareLock relation | 16400 | 9904 | 16678 | AccessShareLock relation | 16400 | 9907 | 16678 | AccessShareLock relation | 16400 | 9436 | 16678 | AccessExclusiveLock relation | 16400 | 29393 | 16678 | AccessShareLock relation | 16400 | 9908 | 16678 | AccessShareLock relation | 16400 | 9912 | 16678 | AccessShareLock relation | 16400 | 9911 | 16678 | AccessShareLock relation | 16400 | 4615 | 16678 | AccessShareLock relation | 16400 | 9128 | 16678 | AccessShareLock (17 rows)testtf=# select locktype,database,pid,relation ,mode from pg_locks where relation=16678; locktype | database | pid | relation | mode ----------+----------+-------+----------+----------------- relation | 16400 | 29393 | 16678 | AccessShareLock testtf=# select locktype,database,pid,relation ,mode from pg_locks where relation=16678; locktype | database | pid | relation | mode ----------+----------+-------+----------+----------------- relation | 16400 | 29393 | 16678 | AccessShareLock 备注：此时表 table_test 只有一个会话(pid=29393)持有 AccessShareLock 锁，这应该是个Select查询; 12345678910111213141516171819202122testtf=# \\d pg_stat_activity; View \"pg_catalog.pg_stat_activity\" Column | Type | Modifiers ------------------+--------------------------+----------- datid | oid | datname | name | procpid | integer | usesysid | oid | usename | name | application_name | text | client_addr | inet | client_port | integer | backend_start | timestamp with time zone | xact_start | timestamp with time zone | query_start | timestamp with time zone | waiting | boolean | current_query | text | testtf=# select usename,current_query ,xact_start,procpid from pg_stat_activity where procpid=29393; usename | current_query | xact_start | procpid ---------+-----------------------+-------------------------------+--------- wapnews | &lt;IDLE&gt; in transaction | 2010-07-08 13:53:07.906297+08 | 29393 备注：从上面可以看出 空闲事务 9436 持有共享锁”AccessShareLock”，这个事务从 2010-07-08 13:53:07 开始到现在还没有结束。 经和开发人员联系，可以KILL这个会话12345testtf=# select pg_terminate_backend(29393); pg_terminate_backend ---------------------- t (1 row) 会话消失1234testtf=# select usename,current_query ,xact_start,procpid from pg_stat_activity where procpid=29393; usename | current_query | xact_start | procpid ---------+---------------+------------+--------- (0 rows) 再次执行DDL，执行成功12testtf=# ALTER TABLE table_test ADD COLUMN column_a integer NOT NULL DEFAULT 0; ALTER TABLE 总结 原因分析: 在生产库执行类似ALTER TABLE 的DDL时应该非常小心，因为此时加的是 Table 级的 ACCESS EXCLUSIVE 锁 ,ACCESS EXCLUSIVE和其它所有类型所有锁都冲突，包括SELECT，所以当执行Select这张表的事务还没有结束时，执行 Alter Table时，表table_test会因获取不到 ACCESS EXCLUSIVE 而发生等侍。 建议方法：当执行ALTER TABLE类似操作时，正确的方法是先查看 pg_locks 是否有这张表所持有的锁，如果有，等系统空闲的时候 再做这种操作 扩展：当在生产库执行DDL时，应该首先查看执行DDL对象当前所持有锁的情况，如果和执行DDL请求的锁冲突，应该等系统空闲的时候再执行DDL操作。 和开发人员沟通，程序代码及时提交事务。","categories":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/categories/PG案例分析/"}],"tags":[{"name":"PG案例分析","slug":"PG案例分析","permalink":"https://postgres.fun/tags/PG案例分析/"}]},{"title":"PostgreSQL dblink的使用","slug":"20100904121139","date":"2010-09-04T04:11:39.000Z","updated":"2018-09-04T01:33:46.004Z","comments":true,"path":"20100904121139.html","link":"","permalink":"https://postgres.fun/20100904121139.html","excerpt":"","text":"PostgreSQL 也有类似 Oracle dblink 的功能，可以远程访问 PostgreSQL 数据库，但需要手工安装dblink, 下面是dblink的安装过程和简单使用。 dblink的安装编译12#cd postgres源码安装目录/contrib/dblink # make 安装dblink123456[root@pg1 dblink]# make install /bin/mkdir -p '/opt/pgsql/lib' /bin/mkdir -p '/opt/pgsql/share/contrib' /bin/sh ../../config/install-sh -c -m 755 dblink.so '/opt/pgsql/lib/dblink.so' /bin/sh ../../config/install-sh -c -m 644 ./uninstall_dblink.sql '/opt/pgsql/share/contrib' /bin/sh ../../config/install-sh -c -m 644 dblink.sql '/opt/pgsql/share/contrib' 安装完成之后，会在目录 /opt/pgsql/lib/ 产生一个lib文件 dblink.so; 同时在目录 $PGHOME/share/contrib下产生一个 dblink.sql文件，这个文件用来创建 dblink 各类函数。 导入 dblink.sql 到目标库123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# chown -R postgres:postgres /opt/pgsql su - postgres [postgres@pg1 contrib]$ psql -d mydb -f dblink.sql SET CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION REVOKE REVOKE CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE TYPE CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION CREATE FUNCTION dblink的简单使用dblink主要函数简介12345dblink_connect ― opens a persistent connection to a remote database dblink_send_query ― sends an async query to a remote database dblink_get_result ― gets an async query result dblink ― executes a query in a remote database dblink_disconnect ― closes a persistent connection to a remote database dblink的使用 目标库: IP=192.168.1.27 port=1921 database=testdb建立连接12345mydb=&gt; select dblink_connect('dblink_testdb','host=192.168.1.27 port=1921 dbname=testdb user=test_user password=test_user' ); dblink_connect ---------------- OK (1 row) 发送异步查询请求12345mydb=&gt; select dblink_send_query('dblink_testdb','select * from test_id'); dblink_send_query ------------------- 1 (1 row) 获取异步查询结果123456mydb=&gt; select * from dblink_get_result('dblink_testdb') as t1 (id integer); id ----- 100 101 (2 rows) Insert1234mydb=&gt; select dblink_exec('dblink_testdb','insert into test_id values (103)'); dblink_exec ------------- INSERT 0 1 或者直接查询1234567mydb=&gt; select * from dblink('dblink_testdb','select * from test_id') as t1 (id integer); id ----- 100 101 103 (3 rows) 断开dblink连接 mydb=&gt; select dblink_disconnect(&apos;dblink_testdb&apos;); dblink_disconnect ------------------- OK (1 row) 以上就是PostgreSQL dblink的简单使用过程，感觉没有Oracle的dblink使用方便,有关 dblink 的详细但要可以查看PostgreSQL官网文档详细介绍。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 建表大小写问题","slug":"20100823194215","date":"2010-08-23T11:42:15.000Z","updated":"2018-09-04T01:33:45.941Z","comments":true,"path":"20100823194215.html","link":"","permalink":"https://postgres.fun/20100823194215.html","excerpt":"","text":"今天被 PostgreSQL 耍了一下午，其实是个表名大小写问题，好再后来终于明白了; 今天应用有个需求，需要从生产库导部分数据到测试库，其中生产库中有几张表比较特殊，表名中含有大写字母和小写字母，然后通过 pg_dump 导表一直报表不存在,以下模拟下今天的情况, 这里表名用 TesT_1 代替。 以 postgis 用户登陆 postgis 库1psql -d postgis -U posgis 创建一张测试表 “TesT_1”12345678postgis=&gt; create table TesT_1 as select * from pg_tables; SELECT postgis=&gt; \\d List of relations Schema | Name | Type | Owner ---------+-------------------+-------+---------- postgis | test_1 | table | postgis 说明：奇怪，明明创建的表名为 TesT_1, PG默认转换为小写了。 再次创建测试表 TesT_11234567891011postgis=&gt; drop table test_1; DROP TABLEpostgis=&gt; create table \"TesT_1\" as select * from pg_tables; SELECTpostgis=&gt; \\d List of relations Schema | Name | Type | Owner ---------+-------------------+-------+---------- postgis | TesT_1 | table | postgis 表名加了双引号后，果然创建了张表名为 TesT_1的表，利用pg_dump 导出表 ,导出脚本如下，一直报表不存在;12[postgres@pgb dump]$ pg_dump -t TesT_1 -U postgis postgis &gt; 1.out pg_dump: No matching tables were found 加上schema前缀也不行12[postgres@pgb dump]$ pg_dump -t postgis.TesT_1 postgis &gt; 1.out pg_dump: No matching tables were found 后来psql连接到库1psql -d postgis -U posgis 查看库中 postgis有哪些表12345postgis=&gt; \\d List of relations Schema | Name | Type | Owner ---------+-------------------+-------+---------- postgis | TesT_1 | table | postgis 查看表结构，报表不存在12postgis-&gt; \\d Test_1; Did not find any relation named \"Test_1\". 表名加下双引号才行1234567891011postgis-&gt; \\d \"TesT_1\" Table \"postgis.TesT_1\" Column | Type | Modifiers -------------+---------+----------- schemaname | name | tablename | name | tableowner | name | tablespace | name | hasindexes | boolean | hasrules | boolean | hastriggers | boolean | 从而想到，可能是PG默认只读小写，当遇到含有大写字母的表时，就读不到了；于是在 pb_dump中，表名加个双引号就行； 加双引号还不行12[postgres@pgb dump]$ pg_dump -t \"TesT_1\" -U postgis postgis &gt; 1.out pg_dump: No matching tables were found 后来在师傅的提示下，加上转义符 才行1[postgres@pgb dump]$ pg_dump -t \\\"TesT_1\\\" -U postgis postgis &gt; 1.out PS : 到了这里，问题终于解决了，真是无语了，PG里表名会区分大小写，SQL语句会自动转换为小写。","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"PostgreSQL 8.4 + PostGIS + Linux 的安装","slug":"20100822190943","date":"2010-08-22T11:09:43.000Z","updated":"2018-09-04T01:33:45.894Z","comments":true,"path":"20100822190943.html","link":"","permalink":"https://postgres.fun/20100822190943.html","excerpt":"","text":"今天弄了好久，终于把Postgis安装完成，Postgresql 8.4的安装请参考官方文档，这里不记录了，下面是Postgis 1.5的安装过程。 环境信息Postgresql: 8.4.4Postgis: 1.5.1OS : Red Hat Enterprise 5 安装 proj-4.7.0下载地址： http://trac.osgeo.org/proj/1234cd proj-4.7.0/ #./configure --prefix=/usr/local/pg_tool/proj # make # make install 安装 geos-3.2.2下载地址: http://trac.osgeo.org/geos/1234# ./configure --prefix=/usr/local/pg_tool/geos --enable-python --enable-ruby # make # make check # make install 安装 postgis-1.5.1下载地址:http://postgis.refractions.net/download/123456789101112131415161718192021222324./configure --prefix=/usr/local/pg_tool/postgis --with-pgconfig=/opt/pgsql/bin/pg_config --with-projdir=/usr/local/pg_tool/proj --with-geosconfig=/usr/local/pg_tool/geos/bin/geos-configconigure 成功后，会出现以下界面： PostGIS is now configured for i686-pc-linux-gnu-------------- Compiler Info ------------- C compiler: gcc -g -O2 C++ compiler: g++ -g -O2-------------- Dependencies -------------- GEOS config: /usr/local/pg_tool/geos/bin/geos-config GEOS version: 3.2.2 PostgreSQL config: /opt/pgsql/bin/pg_config PostgreSQL version: PostgreSQL 8.4.4 PROJ4 version: 47 Libxml2 config: /usr/bin/xml2-config Libxml2 version: 2.6.26 PostGIS debug level: 0-------- Documentation Generation -------- xsltproc: /usr/bin/xsltproc xsl style sheets: /usr/share/sgml/docbook/xsl-stylesheets dblatex: convert: /usr/bin/convert# make # make check # make install 说明：安装成功后会在目录$PGHOME/share/contrib/下产生一个名为postgis-1.5的目录。 测试新建用户，表空间，数据库，language12345postgres=#create role postgis login with encrypted password 'postgis' nosuperuser nocreatedb nocreaterole noinherit ; postgres=# create tablespace tbs_postgis_data owner postgis LOCATION '/opt/pgdata/tbs_postgis_data'; postgres=# create tablespace tbs_postgis_idx owner postgis LOCATION '/opt/pgdata/tbs_postgis_idx'; postgres=#create database postgis owner=postgis template=template0 ENCODING='UTF8' tablespace=tbs_postgis_data; create language plpgsql; 新增加环境变量 ，修改文件 .bash_profile12345PROJ_HOME=/usr/local/pg_tool/proj GEOS_HOME=/usr/local/pg_tool/geos POSTGIS_HOME=/usr/local/pg_tool/postgisLD_LIBRARY_PATH=$PGSQL_HOME/lib:$PROJ_HOME/lib:$GEOS_HOME/lib:$POSTGIS_HOME/lib PATH=$PGSQL_HOME/bin:$PATH:$HOME/bin 编缉完成后，执行.bash_profile 立即生效。 载入 PostGIS 对象和函数执行脚本 postgis.sql12cd $PGHOME/share/contrib/postgis-1.5 $ psql -d postgis -f postgis.sql &gt; 1.out 说明： 本人在这里执行不成功，报以下ERROR,12psql:postgis.sql:59: ERROR: could not load library \"/opt/pgsql/lib/postgis-1.5.so\": libgeos_c.so.1: cannot open shared object file: No such file or directory 查了相关文档，说是要配置环境变量 LD_LIBRARY_PATH 的 proj 和 geos 的库的路径, 但按照要求配置，还是不成功，从错误提示上看，脚本找不到libgeos_c.so.1文件，这个文件在 $GEOS_HOME/lib 目录下，折腾了好久都不成功；最后根据提示，将文件libgeos_c.so.1 COPY到目录/opt/pgsql/lib后，然后再次执行 psql -d postgis -f postgis.sql，终于成功了但为什么程序不会从LD_LIBRARY_PATH找呢？非常奇怪，以后要是有哪位读者知道的话，可以留言给我,谢谢! 执行脚本 spatial_ref_sys1psql -d [yourdatabase] -f spatial_ref_sys.sql 关于文档Postgis的详细介绍，在安装包里有非常详细的HTML文档,包括安装，使用，和常见问题等.目录： postgis-1.5.1dochtml","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostGIS","slug":"PostGIS","permalink":"https://postgres.fun/tags/PostGIS/"}]},{"title":"PgBouncer 连接池的使用","slug":"20100821173317","date":"2010-08-21T09:33:17.000Z","updated":"2018-09-04T01:33:45.832Z","comments":true,"path":"20100821173317.html","link":"","permalink":"https://postgres.fun/20100821173317.html","excerpt":"","text":"今天学习了下PgBouncer的使用， PgBouncer可以在后端数据库和前端应用间建立连接的桥梁，由PgBouncer去处理和后端数据库的连接关系。 对客户端连接进行限制，预防过多或者恶意的连接请求。 PgBouncer 的特点 内存消耗低(默认为2k/连接)，因为Bouncer不需要每次都接受完整的数据包 可以把不同的数据库连接到一个机器上，而对客户端保持透明 支持在线的重新配置而无须重启 仅支持V3协议，因此后端版本须&gt;=7.4 以下是Pgbouncer的安装过程。 安装准备下载源文件http://pgfoundry.org/frs/?group_id=1000258&amp;release_id=1645 安装libevent组件http://monkey.org/~provos/libevent/下载，安装过程查看手册。 安装 PgBouncer123$./configure --prefix=/opt/pgbouncer --with-libevent=/usr/local/ $make $make install 配置 PgBouncer编写配置文件123$ mkdir -p $PGDATA/pgbouncer_config $ mkdir -p $PGDATA/pgbouncer_config/run cd $PGDATA/pgbouncer_config 配置文件 config.ini12345678910111213141516171819202122[databases] skytf = host=127.0.0.1 dbname=mydb port=1999 pool_size=500 postgres = host=127.0.0.1 dbname=postgres port=1999 pool_size=25[pgbouncer] pool_mode = session listen_port = 1999 listen_addr = * unix_socket_dir = /database/pgdata/pg_root/pgbouncer_config/run auth_type = md5 auth_file = /database/pgdata/pg_root/pgbouncer_config/users.txt logfile = /var/applog/pg_log/pgbouncer.log #logfile = /dev/null pidfile = /database/pgdata/pg_root/pgbouncer_config/pgbouncer.pid max_client_conn = 6500 default_pool_size = 900 reserve_pool_timeout = 0 reserve_pool_size = 30 server_reset_query = DISCARD ALL; admin_users = pgbouncer_admin stats_users = pgbouncer_guest ignore_startup_parameters = extra_float_digits,application_name,geqo stats_period = 30 说明： 关于参数的解释在pgbouncer的源文件中有非常详细的文档，这里就不说明了。 配置文件 users.txt12\"pgbouncer_admin\" \"pgbouncer_admin\" \"skytf\" \"skytf\" 启动 PgBouncer1./opt/pgbouncer/bin/pgbouncer -d /database/pgdata/pg_root/pgbouncer_config/config.ini 测试通过pgbouncer连接是否正常1psql -h 127.0.0.1 -p 1999 -d mydb -U skytf 管理 PgBouncer以管理用户进入pgbouncer1psql -h 127.0.0.1 -p 1999 -U pgbouncer_admin -d pgbouncer 查看pgbouncer运行情况1234pgbouncer=# show stats; pgbouncer=# show lists; pgbouncer=# show pools; pgbouncer=# show databases; 参考目录结构123456[postgres@pg1 pgbouncer_config]$ ll 总计 16K -rw-rw-r-- 1 postgres postgres 734 08-19 13:47 config.ini -rw-r--r-- 1 postgres postgres 5 08-19 14:01 pgbouncer.pid drwxrwxr-x 2 postgres postgres 4.0K 08-19 14:01 run -rw-rw-r-- 1 postgres postgres 52 08-19 14:09 users.txt","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PgBouncer","slug":"PgBouncer","permalink":"https://postgres.fun/tags/PgBouncer/"}]},{"title":"PG statspack 的安装和使用","slug":"20100810142325","date":"2010-08-10T06:23:25.000Z","updated":"2018-09-04T01:33:45.769Z","comments":true,"path":"20100810142325.html","link":"","permalink":"https://postgres.fun/20100810142325.html","excerpt":"","text":"今天学习了下postgres 的statpack 功能，设置PG的statpack功能需要下载一个包,并安装;PG的statspack功能和Oracle 的Statspack功能相似，用来统计一个时间段内数据库的运行指标只是PG 的statpack报告内容相比Oracle的没有那么详尽,下面介绍下PG statspack安装和使用过程。 安装 pgstatspack1.1 下载 pgstatspack tar包 ，下载地址http://pgfoundry.org/frs/?group_id=1000375&amp;release_id=1321我下载的是pgstatspack_version_2.1.tar.gz, 是目前的最新版本; 1.2 创建一个超级用户，如果使用postgres用户，则不用新建； 1.3 开始安装,执行脚本 install_pgstats.sh (以postgres OS用户执行脚本)12cd pgstatspack的解压目录 ./install_pgstats.sh 这个脚本将在各个数据库(postgres.templates库除外)上安装统计信息表和函数; 1.4 创建 snapshot,有两种方式: 方式一： 手工方式创建 snapshot12Snapshot with comment: select pgstatspack_snap('my comment'); 方式二：crontab 方式自动创建 snapshots 以下是我本机的crontab,每15分钟创建一个snapshot12#Automated snapshots every 15 mins */15 * * * * /home/postgres/pgstatspack/bin/snapshot.sh 1 &gt; /home/postgres/pgstatspack/log/1.log 2&gt;&amp;1 生成报表123cd pgstatspack的解压目录/bin ./pgstatspack_report.sh &lt;username&gt; &lt;database&gt; pgstatspack_report.sh -u postgres -d mydb 执行这个脚本后，就会提示输入begin snapid,end snapid,这里和生成Oracle 的statspack报告内似; 这个脚本生成的报告默认是在/tmp目录下，可以修改下这个脚本，将目录生成在指定目录1234vi pgstatspack_report.sh,修改FILENAME变量即为报告存放目录。 .. FILENAME=/home/postgres/pgstatspack/report/pgstatreport_$&#123;CUR_DATE&#125;_$&#123;STARTSNAP&#125;_$&#123;STOPSNAP&#125;.txt .. 报告内容123456789101112131415161718192021222324252627282930napshot information Begin snapshot : snapid | ts | description --------+----------------------------+--------------------- 4 | 2010-08-07 04:36:38.303123 | cron based snapshot (1 行)End snapshot : snapid | ts | description --------+----------------------------+--------------------- 5 | 2010-08-07 04:51:33.874608 | cron based snapshot (1 行)Seconds in snapshot: 895.571485 Database version version --------------------------------------------------------------------------------------------------------------- PostgreSQL 9.0beta3 on i686-pc-linux-gnu, compiled by GCC gcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-48), 32-bit (1 行)current_database | dbsize ------------------+-------- mydb | 29 MB (1 行)Database statistics database | tps | hitrate | lio_ps | pio_ps | rollbk_ps ---------------+-------+---------+----------+--------+----------- mydb | 75.10 | 99.00 | 11420.30 | 0.35 | 0.00 template1 | 0.03 | 99.00 | 2.03 | 0.00 | 0.00 postgres | 0.03 | 99.00 | 2.03 | 0.00 | 0.00 template0 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 (4 行) 注意： 以下是报告的部分内容, 详细内容可以参照报告模板，解压包的目录下有份报告模板 pgstatspack_sample_report.txt。 报告字段解释 tps - transactions per secondNumber of committed transactions between snapshots, devided by the number of seconds between the two snapshots.pg_stat_database.xact_commit - number of committed transactionshitrate : cache命中率Number of cache reads as a percentage of the total number of reads (cache and physical) between the two snapshots.pg_stat_database.blks_hit - cache readspg_stat_database.blks_read - physical readslio_ps: 逻辑读Number of logical reads between snapshots, devided by the number of seconds between the two snapshots.Logical reads is in postgresql the number of physical reads + the number of cache reads.pg_stat_database.blks_hit - cache readspg_stat_database.blks_read - physical readspio_ps: 物理读Number of physical reads between snapshots, devided by the number of seconds between the two snapshots.pg_stat_database.blks_read - physical readsrollbk_psNumber of rollbacks between snapshots, devided by the number of seconds between the two snapshots.pg_stat_database.xact_rollback Pgstatspack 相关表1234567pgstatspack_snap pgstatspack_database pgstatspack_tables pgstatspack_indexes pgstatspack_sequences pgstatspack_settings pgstatspackid","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"Vmware 虚机环境添加硬盘","slug":"20100807134112","date":"2010-08-07T05:41:12.000Z","updated":"2018-09-04T01:33:45.722Z","comments":true,"path":"20100807134112.html","link":"","permalink":"https://postgres.fun/20100807134112.html","excerpt":"","text":"今天学习了下Linux 文件系统的基础知识，这篇日志不会详细介绍LINUX文件系统知识，接下来将以一个实例，讲述如何在Vmware 上新增加一块disk ,并按照指定格式化，然后mount到指定目录。 1 在Vmware 上新增加一块disk,详细步骤略,在加硬盘过程中遇到点小麻烦, 新增硬盘后在操作系统通过 “fdisk -l “ 时读不到,后来才发现新增的硬盘需要改成IDE类型才能读到，因为主硬盘（linux安装时的disk）是 IDE模式吧，我想新增的硬盘应该与主硬盘类型应该一样，如果哪位大侠了解的话，可以回贴指教下啊; 2 在步骤1中新增了一块容量为3G的硬盘, 规划如下: 分区 大小 类型 挂载点 /dev/hdb3 2048M ext3 /archive/pga /dev/hdb1 1024M ext3 /pgdata 3 查看当前系统硬盘情况1234567891011[root@pg1 ~]# fdisk -l Disk /dev/hda: 16.1 GB, 16106127360 bytes 255 heads, 63 sectors/track, 1958 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Device Boot Start End Blocks Id System /dev/hda1 * 1 1827 14675346 83 Linux /dev/hda2 1828 1958 1052257+ 82 Linux swap / SolarisDisk /dev/hdb: 3221 MB, 3221225472 bytes 16 heads, 63 sectors/track, 6241 cylinders Units = cylinders of 1008 * 512 = 516096 bytesDisk /dev/hdb doesn't contain a valid partition table ----这句话说明disk /dev/hdb 还没有分区，可以使用; 4 给/dev/hdb 添加一个分区12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@pg1 ~]# fdisk /dev/hdb Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel Building a new DOS disklabel. Changes will remain in memory only, until you decide to write them. After that, of course, the previous content won't be recoverable. The number of cylinders for this disk is set to 6241. There is nothing wrong with that, but this is larger than 1024, and could in certain setups cause problems with: 1) software that runs at boot time (e.g., old versions of LILO) 2) booting and partitioning software from other OSs (e.g., DOS FDISK, OS/2 FDISK) Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)Command (m for help): m Command action a toggle a bootable flag b edit bsd disklabel c toggle the dos compatibility flag d delete a partition l list known partition types m print this menu n add a new partition o create a new empty DOS partition table p print the partition table q quit without saving changes s create a new empty Sun disklabel t change a partition's system id u change display/entry units v verify the partition table w write table to disk and exit x extra functionality (experts only)Command (m for help): p ---查看分区情况Disk /dev/hdb: 3221 MB, 3221225472 bytes 16 heads, 63 sectors/track, 6241 cylinders Units = cylinders of 1008 * 512 = 516096 bytes Device Boot Start End Blocks Id SystemCommand (m for help): n Command action e extended p primary partition (1-4) ----选择主分区 p Partition number (1-4): 3----选择分区编号 First cylinder (1-6241, default 1): Using default value 1 Last cylinder or +size or +sizeM or +sizeK (1-6241, default 6241): +2048M ----选择大小，我这里设置成2GCommand (m for help): pDisk /dev/hdb: 3221 MB, 3221225472 bytes 16 heads, 63 sectors/track, 6241 cylinders Units = cylinders of 1008 * 512 = 516096 bytes Device Boot Start End Blocks Id System /dev/hdb3 1 3969 2000344+ 83 LinuxCommand (m for help): w ----写入分区表The partition table has been altered!Calling ioctl() to re-read partition table.4 格式化分区，设置成 ext3 格式 [root@pg1 ~]# mkfs -t ext3 /dev/hdb3 mke2fs 1.39 (29-May-2006) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) 250368 inodes, 500086 blocks 25004 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=515899392 16 block groups 32768 blocks per group, 32768 fragments per group 15648 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 25 mounts or 180 days, whichever comes first. Use tune2fs -c or -i to override. 5 挂载1[root@pg1 ~]# mount -t ext3 /dev/hdb3 /archive/pga 这里挂栽到目录 /archive/pga，同样的方法将 /dev/hdb1 mount到 /pgdata, 这里不一一列出。 6 开机自动挂载:12vim /etc/fstab /dev/hdb3 /archive/pga ext3 defaults 0 0 7 顺便介绍下umountumount只能卸载“不忙”的文件系统，如果有任务进程在文件有有读，写操作，这个文件系统都算“繁忙” 可以通过 fuser 命令找出读此目录的进程 –以下umount /pgdata目录12345678910111213141516171819202122232425[root@pg1 /]# df -hv 文件系统 容量 已用 可用 已用% 挂载点 /dev/hda1 14G 6.2G 6.8G 48% / tmpfs 217M 0 217M 0% /dev/shm none 217M 104K 217M 1% /var/lib/xenstored /dev/hdb1 1.1G 34M 1012M 4% /pgdata /dev/hdb3 1.9G 35M 1.8G 2% /archive/pga[root@pg1 /]# umount /pgdata umount: /pgdata: device is busy umount: /pgdata: device is busy[root@pg1 /]# fuser -m /pgdata /pgdata: 3441c [root@pg1 /]# ps -ef | grep 3441 root 949 3241 0 08:04 pts/1 00:00:00 grep 3441 root 3441 3439 0 07:16 pts/2 00:00:00 -bash [root@pg1 /]# kill -9 3441 [root@pg1 /]# umount /pgdata[root@pg1 /]# df -hv 文件系统 容量 已用 可用 已用% 挂载点 /dev/hda1 14G 6.2G 6.8G 48% / tmpfs 217M 0 217M 0% /dev/shm none 217M 104K 217M 1% /var/lib/xenstored /dev/hdb3 1.9G 35M 1.8G 2% /archive/pga 再次查看发现,杀掉进程3441后， /pgdata目录已经 umount了。","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"Pgfincore: cache data to OS cache","slug":"20100805161611","date":"2010-08-05T08:16:11.000Z","updated":"2018-09-04T01:33:45.660Z","comments":true,"path":"20100805161611.html","link":"","permalink":"https://postgres.fun/20100805161611.html","excerpt":"","text":"最近从师傅那学习一种非常实用的数据库缓存机制，能够将数据库里表缓存到OS的cache里，注意不是数据库的BUFFER，是操作系统层面的cache, 众所周知，数据库从内存读要比从硬盘读数据快很多，虽然以前学sybase时有也有将少量经常查询的表放到数据库 ache里，Oracle里也有个Keep用来CACHE经常访问的小表，但今天要讲的机制不一样了，它是将数据库表，索引CACHE到OS层面的缓存里，只要内存足够大，可以将需要的数据都CACHE到OS 内存中，这极到的提高了应用的处理速度；采用这种方法测试了一套应用系统，速度要好快几十倍。这个工具叫 pgfincore,下面简单的介绍下它的用法。 Pgfincore安装1.1 解压介质 (postgres), 并COPY 到 ostgresql的源代码目录/contrib123cd postgresql的源代码目录/contrib/pgfincore $USE_PGXS=1 make clean $USE_PGXS=1 make 1.2 instll (by root)123#cd /home/postgres #source .bash_profile #USE_PGXS=1 make install 1.3 导入pgfincore.sql文件1234su - postgres cd postgresql的安装目录/share/postgresql/contrib chown -R postgres:postgres /database psql mydb -f pgfincore.sql 主要函数介绍 pgsysconf –查看操作系统CACHE情况 pgmincore –查看对象（表，索引）CACHE情况 pgfadv_willneed –将数据库对象（表，索引）载入OS CACHE pgfadv_dontneed –将数据库对象（表，索引）刷出OS CACHE pgfadv_normal –This function set //NORMAL// flag on the current relation. () pgmincore_snapshot –保存对象块照情况 pgfadv_willneed_snapshot –还原对象快照情况 函数返回的列 relpath : the relation pathblock_size : the size of one block disk?block_disk : the total number of file system blocks of the relationblock_mem : the total number of file system blocks of the relation in buffer cache. (not the shared buffers from PostgreSQL but the OS cache)group_mem : the number of groups of adjacent block_mem 2.1 pgmincore查询表CACHE情况12345mydb=&gt; select * from pgmincore ('test_3'); relpath | block_size | block_disk | block_mem | group_mem ------------------+------------+------------+-----------+----------- base/16385/16394 | 4096 | 2 | 0 | 0 (1 row) 2.2 pgsysconf 查询当前操作系统的块大小，剩余多少可用的CACHE（块）。1234cedric=# select * from pgsysconf(); block_size | block_free ------------+------------ 4096 | 417534 2.3 pgfadv_willneed将数据库对象（表，索引）载入OS CACHE。12345cedric=# select * from pgfadv_willneed('pgbench_accounts'); relpath | block_size | block_disk | block_free --------------------+------------+------------+------------ base/16384/16603 | 4096 | 262144 | 3744 base/16384/16603.1 | 4096 | 65726 | 4236 2.4 pgfadv_dontneed将数据库对象（表，索引）刷出OS CACHE12345cedric=# select * from pgfadv_dontneed('pgbench_accounts'); relpath | block_size | block_disk | block_free --------------------+------------+------------+------------ base/16384/24598 | 4096 | 262144 | 178743 base/16384/24598.1 | 4096 | 55318 | 234078 2.5 pgfadv_normal使用普通内存方式,再pgfadv_dontneed方式刷出对象后，建议执行这个函数，将内存方式改为普通方式；1select * from pgfadv_normal('test_1'); 2.6 pgmincore_snapshot and pgfadv_willneed_snapshot当数据库刚启动的时候，查询会比较慢，应为PostgreSQL和OS级别的缓存都还没有缓存需要的BLOCK。我们可以用pgmincore_snapshot保存一个快照，在数据库起来时用 pgfadv_willneed_snapshot还原。12345678910111213-- Snapshot cedric=# select * from pgmincore_snapshot('pgbench_accounts'); relpath | block_size | block_disk | block_mem | group_mem --------------------+------------+------------+-----------+----------- base/16384/24598 | 4096 | 262144 | 131745 | 1 base/16384/24598.1 | 4096 | 55318 | 55318 | 1-- Restore cedric=# select * from pgfadv_willneed_snapshot('pgbench_accounts'); relpath | block_size | block_disk | block_free --------------------+------------+------------+------------ base/16384/24598 | 4096 | 262144 | 105335 base/16384/24598.1 | 4096 | 55318 | 50217 (2 rows) 测试下2.6步骤中的函数，先查询表OS CACHE情况， block_mem=0,表明没有缓存到OS cache中1234567891011121314151617181920212223mydb=&gt; select * from pgmincore ('test_3'); relpath | block_size | block_disk | block_mem | group_mem ------------------+------------+------------+-----------+----------- base/16385/16394 | 4096 | 2 | 0 | 0 (1 row)--保存表CACHE 情况快照 mydb=&gt; select * from pgmincore_snapshot('test_3'); relpath | block_size | block_disk | block_mem | group_mem ------------------+------------+------------+-----------+----------- base/16385/16394 | 4096 | 2 | 0 | 0 (1 row)--查询表 mydb=&gt; select * From test_3; id ---- 1 (1 row)--再次观察CACHE情况,发现block_mem=2,表明表test_3 已经CACHE了 mydb=&gt; select * from pgmincore ('test_3'); relpath | block_size | block_disk | block_mem | group_mem ------------------+------------+------------+-----------+----------- base/16385/16394 | 4096 | 2 | 2 | 1 (1 row) 说明： PG数据库客户端执行查询后，数据除了会缓存到数据库CACHE 中还会缓存到OS CACHE中,不知道ORACLE是否会缓存到OS CACHE中，知道的大侠可以告诉我下，谢谢。 还原表TEST_3的CACHE情况到做快照时123456789101112mydb=&gt; select * from pgfadv_willneed_snapshot('test_3'); relpath | block_size | block_disk | block_free ------------------+------------+------------+------------ base/16385/16394 | 4096 | 2 | 909 (1 row)--再次观察CACHE情况，和做快照时一样了 mydb=&gt; select * from pgmincore ('test_3'); relpath | block_size | block_disk | block_mem | group_mem ------------------+------------+------------+-----------+----------- base/16385/16394 | 4096 | 2 | 0 | 0 (1 row) 安装过程中遇到的错误12345[postgres@pg1](mailto:postgres@pg1) pgfincore]$ USE_PGXS=1 make clean make: Warning: File `/database/pgdata/tbs1/pg_root/lib/postgresql/pgxs/src/makefiles/../../src/Makefile.global' has modification time 1.4e+08 s in the future rm -f pgfincore.so pgfincore.o rm -f pgfincore.sql uninstall_pgfincore.sql make: 警告：检测到时钟错误。您的创建可能是不完整的。 解决方法网上查了下，是因为时间问题，需要修改文操作系统时间和文件创建时间；123#date -s \"2010-08-05 14:37:22\" $cd /database/pgdata/tbs1/pg_root/lib/postgresql/pgxs/src/makefiles/../../src find . -type f -exec touch &#123;&#125;","categories":[{"name":"PG性能优化","slug":"PG性能优化","permalink":"https://postgres.fun/categories/PG性能优化/"}],"tags":[{"name":"pgfincore","slug":"pgfincore","permalink":"https://postgres.fun/tags/pgfincore/"}]},{"title":"如何查看表、索引、表空间、数据库大小？","slug":"20100804173000","date":"2010-08-04T09:30:00.000Z","updated":"2018-09-04T01:33:45.597Z","comments":true,"path":"20100804173000.html","link":"","permalink":"https://postgres.fun/20100804173000.html","excerpt":"","text":"查询一个索引大小1select pg_size_pretty(pg_relation_size('indexname)) 查看一张表和它的索引总大小1select pg_size_pretty(pg_total_relation_size('tablename')); 查看所有schema里索引大小，按从大到小排列123select indexrelname,pg_size_pretty( pg_relation_size(relid)) from pg_stat_user_indexes where schemaname = 'schemaname' order by pg_relation_size(relid) desc; 查看所有schema里表大小，按从大到小排列123select relname, pg_size_pretty(pg_relation_size(relid)) from pg_stat_user_tables where schemaname = 'schemaname' order by pg_relation_size(relid) desc; 查看数据库大小12select pg_database.datname,pg_size_pretty(pg_database_size(pg_database.datname)) AS size from pg_database; 查看表空间大小1select pg_tablespace_size('tbs_index')/1024/1024 as \"SIZE M\";","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]},{"title":"如何查看CPU信息 ?","slug":"20100804172728","date":"2010-08-04T09:27:28.000Z","updated":"2018-09-04T01:33:45.519Z","comments":true,"path":"20100804172728.html","link":"","permalink":"https://postgres.fun/20100804172728.html","excerpt":"","text":"以前对如何看CPU个数和每个CPU是多少核的不太清楚，今天网上查了些资料,总算有些概念。查看CPU信息主要通过 /proc/cpuinfo 文件来查看，以下为该文件内容里的一些参数说明： processor ： 条目包括这一逻辑处理器的唯一标识符。 physical id : 条目包括每个物理封装的唯一标识符。 core id: 条目保存每个内核的唯一标识符。 cpu cores 条目包含位于相同物理封装中的内核数量 siblings 条目列出了位于相同物理封装中的逻辑处理器的数量。 其中前四项经常用到。 12345查看物理CPU个数 cat /proc/cpuinfo | grep \"physical id\" | sort | uniq | wc -l查看逻辑CPU个数 cat /proc/cpuinfo | grep \"processor\" | sort | uniq | wc -l /proc/cpuinfo文件信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647processor : 0 --表示逻辑CPU vendor_id : GenuineIntel cpu family : 6 model : 26 model name : Intel(R) Xeon(R) CPU E5504 @ 2.00GHz stepping : 5 cpu MHz : 2000.119 cache size : 4096 KB physical id : 0--表示物理CPU编号 siblings : 4 core id : 0 cpu cores : 4 --表示物理CPU有多少核 apicid : 0 fpu : yes fpu_exception : yes cpuid level : 11 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm syscall nx rdtscp lm constant_tsc nonstop_tsc pni monitor ds_cpl vmx est tm2 cx16 xtpr popcnt lahf_lm bogomips : 4000.23 clflush size : 64 cache_alignment : 64 address sizes : 40 bits physical, 48 bits virtual power management: [8] …..中间部分省略vendor_id : GenuineIntel cpu family : 6 model : 26 model name : Intel(R) Xeon(R) CPU E5504 @ 2.00GHz stepping : 5 cpu MHz : 2000.119 cache size : 4096 KB physical id : 1 siblings : 4 core id : 3 cpu cores : 4 apicid : 22 fpu : yes fpu_exception : yes cpuid level : 11 wp : yes flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm syscall nx rdtscp lm constant_tsc nonstop_tsc pni monitor ds_cpl vmx est tm2 cx16 xtpr popcnt lahf_lm bogomips : 4000.11 clflush size : 64 cache_alignment : 64 address sizes : 40 bits physical, 48 bits virtual power management: [8] 从这个文件可以看出，有2个物理CPU，每颗4核。 另一种方法查看1234567891011121314151617181920212223242526272829[root@pc-zjndyh01 ~]# lscpuArchitecture: x86_64CPU op-mode(s): 32-bit, 64-bitByte Order: Little EndianCPU(s): 32On-line CPU(s) list: 0-31Thread(s) per core: 1Core(s) per socket: 8Socket(s): 4NUMA node(s): 8Vendor ID: AuthenticAMDCPU family: 16Model: 9Stepping: 1CPU MHz: 2400.143BogoMIPS: 4800.11Virtualization: AMD-VL1d cache: 64KL1i cache: 64KL2 cache: 512KL3 cache: 5118KNUMA node0 CPU(s): 0,4,8,12NUMA node1 CPU(s): 16,20,24,28NUMA node2 CPU(s): 1,5,9,13NUMA node3 CPU(s): 17,21,25,29NUMA node4 CPU(s): 2,6,10,14NUMA node5 CPU(s): 18,22,26,30NUMA node6 CPU(s): 19,23,27,31NUMA node7 CPU(s): 3,7,11,15 socket就代表cpu插槽，如果有8个socket那就是八路的","categories":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/categories/RHEL/"}],"tags":[{"name":"RHEL","slug":"RHEL","permalink":"https://postgres.fun/tags/RHEL/"}]},{"title":"PostgeSQL 数据库备份与恢复实验 (PITR)","slug":"20100802210528","date":"2010-08-02T13:05:28.000Z","updated":"2018-09-04T01:33:45.457Z","comments":true,"path":"20100802210528.html","link":"","permalink":"https://postgres.fun/20100802210528.html","excerpt":"","text":"一、概述此文档描述了Postgresql数据库备份方案, 当数据库发生崩溃时，如何利用备份恢复到崩溃前，保证数据库最少数据量丢失甚至不丢失； 二、备份恢复原理运用在线备份以及即时恢复(PITR)原理,利用Postgresql 数据库的WAL（Write Ahead Logging ）预写日志和基础备份( $PGDATA目录文件tar包 )，恢复到数据库崩溃前时间点，保证数据量最少丢失或者不丢失，如果数据库崩溃，我们就可以通过热备产生的备份文件data_bak.tar包 ($PGDATA目录文件tar包)和archive_command产生的WAL及我们自己备份的WAL(pg_xlog)来进行数据库的 recovery 。 三、在线热备3.1 配置归档模式配置归档需要编辑postgresql.conf文件，默认与 /usr/local/pgsql/data/ 目录下vi $PGDATA/postgresql.conf ，修改以下12archive_mode = onarchive_command = 'cp -i %p /home/postgres/archive/%f &lt;/dev/null' 注意：这里将WAL日志备份到 /home/postgres/archive 目录下,pg_xlog/下的目录下可能还有未备份的数据，需要另外备份, 详见3.7。 3.2 重启动数据库12pg_ctl –D $PGDATA stoppg_ctl –D $PGDATA sart 3.3 创建基础备份1postgres=# select pg_start_backup('bakup_test'); 3.4 备份整个data目录 (tar方式)1tar cvf /database/pgdata/tbs1/pg_root/base_data.tar /database/pgdata/tbs1/pg_root/data 3.5 停止备份1postgres=# select pg_stop_backup(); 3.6 切换日志1postgres=# pg_switch_xlog(); 3.7 定期备份WAL新生成日志如果单独通过archive_command来备份WAL的话, 能根本就做不到PITR, 因为pg_xlog/下面可能还有数据没有备份到archive_command指定的目录里；所以需要另外写脚本把/data/pg_xlog/下的WAL日志文件备份到预设的归档目录下，保证产生的WAL日志都已归档， 这里写了个脚本，每五分钟执行一次 每5分钟备份 (通过Crontab执行)12345vi cp_pg_xlog.shcp -f /database/pgdata/tbs1/pg_root/data/pg_xlog/[0-9]* /home/postgres/archive/archive_bak2/crontab – e*/5 * * * * /home/postgres/script/cp_pg_xlog.sh &gt; /dev/null 2&gt;&amp;1 3.8 编写备份脚本hot_bak_post.sh (供参考)附件：以上操作可以写个脚本，定时执行，热备脚本为 hot_bak_post.sh ,定时执行写在CRONTABL里，每周六零晨做一次BASE 备份。脚本内容如下，参考123456789101112131415161718192021#!/bin/bashCUR_DATE=`date +%Y%m%d`export PGPORT=1921export PGHOME=/database/pgdata/tbs1/pg_rootexport PATH=/database/pgdata/tbs1/pg_root/bin:$PATHexport DATA_BAK_DIR=/home/postgres/archive/data_bakexport LOG_PATH=/home/postgres/archive/logARCHIVE_LOG=\"$&#123;LOG_PATH&#125;/bak_pg_$&#123;CUR_DATE&#125;.log\"echo \"*************** Begin backup ,please wait *************************\" |tee -a $&#123;ARCHIVE_LOG&#125;psql -c \"select pg_start_backup('backup_test');\"cd $PGHOMEtar cvf data_$&#123;CUR_DATE&#125;.tar datamv *.tar $&#123;DATA_BAK_DIR&#125;psql -c \"select pg_stop_backup();\"psql -c \"select pg_switch_xlog();\"echo \"Backup completed!\" 说明：脚本已经测试，可以实现功能，目前还在完善中。 四、恢复如果数据库崩溃，我们就可以使用热备产生的data_bak.tar包(即$PGDATA目录文件tar包)和archive_command产生的WAL及我们自己备份的WAL(pg_xlog)来进行数据库的 recovery，以下实验模拟数据库崩溃时的恢复过程 4.1 创建一张测试表12345678910111213141516171819202122232425psql -dmydb -Uskytfmydb=&gt; \\d List of relations Schema | Name | Type | Owner--------+-------------+-------+------- skytf | test2 | table | skytf skytf | test_1 | table | skytf skytf | test_3 | table | skytfmydb=&gt; create table test_backup (id int,remark varchar(32));CREATE TABLEmydb=&gt; insert into test_backup values (1 ,'a');INSERT 0 1mydb=&gt; insert into test_backup values (2 ,'b');INSERT 0 1mydb=&gt; insert into test_backup values (3 ,'c');mydb=&gt; select * from test_backup; id | remark----+-------- 1 | a 2 | b 3 | c(3 rows) 4.2 停数据库1pg_ctl –D $PGDATA stop 4.3 删除data目录12cd $PGDATArm -rf data 4.4 恢复备份1tar xvf –f database/pgdata/tbs1/pg_root/data_bak.tar 4.5 修改 pg_hba.conf，避免普通用户服务连接 4.6 清空/data/pg_xlog/目录下所有文件1rm –r $PGDATA/pg_xlog/ 4.7 创建/pg_xlog/及其下面的archive_status目录1mkdir -p /database/pgdata/tbs1/pg_root/pg_xlog/archive_status 4.8 在/data/目录下创建recovery.conf vi database/pgdata/tbs1/pg_root/data/recovery.conf写入以下行1restore_command = ' cp /home/postgres/archive/%f \"%p\"' 4.9 启动数据库1pg_ctl -D $PGDATA start 注意：切正常的话数据库就会自动应用WAL日志进行恢复启动过程如有异常可以查看CSV日志,参数log_directory指定了日志目录（前提是你设置了日志记录功能）； 4.10 查看数据库是否恢复1234567891011121314151617mydb=&gt; \\d List of relations Schema | Name | Type | Owner--------+-------------+-------+------- skytf | test2 | table | skytf skytf | test_1 | table | skytf skytf | test_3 | table | skytf skytf | test_backup | table | skytf(4 rows)mydb=&gt; select * from test_backup; id | remark----+-------- 1 | a 2 | b 3 | c(3 rows) 4.11 库分析1vacuumdb -z -d mydb 至此，数据已成功恢复！ 五、后续工作 清理脚本由于备份的tar包会越来越大，后续可以写些清理脚本，例如只保留最近几个tar包，其它的都删除掉等; 由于WAL日志文件比较大，可以制定删除策略，定期删除备份目录WAL日志。","categories":[{"name":"PG备份与恢复","slug":"PG备份与恢复","permalink":"https://postgres.fun/categories/PG备份与恢复/"}],"tags":[{"name":"备份恢复","slug":"备份恢复","permalink":"https://postgres.fun/tags/备份恢复/"}]},{"title":"RedHat Enterprise 5上安装 PostgreSQL","slug":"20100731115100","date":"2010-07-31T03:51:00.000Z","updated":"2018-09-04T01:33:45.378Z","comments":true,"path":"20100731115100.html","link":"","permalink":"https://postgres.fun/20100731115100.html","excerpt":"","text":"以下是第一次在红帽子五上安装 PostgreSQL 笔记, 该篇日志描述了在Red Hat Enterprise 5上安装PostgreSQL-9.0beat3的详细安装过程， 操作系统安装步骤已省略。 安装环境虚拟机 : Vmware Workstation 5操作系统： Red Hat Enterprise 5数据库： PostgreSQL-9.0beta3 安装准备修改操作系统参数 ，修改 /etc/sysctl.conf1234567891011121314kernel.shmmni = 4096 kernel.sem = 501000 6412800000 501000 12800 fs.file-max = 767246 net.ipv4.ip_local_port_range = 1024 65000 net.core.rmem_default = 1048576 net.core.rmem_max = 1048576 net.core.wmem_default = 262144 net.core.wmem_max = 262144 net.ipv4.tcp_tw_recycle=1 net.ipv4.tcp_max_syn_backlog=4096 net.core.netdev_max_backlog=10000 vm.overcommit_memory=0 net.ipv4.ip_conntrack_max=655360sysctl -p 生效 修改 /etc/security/limits.conf12345678* soft nofile 131072 * hard nofile 131072 * soft nproc 131072 * hard nproc 131072 * soft core unlimited * hard core unlimited * soft memlock 50000000 * hard memlock 50000000 修改/etc/pam.d/login1session required pam_limits.so 添加用户和组123#groupadd postgresql #useradd -g postgresql postgresql # passwd postgres 创建目录123456mkdir -p /opt/pg_root/data mkdir -p /opt/pg_root/log touch /opt/pg_root/log/pgsql.logcd /usr/local chown -R postgres:postgres pg_root ln -sf /opt/pg_root /database/pgdata/pg_root 配置环境变量123456789101112su - postgres vi .bash_profile export DATE=`date +\"%Y%m%d%H%M\"`export PGPORT=1921 export PGDATA=/database/pgdata/tbs1/pg_root/data export PGHOME=/opt/pgsql_9beta3 export PATH=/database/pgdata/tbs1/pg_root/bin:$PATHexport LD_LIBRARY_PATH=export DATE=`date +\"%Y%m%d%H%M\"` export MANPATH=$PGHOME/share/man:$MANPATH export LANG=en_US.utf8 alias rm='rm -i' alias ll='ls -lh' 安装PostgreSQL解压安装包到 /opt/postgresql-9.0beta3：1# tar xvf postgresql-9.0beta3.tar.gz 进行安装配置12cd /opt/postgresql-9.0beta3 ./configure --prefix=/opt/pg_root --with-pgport=1921 --with-segsize=8 --with-wal-segsize=64 --with-wal-blocksize=64 --with-perl --with-python --with-openssl --with-pam --with-ldap --with-libxml --with-libxslt --enable-thread-safety 注意：Configure 脚本详细选项可以查看帮助：./configure --help 如查看版本信息：./configure -V ， 如果报缺少相应包，则 yum 安装相应包即可。 编译1# gmake (12:46) 这个过程比较长，大概在二十分钟以上没有任何问题的话，我们可以看到最后一句提示信息1“All of PostgreSQL successfully made. Ready to install.” 安装1# gmake install-world 成功安装后能看到最后一句提示信息”PostgreSQL installation complete.” 初始化数据库(PSQL用户)12root# su - postgres initdb -D /opt/pg_root/data -E UTF8 --locale=C -U postgres -W 注意: initdb 脚本详细选项可以查看 initdb --help ,例如查看 initdb 脚本版本命令如下 initdb -V。 命令执行完后提示1234Success. You can now start the database server using: postgres -D /usr/local/pgsql/data or pg_ctl -D /usr/local/pgsql/data -l logfile start 数据库的启动与关闭启动数据库1pg_ctl -D /database/pgdata/pg_root/data-l /database/pgdata/pg_root/log/pgsql.log start 关闭数据库 (参考步骤，这步只是为了显示关闭数据库命令)1pg_ctl -D /database/pgdata/pg_root/data 创建数据库用户、数据库创建数据库用户123CREATE ROLE usera LOGIN ENCRYPTED PASSWORD 'usera' nosuperuser inherit nocreatedb nocreaterole; 创建数据库1createdb mydb -O usera 登陆测试123456789psql -d mydb -U usera psql (9.0beta3) Type \"help\" for help.oup=&gt; select version(); version ---------------------------------------------------------------------------------------------------------------------- PostgreSQL 9.0beta3 on x86_64-unknown-linux-gnu, compiled by GCC gcc (GCC) 4.1.2 20080704 (Red Hat 4.1.2-46), 64-bit (1 row)","categories":[{"name":"Postgres基础","slug":"Postgres基础","permalink":"https://postgres.fun/categories/Postgres基础/"}],"tags":[{"name":"PostgreSQL","slug":"PostgreSQL","permalink":"https://postgres.fun/tags/PostgreSQL/"}]}]}